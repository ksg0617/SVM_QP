{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ticker</th>\n",
       "      <th>pe_exi</th>\n",
       "      <th>de_ratio</th>\n",
       "      <th>close_price</th>\n",
       "      <th>exp_ret</th>\n",
       "      <th>vol</th>\n",
       "      <th>mom</th>\n",
       "      <th>Mkt-RF</th>\n",
       "      <th>SMB</th>\n",
       "      <th>HML</th>\n",
       "      <th>RMW</th>\n",
       "      <th>CMA</th>\n",
       "      <th>RF</th>\n",
       "      <th>ret_excess</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-02-29</td>\n",
       "      <td>AAP</td>\n",
       "      <td>23.193750</td>\n",
       "      <td>2.305863</td>\n",
       "      <td>131.959274</td>\n",
       "      <td>-0.000684</td>\n",
       "      <td>0.077602</td>\n",
       "      <td>-0.042258</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.88</td>\n",
       "      <td>-0.57</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2.02</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>-0.000884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-03-31</td>\n",
       "      <td>AAP</td>\n",
       "      <td>25.053125</td>\n",
       "      <td>2.305863</td>\n",
       "      <td>142.591949</td>\n",
       "      <td>-0.000529</td>\n",
       "      <td>0.077550</td>\n",
       "      <td>-0.040434</td>\n",
       "      <td>6.96</td>\n",
       "      <td>1.07</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0.77</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>-0.000729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-04-30</td>\n",
       "      <td>AAP</td>\n",
       "      <td>24.390625</td>\n",
       "      <td>2.305863</td>\n",
       "      <td>138.821274</td>\n",
       "      <td>0.008972</td>\n",
       "      <td>0.080094</td>\n",
       "      <td>0.072751</td>\n",
       "      <td>0.91</td>\n",
       "      <td>1.23</td>\n",
       "      <td>3.28</td>\n",
       "      <td>-2.97</td>\n",
       "      <td>1.90</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.008872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-05-31</td>\n",
       "      <td>AAP</td>\n",
       "      <td>23.522936</td>\n",
       "      <td>2.362231</td>\n",
       "      <td>136.811432</td>\n",
       "      <td>0.010492</td>\n",
       "      <td>0.079151</td>\n",
       "      <td>0.093242</td>\n",
       "      <td>1.78</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>-1.66</td>\n",
       "      <td>-1.09</td>\n",
       "      <td>-2.49</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.010392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-06-30</td>\n",
       "      <td>AAP</td>\n",
       "      <td>24.714067</td>\n",
       "      <td>2.362231</td>\n",
       "      <td>143.795746</td>\n",
       "      <td>0.003330</td>\n",
       "      <td>0.076990</td>\n",
       "      <td>0.005550</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-1.48</td>\n",
       "      <td>1.41</td>\n",
       "      <td>1.94</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.003130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13244</th>\n",
       "      <td>2024-08-31</td>\n",
       "      <td>XOM</td>\n",
       "      <td>14.107656</td>\n",
       "      <td>0.771855</td>\n",
       "      <td>114.883614</td>\n",
       "      <td>0.012588</td>\n",
       "      <td>0.052356</td>\n",
       "      <td>0.144777</td>\n",
       "      <td>1.61</td>\n",
       "      <td>-3.65</td>\n",
       "      <td>-1.13</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.007788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13245</th>\n",
       "      <td>2024-09-30</td>\n",
       "      <td>XOM</td>\n",
       "      <td>14.021531</td>\n",
       "      <td>0.771855</td>\n",
       "      <td>114.182266</td>\n",
       "      <td>0.009021</td>\n",
       "      <td>0.051371</td>\n",
       "      <td>0.097898</td>\n",
       "      <td>1.74</td>\n",
       "      <td>-1.02</td>\n",
       "      <td>-2.59</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.005021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13246</th>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>XOM</td>\n",
       "      <td>13.968900</td>\n",
       "      <td>0.771855</td>\n",
       "      <td>113.753670</td>\n",
       "      <td>0.003723</td>\n",
       "      <td>0.049151</td>\n",
       "      <td>0.031893</td>\n",
       "      <td>-0.97</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>0.89</td>\n",
       "      <td>-1.38</td>\n",
       "      <td>1.03</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>-0.000177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13247</th>\n",
       "      <td>2024-11-30</td>\n",
       "      <td>XOM</td>\n",
       "      <td>14.689913</td>\n",
       "      <td>0.737662</td>\n",
       "      <td>115.847267</td>\n",
       "      <td>0.011724</td>\n",
       "      <td>0.037114</td>\n",
       "      <td>0.141942</td>\n",
       "      <td>6.51</td>\n",
       "      <td>4.78</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-2.62</td>\n",
       "      <td>-2.17</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.007724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13248</th>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>XOM</td>\n",
       "      <td>13.396015</td>\n",
       "      <td>0.737662</td>\n",
       "      <td>103.865776</td>\n",
       "      <td>0.014966</td>\n",
       "      <td>0.035716</td>\n",
       "      <td>0.187306</td>\n",
       "      <td>-3.17</td>\n",
       "      <td>-3.87</td>\n",
       "      <td>-2.95</td>\n",
       "      <td>1.82</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.011266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13249 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date ticker     pe_exi  de_ratio  close_price   exp_ret  \\\n",
       "0      2016-02-29    AAP  23.193750  2.305863   131.959274 -0.000684   \n",
       "1      2016-03-31    AAP  25.053125  2.305863   142.591949 -0.000529   \n",
       "2      2016-04-30    AAP  24.390625  2.305863   138.821274  0.008972   \n",
       "3      2016-05-31    AAP  23.522936  2.362231   136.811432  0.010492   \n",
       "4      2016-06-30    AAP  24.714067  2.362231   143.795746  0.003330   \n",
       "...           ...    ...        ...       ...          ...       ...   \n",
       "13244  2024-08-31    XOM  14.107656  0.771855   114.883614  0.012588   \n",
       "13245  2024-09-30    XOM  14.021531  0.771855   114.182266  0.009021   \n",
       "13246  2024-10-31    XOM  13.968900  0.771855   113.753670  0.003723   \n",
       "13247  2024-11-30    XOM  14.689913  0.737662   115.847267  0.011724   \n",
       "13248  2024-12-31    XOM  13.396015  0.737662   103.865776  0.014966   \n",
       "\n",
       "            vol       mom  Mkt-RF   SMB   HML   RMW   CMA      RF  ret_excess  \n",
       "0      0.077602 -0.042258   -0.07  0.88 -0.57  3.25  2.02  0.0002   -0.000884  \n",
       "1      0.077550 -0.040434    6.96  1.07  1.19  0.77 -0.08  0.0002   -0.000729  \n",
       "2      0.080094  0.072751    0.91  1.23  3.28 -2.97  1.90  0.0001    0.008872  \n",
       "3      0.079151  0.093242    1.78 -0.61 -1.66 -1.09 -2.49  0.0001    0.010392  \n",
       "4      0.076990  0.005550   -0.05  0.45 -1.48  1.41  1.94  0.0002    0.003130  \n",
       "...         ...       ...     ...   ...   ...   ...   ...     ...         ...  \n",
       "13244  0.052356  0.144777    1.61 -3.65 -1.13  0.85  0.86  0.0048    0.007788  \n",
       "13245  0.051371  0.097898    1.74 -1.02 -2.59  0.04 -0.26  0.0040    0.005021  \n",
       "13246  0.049151  0.031893   -0.97 -0.88  0.89 -1.38  1.03  0.0039   -0.000177  \n",
       "13247  0.037114  0.141942    6.51  4.78 -0.05 -2.62 -2.17  0.0040    0.007724  \n",
       "13248  0.035716  0.187306   -3.17 -3.87 -2.95  1.82 -1.10  0.0037    0.011266  \n",
       "\n",
       "[13249 rows x 15 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"feature_data/data.csv\")\n",
    "\n",
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test sets\n",
    "#train_df = df[df['date'] < '2024-01-01']\n",
    "#test_df = df[df['date'] >= '2024-01-01']\n",
    "\n",
    "#train_df = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate returns & Covariances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "import mgarch\n",
    "factor_cols = [\"Mkt-RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\"]\n",
    "\n",
    "\n",
    "# ----- helper #1 :   β̂  and  F̂  --------------------------------\n",
    "def fit_beta(X: np.ndarray, Y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        # 1) strip rows that contain any non-finite value\n",
    "    row_mask = np.isfinite(X).all(axis=1) & np.isfinite(Y).all(axis=1)\n",
    "    Xc, Yc   = X[row_mask], Y[row_mask]\n",
    "\n",
    "    beta, *_ = np.linalg.lstsq(Xc, Yc, rcond=None)               # (d_x, d_y)\n",
    "    resid    = Yc - Xc @ beta                                    # (n, d_y)\n",
    "    F_hat    = np.diag(resid.var(axis=0, ddof=1))              # (d_y, d_y)\n",
    "    return beta, F_hat\n",
    "\n",
    "\n",
    "# ----- helper #2 :   one-step mean & cov ------------------------\n",
    "def forecast_one_step(x_i, W_i, beta, F_hat):\n",
    "    y_hat = x_i @ beta                                         # (d_y,)\n",
    "    V_hat = beta.T @ W_i @ beta + F_hat                        # (d_y, d_y)\n",
    "    return y_hat, V_hat\n",
    "\n",
    "# ---- helper #3 : DCC-GARCH forecast of factor covariance ------\n",
    "def dcc_garch_cov(X, ndays=1):\n",
    "    dist = 't'\n",
    "    vol = mgarch.mgarch(dist)\n",
    "    vol.fit(X)\n",
    "    W_t = vol.predict(ndays)['cov']\n",
    "    return W_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df.groupby('ticker')['close_price'].shift(-1).gt(df['close_price']).astype(int) * 2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ret'] = df.groupby('ticker')['close_price'].pct_change().shift(-1).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ret_excess'] = df['ret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label counts:\n",
      "label\n",
      " 1    7321\n",
      "-1    5928\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# get the how many 1s and 0s in the label column\n",
    "label_counts = df['label'].value_counts()\n",
    "print(\"Label counts:\")\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ticker</th>\n",
       "      <th>pe_exi</th>\n",
       "      <th>de_ratio</th>\n",
       "      <th>close_price</th>\n",
       "      <th>exp_ret</th>\n",
       "      <th>vol</th>\n",
       "      <th>mom</th>\n",
       "      <th>Mkt-RF</th>\n",
       "      <th>SMB</th>\n",
       "      <th>HML</th>\n",
       "      <th>RMW</th>\n",
       "      <th>CMA</th>\n",
       "      <th>RF</th>\n",
       "      <th>ret_excess</th>\n",
       "      <th>label</th>\n",
       "      <th>ret</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-02-29</td>\n",
       "      <td>AAP</td>\n",
       "      <td>23.193750</td>\n",
       "      <td>2.305863</td>\n",
       "      <td>131.959274</td>\n",
       "      <td>-0.000684</td>\n",
       "      <td>0.077602</td>\n",
       "      <td>-0.042258</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.88</td>\n",
       "      <td>-0.57</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2.02</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.080575</td>\n",
       "      <td>1</td>\n",
       "      <td>0.080575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-03-31</td>\n",
       "      <td>AAP</td>\n",
       "      <td>25.053125</td>\n",
       "      <td>2.305863</td>\n",
       "      <td>142.591949</td>\n",
       "      <td>-0.000529</td>\n",
       "      <td>0.077550</td>\n",
       "      <td>-0.040434</td>\n",
       "      <td>6.96</td>\n",
       "      <td>1.07</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0.77</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>-0.026444</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.026444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-04-30</td>\n",
       "      <td>AAP</td>\n",
       "      <td>24.390625</td>\n",
       "      <td>2.305863</td>\n",
       "      <td>138.821274</td>\n",
       "      <td>0.008972</td>\n",
       "      <td>0.080094</td>\n",
       "      <td>0.072751</td>\n",
       "      <td>0.91</td>\n",
       "      <td>1.23</td>\n",
       "      <td>3.28</td>\n",
       "      <td>-2.97</td>\n",
       "      <td>1.90</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.014478</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.014478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-05-31</td>\n",
       "      <td>AAP</td>\n",
       "      <td>23.522936</td>\n",
       "      <td>2.362231</td>\n",
       "      <td>136.811432</td>\n",
       "      <td>0.010492</td>\n",
       "      <td>0.079151</td>\n",
       "      <td>0.093242</td>\n",
       "      <td>1.78</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>-1.66</td>\n",
       "      <td>-1.09</td>\n",
       "      <td>-2.49</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.051051</td>\n",
       "      <td>1</td>\n",
       "      <td>0.051051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-06-30</td>\n",
       "      <td>AAP</td>\n",
       "      <td>24.714067</td>\n",
       "      <td>2.362231</td>\n",
       "      <td>143.795746</td>\n",
       "      <td>0.003330</td>\n",
       "      <td>0.076990</td>\n",
       "      <td>0.005550</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-1.48</td>\n",
       "      <td>1.41</td>\n",
       "      <td>1.94</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.050918</td>\n",
       "      <td>1</td>\n",
       "      <td>0.050918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13244</th>\n",
       "      <td>2024-08-31</td>\n",
       "      <td>XOM</td>\n",
       "      <td>14.107656</td>\n",
       "      <td>0.771855</td>\n",
       "      <td>114.883614</td>\n",
       "      <td>0.012588</td>\n",
       "      <td>0.052356</td>\n",
       "      <td>0.144777</td>\n",
       "      <td>1.61</td>\n",
       "      <td>-3.65</td>\n",
       "      <td>-1.13</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>-0.006105</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.006105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13245</th>\n",
       "      <td>2024-09-30</td>\n",
       "      <td>XOM</td>\n",
       "      <td>14.021531</td>\n",
       "      <td>0.771855</td>\n",
       "      <td>114.182266</td>\n",
       "      <td>0.009021</td>\n",
       "      <td>0.051371</td>\n",
       "      <td>0.097898</td>\n",
       "      <td>1.74</td>\n",
       "      <td>-1.02</td>\n",
       "      <td>-2.59</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>-0.003754</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.003754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13246</th>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>XOM</td>\n",
       "      <td>13.968900</td>\n",
       "      <td>0.771855</td>\n",
       "      <td>113.753670</td>\n",
       "      <td>0.003723</td>\n",
       "      <td>0.049151</td>\n",
       "      <td>0.031893</td>\n",
       "      <td>-0.97</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>0.89</td>\n",
       "      <td>-1.38</td>\n",
       "      <td>1.03</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.018405</td>\n",
       "      <td>1</td>\n",
       "      <td>0.018405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13247</th>\n",
       "      <td>2024-11-30</td>\n",
       "      <td>XOM</td>\n",
       "      <td>14.689913</td>\n",
       "      <td>0.737662</td>\n",
       "      <td>115.847267</td>\n",
       "      <td>0.011724</td>\n",
       "      <td>0.037114</td>\n",
       "      <td>0.141942</td>\n",
       "      <td>6.51</td>\n",
       "      <td>4.78</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-2.62</td>\n",
       "      <td>-2.17</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>-0.103425</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.103425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13248</th>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>XOM</td>\n",
       "      <td>13.396015</td>\n",
       "      <td>0.737662</td>\n",
       "      <td>103.865776</td>\n",
       "      <td>0.014966</td>\n",
       "      <td>0.035716</td>\n",
       "      <td>0.187306</td>\n",
       "      <td>-3.17</td>\n",
       "      <td>-3.87</td>\n",
       "      <td>-2.95</td>\n",
       "      <td>1.82</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13249 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date ticker     pe_exi  de_ratio  close_price   exp_ret  \\\n",
       "0      2016-02-29    AAP  23.193750  2.305863   131.959274 -0.000684   \n",
       "1      2016-03-31    AAP  25.053125  2.305863   142.591949 -0.000529   \n",
       "2      2016-04-30    AAP  24.390625  2.305863   138.821274  0.008972   \n",
       "3      2016-05-31    AAP  23.522936  2.362231   136.811432  0.010492   \n",
       "4      2016-06-30    AAP  24.714067  2.362231   143.795746  0.003330   \n",
       "...           ...    ...        ...       ...          ...       ...   \n",
       "13244  2024-08-31    XOM  14.107656  0.771855   114.883614  0.012588   \n",
       "13245  2024-09-30    XOM  14.021531  0.771855   114.182266  0.009021   \n",
       "13246  2024-10-31    XOM  13.968900  0.771855   113.753670  0.003723   \n",
       "13247  2024-11-30    XOM  14.689913  0.737662   115.847267  0.011724   \n",
       "13248  2024-12-31    XOM  13.396015  0.737662   103.865776  0.014966   \n",
       "\n",
       "            vol       mom  Mkt-RF   SMB   HML   RMW   CMA      RF  ret_excess  \\\n",
       "0      0.077602 -0.042258   -0.07  0.88 -0.57  3.25  2.02  0.0002    0.080575   \n",
       "1      0.077550 -0.040434    6.96  1.07  1.19  0.77 -0.08  0.0002   -0.026444   \n",
       "2      0.080094  0.072751    0.91  1.23  3.28 -2.97  1.90  0.0001   -0.014478   \n",
       "3      0.079151  0.093242    1.78 -0.61 -1.66 -1.09 -2.49  0.0001    0.051051   \n",
       "4      0.076990  0.005550   -0.05  0.45 -1.48  1.41  1.94  0.0002    0.050918   \n",
       "...         ...       ...     ...   ...   ...   ...   ...     ...         ...   \n",
       "13244  0.052356  0.144777    1.61 -3.65 -1.13  0.85  0.86  0.0048   -0.006105   \n",
       "13245  0.051371  0.097898    1.74 -1.02 -2.59  0.04 -0.26  0.0040   -0.003754   \n",
       "13246  0.049151  0.031893   -0.97 -0.88  0.89 -1.38  1.03  0.0039    0.018405   \n",
       "13247  0.037114  0.141942    6.51  4.78 -0.05 -2.62 -2.17  0.0040   -0.103425   \n",
       "13248  0.035716  0.187306   -3.17 -3.87 -2.95  1.82 -1.10  0.0037    0.000000   \n",
       "\n",
       "       label       ret  \n",
       "0          1  0.080575  \n",
       "1         -1 -0.026444  \n",
       "2         -1 -0.014478  \n",
       "3          1  0.051051  \n",
       "4          1  0.050918  \n",
       "...      ...       ...  \n",
       "13244     -1 -0.006105  \n",
       "13245     -1 -0.003754  \n",
       "13246      1  0.018405  \n",
       "13247     -1 -0.103425  \n",
       "13248     -1  0.000000  \n",
       "\n",
       "[13249 rows x 17 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗑  Dropping 43 tickers with missing returns:\n",
      "ABM, ACA, ACT, ADEA, ADMA, AESI, AHCO, ALRM, AMN, AMPH, AMR, ANGI, AORT, ARLO, AROC, ASIX, ASO, ATGE, AUB, AX, BAC, BH, BL, BTU, CABO, CALM, CARG, CARS, CC, CHEF, CLB, CNR, CRC, CRGY, CRK, CWEN, ENR, EPC, ESI, HCC, IAC, LUMN, MSGS\n",
      "✅  Y_df is now NaN-free and has 98 tickers.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# 1.  Build monthly factor (X_df) and asset-return (Y_df) tables\n",
    "\n",
    "Y_df = (df\n",
    "        .pivot(index='date', columns='ticker', values='ret_excess') # ret_excess can be used or ret\n",
    "        .sort_index())\n",
    "\n",
    "X_df = (df[['date'] + factor_cols]\n",
    "        .drop_duplicates('date')\n",
    "        .set_index('date')\n",
    "        .sort_index())\n",
    "# create a label_df\n",
    "label_df = (df\n",
    "            .pivot(index='date', columns='ticker', values='label')\n",
    "            .sort_index())\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1.  Find ticker columns with *any* NaNs in Y_df\n",
    "# ------------------------------------------------------------------\n",
    "bad_tickers = Y_df.columns[Y_df.isna().any()]\n",
    "\n",
    "print(f\"🗑  Dropping {len(bad_tickers)} tickers with missing returns:\")\n",
    "print(\", \".join(map(str, bad_tickers)))\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2.  Drop them from Y_df  (axis=1 ⇒ columns)\n",
    "# ------------------------------------------------------------------\n",
    "Y_df = Y_df.drop(columns=bad_tickers)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3.  OPTIONAL: keep your other pivot tables in sync\n",
    "#     (uncomment if you have X_df, label_df, etc. with same columns)\n",
    "# ------------------------------------------------------------------\n",
    "# X_df      = X_df.drop(columns=bad_tickers, errors=\"ignore\")\n",
    "label_df  = label_df.drop(columns=bad_tickers, errors=\"ignore\")\n",
    "# Sigma_fore = Sigma_fore[np.ix_(good_mask, good_mask)]  # inside loop\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4.  Verify — there should be zero NaNs left\n",
    "# ------------------------------------------------------------------\n",
    "assert not Y_df.isna().any().any(), \"Still NaNs lurking in Y_df!\"\n",
    "print(\"✅  Y_df is now NaN-free and has\", Y_df.shape[1], \"tickers.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) make sure every index really *is* a DatetimeIndex\n",
    "X_df.index      = pd.to_datetime(X_df.index)\n",
    "Y_df.index      = pd.to_datetime(Y_df.index)\n",
    "label_df.index  = pd.to_datetime(label_df.index)\n",
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every row is month-end already ⇒ just iterate over the index\n",
    "month_ends = X_df.index.sort_values()\n",
    "\n",
    "lookback = 12          # e.g. use the past 12 months\n",
    "month_ends = month_ends[lookback:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2017-02-28', '2017-03-31', '2017-04-30', '2017-05-31',\n",
       "               '2017-06-30', '2017-07-31', '2017-08-31', '2017-09-30',\n",
       "               '2017-10-31', '2017-11-30', '2017-12-31', '2018-01-31',\n",
       "               '2018-02-28', '2018-03-31', '2018-04-30', '2018-05-31',\n",
       "               '2018-06-30', '2018-07-31', '2018-08-31', '2018-09-30',\n",
       "               '2018-10-31', '2018-11-30', '2018-12-31', '2019-01-31',\n",
       "               '2019-02-28', '2019-03-31', '2019-04-30', '2019-05-31',\n",
       "               '2019-06-30', '2019-07-31', '2019-08-31', '2019-09-30',\n",
       "               '2019-10-31', '2019-11-30', '2019-12-31', '2020-01-31',\n",
       "               '2020-02-29', '2020-03-31', '2020-04-30', '2020-05-31',\n",
       "               '2020-06-30', '2020-07-31', '2020-08-31', '2020-09-30',\n",
       "               '2020-10-31', '2020-11-30', '2020-12-31', '2021-01-31',\n",
       "               '2021-02-28', '2021-03-31', '2021-04-30', '2021-05-31',\n",
       "               '2021-06-30', '2021-07-31', '2021-08-31', '2021-09-30',\n",
       "               '2021-10-31', '2021-11-30', '2021-12-31', '2022-01-31',\n",
       "               '2022-02-28', '2022-03-31', '2022-04-30', '2022-05-31',\n",
       "               '2022-06-30', '2022-07-31', '2022-08-31', '2022-09-30',\n",
       "               '2022-10-31', '2022-11-30', '2022-12-31', '2023-01-31',\n",
       "               '2023-02-28', '2023-03-31', '2023-04-30', '2023-05-31',\n",
       "               '2023-06-30', '2023-07-31', '2023-08-31', '2023-09-30',\n",
       "               '2023-10-31', '2023-11-30', '2023-12-31', '2024-01-31',\n",
       "               '2024-02-29', '2024-03-31', '2024-04-30', '2024-05-31',\n",
       "               '2024-06-30', '2024-07-31', '2024-08-31', '2024-09-30',\n",
       "               '2024-10-31', '2024-11-30', '2024-12-31'],\n",
       "              dtype='datetime64[ns]', name='date', freq=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "month_ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "tickers = Y_df.columns.unique()\n",
    "for me_date in month_ends:\n",
    "\n",
    "    # ---------- 1) pick the estimation window ------------\n",
    "    win_mask  = (X_df.index <= me_date)                     & \\\n",
    "                (X_df.index >  me_date - pd.offsets.MonthEnd(lookback))\n",
    "\n",
    "    X_window  = X_df.loc[win_mask]\n",
    "    Y_window  = Y_df.loc[win_mask]\n",
    "\n",
    "    if len(X_window) < 2:\n",
    "        continue   # still not enough data – skip\n",
    "\n",
    "    # ---------- 2)  β̂ , F̂  from the window --------------\n",
    "    beta_hat, F_hat = fit_beta(X_window.values, Y_window.values)\n",
    "\n",
    "    # ---------- 3)  Σ̂ (covariance)  ----------------------\n",
    "    # If you still want DCC-GARCH you can feed monthly returns straight in,\n",
    "    # but most people just use a sample/Exp-Wtd cov here:\n",
    "    W_hat = np.cov(X_window.values.T, ddof=1)\n",
    "    #W_hat = dcc_garch_cov(X_window.values)\n",
    "    \n",
    "    x_today      = X_df.loc[me_date].values\n",
    "    mu_fore, Sigma_fore = forecast_one_step(x_today, W_hat, beta_hat, F_hat)\n",
    "    #  └─ make sure your forecast function is set up for “+1 month”,\n",
    "    #     not “+1 trading day”.\n",
    "\n",
    "    W = 12\n",
    "    R_hist = (Y_df.loc[Y_df.index <= me_date]\n",
    "                .tail(W)         # 12 rows = 12 months\n",
    "                .T.values)       # (n_assets × W)\n",
    "               \n",
    "    # 5) features ----------------------------------------\n",
    "    feature_list = ['exp_ret', 'vol','mom', 'pe_exi', 'de_ratio', 'Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']\n",
    "\n",
    "    monthly = (df\n",
    "               .set_index(['date','ticker'])\n",
    "               .loc[me_date]\n",
    "               .reindex(tickers))\n",
    "    X_feat = monthly[feature_list].values\n",
    "\n",
    "    # 6) labels ------------------------------------------\n",
    "    y_vec = label_df.loc[me_date, tickers].values\n",
    "\n",
    "    # 7) store snapshot ----------------------------------\n",
    "    results.append(dict(date         = me_date,\n",
    "                        X_feat       = X_feat,\n",
    "                        returns_hist = R_hist,\n",
    "                        y            = y_vec.astype(np.double),\n",
    "                        mu_fore      = mu_fore.astype(np.double),\n",
    "                        Sigma_fore   = Sigma_fore.astype(np.double),\n",
    "                        beta         = beta_hat,\n",
    "                        F_hat        = F_hat,\n",
    "                        W_hat        = W_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Σ̂ SPD check: 95/95 pass, 0 fail\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- helper --------------------------------------------------------\n",
    "def is_spd(mat: np.ndarray, tol: float = 1e-8) -> bool:\n",
    "    \"\"\"\n",
    "    Return True if `mat` is symmetric-positive-definite.\n",
    "    • First check symmetry (fast) – avoids false negatives from tiny asymmetry\n",
    "    • Then try a Cholesky factorisation.  If it succeeds, SPD.\n",
    "    • `tol` lets you ignore round-off noise on symmetry test.\n",
    "    \"\"\"\n",
    "    if not np.allclose(mat, mat.T, atol=tol, rtol=0):\n",
    "        return False\n",
    "    try:\n",
    "        np.linalg.cholesky(mat)\n",
    "        return True\n",
    "    except np.linalg.LinAlgError:\n",
    "        return False\n",
    "\n",
    "# --- scan the results list ----------------------------------------\n",
    "bad = []                 # collect (index, date) for matrices that fail\n",
    "for i, snap in enumerate(results):\n",
    "    if not is_spd(snap[\"Sigma_fore\"]):\n",
    "        bad.append((i, snap[\"date\"]))\n",
    "\n",
    "# --- quick report --------------------------------------------------\n",
    "total = len(results)\n",
    "print(f\"Σ̂ SPD check: {total - len(bad)}/{total} pass, {len(bad)} fail\")\n",
    "\n",
    "if bad:\n",
    "    print(\"First few failures:\")\n",
    "    for idx, d in bad[:10]:\n",
    "        print(f\"  #{idx:3d}  {d}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from qpth.qp import QPFunction\n",
    "torch.manual_seed(99)\n",
    "\n",
    "def _check(tag, T):\n",
    "    if torch.isnan(T).any() or torch.isinf(T).any():\n",
    "        bad = torch.logical_or(torch.isnan(T), torch.isinf(T))\n",
    "        idx = torch.nonzero(bad, as_tuple=True)\n",
    "        print(f\"[{tag}] bad @ indices:\", idx, \"values:\", T[idx][:5])\n",
    "        raise ValueError(f\"{tag} contains NaN/Inf\")\n",
    "    \n",
    "def sanitise_spd(M: torch.Tensor,\n",
    "                 name: str = \"Q\",\n",
    "                 eps: float = 1e-6) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    1. reports NaN/Inf, duplicate rows, condition number, min/max eigenvalues\n",
    "    2. drops exact duplicate rows/cols (they kill Cholesky)\n",
    "    3. adds minimal jitter to make SPD\n",
    "    \"\"\"\n",
    "    def _report(tag, val):  # compact helper\n",
    "        print(f\"[{name}] {tag}: {val}\")\n",
    "\n",
    "    # 0) NaN / Inf check -------------------------------------------------\n",
    "    if torch.isnan(M).any() or torch.isinf(M).any():\n",
    "        raise ValueError(f\"[{name}] contains NaN or Inf – cannot factorise\")\n",
    "\n",
    "    # 1) deduplicate rows/cols ------------------------------------------\n",
    "    #    (X rows that are numerically identical give zero-variance directions)\n",
    "    # NOTE: this assumes you constructed M as  y yᵀ ⊙ X Xᵀ\n",
    "    #       If that is not the case just delete the dedup block.\n",
    "    with torch.no_grad():\n",
    "        diag = torch.diag(M)\n",
    "        mask_nonzero = diag > 0      # rows with all-zero features have 0 on diag\n",
    "        if mask_nonzero.sum() < len(mask_nonzero):\n",
    "            _report(\"duplicate/zero rows removed\",\n",
    "                    int(len(mask_nonzero) - mask_nonzero.sum()))\n",
    "            M = M[mask_nonzero][:, mask_nonzero]\n",
    "\n",
    "    # 2) minimal jitter --------------------------------------------------\n",
    "    eigvals = torch.linalg.eigvalsh(M)\n",
    "    λmin    = eigvals.min().item()\n",
    "    λmax    = eigvals.max().item()\n",
    "    cond    = λmax / max(λmin, eps)\n",
    "    #_report(\"λ_min\",  λmin)\n",
    "    #_report(\"λ_max\",  λmax)\n",
    "    #_report(\"cond\",   cond)\n",
    "\n",
    "    jitter  = max(eps, -λmin + eps)\n",
    "    M       = M + jitter * torch.eye(M.size(0), device=M.device, dtype=M.dtype)\n",
    "\n",
    "    # final safety: Cholesky must succeed now\n",
    "    try:\n",
    "        torch.linalg.cholesky(M)\n",
    "    except RuntimeError as e:\n",
    "        raise RuntimeError(f\"[{name}] still not SPD even after jitter = {jitter}\") from e\n",
    "\n",
    "    return M\n",
    "\n",
    "\n",
    "class EndToEndSVM_MVO_Sigmoid(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 C_svm: float,\n",
    "                 eps: float,\n",
    "                 tau: float\n",
    "            ):\n",
    "        \"\"\"\n",
    "        in_features : number of raw features per asset\n",
    "        C_svm       : SVM dual box-constraint\n",
    "        eps         : jitter to ensure all Q-matrices are SPD\n",
    "        tau         : sigmoid temperature for soft gating\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # 1)  make the learnable projection W (d × d, no bias)\n",
    "        self.embed = nn.Linear(in_features, in_features, bias=False).double()\n",
    "\n",
    "        # 2)  start it as an identity matrix instead of tiny random numbers\n",
    "        nn.init.eye_(self.embed.weight)          # ← this line\n",
    "\n",
    "        self.C   = C_svm\n",
    "        self.eps = eps\n",
    "        self.tau = tau\n",
    "\n",
    "    def forward(self,\n",
    "                X_feat: torch.Tensor,     # (n,d)\n",
    "                y: torch.Tensor,          # ±1\n",
    "                mu_est: torch.Tensor,     # (n,)  \n",
    "                Sigma_est: torch.Tensor,  # (n,n) \n",
    "                return_goal: float\n",
    "               ) -> torch.Tensor:\n",
    "        n, d = X_feat.shape\n",
    "        # A. raw input\n",
    "        #_check(\"X_feat\", X_feat)\n",
    "\n",
    "\n",
    "        # 1) feature embedding\n",
    "        Xp = self.embed(X_feat.double())\n",
    "        #_check(\"Xp\", Xp)\n",
    "        y  = y.view(-1).double()           # now y is float64 ±1\n",
    "\n",
    "        # 2) SVM dual QP\n",
    "        K      = Xp @ Xp.t()                             # (n, n)\n",
    "        #_check(\"K\", K)\n",
    "\n",
    "        yy    = y.unsqueeze(1) * y.unsqueeze(0)  # (n,n) float64\n",
    "        Q_svm  = sanitise_spd((y[:,None] * y[None,:]) * K, name=\"Q_svm\", eps=self.eps)\n",
    "        #_check(\"Q_svm\", Q_svm)\n",
    "        p_svm  = -torch.ones(n, device=Xp.device, dtype=Xp.dtype)\n",
    "\n",
    "        G_svm  = torch.cat([\n",
    "            -torch.eye(n, device=Xp.device, dtype=Xp.dtype),\n",
    "             torch.eye(n, device=Xp.device, dtype=Xp.dtype)\n",
    "        ], dim=0)                                        # (2n, n)\n",
    "        h_svm  = torch.cat([\n",
    "            torch.zeros(n, device=Xp.device, dtype=Xp.dtype),\n",
    "            self.C * torch.ones(n, device=Xp.device, dtype=Xp.dtype)\n",
    "        ], dim=0)                                        # (2n,)\n",
    "\n",
    "        A_svm  = y.view(1,-1)#.to(Xp)                     # (1, n)\n",
    "        b_svm  = torch.zeros(1, device=Xp.device, dtype=Xp.dtype)\n",
    "        # ---------------------------------------------------------------\n",
    "        # handle single-class case (all +1  *or*  all –1)\n",
    "        # ---------------------------------------------------------------\n",
    "        if (y == y[0]).all():              # every label identical\n",
    "            A_svm = torch.empty(0, n, device=Xp.device, dtype=Xp.dtype)  # shape (0, n)\n",
    "            b_svm = torch.empty(0,       device=Xp.device, dtype=Xp.dtype)  # shape (0,)\n",
    "            print(\"Warning: all labels identical, no SVM hyperplane constructed.\")\n",
    "        else:\n",
    "            A_svm = y.view(1, -1)                                           # (1, n)\n",
    "            b_svm = torch.zeros(1, device=Xp.device, dtype=Xp.dtype)        # (1,)\n",
    "\n",
    "        alpha      = QPFunction(verbose=False)(\n",
    "                    Q_svm, p_svm, G_svm, h_svm, A_svm, b_svm\n",
    "                 )                                  # (n,)\n",
    "        alpha      = torch.clamp(alpha, 0.0, self.C)\n",
    "\n",
    "        # 3) build hyperplane and score\n",
    "        # after solving for alpha\n",
    "        # construct w_svm properly:\n",
    "        # make sure alphas is a 1-D tensor of length \n",
    "        alpha = alpha.view(-1)               \n",
    "\n",
    "        # ensure y is double or double to match alphas dtype\n",
    "        y = y.to(alpha.dtype)            \n",
    "\n",
    "        # elementwise product alpha_i * y_i\n",
    "        alpha_y = alpha * y                \n",
    "        w_svm = Xp.t().mv(alpha_y)                   # or torch.matmul(X.t(), alpha_y)\n",
    "\n",
    "        scores = Xp @ w_svm                 # (n_assets,)\n",
    "\n",
    "        # diagnostic\n",
    "        with torch.no_grad():\n",
    "            print(\"‖w_svm‖₂       :\", w_svm.norm().item())\n",
    "            print(\"‖alpha‖₁       :\", alpha.abs().sum().item())\n",
    "            print(\"scores min/max :\", scores.min().item(), scores.max().item())\n",
    "            \n",
    "        hinge = torch.clamp(1.0 - y * scores, min=0.0).mean()\n",
    "\n",
    "\n",
    "\n",
    "        # 4) differentiable sigmoid gate\n",
    "        mask   = torch.sigmoid(scores / self.tau)        # (n,) in (0,1)\n",
    "        print(\"Mask mean value: \", mask.mean())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # 5) MVO QP *over all assets* with w_i ≤ mask_i\n",
    "        #    compute moments for every asset\n",
    "        # ---------- 2) MVO QP using *forecast* μ, Σ ----------\n",
    "        mu     = mu_est                             # (n,)\n",
    "        Sigma  = Sigma_est\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # upper-bound portfolio return with current mask\n",
    "            r_max = (mu * mask).sum()          # all weight mass on highest μ allowed\n",
    "            print(f\"max feasible return = {r_max.item():.4f}  |  goal = {return_goal}\")\n",
    "\n",
    "            if r_max < return_goal - 1e-8:\n",
    "                print(\"⚠️  Target return is infeasible for this mask snapshot.\")\n",
    "\n",
    "        P_mvo  = Sigma\n",
    "        q_mvo  = torch.zeros(n, device=Sigma.device, dtype=Sigma.dtype)\n",
    "\n",
    "        # box constraints: 0 ≤ w ≤ mask\n",
    "        G_box  = torch.cat([\n",
    "            -torch.eye(n, device=Sigma.device, dtype=Sigma.dtype),  # -w ≤ 0\n",
    "             torch.eye(n, device=Sigma.device, dtype=Sigma.dtype)   #  w ≤ mask\n",
    "        ], dim=0)                                                   # (2n, n)\n",
    "        h_box  = torch.cat([\n",
    "            torch.zeros(n, device=Sigma.device, dtype=Sigma.dtype),\n",
    "            mask\n",
    "        ], dim=0)                                                   # (2n,)\n",
    "\n",
    "        # ------------------ NEW inequality: μᵀw ≥ return_goal -------------\n",
    "        G_ret = -mu.unsqueeze(0)                                     # (1, n)\n",
    "        h_ret = -torch.tensor([return_goal],\n",
    "                            device=Sigma.device, dtype=Sigma.dtype)\n",
    "\n",
    "        # concat all inequalities\n",
    "        G_ineq = torch.cat([G_box, G_ret], dim=0)                    # (2n+1, n)\n",
    "        h_ineq = torch.cat([h_box, h_ret], dim=0)                    # (2n+1,)\n",
    "\n",
    "        # equality: sum(w)=1\n",
    "        A_eq = torch.ones(1, n, device=Sigma.device, dtype=Sigma.dtype)  # (1, n)                                             \n",
    "        b_eq = torch.tensor([1.0], device=Sigma.device, dtype=Sigma.dtype)\n",
    "\n",
    "        w_opt  = QPFunction(verbose=False)(\n",
    "                    P_mvo, q_mvo, G_ineq, h_ineq, A_eq, b_eq\n",
    "                 )                                  # (n,)\n",
    "        # 2. Check return target vs feasible region\n",
    "        #print(\"mask_min/max:\", mask.min().item(), mask.max().item())\n",
    "        #print(\"mu_min/max:\",   mu.min().item(),   mu.max().item(), \"goal:\", return_goal)\n",
    "        \n",
    "        return w_opt.view(-1), hinge             # (n,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "snapshots = sorted(results, key=lambda s: s[\"date\"])   # ensure sorted\n",
    "\n",
    "val_fraction = 0.2\n",
    "n_total      = len(snapshots)\n",
    "n_val        = int(n_total * val_fraction)\n",
    "\n",
    "train_snaps  = snapshots[:-n_val]          # earlier dates\n",
    "val_snaps    = snapshots[-n_val:]          # most recent dates\n",
    "\n",
    "# ---------- 1.  tiny helper --------------------------------------------------\n",
    "def to_tensor(x, *, dtype=torch.float64):\n",
    "    \"\"\"\n",
    "    NumPy → torch, replace NaN/Inf with finite numbers\n",
    "    (you can swap 'nan=0.0' for any imputation of your choice).\n",
    "    \"\"\"\n",
    "    x = np.nan_to_num(x, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "    return torch.as_tensor(x, dtype=dtype)\n",
    "\n",
    "\n",
    "# ---------- 2.  custom Dataset ----------------------------------------------\n",
    "class SnapshotDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Each item is one time-snapshot:\n",
    "        X_feat, y, mu_fore, Sigma_fore, return_goal\n",
    "    Shapes:\n",
    "        X_feat      (n, d)   – features\n",
    "        y           (n,)     – ±1 labels\n",
    "        mu_fore     (n,)\n",
    "        Sigma_fore  (n, n)\n",
    "    \"\"\"\n",
    "    def __init__(self, results, return_goal):\n",
    "        self.data = []\n",
    "\n",
    "        for snap in results:\n",
    "            X  = to_tensor(snap[\"X_feat\"])      # (n, d)\n",
    "            y  = to_tensor(snap[\"y\"]).view(-1)  # (n,)\n",
    "            mu = to_tensor(snap[\"mu_fore\"])     # (n,)\n",
    "            S  = to_tensor(snap[\"Sigma_fore\"])  # (n, n)\n",
    "\n",
    "            # ---------- basic sanity: drop rows that are still all-zero ------\n",
    "            # (happens if the original had only NaNs)\n",
    "            keep = (X.abs().sum(dim=1) > 0)\n",
    "            if keep.sum() < 2:                  # need ≥2 points for an SVM\n",
    "                continue                         # skip this snapshot\n",
    "\n",
    "            X, y, mu = X[keep], y[keep], mu[keep]\n",
    "            S        = S[keep][:, keep]\n",
    "\n",
    "            self.data.append((X, y, mu, S, float(return_goal)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]          # batch = tuple of 5 tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------Epoch:  1 ----------------------------------------\n",
      "‖w_svm‖₂       : 0.023789445054010108\n",
      "‖alpha‖₁       : 0.8599999999999828\n",
      "scores min/max : -0.051389728619721305 0.07023443335975885\n",
      "Mask mean value:  tensor(0.4880, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6506  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.316e-14\n",
      "‖w_svm‖₂       : 0.11503395115845469\n",
      "‖alpha‖₁       : 0.6485480408601536\n",
      "scores min/max : -17.696820368066064 2.096458227684054\n",
      "Mask mean value:  tensor(0.7404, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1721  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.874e-09\n",
      "‖w_svm‖₂       : 0.0048202231775053336\n",
      "‖alpha‖₁       : 0.3799999999999992\n",
      "scores min/max : -0.008563399142324919 0.0017885251170978126\n",
      "Mask mean value:  tensor(0.4954, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8826  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.501e-14\n",
      "‖w_svm‖₂       : 8.184956798121821e-08\n",
      "‖alpha‖₁       : 0.38\n",
      "scores min/max : -5.6913121187201806e-09 1.2446018537991292e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.459e-22\n",
      "‖w_svm‖₂       : 1.4218971620291085e-07\n",
      "‖alpha‖₁       : 0.37999999999999345\n",
      "scores min/max : -9.449511031292122e-09 5.5470424740013315e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.368e-07\n",
      "‖w_svm‖₂       : 7.409005264396887e-08\n",
      "‖alpha‖₁       : 0.13999999999998555\n",
      "scores min/max : -4.386717999807475e-09 1.4597005241632191e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.884e-08\n",
      "‖w_svm‖₂       : 0.04856460666812842\n",
      "‖alpha‖₁       : 0.8263074351954468\n",
      "scores min/max : -1.9890633209321404 0.3721426466798614\n",
      "Mask mean value:  tensor(0.3825, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0269  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.539e-02\n",
      "‖w_svm‖₂       : 0.004552556486472456\n",
      "‖alpha‖₁       : 0.7005547376574455\n",
      "scores min/max : -2.0070170420602067 0.016359756494072952\n",
      "Mask mean value:  tensor(0.4401, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2473  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.547e-05\n",
      "‖w_svm‖₂       : 0.05926523806844616\n",
      "‖alpha‖₁       : 0.9199999999999942\n",
      "scores min/max : -0.44078016174113116 0.8923463545204302\n",
      "Mask mean value:  tensor(0.6176, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1449  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.097e-03\n",
      "‖w_svm‖₂       : 0.060090047799591824\n",
      "‖alpha‖₁       : 0.7293283914699276\n",
      "scores min/max : -0.23083615339435332 2.048672339159679\n",
      "Mask mean value:  tensor(0.7673, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.4175  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 7.429e-04\n",
      "‖w_svm‖₂       : 1.127372858777745e-06\n",
      "‖alpha‖₁       : 0.499999999999983\n",
      "scores min/max : -4.445297622143073e-08 1.4866162886597238e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.131e-19\n",
      "‖w_svm‖₂       : 2.907591465193911e-07\n",
      "‖alpha‖₁       : 0.66\n",
      "scores min/max : -7.078206016270554e-08 8.084485230062353e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.171e-18\n",
      "‖w_svm‖₂       : 2.45549632195567e-07\n",
      "‖alpha‖₁       : 0.6399999999999797\n",
      "scores min/max : -1.5781106369725052e-08 5.125496703103776e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.136e-20\n",
      "‖w_svm‖₂       : 1.7808293909428208e-07\n",
      "‖alpha‖₁       : 0.239999999999989\n",
      "scores min/max : -1.2355569922917888e-08 3.0782056994651687e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.432e-20\n",
      "‖w_svm‖₂       : 1.106722760438422e-07\n",
      "‖alpha‖₁       : 0.23999999999997562\n",
      "scores min/max : -2.1540482375213488e-09 8.858366302247197e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.536e-21\n",
      "‖w_svm‖₂       : 8.374973180059502e-08\n",
      "‖alpha‖₁       : 0.6599999999999951\n",
      "scores min/max : -1.5754774237819838e-08 1.2900701145909744e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.378e-09\n",
      "‖w_svm‖₂       : 0.0008099513748216786\n",
      "‖alpha‖₁       : 0.8199999999999991\n",
      "scores min/max : -0.001258529584865573 0.0002432337587374865\n",
      "Mask mean value:  tensor(0.4984, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5321  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.709e-17\n",
      "‖w_svm‖₂       : 0.00018331978420824862\n",
      "‖alpha‖₁       : 0.8199999999999958\n",
      "scores min/max : -1.2888981541390127e-05 1.2549761359679429e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6978  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.136e-17\n",
      "‖w_svm‖₂       : 0.020483973412486506\n",
      "‖alpha‖₁       : 0.5962222954058374\n",
      "scores min/max : -2.9407399919645387 2.1806597077236254\n",
      "Mask mean value:  tensor(0.8440, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3241  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.824e-02\n",
      "‖w_svm‖₂       : 0.0002063248485876401\n",
      "‖alpha‖₁       : 0.6199999999971116\n",
      "scores min/max : -7.343163325798104e-06 7.746915777740238e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.600e-17\n",
      "‖w_svm‖₂       : 1.1533581989295752e-07\n",
      "‖alpha‖₁       : 0.5199999999999988\n",
      "scores min/max : -1.906089068020053e-08 2.341126689948872e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.056e-19\n",
      "‖w_svm‖₂       : 0.082647311034048\n",
      "‖alpha‖₁       : 0.6574511767265585\n",
      "scores min/max : -1.9305686418884704 3.7356606795494476\n",
      "Mask mean value:  tensor(0.5862, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.7206  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.633e-06\n",
      "‖w_svm‖₂       : 2.3828227393214618e-07\n",
      "‖alpha‖₁       : 0.5799999999999863\n",
      "scores min/max : 5.716383448648295e-10 1.625144600349697e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.561e-19\n",
      "‖w_svm‖₂       : 1.9080361717925232e-07\n",
      "‖alpha‖₁       : 0.37999999999999934\n",
      "scores min/max : -3.593583827593649e-08 -2.479125819914925e-10\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.397e-19\n",
      "‖w_svm‖₂       : 0.005847747539087034\n",
      "‖alpha‖₁       : 0.6076575625261624\n",
      "scores min/max : -1.980434739175809 0.3065857743663597\n",
      "Mask mean value:  tensor(0.5994, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2424  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.366e-15\n",
      "‖w_svm‖₂       : 4.274261632149107e-07\n",
      "‖alpha‖₁       : 0.7199999999999998\n",
      "scores min/max : -6.634612746011089e-08 -9.10176759143239e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.532e-21\n",
      "‖w_svm‖₂       : 0.13863420467741777\n",
      "‖alpha‖₁       : 0.8728827856505408\n",
      "scores min/max : -12.193451121324381 2.1240751825340136\n",
      "Mask mean value:  tensor(0.5133, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.5085  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 2.173e-01\n",
      "‖w_svm‖₂       : 2.11096382721532e-07\n",
      "‖alpha‖₁       : 0.4199999999999682\n",
      "scores min/max : -3.50903876205805e-08 -1.2737208806990064e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.452e-19\n",
      "‖w_svm‖₂       : 0.025739083711066413\n",
      "‖alpha‖₁       : 0.8150783924817958\n",
      "scores min/max : -11.502362775791502 2.0455524768080937\n",
      "Mask mean value:  tensor(0.7421, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9662  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.953e-11\n",
      "‖w_svm‖₂       : 0.023817121502083097\n",
      "‖alpha‖₁       : 0.8233062046384336\n",
      "scores min/max : -1.8647534945534774 1.5379889031970966\n",
      "Mask mean value:  tensor(0.7836, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.5967  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.074e-05\n",
      "‖w_svm‖₂       : 0.05123475972491124\n",
      "‖alpha‖₁       : 0.8972181268731909\n",
      "scores min/max : -1.7831128018982603 3.6166491232703684\n",
      "Mask mean value:  tensor(0.2800, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4492  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.791e-03\n",
      "‖w_svm‖₂       : 1.0575581265303735e-06\n",
      "‖alpha‖₁       : 0.5999999999999749\n",
      "scores min/max : -5.666104131845813e-08 -5.703005007678287e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.464e-19\n",
      "‖w_svm‖₂       : 1.177723365201369e-07\n",
      "‖alpha‖₁       : 0.2999999999999862\n",
      "scores min/max : 4.216463081893895e-09 1.3275109327648497e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.159e-07\n",
      "‖w_svm‖₂       : 0.05500020402157793\n",
      "‖alpha‖₁       : 0.5753528201224337\n",
      "scores min/max : -1.9460455937118097 0.8765033644917635\n",
      "Mask mean value:  tensor(0.7320, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.0033  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.334e-04\n",
      "‖w_svm‖₂       : 5.617740621003199e-08\n",
      "‖alpha‖₁       : 0.23999999999997818\n",
      "scores min/max : -8.69581462381548e-09 5.854958257161024e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.027e-20\n",
      "‖w_svm‖₂       : 0.024936639808456233\n",
      "‖alpha‖₁       : 0.19642302146620458\n",
      "scores min/max : -2.0873309334576637 0.13928063139463873\n",
      "Mask mean value:  tensor(0.3198, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.0639  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.079e-09\n",
      "‖w_svm‖₂       : 4.764448440239928e-08\n",
      "‖alpha‖₁       : 0.17999999999999883\n",
      "scores min/max : -1.7449032970535139e-09 1.287040429635307e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 4.347e-08\n",
      "‖w_svm‖₂       : 0.046189494911893864\n",
      "‖alpha‖₁       : 0.9387649356472215\n",
      "scores min/max : -2.480153871710273 1.554131585078983\n",
      "Mask mean value:  tensor(0.1294, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3499  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.005e-03\n",
      "‖w_svm‖₂       : 2.3464549887911975e-07\n",
      "‖alpha‖₁       : 0.2599999999999886\n",
      "scores min/max : 3.466696158911477e-08 4.7498445530874524e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.849e-17\n",
      "‖w_svm‖₂       : 5.7989598457158686e-08\n",
      "‖alpha‖₁       : 0.1199999999999905\n",
      "scores min/max : -1.14985534858377e-08 1.9308018084145804e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.562e-08\n",
      "‖w_svm‖₂       : 6.021500746200314e-08\n",
      "‖alpha‖₁       : 0.43999999999999534\n",
      "scores min/max : -1.1149925596783693e-07 -7.363193567819594e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.366e-20\n",
      "‖w_svm‖₂       : 0.019201740296767168\n",
      "‖alpha‖₁       : 0.6814739939342587\n",
      "scores min/max : -1.9935075082527387 0.04441101912677492\n",
      "Mask mean value:  tensor(0.5107, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1882  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.769e-12\n",
      "‖w_svm‖₂       : 0.002951386905725568\n",
      "‖alpha‖₁       : 0.5799999999999998\n",
      "scores min/max : -0.0009189887561900777 0.0001988651199040653\n",
      "Mask mean value:  tensor(0.4998, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3178  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.995e-05\n",
      "‖w_svm‖₂       : 4.002414209245465e-07\n",
      "‖alpha‖₁       : 0.27999999999999725\n",
      "scores min/max : -1.111316194010338e-07 -4.942692379513259e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.865e-19\n",
      "‖w_svm‖₂       : 0.00015579984732452543\n",
      "‖alpha‖₁       : 0.6399999999999734\n",
      "scores min/max : -2.6872368345508686e-05 -1.7608938244211895e-05\n",
      "Mask mean value:  tensor(0.4999, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7642  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.031e-17\n",
      "‖w_svm‖₂       : 0.0003037687306641851\n",
      "‖alpha‖₁       : 0.4199999999999977\n",
      "scores min/max : -0.0003168973681370353 -4.217582583848192e-06\n",
      "Mask mean value:  tensor(0.4998, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9952  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.511e-15\n",
      "‖w_svm‖₂       : 0.022889727436299728\n",
      "‖alpha‖₁       : 0.3833179589003174\n",
      "scores min/max : -1.977376999617345 0.21840693390152624\n",
      "Mask mean value:  tensor(0.6419, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.009e-13\n",
      "‖w_svm‖₂       : 0.0447393252700232\n",
      "‖alpha‖₁       : 0.9401047823652389\n",
      "scores min/max : -1.822617914769313 0.31508558959928035\n",
      "Mask mean value:  tensor(0.4543, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3559  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.038e-15\n",
      "‖w_svm‖₂       : 0.031471954678258474\n",
      "‖alpha‖₁       : 0.8982854444150619\n",
      "scores min/max : -0.8287285867596603 1.934094654711487\n",
      "Mask mean value:  tensor(0.6900, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5294  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.235e-06\n",
      "‖w_svm‖₂       : 0.00048228331634318517\n",
      "‖alpha‖₁       : 0.43999999999999995\n",
      "scores min/max : -4.2449957745711826e-05 0.0001875715094223649\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6552  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.583e-16\n",
      "‖w_svm‖₂       : 0.06883211939581166\n",
      "‖alpha‖₁       : 0.5788805669804571\n",
      "scores min/max : -3.3109786254657974 1.8654682462272503\n",
      "Mask mean value:  tensor(0.1342, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2754  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.645e-04\n",
      "‖w_svm‖₂       : 0.027741813340308782\n",
      "‖alpha‖₁       : 0.5509271706115271\n",
      "scores min/max : -3.604589017268264 0.9704286558357864\n",
      "Mask mean value:  tensor(0.0970, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8225  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.743e-03\n",
      "‖w_svm‖₂       : 1.5546998402389517e-05\n",
      "‖alpha‖₁       : 0.3599999999926343\n",
      "scores min/max : -1.4674267320271098e-07 7.554633686344886e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3078  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.794e-17\n",
      "‖w_svm‖₂       : 0.0003448533239520723\n",
      "‖alpha‖₁       : 0.7399999999999982\n",
      "scores min/max : -2.7701102019780147e-05 -1.2531768889895505e-05\n",
      "Mask mean value:  tensor(0.4999, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0641  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.952e-17\n",
      "‖w_svm‖₂       : 0.005395937513553804\n",
      "‖alpha‖₁       : 0.5599999999999727\n",
      "scores min/max : 0.0005413283226056913 0.002045256031799637\n",
      "Mask mean value:  tensor(0.5041, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0566  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 2.695e-04\n",
      "‖w_svm‖₂       : 0.06732777602563866\n",
      "‖alpha‖₁       : 0.46953410286140373\n",
      "scores min/max : -1.9151396084139463 3.168439243306221\n",
      "Mask mean value:  tensor(0.8410, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 7.8926  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.458e-09\n",
      "‖w_svm‖₂       : 0.11975486958402691\n",
      "‖alpha‖₁       : 0.7521792047996203\n",
      "scores min/max : -1.7083701006664624 2.274567146149382\n",
      "Mask mean value:  tensor(0.9141, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3428  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.862e-12\n",
      "‖w_svm‖₂       : 0.018843203352315183\n",
      "‖alpha‖₁       : 0.7799999999999746\n",
      "scores min/max : -0.001591993570569123 0.0712542579142827\n",
      "Mask mean value:  tensor(0.5234, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.2703  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.817e-15\n",
      "‖w_svm‖₂       : 0.16893301323504448\n",
      "‖alpha‖₁       : 0.8746823192858512\n",
      "scores min/max : -3.079915130475414 6.45635200272294\n",
      "Mask mean value:  tensor(0.1055, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1771  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.627e-05\n",
      "‖w_svm‖₂       : 0.056282870872112666\n",
      "‖alpha‖₁       : 0.9054173169982169\n",
      "scores min/max : -1.1612470394835974 1.7535465317503818\n",
      "Mask mean value:  tensor(0.1604, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2977  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.489e-03\n",
      "‖w_svm‖₂       : 9.50178241311783e-08\n",
      "‖alpha‖₁       : 0.17999999999999897\n",
      "scores min/max : -2.8456842361640714e-09 2.3091001022152223e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.938e-21\n",
      "‖w_svm‖₂       : 0.004040655402479383\n",
      "‖alpha‖₁       : 0.4599999999999991\n",
      "scores min/max : -0.0019929742864206674 -0.0003117821074589064\n",
      "Mask mean value:  tensor(0.4965, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.1583  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.712e-16\n",
      "‖w_svm‖₂       : 0.03007540613429636\n",
      "‖alpha‖₁       : 0.6599999999999921\n",
      "scores min/max : -0.058957195033828215 0.1120133910756137\n",
      "Mask mean value:  tensor(0.4932, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.9416  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.186e-15\n",
      "‖w_svm‖₂       : 2.4192078314795954e-08\n",
      "‖alpha‖₁       : 0.11999999999999686\n",
      "scores min/max : -1.6729517348203745e-09 1.2858777583186986e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.958e-09\n",
      "‖w_svm‖₂       : 8.180543199050361e-08\n",
      "‖alpha‖₁       : 0.5399999999999997\n",
      "scores min/max : -2.4602165190907608e-08 -3.0757488769583364e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.564e-20\n",
      "‖w_svm‖₂       : 6.792648499849102e-07\n",
      "‖alpha‖₁       : 0.39999999999999847\n",
      "scores min/max : -1.275556233485277e-07 1.784313816416377e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.121e-09\n",
      "‖w_svm‖₂       : 4.614309936695845e-06\n",
      "‖alpha‖₁       : 0.41999999998652204\n",
      "scores min/max : -1.2644811011073844e-06 6.646706853677764e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1230  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.966e-05\n",
      "‖w_svm‖₂       : 0.022486860653401573\n",
      "‖alpha‖₁       : 0.8520932563862181\n",
      "scores min/max : -3.0379922161669253 1.443835177017273\n",
      "Mask mean value:  tensor(0.1180, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8341  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.320e-03\n",
      "‖w_svm‖₂       : 0.07671691016074325\n",
      "‖alpha‖₁       : 0.41729126674201283\n",
      "scores min/max : -1.6574812616596324 3.656542656736513\n",
      "Mask mean value:  tensor(0.8837, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8908  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.760e-11\n",
      "‖w_svm‖₂       : 0.04087052382369471\n",
      "‖alpha‖₁       : 0.8894842461680009\n",
      "scores min/max : -2.024659152573376 0.32246835495080023\n",
      "Mask mean value:  tensor(0.2247, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0634  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.646e-03\n",
      "‖w_svm‖₂       : 0.15991138914429104\n",
      "‖alpha‖₁       : 0.8776428575431686\n",
      "scores min/max : -2.0878490560903953 5.080038511723575\n",
      "Mask mean value:  tensor(0.1186, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1787  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.724e-03\n",
      "‖w_svm‖₂       : 0.06182621685883089\n",
      "‖alpha‖₁       : 0.6599999999999999\n",
      "scores min/max : -0.5178767001469985 0.3941508693413547\n",
      "Mask mean value:  tensor(0.5209, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8710  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.651e-14\n",
      "‖w_svm‖₂       : 0.01704233626978669\n",
      "‖alpha‖₁       : 0.8599999999999999\n",
      "scores min/max : -0.020507632277063002 -0.0007579764000145775\n",
      "Mask mean value:  tensor(0.4682, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1148  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.590e-15\n",
      "‖w_svm‖₂       : 0.00012895873732157606\n",
      "‖alpha‖₁       : 0.43999999999992556\n",
      "scores min/max : -7.974092973301613e-05 4.1451054327593226e-05\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0288  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.422e-17\n",
      "‖w_svm‖₂       : 7.052164864035976e-08\n",
      "‖alpha‖₁       : 0.4199999999999989\n",
      "scores min/max : -4.63523501154567e-08 7.961059171284025e-10\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 4.892e-08\n",
      "‖w_svm‖₂       : 1.1127370183381681e-06\n",
      "‖alpha‖₁       : 0.31999999999977446\n",
      "scores min/max : -1.4703639445318943e-07 -3.984243912145174e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4899  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.412e-18\n",
      "‖w_svm‖₂       : 0.07252298872124084\n",
      "‖alpha‖₁       : 0.5755552646668228\n",
      "scores min/max : -1.8922563585662422 6.059816281249092\n",
      "Mask mean value:  tensor(0.8558, dtype=torch.float64)\n",
      "max feasible return = 0.4738  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.175982750743913e-07\n",
      "‖alpha‖₁       : 0.5799999999999998\n",
      "scores min/max : -4.855174263599755e-08 4.217641487264062e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0583386299173653e-07\n",
      "‖alpha‖₁       : 0.2999999999999997\n",
      "scores min/max : -4.563248145371564e-09 3.997810165475984e-10\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 7.730175740655192e-08\n",
      "‖alpha‖₁       : 0.5999999999999998\n",
      "scores min/max : -3.359621633427723e-08 3.132876342651435e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 5.1008712332856384e-08\n",
      "‖alpha‖₁       : 0.3799999999999991\n",
      "scores min/max : -9.927729706910885e-09 -5.470216014151722e-10\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.9413472110348676e-06\n",
      "‖alpha‖₁       : 0.319999999980963\n",
      "scores min/max : -1.3954349383512672e-06 1.8023248110935785e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4257  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.06913814728232948\n",
      "‖alpha‖₁       : 0.739626841858414\n",
      "scores min/max : -3.4884405741787363 2.1178606714095762\n",
      "Mask mean value:  tensor(0.8484, dtype=torch.float64)\n",
      "max feasible return = -0.8951  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.0324924085060707\n",
      "‖alpha‖₁       : 0.6148798255141724\n",
      "scores min/max : -1.8824569932569595 1.2477462789778684\n",
      "Mask mean value:  tensor(0.8445, dtype=torch.float64)\n",
      "max feasible return = 2.9779  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.928205535293137e-07\n",
      "‖alpha‖₁       : 0.4599999999995189\n",
      "scores min/max : -1.4170068618646197e-07 4.641572188320021e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.37013537163882e-07\n",
      "‖alpha‖₁       : 0.5199999999999949\n",
      "scores min/max : -1.2479287227121639e-09 1.1646272649876339e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.6089589615054566e-07\n",
      "‖alpha‖₁       : 0.5799999999999987\n",
      "scores min/max : -1.0932751906233515e-08 2.5207000855310377e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.039857264404822526\n",
      "‖alpha‖₁       : 0.6870889112221268\n",
      "scores min/max : -0.3345741457345845 2.058307632199502\n",
      "Mask mean value:  tensor(0.7723, dtype=torch.float64)\n",
      "max feasible return = -0.2591  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.036212767319759545\n",
      "‖alpha‖₁       : 0.4220640876159495\n",
      "scores min/max : -3.940027694913938 5.143210457049406\n",
      "Mask mean value:  tensor(0.0636, dtype=torch.float64)\n",
      "max feasible return = 0.4023  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.049509104067639885\n",
      "‖alpha‖₁       : 0.8211039534304752\n",
      "scores min/max : -5.079099457093813 3.075707154309234\n",
      "Mask mean value:  tensor(0.9435, dtype=torch.float64)\n",
      "max feasible return = -3.5684  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.010957576580863427\n",
      "‖alpha‖₁       : 0.7999999999999985\n",
      "scores min/max : -0.06252089333742432 0.015306936873624565\n",
      "Mask mean value:  tensor(0.4785, dtype=torch.float64)\n",
      "max feasible return = -0.1741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.23514272990451e-07\n",
      "‖alpha‖₁       : 0.6199999999999843\n",
      "scores min/max : -3.298769213000312e-08 -1.653269149749744e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.6870203756726484e-08\n",
      "‖alpha‖₁       : 0.4399999999999612\n",
      "scores min/max : -3.0759962321022668e-09 6.506736602497912e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.309528797469823e-07\n",
      "‖alpha‖₁       : 0.2799999999999993\n",
      "scores min/max : 9.82033523175085e-09 2.7418350802656627e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch   1 | train 0.005654 | val 0.006823\n",
      "-----------------------------------------Epoch:  2 ----------------------------------------\n",
      "‖w_svm‖₂       : 0.16011603975960906\n",
      "‖alpha‖₁       : 0.8777202763197184\n",
      "scores min/max : -2.086223553175941 5.080251562865225\n",
      "Mask mean value:  tensor(0.1200, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1805  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.723e-03\n",
      "‖w_svm‖₂       : 0.03007036417872475\n",
      "‖alpha‖₁       : 0.659999999999992\n",
      "scores min/max : -0.058904420405596715 0.11198813711542846\n",
      "Mask mean value:  tensor(0.4934, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.9427  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.185e-15\n",
      "‖w_svm‖₂       : 0.048398404782549795\n",
      "‖alpha‖₁       : 0.826289351439235\n",
      "scores min/max : -1.9860804455746095 0.37525968529905157\n",
      "Mask mean value:  tensor(0.3927, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0260  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.522e-02\n",
      "‖w_svm‖₂       : 8.180028136108803e-08\n",
      "‖alpha‖₁       : 0.5399999999999997\n",
      "scores min/max : -2.475425188601598e-08 -3.224277202556867e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.563e-20\n",
      "‖w_svm‖₂       : 0.00048215706473061083\n",
      "‖alpha‖₁       : 0.43999999999999356\n",
      "scores min/max : -4.394428636116046e-05 0.00018595809960905637\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6552  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.583e-16\n",
      "‖w_svm‖₂       : 0.019216142963226445\n",
      "‖alpha‖₁       : 0.6814747393117956\n",
      "scores min/max : -1.9936359980595597 0.04427832934131118\n",
      "Mask mean value:  tensor(0.5101, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1880  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.569e-14\n",
      "‖w_svm‖₂       : 0.06917241792857186\n",
      "‖alpha‖₁       : 0.578922410770394\n",
      "scores min/max : -3.311374659354692 1.8648416381758108\n",
      "Mask mean value:  tensor(0.1335, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2742  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.659e-04\n",
      "‖w_svm‖₂       : 4.0042172878997676e-07\n",
      "‖alpha‖₁       : 0.27999999999999714\n",
      "scores min/max : -1.1319948141416751e-07 -5.1492057962026237e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.874e-19\n",
      "‖w_svm‖₂       : 0.022492055038791543\n",
      "‖alpha‖₁       : 0.8520935937433247\n",
      "scores min/max : -3.0368878157528068 1.4449634077296998\n",
      "Mask mean value:  tensor(0.1183, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8359  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.284e-03\n",
      "‖w_svm‖₂       : 8.217775598238689e-08\n",
      "‖alpha‖₁       : 0.3799999999999979\n",
      "scores min/max : 6.648325461678099e-09 2.480570187061887e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.473e-22\n",
      "‖w_svm‖₂       : 5.7994754558946565e-08\n",
      "‖alpha‖₁       : 0.11999999999999049\n",
      "scores min/max : -1.1831440621891964e-08 1.8970041639964805e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.397e-08\n",
      "‖w_svm‖₂       : 0.004032991406881697\n",
      "‖alpha‖₁       : 0.45999999999999885\n",
      "scores min/max : -0.002003783531345216 -0.00032791557170173784\n",
      "Mask mean value:  tensor(0.4964, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.1579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.705e-16\n",
      "‖w_svm‖₂       : 2.119117340386903e-07\n",
      "‖alpha‖₁       : 0.4199999999999686\n",
      "scores min/max : -4.9791015449452504e-08 -2.7440815731936203e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.467e-19\n",
      "‖w_svm‖₂       : 4.28674846540136e-07\n",
      "‖alpha‖₁       : 0.7199999999999998\n",
      "scores min/max : -8.745527747702181e-08 -3.023087359302547e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.558e-21\n",
      "‖w_svm‖₂       : 0.041039726954600064\n",
      "‖alpha‖₁       : 0.8895128116690676\n",
      "scores min/max : -2.0238115032954243 0.32330674599415776\n",
      "Mask mean value:  tensor(0.2267, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0638  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.502e-03\n",
      "‖w_svm‖₂       : 0.004616315816296862\n",
      "‖alpha‖₁       : 0.7005552263463972\n",
      "scores min/max : -2.0072064113614267 0.016170595471516017\n",
      "Mask mean value:  tensor(0.4392, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2468  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.767e-05\n",
      "‖w_svm‖₂       : 0.027741191369230242\n",
      "‖alpha‖₁       : 0.550929743864473\n",
      "scores min/max : -3.603834981585907 0.9710794511218865\n",
      "Mask mean value:  tensor(0.0972, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8244  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.760e-03\n",
      "‖w_svm‖₂       : 0.05673197393465054\n",
      "‖alpha‖₁       : 0.9054887736888215\n",
      "scores min/max : -1.1589012821419937 1.7556612165964482\n",
      "Mask mean value:  tensor(0.1638, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3057  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.405e-03\n",
      "‖w_svm‖₂       : 0.00018245741689966992\n",
      "‖alpha‖₁       : 0.8199999999999954\n",
      "scores min/max : -7.125384881959159e-06 6.884859454002962e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6979  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.132e-17\n",
      "‖w_svm‖₂       : 0.059724306806345474\n",
      "‖alpha‖₁       : 0.7293222754540634\n",
      "scores min/max : -0.23048845070161175 2.0491503918336447\n",
      "Mask mean value:  tensor(0.7680, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.4222  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 7.402e-04\n",
      "‖w_svm‖₂       : 1.4247486603127592e-07\n",
      "‖alpha‖₁       : 0.37999999999999245\n",
      "scores min/max : -3.378969688591705e-08 -1.8782184554489414e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.810e-07\n",
      "‖w_svm‖₂       : 0.02279470546379619\n",
      "‖alpha‖₁       : 0.3833138752000692\n",
      "scores min/max : -1.9782800868531853 0.21750385637457176\n",
      "Mask mean value:  tensor(0.6382, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1314  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.040e-13\n",
      "‖w_svm‖₂       : 0.005892302359295479\n",
      "‖alpha‖₁       : 0.60765807493722\n",
      "scores min/max : -1.980281551271199 0.3067391074665829\n",
      "Mask mean value:  tensor(0.6001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2426  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.363e-15\n",
      "‖w_svm‖₂       : 1.1587120995373011e-07\n",
      "‖alpha‖₁       : 0.29999999999999866\n",
      "scores min/max : 7.544917359364636e-09 1.644183321909989e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.142e-07\n",
      "‖w_svm‖₂       : 0.01879543680286536\n",
      "‖alpha‖₁       : 0.78\n",
      "scores min/max : -0.0028919204772704446 0.06954384207294413\n",
      "Mask mean value:  tensor(0.5167, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.2425  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.884e-15\n",
      "‖w_svm‖₂       : 0.004832570715411773\n",
      "‖alpha‖₁       : 0.37999999999999845\n",
      "scores min/max : -0.007886597226616027 0.002508292319821649\n",
      "Mask mean value:  tensor(0.4990, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8890  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.442e-14\n",
      "‖w_svm‖₂       : 2.4641176915437155e-07\n",
      "‖alpha‖₁       : 0.6399999999999807\n",
      "scores min/max : -1.6850634429861034e-08 4.051642500214348e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.138e-20\n",
      "‖w_svm‖₂       : 2.9189406706168193e-07\n",
      "‖alpha‖₁       : 0.6599999999999999\n",
      "scores min/max : -7.962772683839208e-08 7.201004305547395e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.172e-18\n",
      "‖w_svm‖₂       : 5.631567504115809e-08\n",
      "‖alpha‖₁       : 0.23999999999997818\n",
      "scores min/max : -7.954606673654782e-09 6.596605524587131e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.033e-20\n",
      "‖w_svm‖₂       : 7.423674395809101e-08\n",
      "‖alpha‖₁       : 0.13999999999999305\n",
      "scores min/max : -6.27040100309461e-09 -4.6296425955370936e-10\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.894e-08\n",
      "‖w_svm‖₂       : 0.0773775032106171\n",
      "‖alpha‖₁       : 0.41743345041394075\n",
      "scores min/max : -1.6580172437789442 3.6595945573163724\n",
      "Mask mean value:  tensor(0.8835, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8904  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.651e-11\n",
      "‖w_svm‖₂       : 0.12135661399526944\n",
      "‖alpha‖₁       : 0.7525892248014934\n",
      "scores min/max : -1.708604932447712 2.275617904431113\n",
      "Mask mean value:  tensor(0.9140, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3470  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.179e-12\n",
      "‖w_svm‖₂       : 0.02378527865317218\n",
      "‖alpha‖₁       : 0.823307562542702\n",
      "scores min/max : -1.8648979878513148 1.537774440876204\n",
      "Mask mean value:  tensor(0.7833, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.5955  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.071e-05\n",
      "‖w_svm‖₂       : 0.05896000505095565\n",
      "‖alpha‖₁       : 0.9199999999999974\n",
      "scores min/max : -0.42228257488259213 0.9803588061798161\n",
      "Mask mean value:  tensor(0.6176, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1193  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 3.184e-02\n",
      "‖w_svm‖₂       : 4.589618498691343e-06\n",
      "‖alpha‖₁       : 0.41999999998675774\n",
      "scores min/max : -1.2664626669992667e-06 6.575978111342124e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1230  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.956e-05\n",
      "‖w_svm‖₂       : 0.0448455014393324\n",
      "‖alpha‖₁       : 0.9401032015616815\n",
      "scores min/max : -1.8193440177972076 0.31845703938412984\n",
      "Mask mean value:  tensor(0.4675, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3559  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.106e-10\n",
      "‖w_svm‖₂       : 0.06182315484833645\n",
      "‖alpha‖₁       : 0.6599999999999998\n",
      "scores min/max : -0.5180093566451348 0.3939773556121608\n",
      "Mask mean value:  tensor(0.5202, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8697  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.723e-14\n",
      "‖w_svm‖₂       : 1.9132231259762753e-07\n",
      "‖alpha‖₁       : 0.3799999999999994\n",
      "scores min/max : -1.2768861986110545e-08 2.2917004889582702e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.402e-19\n",
      "‖w_svm‖₂       : 0.00030320577476299803\n",
      "‖alpha‖₁       : 0.41999999999999965\n",
      "scores min/max : -0.0003180317533265428 -6.511513130472106e-06\n",
      "Mask mean value:  tensor(0.4998, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9952  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.521e-15\n",
      "‖w_svm‖₂       : 0.00020540423589655445\n",
      "‖alpha‖₁       : 0.6199999999914445\n",
      "scores min/max : -7.038948898466186e-06 7.919182278759852e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.607e-17\n",
      "‖w_svm‖₂       : 0.0001292372233484795\n",
      "‖alpha‖₁       : 0.4399999999998548\n",
      "scores min/max : -8.00672970451816e-05 4.165372389556123e-05\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0288  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.423e-17\n",
      "‖w_svm‖₂       : 2.4194757199018618e-08\n",
      "‖alpha‖₁       : 0.11999999999999669\n",
      "scores min/max : -2.9703016173792215e-09 1.1571936110220565e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.961e-09\n",
      "‖w_svm‖₂       : 0.0008126709588442865\n",
      "‖alpha‖₁       : 0.8199999999999993\n",
      "scores min/max : -0.0011153588840163856 0.0004006680017704799\n",
      "Mask mean value:  tensor(0.4992, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5360  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.659e-17\n",
      "‖w_svm‖₂       : 0.016980104108188284\n",
      "‖alpha‖₁       : 0.8599999999999998\n",
      "scores min/max : -0.02114914636324067 -0.0015462838605899767\n",
      "Mask mean value:  tensor(0.4644, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1139  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.679e-15\n",
      "‖w_svm‖₂       : 1.7867233071767003e-07\n",
      "‖alpha‖₁       : 0.23999999999998967\n",
      "scores min/max : -6.9771684339891865e-09 8.442712487617312e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.454e-20\n",
      "‖w_svm‖₂       : 0.05539070237205518\n",
      "‖alpha‖₁       : 0.5754089091522248\n",
      "scores min/max : -1.9491770233228582 0.8733302241030101\n",
      "Mask mean value:  tensor(0.7225, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9761  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.929e-04\n",
      "‖w_svm‖₂       : 1.112929483583467e-06\n",
      "‖alpha‖₁       : 0.31999999999976086\n",
      "scores min/max : -1.4175938185645703e-07 -3.446766216898859e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4899  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.413e-18\n",
      "‖w_svm‖₂       : 1.1589189245653966e-07\n",
      "‖alpha‖₁       : 0.5199999999999989\n",
      "scores min/max : -1.720989612152392e-08 4.192085565441734e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.072e-19\n",
      "‖w_svm‖₂       : 0.00034528469094955805\n",
      "‖alpha‖₁       : 0.739999999999924\n",
      "scores min/max : -3.3466073445641476e-05 -1.8258778708933958e-05\n",
      "Mask mean value:  tensor(0.4999, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0641  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.948e-17\n",
      "‖w_svm‖₂       : 4.780595236356344e-08\n",
      "‖alpha‖₁       : 0.17999999999999872\n",
      "scores min/max : 2.6673829413728723e-09 1.7284472984516707e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 4.396e-08\n",
      "‖w_svm‖₂       : 0.02517889297204941\n",
      "‖alpha‖₁       : 0.1964375458746532\n",
      "scores min/max : -2.091295713267889 0.13532366503898277\n",
      "Mask mean value:  tensor(0.3048, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9697  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.904e-09\n",
      "‖w_svm‖₂       : 0.005373651076628397\n",
      "‖alpha‖₁       : 0.5599999999999943\n",
      "scores min/max : 0.0005013512183483612 0.001993230078167922\n",
      "Mask mean value:  tensor(0.5039, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0561  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 2.723e-04\n",
      "‖w_svm‖₂       : 8.402609954191702e-08\n",
      "‖alpha‖₁       : 0.6599999999999949\n",
      "scores min/max : -1.3734057202052914e-08 1.3102117117072127e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.385e-09\n",
      "‖w_svm‖₂       : 0.06875833064227625\n",
      "‖alpha‖₁       : 0.46974898774639695\n",
      "scores min/max : -1.939869659981384 3.142076678943733\n",
      "Mask mean value:  tensor(0.7909, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 7.4348  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.438e-09\n",
      "‖w_svm‖₂       : 0.05190180126623534\n",
      "‖alpha‖₁       : 0.8973052887491244\n",
      "scores min/max : -1.779380656276226 3.6204397988766948\n",
      "Mask mean value:  tensor(0.2851, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4542  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.212e-03\n",
      "‖w_svm‖₂       : 2.3495395603866382e-07\n",
      "‖alpha‖₁       : 0.2599999999999889\n",
      "scores min/max : 4.7170409032608935e-08 5.999951472714151e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.880e-17\n",
      "‖w_svm‖₂       : 0.00015565971130433384\n",
      "‖alpha‖₁       : 0.6399999999999703\n",
      "scores min/max : -3.0439776589512712e-05 -2.119302886226769e-05\n",
      "Mask mean value:  tensor(0.4999, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7642  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.035e-17\n",
      "‖w_svm‖₂       : 2.392805083501026e-07\n",
      "‖alpha‖₁       : 0.5799999999999912\n",
      "scores min/max : 1.54548086744139e-08 3.112264772844989e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.578e-19\n",
      "‖w_svm‖₂       : 7.078324570566203e-08\n",
      "‖alpha‖₁       : 0.42\n",
      "scores min/max : -4.782310832844497e-08 -6.983648682181736e-10\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 4.946e-08\n",
      "‖w_svm‖₂       : 0.16900071987381576\n",
      "‖alpha‖₁       : 0.8752517237450403\n",
      "scores min/max : -3.340448445384366 6.299372359436649\n",
      "Mask mean value:  tensor(0.1073, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1801  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.279e-04\n",
      "‖w_svm‖₂       : 6.831026104155816e-07\n",
      "‖alpha‖₁       : 0.39999999999999974\n",
      "scores min/max : -1.1702316872446101e-07 1.8918965683421775e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.145e-09\n",
      "‖w_svm‖₂       : 0.0029488336123321864\n",
      "‖alpha‖₁       : 0.5799999999999998\n",
      "scores min/max : -0.0005917272698370527 0.0005233453725269691\n",
      "Mask mean value:  tensor(0.5015, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3189  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.216e-05\n",
      "‖w_svm‖₂       : 0.046636604986174446\n",
      "‖alpha‖₁       : 0.9388029473239233\n",
      "scores min/max : -2.4727373801165107 1.5618449690865992\n",
      "Mask mean value:  tensor(0.1333, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3615  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.200e-03\n",
      "‖w_svm‖₂       : 0.023660948358562608\n",
      "‖alpha‖₁       : 0.8599999999999843\n",
      "scores min/max : -0.05084610797576088 0.06950828478117412\n",
      "Mask mean value:  tensor(0.4881, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6501  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.939e-14\n",
      "‖w_svm‖₂       : 0.03167039299022138\n",
      "‖alpha‖₁       : 0.8982990082701381\n",
      "scores min/max : -0.8269302622531909 1.9357671397687088\n",
      "Mask mean value:  tensor(0.6941, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5277  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.295e-06\n",
      "‖w_svm‖₂       : 0.11693109188613744\n",
      "‖alpha‖₁       : 0.6489786395676501\n",
      "scores min/max : -17.736471779650376 2.05530216090004\n",
      "Mask mean value:  tensor(0.6630, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1251  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.798e-07\n",
      "‖w_svm‖₂       : 5.999806361237299e-08\n",
      "‖alpha‖₁       : 0.439999999999997\n",
      "scores min/max : -1.1220071591483173e-07 -8.102399376815352e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.361e-20\n",
      "‖w_svm‖₂       : 0.020435149334982416\n",
      "‖alpha‖₁       : 0.5962263466655836\n",
      "scores min/max : -2.9424409098611197 2.1817350659334482\n",
      "Mask mean value:  tensor(0.8453, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3285  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.808e-02\n",
      "‖w_svm‖₂       : 1.1316968572944785e-06\n",
      "‖alpha‖₁       : 0.4999999999999849\n",
      "scores min/max : -9.019891088305452e-08 1.0283482639916908e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.165e-19\n",
      "‖w_svm‖₂       : 0.13788513758390286\n",
      "‖alpha‖₁       : 0.8728871092303279\n",
      "scores min/max : -12.222221440337846 2.098737000911586\n",
      "Mask mean value:  tensor(0.4535, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3402  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 7.008e-02\n",
      "‖w_svm‖₂       : 0.025687961933710066\n",
      "‖alpha‖₁       : 0.8150790632316055\n",
      "scores min/max : -11.507047619879833 2.0416863522131083\n",
      "Mask mean value:  tensor(0.7341, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9469  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.080e-08\n",
      "‖w_svm‖₂       : 9.592039288321223e-08\n",
      "‖alpha‖₁       : 0.17999999999999344\n",
      "scores min/max : -1.6655315598012087e-09 2.3336205701470652e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.994e-21\n",
      "‖w_svm‖₂       : 0.08249943782311414\n",
      "‖alpha‖₁       : 0.6575074887211726\n",
      "scores min/max : -1.9386452188676262 3.7467430845849026\n",
      "Mask mean value:  tensor(0.5660, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.6029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.856e-06\n",
      "‖w_svm‖₂       : 1.0621751320154816e-06\n",
      "‖alpha‖₁       : 0.5999999999999714\n",
      "scores min/max : -7.411623281919669e-08 -2.3147703933516346e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.484e-19\n",
      "‖w_svm‖₂       : 1.1150669607569014e-07\n",
      "‖alpha‖₁       : 0.23999999999997473\n",
      "scores min/max : 1.1900564783070606e-08 2.2916347694737118e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.609e-21\n",
      "‖w_svm‖₂       : 1.545710759398151e-05\n",
      "‖alpha‖₁       : 0.35999999999291127\n",
      "scores min/max : 3.763021578852377e-07 1.2713662439691275e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3078  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.758e-17\n",
      "‖w_svm‖₂       : 0.07380880224600313\n",
      "‖alpha‖₁       : 0.575755261844567\n",
      "scores min/max : -1.9016337104598364 6.049898972921299\n",
      "Mask mean value:  tensor(0.8403, dtype=torch.float64)\n",
      "max feasible return = 0.4569  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.177119253415682e-07\n",
      "‖alpha‖₁       : 0.58\n",
      "scores min/max : -6.004136363806011e-08 3.0677518463329885e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0599960276710912e-07\n",
      "‖alpha‖₁       : 0.29999999999999977\n",
      "scores min/max : -4.2962891349643095e-09 6.670228660116939e-10\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 7.710973395878616e-08\n",
      "‖alpha‖₁       : 0.5999999999999998\n",
      "scores min/max : -3.2264246877614856e-08 4.464028546849687e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 5.094694168101997e-08\n",
      "‖alpha‖₁       : 0.3799999999999991\n",
      "scores min/max : -9.642271989923281e-09 -2.614656287287844e-10\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.065253770674124e-06\n",
      "‖alpha‖₁       : 0.31999999997788764\n",
      "scores min/max : -1.3788493195972892e-06 2.9278378921979577e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4257  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.06898986800752727\n",
      "‖alpha‖₁       : 0.7396235806549267\n",
      "scores min/max : -3.4871522982014835 2.119418300125687\n",
      "Mask mean value:  tensor(0.8501, dtype=torch.float64)\n",
      "max feasible return = -0.8968  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.0325074237822411\n",
      "‖alpha‖₁       : 0.6148802623079964\n",
      "scores min/max : -1.8824507020754802 1.2477392736441273\n",
      "Mask mean value:  tensor(0.8445, dtype=torch.float64)\n",
      "max feasible return = 2.9778  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.937666715208711e-07\n",
      "‖alpha‖₁       : 0.45999999999950514\n",
      "scores min/max : -1.3813222421981424e-07 4.6799065585068916e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.3736713063715717e-07\n",
      "‖alpha‖₁       : 0.5199999999999947\n",
      "scores min/max : 1.148880715337741e-09 1.4043656121100914e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.61396782317198e-07\n",
      "‖alpha‖₁       : 0.5799999999999987\n",
      "scores min/max : -1.7880238261070445e-08 1.8261366209492276e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.040342624305031397\n",
      "‖alpha‖₁       : 0.6871215462502864\n",
      "scores min/max : -0.3382356827739512 2.054631222324714\n",
      "Mask mean value:  tensor(0.7612, dtype=torch.float64)\n",
      "max feasible return = -0.2519  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03627661455816913\n",
      "‖alpha‖₁       : 0.4220641204203933\n",
      "scores min/max : -3.939936324234323 5.142470859776018\n",
      "Mask mean value:  tensor(0.0635, dtype=torch.float64)\n",
      "max feasible return = 0.4019  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04986739389255482\n",
      "‖alpha‖₁       : 0.8211189842818283\n",
      "scores min/max : -5.084157675395643 3.070282328794424\n",
      "Mask mean value:  tensor(0.9423, dtype=torch.float64)\n",
      "max feasible return = -3.5610  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.010919474344510138\n",
      "‖alpha‖₁       : 0.7999999999999985\n",
      "scores min/max : -0.06185905327766833 0.015421136251580198\n",
      "Mask mean value:  tensor(0.4797, dtype=torch.float64)\n",
      "max feasible return = -0.1745  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.240808273900993e-07\n",
      "‖alpha‖₁       : 0.6199999999999843\n",
      "scores min/max : -3.511181536693432e-08 -1.8657320306388495e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.697779045840423e-08\n",
      "‖alpha‖₁       : 0.4399999999999612\n",
      "scores min/max : -2.385154719848079e-09 7.19749905348593e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.3643863354276101e-07\n",
      "‖alpha‖₁       : 0.27999999999996744\n",
      "scores min/max : -3.0508280339728538e-09 1.4939307781150752e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch   2 | train 0.005615 | val 0.006825\n",
      "-----------------------------------------Epoch:  3 ----------------------------------------\n",
      "‖w_svm‖₂       : 0.03001387548526047\n",
      "‖alpha‖₁       : 0.6599999999999939\n",
      "scores min/max : -0.058729927764030895 0.11125235623866934\n",
      "Mask mean value:  tensor(0.4929, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.9389  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.191e-15\n",
      "‖w_svm‖₂       : 0.02304456111160896\n",
      "‖alpha‖₁       : 0.3833252547730889\n",
      "scores min/max : -1.977895315130536 0.21787862996544555\n",
      "Mask mean value:  tensor(0.6398, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1316  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.045e-13\n",
      "‖w_svm‖₂       : 0.03158234475018392\n",
      "‖alpha‖₁       : 0.8982988664611712\n",
      "scores min/max : -0.8260133813951028 1.936584458945033\n",
      "Mask mean value:  tensor(0.6962, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5267  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.271e-06\n",
      "‖w_svm‖₂       : 0.0029654576039132457\n",
      "‖alpha‖₁       : 0.5799999999999998\n",
      "scores min/max : -0.00064007150521837 0.00048806514744075685\n",
      "Mask mean value:  tensor(0.5013, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3187  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.317e-05\n",
      "‖w_svm‖₂       : 4.5747202713199616e-06\n",
      "‖alpha‖₁       : 0.41999999998699816\n",
      "scores min/max : -1.0398223270931973e-06 8.754659153960701e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1230  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.959e-05\n",
      "‖w_svm‖₂       : 0.06154560769938443\n",
      "‖alpha‖₁       : 0.6599999999999999\n",
      "scores min/max : -0.51329597023052 0.3902790288989899\n",
      "Mask mean value:  tensor(0.5198, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8702  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.761e-14\n",
      "‖w_svm‖₂       : 8.427972918913275e-08\n",
      "‖alpha‖₁       : 0.6599999999999948\n",
      "scores min/max : -6.675367817462693e-09 1.380663762684906e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.404e-09\n",
      "‖w_svm‖₂       : 0.04891954231429164\n",
      "‖alpha‖₁       : 0.8263578687157859\n",
      "scores min/max : -1.9843265258227265 0.3764194389721281\n",
      "Mask mean value:  tensor(0.3986, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0249  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.558e-02\n",
      "‖w_svm‖₂       : 0.01704231488290734\n",
      "‖alpha‖₁       : 0.8599999999999997\n",
      "scores min/max : -0.024220721192630894 -0.004476246757101822\n",
      "Mask mean value:  tensor(0.4497, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1090  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.172e-15\n",
      "‖w_svm‖₂       : 4.787249336893801e-08\n",
      "‖alpha‖₁       : 0.17999999999999908\n",
      "scores min/max : 7.470736326338789e-09 2.209606941215289e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 4.453e-08\n",
      "‖w_svm‖₂       : 0.00015532918274528034\n",
      "‖alpha‖₁       : 0.6399999999999919\n",
      "scores min/max : -3.3647147242324824e-05 -2.443954876034832e-05\n",
      "Mask mean value:  tensor(0.4999, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7642  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.043e-17\n",
      "‖w_svm‖₂       : 0.163467421549891\n",
      "‖alpha‖₁       : 0.8788686973983502\n",
      "scores min/max : -2.0767486444713894 5.087960526648802\n",
      "Mask mean value:  tensor(0.1298, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.658e-03\n",
      "‖w_svm‖₂       : 0.04132521440598973\n",
      "‖alpha‖₁       : 0.8895338145693236\n",
      "scores min/max : -2.020513240447046 0.32654233409122757\n",
      "Mask mean value:  tensor(0.2354, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0680  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.014e-03\n",
      "‖w_svm‖₂       : 5.6286144863614806e-08\n",
      "‖alpha‖₁       : 0.2399999999999769\n",
      "scores min/max : -8.430707092181989e-09 6.128083531456963e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.035e-20\n",
      "‖w_svm‖₂       : 0.046612021724133305\n",
      "‖alpha‖₁       : 0.9388038447151068\n",
      "scores min/max : -2.471975406619473 1.5628030080595723\n",
      "Mask mean value:  tensor(0.1338, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3630  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.228e-03\n",
      "‖w_svm‖₂       : 2.3882844679796994e-07\n",
      "‖alpha‖₁       : 0.5799999999999982\n",
      "scores min/max : 1.3405891750637934e-09 1.697177510811876e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.584e-19\n",
      "‖w_svm‖₂       : 1.1103566629752497e-06\n",
      "‖alpha‖₁       : 0.319999999999841\n",
      "scores min/max : -2.0239648882588246e-07 -9.560343477610945e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4899  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.414e-18\n",
      "‖w_svm‖₂       : 5.807412280326759e-08\n",
      "‖alpha‖₁       : 0.11999999999999064\n",
      "scores min/max : -1.617068883369967e-08 1.4634704994346254e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.594e-08\n",
      "‖w_svm‖₂       : 0.00012895176742673138\n",
      "‖alpha‖₁       : 0.4399999999998488\n",
      "scores min/max : -8.009055590374112e-05 4.1096246367021416e-05\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0288  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.428e-17\n",
      "‖w_svm‖₂       : 0.0008157502975008787\n",
      "‖alpha‖₁       : 0.8199999999999992\n",
      "scores min/max : -0.0008697740763679105 0.0006637032684382155\n",
      "Mask mean value:  tensor(0.5005, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5425  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.658e-17\n",
      "‖w_svm‖₂       : 0.005901258870566382\n",
      "‖alpha‖₁       : 0.6076581783857292\n",
      "scores min/max : -1.9802298535256504 0.3067908348924331\n",
      "Mask mean value:  tensor(0.6003, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2427  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.382e-15\n",
      "‖w_svm‖₂       : 0.022848953421265993\n",
      "‖alpha‖₁       : 0.8521058598267943\n",
      "scores min/max : -3.030318871524705 1.4512825292994267\n",
      "Mask mean value:  tensor(0.1196, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8454  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.170e-03\n",
      "‖w_svm‖₂       : 1.1613836918382185e-07\n",
      "‖alpha‖₁       : 0.29999999999999843\n",
      "scores min/max : 1.1304147282220943e-08 2.0201907697098657e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.145e-07\n",
      "‖w_svm‖₂       : 4.2945304621570415e-07\n",
      "‖alpha‖₁       : 0.7199999999999996\n",
      "scores min/max : -7.101493829839128e-08 -1.3809437541147948e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.592e-21\n",
      "‖w_svm‖₂       : 0.000484634694921927\n",
      "‖alpha‖₁       : 0.43999999999999806\n",
      "scores min/max : -0.00011245826115161028 0.00011981779129620497\n",
      "Mask mean value:  tensor(0.4996, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6541  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.614e-16\n",
      "‖w_svm‖₂       : 0.06923339493262132\n",
      "‖alpha‖₁       : 0.4698148961062494\n",
      "scores min/max : -1.9546191185780228 3.1276515058416714\n",
      "Mask mean value:  tensor(0.7569, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 7.1227  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.812e-09\n",
      "‖w_svm‖₂       : 0.004865803461064958\n",
      "‖alpha‖₁       : 0.37999999999999784\n",
      "scores min/max : -0.0077747666803233 0.0027652245658508975\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8908  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.458e-14\n",
      "‖w_svm‖₂       : 7.052076679174875e-08\n",
      "‖alpha‖₁       : 0.42000000000000004\n",
      "scores min/max : -5.0550030437817105e-08 -3.464206382247106e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 4.961e-08\n",
      "‖w_svm‖₂       : 0.025663701574558887\n",
      "‖alpha‖₁       : 0.8150794377918351\n",
      "scores min/max : -11.510306372788406 2.0389707545968565\n",
      "Mask mean value:  tensor(0.7284, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.855e-06\n",
      "‖w_svm‖₂       : 0.13731575990360628\n",
      "‖alpha‖₁       : 0.8728933379348953\n",
      "scores min/max : -12.23727634681837 2.0860782959256547\n",
      "Mask mean value:  tensor(0.4220, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2538  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 2.991e-02\n",
      "‖w_svm‖₂       : 0.057843913026777286\n",
      "‖alpha‖₁       : 0.9056274720659979\n",
      "scores min/max : -1.1551820399136208 1.7595470774340396\n",
      "Mask mean value:  tensor(0.1700, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3174  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.283e-03\n",
      "‖w_svm‖₂       : 0.023782230761715914\n",
      "‖alpha‖₁       : 0.823310066214285\n",
      "scores min/max : -1.8661166805116371 1.536422372614149\n",
      "Mask mean value:  tensor(0.7808, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.5889  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.057e-05\n",
      "‖w_svm‖₂       : 0.07764046007636208\n",
      "‖alpha‖₁       : 0.41751097272480764\n",
      "scores min/max : -1.6639989017804417 3.6564287374326487\n",
      "Mask mean value:  tensor(0.8817, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8862  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.359e-10\n",
      "‖w_svm‖₂       : 0.018892643703938464\n",
      "‖alpha‖₁       : 0.7799999999999997\n",
      "scores min/max : -0.0021918485603676704 0.07095614514616763\n",
      "Mask mean value:  tensor(0.5205, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.2586  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.871e-15\n",
      "‖w_svm‖₂       : 8.206768475826221e-08\n",
      "‖alpha‖₁       : 0.5399999999999523\n",
      "scores min/max : -2.8010865567240374e-08 -6.398532242424773e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.588e-20\n",
      "‖w_svm‖₂       : 0.059273412031440764\n",
      "‖alpha‖₁       : 0.7293256281400471\n",
      "scores min/max : -0.23048231424852442 2.049372764130901\n",
      "Mask mean value:  tensor(0.7680, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.4235  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 7.461e-04\n",
      "‖w_svm‖₂       : 1.0670077866219786e-06\n",
      "‖alpha‖₁       : 0.5999999999999394\n",
      "scores min/max : -7.138531586514997e-08 -2.026648196351512e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.498e-19\n",
      "‖w_svm‖₂       : 0.02345671439329025\n",
      "‖alpha‖₁       : 0.8599999999999857\n",
      "scores min/max : -0.04969766119973915 0.06858785321229413\n",
      "Mask mean value:  tensor(0.4897, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6507  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.271e-15\n",
      "‖w_svm‖₂       : 2.363222564247388e-07\n",
      "‖alpha‖₁       : 0.259999999999989\n",
      "scores min/max : 3.8927423392853154e-08 5.175659029176374e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.959e-17\n",
      "‖w_svm‖₂       : 0.01938471586776396\n",
      "‖alpha‖₁       : 0.6814805719922141\n",
      "scores min/max : -1.9926565022147809 0.0452762410901051\n",
      "Mask mean value:  tensor(0.5148, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1902  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.927e-13\n",
      "‖w_svm‖₂       : 0.004655227479484388\n",
      "‖alpha‖₁       : 0.700555583478868\n",
      "scores min/max : -2.0073059357635974 0.01606939904389261\n",
      "Mask mean value:  tensor(0.4387, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2466  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.902e-05\n",
      "‖w_svm‖₂       : 0.11787846713380734\n",
      "‖alpha‖₁       : 0.64921093176985\n",
      "scores min/max : -17.760184510215204 2.033260520066217\n",
      "Mask mean value:  tensor(0.6170, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1197  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.935e-05\n",
      "‖w_svm‖₂       : 0.0002038813548942311\n",
      "‖alpha‖₁       : 0.6199999999941973\n",
      "scores min/max : -2.406862852083149e-06 1.2329851523075359e-05\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.619e-17\n",
      "‖w_svm‖₂       : 0.00018096772824442104\n",
      "‖alpha‖₁       : 0.8200000000000001\n",
      "scores min/max : -5.238905033621362e-06 8.53186358162192e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6979  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.127e-17\n",
      "‖w_svm‖₂       : 1.8011990220027969e-07\n",
      "‖alpha‖₁       : 0.2399999999999865\n",
      "scores min/max : 1.2985698251882873e-08 2.8460491397765573e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.523e-20\n",
      "‖w_svm‖₂       : 1.1675137082598576e-07\n",
      "‖alpha‖₁       : 0.5199999999999989\n",
      "scores min/max : 1.4626455545769325e-09 2.2863325697726574e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.112e-19\n",
      "‖w_svm‖₂       : 0.08223401795366773\n",
      "‖alpha‖₁       : 0.6575394651412849\n",
      "scores min/max : -1.9415031747436544 3.763361737136306\n",
      "Mask mean value:  tensor(0.5590, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.5619  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.591e-06\n",
      "‖w_svm‖₂       : 0.02786497890662537\n",
      "‖alpha‖₁       : 0.5509351253927245\n",
      "scores min/max : -3.603025501319471 0.9719724444401012\n",
      "Mask mean value:  tensor(0.0975, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8275  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.809e-03\n",
      "‖w_svm‖₂       : 7.496419148606647e-08\n",
      "‖alpha‖₁       : 0.13999999999997864\n",
      "scores min/max : -6.720819990835163e-09 -8.382559676981698e-10\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.982e-08\n",
      "‖w_svm‖₂       : 1.0737458432266364e-07\n",
      "‖alpha‖₁       : 0.23999999999999966\n",
      "scores min/max : 1.1330560911233283e-08 2.205828596833134e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.454e-21\n",
      "‖w_svm‖₂       : 0.0003005137426646529\n",
      "‖alpha‖₁       : 0.4199999999997279\n",
      "scores min/max : -0.0002873367625649064 1.8673252660705375e-05\n",
      "Mask mean value:  tensor(0.4999, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9954  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.524e-15\n",
      "‖w_svm‖₂       : 6.803247847723488e-07\n",
      "‖alpha‖₁       : 0.39999999999999997\n",
      "scores min/max : -1.2315801150087492e-07 1.8291275194855637e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.110e-09\n",
      "‖w_svm‖₂       : 2.395642040866119e-08\n",
      "‖alpha‖₁       : 0.11999999999999712\n",
      "scores min/max : -4.229513495395753e-09 1.0292423191879925e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.948e-09\n",
      "‖w_svm‖₂       : 5.990647446387097e-08\n",
      "‖alpha‖₁       : 0.43999999999999856\n",
      "scores min/max : -1.1054173878630271e-07 -6.381782503072111e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.361e-20\n",
      "‖w_svm‖₂       : 0.005409104619452549\n",
      "‖alpha‖₁       : 0.5599999999999821\n",
      "scores min/max : 0.0008471245767329681 0.002359392788897901\n",
      "Mask mean value:  tensor(0.5056, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0598  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 2.482e-04\n",
      "‖w_svm‖₂       : 4.0330909491089354e-07\n",
      "‖alpha‖₁       : 0.2799999999999957\n",
      "scores min/max : -2.040383131736528e-07 -1.4229145279954533e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.951e-19\n",
      "‖w_svm‖₂       : 0.05822427523194602\n",
      "‖alpha‖₁       : 0.9199999999999999\n",
      "scores min/max : -0.4306873078510889 0.8564502022203563\n",
      "Mask mean value:  tensor(0.5972, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.0341  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.452e-03\n",
      "‖w_svm‖₂       : 0.0003425894925850598\n",
      "‖alpha‖₁       : 0.7399999999993636\n",
      "scores min/max : -3.670068573436001e-05 -2.1729830689317857e-05\n",
      "Mask mean value:  tensor(0.4999, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0640  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.939e-17\n",
      "‖w_svm‖₂       : 1.928972831302267e-07\n",
      "‖alpha‖₁       : 0.37999999999999934\n",
      "scores min/max : -9.241459169796285e-09 2.644202300674578e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.447e-19\n",
      "‖w_svm‖₂       : 1.1403612284822672e-06\n",
      "‖alpha‖₁       : 0.4999999999999738\n",
      "scores min/max : -1.1103931241944993e-07 8.219039355785685e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.230e-19\n",
      "‖w_svm‖₂       : 2.486233201604191e-07\n",
      "‖alpha‖₁       : 0.6399999999999679\n",
      "scores min/max : 4.4532757880961154e-10 2.137712876391438e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.252e-20\n",
      "‖w_svm‖₂       : 0.044533608692184694\n",
      "‖alpha‖₁       : 0.9401265740877788\n",
      "scores min/max : -1.8187341367358163 0.31872330709522934\n",
      "Mask mean value:  tensor(0.4693, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3537  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.742e-15\n",
      "‖w_svm‖₂       : 0.07109058528313918\n",
      "‖alpha‖₁       : 0.5791751659013495\n",
      "scores min/max : -3.316783758348093 1.8593595798743063\n",
      "Mask mean value:  tensor(0.1265, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2635  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.650e-04\n",
      "‖w_svm‖₂       : 8.299874585242933e-08\n",
      "‖alpha‖₁       : 0.3799999999999989\n",
      "scores min/max : 9.810346716624484e-09 2.7966079391739254e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.544e-22\n",
      "‖w_svm‖₂       : 2.1331271892251556e-07\n",
      "‖alpha‖₁       : 0.4199999999999702\n",
      "scores min/max : -5.611656305446799e-08 -3.37774494989235e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.492e-19\n",
      "‖w_svm‖₂       : 9.446481022089811e-08\n",
      "‖alpha‖₁       : 0.17999999999999888\n",
      "scores min/max : -7.626656861909138e-10 2.3292158717388902e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.885e-21\n",
      "‖w_svm‖₂       : 0.052396689108520876\n",
      "‖alpha‖₁       : 0.8973656777770093\n",
      "scores min/max : -1.7759055860568096 3.6244607454008317\n",
      "Mask mean value:  tensor(0.2903, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4592  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.635e-03\n",
      "‖w_svm‖₂       : 1.4355715298304245e-07\n",
      "‖alpha‖₁       : 0.37999999999999173\n",
      "scores min/max : -2.6574718504377145e-08 -1.155543500898145e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.493e-07\n",
      "‖w_svm‖₂       : 1.5255872785619885e-05\n",
      "‖alpha‖₁       : 0.35999999999343074\n",
      "scores min/max : 7.720256875650295e-07 1.6489507218155488e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3078  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.709e-17\n",
      "‖w_svm‖₂       : 0.16799562641492868\n",
      "‖alpha‖₁       : 0.8753383950751965\n",
      "scores min/max : -3.5172281398133105 6.2091557858127855\n",
      "Mask mean value:  tensor(0.1072, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1831  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.294e-05\n",
      "‖w_svm‖₂       : 0.004049290933671677\n",
      "‖alpha‖₁       : 0.45999999999999996\n",
      "scores min/max : -0.0023900023773243077 -0.0006956619846627023\n",
      "Mask mean value:  tensor(0.4945, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.1498  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.753e-16\n",
      "‖w_svm‖₂       : 2.946657155536424e-07\n",
      "‖alpha‖₁       : 0.6599999999999963\n",
      "scores min/max : -1.1135272857483221e-07 4.035709534155156e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.190e-18\n",
      "‖w_svm‖₂       : 0.12388541943317213\n",
      "‖alpha‖₁       : 0.753199078798902\n",
      "scores min/max : -1.7056595393037393 2.2767599873246245\n",
      "Mask mean value:  tensor(0.9162, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.994e-12\n",
      "‖w_svm‖₂       : 0.020210856209587492\n",
      "‖alpha‖₁       : 0.5962265450298824\n",
      "scores min/max : -2.943253560036432 2.1863356249406647\n",
      "Mask mean value:  tensor(0.8508, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3460  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.713e-02\n",
      "‖w_svm‖₂       : 0.05540767775867508\n",
      "‖alpha‖₁       : 0.5754559167055768\n",
      "scores min/max : -1.9492028739443414 0.8729398146741649\n",
      "Mask mean value:  tensor(0.7219, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9739  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.929e-04\n",
      "‖w_svm‖₂       : 0.025199318440555247\n",
      "‖alpha‖₁       : 0.1964461013917897\n",
      "scores min/max : -2.0969790591656094 0.12936497768509952\n",
      "Mask mean value:  tensor(0.2838, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.8381  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.641e-09\n",
      "‖w_svm‖₂       : 0.0744759765401093\n",
      "‖alpha‖₁       : 0.5758732962159416\n",
      "scores min/max : -1.9142877069030855 6.035725204570186\n",
      "Mask mean value:  tensor(0.8166, dtype=torch.float64)\n",
      "max feasible return = 0.4312  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.198255485480057e-07\n",
      "‖alpha‖₁       : 0.5800000000000001\n",
      "scores min/max : -6.955119422584007e-08 2.117000936421374e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.1113750298267026e-07\n",
      "‖alpha‖₁       : 0.29999999999996885\n",
      "scores min/max : -4.025718397473162e-09 1.0962525751403471e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 7.696046916076286e-08\n",
      "‖alpha‖₁       : 0.5999999999999996\n",
      "scores min/max : -3.043098594094573e-08 6.299855974307377e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 5.1003747860100376e-08\n",
      "‖alpha‖₁       : 0.379999999999999\n",
      "scores min/max : -8.960653711317658e-09 4.224785080354218e-10\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.14952696354859e-06\n",
      "‖alpha‖₁       : 0.3199999999763857\n",
      "scores min/max : -1.2270843007573464e-06 4.888977800535883e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4257  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.06839627584029152\n",
      "‖alpha‖₁       : 0.7396272393772755\n",
      "scores min/max : -3.4846765503469657 2.1233699319654105\n",
      "Mask mean value:  tensor(0.8542, dtype=torch.float64)\n",
      "max feasible return = -0.9008  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.032811174257901615\n",
      "‖alpha‖₁       : 0.6148995856534254\n",
      "scores min/max : -1.8835930411232602 1.2465732292608074\n",
      "Mask mean value:  tensor(0.8423, dtype=torch.float64)\n",
      "max feasible return = 2.9695  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.972844528706729e-07\n",
      "‖alpha‖₁       : 0.4599999999994928\n",
      "scores min/max : -1.1645201629573715e-07 4.898230198718091e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.3865508376934057e-07\n",
      "‖alpha‖₁       : 0.5199999999999927\n",
      "scores min/max : 1.900447711468359e-09 1.47991147289632e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.6340520697110027e-07\n",
      "‖alpha‖₁       : 0.5799999999999987\n",
      "scores min/max : -2.784138809115935e-08 8.299622015013858e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.04085388640578465\n",
      "‖alpha‖₁       : 0.6871612110073999\n",
      "scores min/max : -0.34359327941562945 2.0492708254224623\n",
      "Mask mean value:  tensor(0.7444, dtype=torch.float64)\n",
      "max feasible return = -0.2408  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03606368764296453\n",
      "‖alpha‖₁       : 0.42206680522601814\n",
      "scores min/max : -3.9399912949230624 5.145270499302003\n",
      "Mask mean value:  tensor(0.0638, dtype=torch.float64)\n",
      "max feasible return = 0.4037  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.049733504464189005\n",
      "‖alpha‖₁       : 0.8211260750941878\n",
      "scores min/max : -5.087625173698458 3.0673042217950792\n",
      "Mask mean value:  tensor(0.9416, dtype=torch.float64)\n",
      "max feasible return = -3.5568  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.010806947014223276\n",
      "‖alpha‖₁       : 0.7999999999999973\n",
      "scores min/max : -0.06060882059551489 0.015075033991652416\n",
      "Mask mean value:  tensor(0.4799, dtype=torch.float64)\n",
      "max feasible return = -0.1746  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.2692837695505835e-07\n",
      "‖alpha‖₁       : 0.6199999999999835\n",
      "scores min/max : -3.0928457661805676e-08 -1.4469510775941597e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.734393380728054e-08\n",
      "‖alpha‖₁       : 0.439999999999961\n",
      "scores min/max : -1.6671067556170604e-09 7.915699736352993e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.3318008181618263e-07\n",
      "‖alpha‖₁       : 0.27999999999999936\n",
      "scores min/max : -2.0936096120701772e-08 -3.3396454914293103e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch   3 | train 0.005602 | val 0.006819\n",
      "-----------------------------------------Epoch:  4 ----------------------------------------\n",
      "‖w_svm‖₂       : 0.1237451692871758\n",
      "‖alpha‖₁       : 0.7531639555360239\n",
      "scores min/max : -1.7058075007314568 2.2766966556252974\n",
      "Mask mean value:  tensor(0.9161, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3445  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.957e-12\n",
      "‖w_svm‖₂       : 0.1649704609207944\n",
      "‖alpha‖₁       : 0.8793863840935845\n",
      "scores min/max : -2.0678937178202035 5.0941199278896905\n",
      "Mask mean value:  tensor(0.1393, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1996  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.687e-03\n",
      "‖w_svm‖₂       : 1.1391684342357806e-06\n",
      "‖alpha‖₁       : 0.3199999999995867\n",
      "scores min/max : -2.40243829315338e-07 -1.314990560312756e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4899  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.461e-18\n",
      "‖w_svm‖₂       : 2.4840384142301464e-07\n",
      "‖alpha‖₁       : 0.6399999999999627\n",
      "scores min/max : 1.2732214297855397e-08 3.3677110384945974e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.238e-20\n",
      "‖w_svm‖₂       : 0.04920617110712181\n",
      "‖alpha‖₁       : 0.8263915182025887\n",
      "scores min/max : -1.9803218812188195 0.3800279652007383\n",
      "Mask mean value:  tensor(0.4125, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0232  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.567e-02\n",
      "‖w_svm‖₂       : 0.055430660377102606\n",
      "‖alpha‖₁       : 0.5754514135159952\n",
      "scores min/max : -1.9478494953358099 0.8743307632010158\n",
      "Mask mean value:  tensor(0.7261, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9859  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.260e-04\n",
      "‖w_svm‖₂       : 0.05815063984237731\n",
      "‖alpha‖₁       : 0.9199999999999999\n",
      "scores min/max : -0.4301599225465914 0.8537448822672133\n",
      "Mask mean value:  tensor(0.5946, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.0201  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.482e-03\n",
      "‖w_svm‖₂       : 0.07120667622834488\n",
      "‖alpha‖₁       : 0.5791938892273826\n",
      "scores min/max : -3.3177613470269653 1.8583924887067191\n",
      "Mask mean value:  tensor(0.1253, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2615  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.628e-04\n",
      "‖w_svm‖₂       : 9.388447385095232e-08\n",
      "‖alpha‖₁       : 0.17999999999999883\n",
      "scores min/max : -8.340537210944539e-10 2.328221344520045e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.825e-21\n",
      "‖w_svm‖₂       : 2.9391904189449484e-07\n",
      "‖alpha‖₁       : 0.6599999999999963\n",
      "scores min/max : -1.400149765168205e-07 1.1707038740775621e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.190e-18\n",
      "‖w_svm‖₂       : 2.4104691879786266e-08\n",
      "‖alpha‖₁       : 0.11999999999999297\n",
      "scores min/max : -4.8200289793852385e-09 9.923460497507286e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.956e-09\n",
      "‖w_svm‖₂       : 7.451525456825271e-08\n",
      "‖alpha‖₁       : 0.1399999999999967\n",
      "scores min/max : -6.1626063613200094e-09 -4.132463469340717e-10\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.942e-08\n",
      "‖w_svm‖₂       : 0.00018082015046233394\n",
      "‖alpha‖₁       : 0.8199999999999975\n",
      "scores min/max : -6.011489706278967e-06 7.72187483064429e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6979  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.127e-17\n",
      "‖w_svm‖₂       : 1.1370907088444593e-06\n",
      "‖alpha‖₁       : 0.4999999999999786\n",
      "scores min/max : -1.2600652438192302e-07 6.713610057041067e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.207e-19\n",
      "‖w_svm‖₂       : 8.178981499553436e-08\n",
      "‖alpha‖₁       : 0.5399999999999999\n",
      "scores min/max : -2.580636647434183e-08 -4.285312104918459e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.574e-20\n",
      "‖w_svm‖₂       : 4.365730076065017e-07\n",
      "‖alpha‖₁       : 0.7199999999998992\n",
      "scores min/max : -2.6168916235364556e-08 3.117418977641884e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.679e-21\n",
      "‖w_svm‖₂       : 0.023370632636504896\n",
      "‖alpha‖₁       : 0.8599999999999848\n",
      "scores min/max : -0.049511939346962845 0.06778818232269301\n",
      "Mask mean value:  tensor(0.4885, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6487  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.351e-15\n",
      "‖w_svm‖₂       : 8.414600300600711e-08\n",
      "‖alpha‖₁       : 0.6599999999999946\n",
      "scores min/max : 2.334232458781619e-09 1.4706813957877428e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.437e-09\n",
      "‖w_svm‖₂       : 2.406902394298476e-07\n",
      "‖alpha‖₁       : 0.5800000000000001\n",
      "scores min/max : -4.2672158962432574e-08 -2.705521217366219e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.633e-19\n",
      "‖w_svm‖₂       : 4.023044506930505e-07\n",
      "‖alpha‖₁       : 0.2799999999999956\n",
      "scores min/max : -2.9383700811485856e-07 -2.3208534244829118e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.970e-19\n",
      "‖w_svm‖₂       : 5.9499522116287376e-08\n",
      "‖alpha‖₁       : 0.43999999999999884\n",
      "scores min/max : -1.0947691443548754e-07 -5.339440127335019e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.339e-20\n",
      "‖w_svm‖₂       : 0.000823536357401461\n",
      "‖alpha‖₁       : 0.8199999999999988\n",
      "scores min/max : -0.0007169130022844682 0.0008440518484972449\n",
      "Mask mean value:  tensor(0.5013, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5468  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.650e-17\n",
      "‖w_svm‖₂       : 1.4279418049083896e-07\n",
      "‖alpha‖₁       : 0.3799999999999916\n",
      "scores min/max : -1.4095350015118261e-08 9.258438521036082e-10\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.743e-07\n",
      "‖w_svm‖₂       : 1.795794137517873e-07\n",
      "‖alpha‖₁       : 0.23999999999998742\n",
      "scores min/max : 3.5668698951642274e-08 5.1127384560200746e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.555e-20\n",
      "‖w_svm‖₂       : 0.061004140657397454\n",
      "‖alpha‖₁       : 0.6599999999999995\n",
      "scores min/max : -0.5026325356537021 0.3847213617229543\n",
      "Mask mean value:  tensor(0.5254, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8842  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.279e-14\n",
      "‖w_svm‖₂       : 5.800010833480404e-08\n",
      "‖alpha‖₁       : 0.11999999999999195\n",
      "scores min/max : -2.1049140064133176e-08 9.729068224544016e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.702e-08\n",
      "‖w_svm‖₂       : 1.9333428621216403e-07\n",
      "‖alpha‖₁       : 0.37999999999999934\n",
      "scores min/max : -1.7152664389749902e-08 1.8534079822953013e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.461e-19\n",
      "‖w_svm‖₂       : 0.02025460145039933\n",
      "‖alpha‖₁       : 0.5962267820744465\n",
      "scores min/max : -2.9357119761211465 2.193475362081534\n",
      "Mask mean value:  tensor(0.8587, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3701  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.505e-02\n",
      "‖w_svm‖₂       : 0.023069921644959992\n",
      "‖alpha‖₁       : 0.8521157233666201\n",
      "scores min/max : -3.0173260609157464 1.4642024404772525\n",
      "Mask mean value:  tensor(0.1224, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8637  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.130e-04\n",
      "‖w_svm‖₂       : 0.11874454936000012\n",
      "‖alpha‖₁       : 0.6494037071511192\n",
      "scores min/max : -17.767862964013496 2.0234963305716587\n",
      "Mask mean value:  tensor(0.5962, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1227  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.482e-04\n",
      "‖w_svm‖₂       : 0.00015488986160593145\n",
      "‖alpha‖₁       : 0.6399999999998824\n",
      "scores min/max : -2.4365156394306273e-05 -1.5209683129425717e-05\n",
      "Mask mean value:  tensor(0.4999, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.033e-17\n",
      "‖w_svm‖₂       : 0.16853529620793778\n",
      "‖alpha‖₁       : 0.8754869882172387\n",
      "scores min/max : -3.51875071196459 6.200730308090848\n",
      "Mask mean value:  tensor(0.1063, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1819  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.941e-05\n",
      "‖w_svm‖₂       : 0.016870982282140578\n",
      "‖alpha‖₁       : 0.8599999999999999\n",
      "scores min/max : -0.024491717901011734 -0.005159382913454573\n",
      "Mask mean value:  tensor(0.4470, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1090  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.193e-15\n",
      "‖w_svm‖₂       : 0.03168816613121438\n",
      "‖alpha‖₁       : 0.8983186253039971\n",
      "scores min/max : -0.8209721882246361 1.941265465333107\n",
      "Mask mean value:  tensor(0.7076, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5211  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.221e-06\n",
      "‖w_svm‖₂       : 0.004686890879235392\n",
      "‖alpha‖₁       : 0.7005558201277483\n",
      "scores min/max : -2.0072934697790354 0.016082166006823443\n",
      "Mask mean value:  tensor(0.4388, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2466  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.936e-05\n",
      "‖w_svm‖₂       : 0.04657220273333429\n",
      "‖alpha‖₁       : 0.9388150583113171\n",
      "scores min/max : -2.471094026099582 1.564291295961769\n",
      "Mask mean value:  tensor(0.1345, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3652  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.333e-03\n",
      "‖w_svm‖₂       : 4.46904271444651e-06\n",
      "‖alpha‖₁       : 0.4199999999883957\n",
      "scores min/max : -8.10463505042106e-07 1.073945285424066e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1230  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.913e-05\n",
      "‖w_svm‖₂       : 1.058144597704073e-06\n",
      "‖alpha‖₁       : 0.5999999999999992\n",
      "scores min/max : -2.6738840601205762e-08 2.4100391870173126e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.478e-19\n",
      "‖w_svm‖₂       : 0.07835562988055801\n",
      "‖alpha‖₁       : 0.4176706948001402\n",
      "scores min/max : -1.6646997709880664 3.6596637777358114\n",
      "Mask mean value:  tensor(0.8815, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8857  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.907e-10\n",
      "‖w_svm‖₂       : 1.1891338069936686e-07\n",
      "‖alpha‖₁       : 0.2999999999998651\n",
      "scores min/max : 1.0213305909936228e-08 1.9289474067451927e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.174e-07\n",
      "‖w_svm‖₂       : 0.023191661553667746\n",
      "‖alpha‖₁       : 0.383331317286672\n",
      "scores min/max : -1.975827994731724 0.21996049484585317\n",
      "Mask mean value:  tensor(0.6483, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1332  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.888e-14\n",
      "‖w_svm‖₂       : 8.314250055029733e-08\n",
      "‖alpha‖₁       : 0.3799999999999992\n",
      "scores min/max : 7.900831473991221e-09 2.6054781584512615e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.559e-22\n",
      "‖w_svm‖₂       : 0.00020483762983973864\n",
      "‖alpha‖₁       : 0.6199999999981903\n",
      "scores min/max : 7.714346190714605e-06 2.2586825337393e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.650e-17\n",
      "‖w_svm‖₂       : 0.00030132011684652247\n",
      "‖alpha‖₁       : 0.4199999999648001\n",
      "scores min/max : -0.0002164016373021448 9.088330996660981e-05\n",
      "Mask mean value:  tensor(0.5003, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9961  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.545e-15\n",
      "‖w_svm‖₂       : 5.696161975952699e-08\n",
      "‖alpha‖₁       : 0.23999999999997582\n",
      "scores min/max : -1.518736863776216e-08 -6.213019657217344e-10\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.076e-20\n",
      "‖w_svm‖₂       : 0.06997899053899713\n",
      "‖alpha‖₁       : 0.4699073250341579\n",
      "scores min/max : -1.9728245200592787 3.108287780122357\n",
      "Mask mean value:  tensor(0.7079, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.6733  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.621e-05\n",
      "‖w_svm‖₂       : 0.00595612732480217\n",
      "‖alpha‖₁       : 0.6076588184721247\n",
      "scores min/max : -1.9803849459577616 0.3066358867718035\n",
      "Mask mean value:  tensor(0.5996, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2425  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.418e-15\n",
      "‖w_svm‖₂       : 0.08239304154986042\n",
      "‖alpha‖₁       : 0.6575576465343689\n",
      "scores min/max : -1.953396072217113 3.7518346504521367\n",
      "Mask mean value:  tensor(0.5291, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.3792  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.269e-06\n",
      "‖w_svm‖₂       : 0.025301047527775917\n",
      "‖alpha‖₁       : 0.19645234660439617\n",
      "scores min/max : -2.102464443281102 0.12385933784938653\n",
      "Mask mean value:  tensor(0.2646, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.7171  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.463e-09\n",
      "‖w_svm‖₂       : 0.0188819839012902\n",
      "‖alpha‖₁       : 0.7799999999999964\n",
      "scores min/max : -0.0013852625715354942 0.0711331471166193\n",
      "Mask mean value:  tensor(0.5241, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.2729  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.882e-15\n",
      "‖w_svm‖₂       : 0.05303454575404974\n",
      "‖alpha‖₁       : 0.8974096018534766\n",
      "scores min/max : -1.7717471056018015 3.628116927148324\n",
      "Mask mean value:  tensor(0.2959, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4645  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.027e-03\n",
      "‖w_svm‖₂       : 0.0446543425140336\n",
      "‖alpha‖₁       : 0.9401279266073046\n",
      "scores min/max : -1.8248988899403136 0.31257425782637366\n",
      "Mask mean value:  tensor(0.4447, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3548  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.043e-14\n",
      "‖w_svm‖₂       : 0.01949267076501498\n",
      "‖alpha‖₁       : 0.681480929009772\n",
      "scores min/max : -1.9894637734251301 0.04855679003387031\n",
      "Mask mean value:  tensor(0.5301, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1977  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.153e-12\n",
      "‖w_svm‖₂       : 6.819630451432773e-07\n",
      "‖alpha‖₁       : 0.39999999999999997\n",
      "scores min/max : -1.298867851699824e-07 1.761246626072055e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.148e-09\n",
      "‖w_svm‖₂       : 0.05922785925695861\n",
      "‖alpha‖₁       : 0.7293254022074667\n",
      "scores min/max : -0.23103367614265458 2.0489328764606336\n",
      "Mask mean value:  tensor(0.7668, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.4178  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 7.544e-04\n",
      "‖w_svm‖₂       : 0.025425965092165374\n",
      "‖alpha‖₁       : 0.8150809908879303\n",
      "scores min/max : -11.511252895549738 2.040218335409913\n",
      "Mask mean value:  tensor(0.7303, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9390  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.098e-06\n",
      "‖w_svm‖₂       : 1.1157867828215532e-07\n",
      "‖alpha‖₁       : 0.23999999999997285\n",
      "scores min/max : 3.4164683654837707e-09 1.4442027588973067e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.609e-21\n",
      "‖w_svm‖₂       : 0.04155105269039424\n",
      "‖alpha‖₁       : 0.8895732186724516\n",
      "scores min/max : -2.0168921409247242 0.33017730291582054\n",
      "Mask mean value:  tensor(0.2451, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0720  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.977e-04\n",
      "‖w_svm‖₂       : 0.00048714238819549493\n",
      "‖alpha‖₁       : 0.43999999999999995\n",
      "scores min/max : -0.00021853153034099584 1.6200962037143986e-05\n",
      "Mask mean value:  tensor(0.4991, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6523  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.681e-16\n",
      "‖w_svm‖₂       : 0.03000775578340232\n",
      "‖alpha‖₁       : 0.6599999999999953\n",
      "scores min/max : -0.0612302561049648 0.10715813710303831\n",
      "Mask mean value:  tensor(0.4793, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.8342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.318e-15\n",
      "‖w_svm‖₂       : 2.343510283130366e-07\n",
      "‖alpha‖₁       : 0.2599999999999949\n",
      "scores min/max : -1.5209409253219456e-08 -2.4221936319765284e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.885e-17\n",
      "‖w_svm‖₂       : 2.1074389589673095e-07\n",
      "‖alpha‖₁       : 0.4199999999999705\n",
      "scores min/max : -6.202861173354877e-08 -3.969482333529617e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.470e-19\n",
      "‖w_svm‖₂       : 7.15637200011835e-08\n",
      "‖alpha‖₁       : 0.42000000000000004\n",
      "scores min/max : -5.3759324044680924e-08 -6.662523136309308e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.147e-08\n",
      "‖w_svm‖₂       : 0.000346565828901703\n",
      "‖alpha‖₁       : 0.7399999999999994\n",
      "scores min/max : -3.602715073100038e-05 -2.070681294573053e-05\n",
      "Mask mean value:  tensor(0.4999, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0640  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.954e-17\n",
      "‖w_svm‖₂       : 0.023990599988220625\n",
      "‖alpha‖₁       : 0.8233117677863522\n",
      "scores min/max : -1.8695443945788248 1.5330030808856034\n",
      "Mask mean value:  tensor(0.7735, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.5726  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.004e-05\n",
      "‖w_svm‖₂       : 0.004054889381309084\n",
      "‖alpha‖₁       : 0.45999999999999985\n",
      "scores min/max : -0.002858128509399254 -0.0011523038317510191\n",
      "Mask mean value:  tensor(0.4922, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.1398  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.903e-16\n",
      "‖w_svm‖₂       : 1.1592207923380114e-07\n",
      "‖alpha‖₁       : 0.519999999999999\n",
      "scores min/max : 5.429401855847861e-08 7.569594074938001e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.142e-19\n",
      "‖w_svm‖₂       : 1.5019440656887635e-05\n",
      "‖alpha‖₁       : 0.3599999999945098\n",
      "scores min/max : 2.5967447086132375e-06 3.4647132606398676e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3078  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.678e-17\n",
      "‖w_svm‖₂       : 0.05778010269228233\n",
      "‖alpha‖₁       : 0.9056570385230398\n",
      "scores min/max : -1.1352002523536509 1.7669791646283453\n",
      "Mask mean value:  tensor(0.1789, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3351  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.400e-03\n",
      "‖w_svm‖₂       : 0.13608935616216172\n",
      "‖alpha‖₁       : 0.8729078196602365\n",
      "scores min/max : -12.222819826044933 2.108243102838241\n",
      "Mask mean value:  tensor(0.4689, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3839  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 9.798e-02\n",
      "‖w_svm‖₂       : 0.028097060188874406\n",
      "‖alpha‖₁       : 0.5509485325903634\n",
      "scores min/max : -3.6009909453508575 0.9740112105460204\n",
      "Mask mean value:  tensor(0.0982, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8340  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.890e-03\n",
      "‖w_svm‖₂       : 0.00543540566542493\n",
      "‖alpha‖₁       : 0.5599999999999996\n",
      "scores min/max : 0.00143585030710284 0.0029639925629614096\n",
      "Mask mean value:  tensor(0.5086, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0660  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 2.102e-04\n",
      "‖w_svm‖₂       : 4.8323608438790235e-08\n",
      "‖alpha‖₁       : 0.17999999999999858\n",
      "scores min/max : 1.175557219557358e-08 2.6416305478552043e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 4.535e-08\n",
      "‖w_svm‖₂       : 0.004915047845278706\n",
      "‖alpha‖₁       : 0.37999999999999967\n",
      "scores min/max : -0.008098549505709332 0.0026080306616522277\n",
      "Mask mean value:  tensor(0.4991, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8889  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.539e-14\n",
      "‖w_svm‖₂       : 0.00012964921678342226\n",
      "‖alpha‖₁       : 0.4399999999999723\n",
      "scores min/max : -8.90037785432759e-05 3.354519076747295e-05\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0284  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.427e-17\n",
      "‖w_svm‖₂       : 0.003011152007178756\n",
      "‖alpha‖₁       : 0.5799999999999998\n",
      "scores min/max : -0.00043740489389908405 0.0007223841602441439\n",
      "Mask mean value:  tensor(0.5024, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3195  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.729e-06\n",
      "‖w_svm‖₂       : 0.07573682833539673\n",
      "‖alpha‖₁       : 0.5760754327178093\n",
      "scores min/max : -1.9235960712989288 6.025296867304092\n",
      "Mask mean value:  tensor(0.7972, dtype=torch.float64)\n",
      "max feasible return = 0.4106  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.171970338287225e-07\n",
      "‖alpha‖₁       : 0.579999999999997\n",
      "scores min/max : -1.1558925353540128e-07 -2.4697207130602987e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.1044892303977902e-07\n",
      "‖alpha‖₁       : 0.29999999999996885\n",
      "scores min/max : -1.5922505167835417e-10 4.96258363854646e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 7.575150117973951e-08\n",
      "‖alpha‖₁       : 0.5999999999999996\n",
      "scores min/max : -1.9661392458080843e-08 1.7069573911442887e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 5.034255510139945e-08\n",
      "‖alpha‖₁       : 0.37999999999999895\n",
      "scores min/max : -3.053742865379402e-09 6.327943986882673e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.11580981626949e-06\n",
      "‖alpha‖₁       : 0.31999999997616835\n",
      "scores min/max : -7.393517216734782e-07 9.744039508018724e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4257  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.06826637115427864\n",
      "‖alpha‖₁       : 0.7396367535808671\n",
      "scores min/max : -3.488264574100177 2.120335695362651\n",
      "Mask mean value:  tensor(0.8508, dtype=torch.float64)\n",
      "max feasible return = -0.8977  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03294231080715807\n",
      "‖alpha‖₁       : 0.6149069939049099\n",
      "scores min/max : -1.882480717527768 1.247659223892169\n",
      "Mask mean value:  tensor(0.8444, dtype=torch.float64)\n",
      "max feasible return = 2.9774  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.927604658267172e-07\n",
      "‖alpha‖₁       : 0.459999999999598\n",
      "scores min/max : -7.460111353635882e-08 5.300201246405411e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.3827634341782437e-07\n",
      "‖alpha‖₁       : 0.5199999999999915\n",
      "scores min/max : 6.0570723278972625e-09 1.895665600761309e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.621860339509151e-07\n",
      "‖alpha‖₁       : 0.5799999999999988\n",
      "scores min/max : -5.4816004464746776e-08 -1.86724753751606e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.04143453945807998\n",
      "‖alpha‖₁       : 0.6872081851334892\n",
      "scores min/max : -0.3499547456168784 2.0429075841067768\n",
      "Mask mean value:  tensor(0.7235, dtype=torch.float64)\n",
      "max feasible return = -0.2263  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.036122274684613306\n",
      "‖alpha‖₁       : 0.42207807924540697\n",
      "scores min/max : -3.9412171372338096 5.14505037420476\n",
      "Mask mean value:  tensor(0.0636, dtype=torch.float64)\n",
      "max feasible return = 0.4025  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.0498782338934199\n",
      "‖alpha‖₁       : 0.8211442931471984\n",
      "scores min/max : -5.097703735287298 3.0573583719263233\n",
      "Mask mean value:  tensor(0.9392, dtype=torch.float64)\n",
      "max feasible return = -3.5419  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.010771353031421217\n",
      "‖alpha‖₁       : 0.7999999999999969\n",
      "scores min/max : -0.05954244707006069 0.015592114034260956\n",
      "Mask mean value:  tensor(0.4832, dtype=torch.float64)\n",
      "max feasible return = -0.1758  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.242536964806317e-07\n",
      "‖alpha‖₁       : 0.6199999999999838\n",
      "scores min/max : -2.77515225019912e-08 -1.1292724687912645e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.739367199271838e-08\n",
      "‖alpha‖₁       : 0.43999999999999995\n",
      "scores min/max : 1.1874330131619176e-10 9.6891531428792e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.3204855234408938e-07\n",
      "‖alpha‖₁       : 0.27999999999999936\n",
      "scores min/max : -6.952082706814225e-08 -5.1925277172123324e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch   4 | train 0.005589 | val 0.006822\n",
      "-----------------------------------------Epoch:  5 ----------------------------------------\n",
      "‖w_svm‖₂       : 2.4038179754202395e-07\n",
      "‖alpha‖₁       : 0.5799999999999963\n",
      "scores min/max : -6.49003079892478e-08 -4.9264708818435696e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.653e-19\n",
      "‖w_svm‖₂       : 0.01938187273551442\n",
      "‖alpha‖₁       : 0.6814815359271695\n",
      "scores min/max : -1.9898326913994187 0.04801548120094752\n",
      "Mask mean value:  tensor(0.5284, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1973  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.092e-12\n",
      "‖w_svm‖₂       : 0.0004900596836494042\n",
      "‖alpha‖₁       : 0.43999999999999984\n",
      "scores min/max : -0.0002567469656400209 -1.9207102524275847e-05\n",
      "Mask mean value:  tensor(0.4989, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6517  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.682e-16\n",
      "‖w_svm‖₂       : 0.04452432510691297\n",
      "‖alpha‖₁       : 0.9401377058799689\n",
      "scores min/max : -1.8233777976432 0.3139808017644158\n",
      "Mask mean value:  tensor(0.4506, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3539  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.675e-15\n",
      "‖w_svm‖₂       : 4.0053222754491163e-07\n",
      "‖alpha‖₁       : 0.27999999999999653\n",
      "scores min/max : -4.397987798475487e-07 -3.780815458844175e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.995e-19\n",
      "‖w_svm‖₂       : 2.341831794410005e-08\n",
      "‖alpha‖₁       : 0.11999999999999866\n",
      "scores min/max : -5.954506685684947e-09 8.459766431578192e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.903e-09\n",
      "‖w_svm‖₂       : 0.00015511799835149192\n",
      "‖alpha‖₁       : 0.6399999999999999\n",
      "scores min/max : -3.416262507442867e-05 -2.498014590582439e-05\n",
      "Mask mean value:  tensor(0.4999, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7642  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.045e-17\n",
      "‖w_svm‖₂       : 0.0008319593858243985\n",
      "‖alpha‖₁       : 0.8199999999999992\n",
      "scores min/max : -0.0003873963110370589 0.001207987709290548\n",
      "Mask mean value:  tensor(0.5031, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5557  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.646e-17\n",
      "‖w_svm‖₂       : 1.7904781132792103e-07\n",
      "‖alpha‖₁       : 0.23999999999998617\n",
      "scores min/max : 6.015306980830208e-08 7.563318553808738e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.584e-20\n",
      "‖w_svm‖₂       : 0.04696692734430033\n",
      "‖alpha‖₁       : 0.9388496394241825\n",
      "scores min/max : -2.4692735403661383 1.5662887877965346\n",
      "Mask mean value:  tensor(0.1356, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3683  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.405e-03\n",
      "‖w_svm‖₂       : 0.13550924802108094\n",
      "‖alpha‖₁       : 0.8729082019733904\n",
      "scores min/max : -12.250899917917181 2.0814990788610404\n",
      "Mask mean value:  tensor(0.4047, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2077  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 9.161e-02\n",
      "‖w_svm‖₂       : 0.16454231444540385\n",
      "‖alpha‖₁       : 0.8799999999999997\n",
      "scores min/max : -2.0082215724744814 4.959902502624188\n",
      "Mask mean value:  tensor(0.1456, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.045e-03\n",
      "‖w_svm‖₂       : 0.004091834889327324\n",
      "‖alpha‖₁       : 0.45999999999999597\n",
      "scores min/max : -0.0030578256943825533 -0.0013216150711255203\n",
      "Mask mean value:  tensor(0.4914, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.1359  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.954e-16\n",
      "‖w_svm‖₂       : 0.057654891614760354\n",
      "‖alpha‖₁       : 0.9199999999999989\n",
      "scores min/max : -0.42539427728913753 0.8368308464861033\n",
      "Mask mean value:  tensor(0.5845, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.9656  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.620e-03\n",
      "‖w_svm‖₂       : 0.05372348893979694\n",
      "‖alpha‖₁       : 0.897498397338301\n",
      "scores min/max : -1.7657023706436348 3.634845521958665\n",
      "Mask mean value:  tensor(0.3052, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4732  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.784e-03\n",
      "‖w_svm‖₂       : 0.019040757347140853\n",
      "‖alpha‖₁       : 0.7799999999999958\n",
      "scores min/max : -0.000762780926814987 0.07297294370423513\n",
      "Mask mean value:  tensor(0.5277, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.2887  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.855e-15\n",
      "‖w_svm‖₂       : 0.06039819900937649\n",
      "‖alpha‖₁       : 0.6599999999999996\n",
      "scores min/max : -0.4923228174998416 0.37702030157294664\n",
      "Mask mean value:  tensor(0.5253, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8871  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.284e-14\n",
      "‖w_svm‖₂       : 0.0786716339762852\n",
      "‖alpha‖₁       : 0.4177956703617796\n",
      "scores min/max : -1.6750300506043958 3.6546914082631803\n",
      "Mask mean value:  tensor(0.8782, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8788  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.676e-11\n",
      "‖w_svm‖₂       : 0.029599839643785278\n",
      "‖alpha‖₁       : 0.6599999999999949\n",
      "scores min/max : -0.0590079496635981 0.10457429533683042\n",
      "Mask mean value:  tensor(0.4824, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.8552  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.232e-15\n",
      "‖w_svm‖₂       : 0.08211847956176614\n",
      "‖alpha‖₁       : 0.6576406701416815\n",
      "scores min/max : -1.9653210672196002 3.771675287616132\n",
      "Mask mean value:  tensor(0.4998, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1888  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.576e-05\n",
      "‖w_svm‖₂       : 2.482085596162382e-07\n",
      "‖alpha‖₁       : 0.6399999999999912\n",
      "scores min/max : 4.48031670846975e-08 6.568714146607082e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.274e-20\n",
      "‖w_svm‖₂       : 1.208866815591932e-06\n",
      "‖alpha‖₁       : 0.31999999999878476\n",
      "scores min/max : -3.8616691849260415e-07 -2.71495486483626e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4899  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.567e-18\n",
      "‖w_svm‖₂       : 0.02385613419633937\n",
      "‖alpha‖₁       : 0.8233146047041457\n",
      "scores min/max : -1.8703991635978352 1.5318642443795194\n",
      "Mask mean value:  tensor(0.7716, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.5666  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.851e-06\n",
      "‖w_svm‖₂       : 1.4864881623298e-05\n",
      "‖alpha‖₁       : 0.359999999994734\n",
      "scores min/max : 2.7154176494962625e-06 3.565675795160018e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3078  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.660e-17\n",
      "‖w_svm‖₂       : 0.004931256305819643\n",
      "‖alpha‖₁       : 0.3799999999999994\n",
      "scores min/max : -0.008143250142339603 0.00262256609294456\n",
      "Mask mean value:  tensor(0.4991, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8889  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.555e-14\n",
      "‖w_svm‖₂       : 0.127432158784944\n",
      "‖alpha‖₁       : 0.7540961773413193\n",
      "scores min/max : -1.7034703069491568 2.279466626981431\n",
      "Mask mean value:  tensor(0.9181, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3453  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.469e-12\n",
      "‖w_svm‖₂       : 1.1723837936406795e-07\n",
      "‖alpha‖₁       : 0.519999999999999\n",
      "scores min/max : 5.3203722963274444e-08 7.460602382403928e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.182e-19\n",
      "‖w_svm‖₂       : 8.418777440001028e-08\n",
      "‖alpha‖₁       : 0.6599999999999944\n",
      "scores min/max : 1.3389081245889215e-08 1.5811094621090275e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.476e-09\n",
      "‖w_svm‖₂       : 4.3705875420327584e-07\n",
      "‖alpha‖₁       : 0.7199999999999999\n",
      "scores min/max : 6.927557800771186e-09 6.414786980526533e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.733e-21\n",
      "‖w_svm‖₂       : 0.016822577966953894\n",
      "‖alpha‖₁       : 0.8599999999999985\n",
      "scores min/max : -0.026238757952486293 -0.007034466133752443\n",
      "Mask mean value:  tensor(0.4379, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1064  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.487e-15\n",
      "‖w_svm‖₂       : 0.027892484202882397\n",
      "‖alpha‖₁       : 0.5509475361562307\n",
      "scores min/max : -3.5985286119366493 0.9760503579754187\n",
      "Mask mean value:  tensor(0.0988, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8397  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.927e-03\n",
      "‖w_svm‖₂       : 0.006040644190975828\n",
      "‖alpha‖₁       : 0.6076598062743626\n",
      "scores min/max : -1.9804614344858842 0.30655975448549383\n",
      "Mask mean value:  tensor(0.5993, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2423  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.472e-15\n",
      "‖w_svm‖₂       : 3.779397986954175e-06\n",
      "‖alpha‖₁       : 0.4199999999925695\n",
      "scores min/max : -6.361002363301781e-07 9.953170530772002e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1230  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.628e-05\n",
      "‖w_svm‖₂       : 0.0002974939201009227\n",
      "‖alpha‖₁       : 0.4199999999927241\n",
      "scores min/max : -0.00020923837240406946 9.055927819024327e-05\n",
      "Mask mean value:  tensor(0.5003, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9961  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.542e-15\n",
      "‖w_svm‖₂       : 0.056554532794387984\n",
      "‖alpha‖₁       : 0.9056564857759692\n",
      "scores min/max : -1.0830865788407227 1.780169662116539\n",
      "Mask mean value:  tensor(0.1901, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3553  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.004e-03\n",
      "‖w_svm‖₂       : 0.004769128724145358\n",
      "‖alpha‖₁       : 0.7005564920007902\n",
      "scores min/max : -2.0075241648609286 0.015852122880532518\n",
      "Mask mean value:  tensor(0.4377, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2460  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.246e-05\n",
      "‖w_svm‖₂       : 0.17228414743668327\n",
      "‖alpha‖₁       : 0.8767972129967053\n",
      "scores min/max : -3.5301913357802057 6.195780776793799\n",
      "Mask mean value:  tensor(0.1044, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1822  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.351e-05\n",
      "‖w_svm‖₂       : 5.924322784179783e-08\n",
      "‖alpha‖₁       : 0.4399999999999988\n",
      "scores min/max : -1.0832850719213016e-07 -4.144193687611294e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.325e-20\n",
      "‖w_svm‖₂       : 5.74381694853232e-08\n",
      "‖alpha‖₁       : 0.23999999999997376\n",
      "scores min/max : -1.5444049429593943e-08 -8.655999631609828e-10\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.094e-20\n",
      "‖w_svm‖₂       : 2.955056359287233e-07\n",
      "‖alpha‖₁       : 0.6599999999999957\n",
      "scores min/max : -1.9120144761398565e-07 -3.94916975519114e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.212e-18\n",
      "‖w_svm‖₂       : 0.041977385560553124\n",
      "‖alpha‖₁       : 0.8896347978271049\n",
      "scores min/max : -2.014273862259597 0.3328203909116528\n",
      "Mask mean value:  tensor(0.2521, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0745  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.184e-05\n",
      "‖w_svm‖₂       : 0.02299876738764737\n",
      "‖alpha‖₁       : 0.8599999999999866\n",
      "scores min/max : -0.04798810438639745 0.06554125091228666\n",
      "Mask mean value:  tensor(0.4885, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6464  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.434e-15\n",
      "‖w_svm‖₂       : 0.00020227240673383696\n",
      "‖alpha‖₁       : 0.6199999999984716\n",
      "scores min/max : 6.948134814789434e-06 2.1450441874259967e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.659e-17\n",
      "‖w_svm‖₂       : 0.02292807907658163\n",
      "‖alpha‖₁       : 0.38333462060178747\n",
      "scores min/max : -1.97645969068888 0.21985300703985905\n",
      "Mask mean value:  tensor(0.6465, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1338  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.179e-14\n",
      "‖w_svm‖₂       : 0.12236766432056033\n",
      "‖alpha‖₁       : 0.6503053818676565\n",
      "scores min/max : -17.79114082595457 2.005033290809897\n",
      "Mask mean value:  tensor(0.5562, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1322  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.409e-04\n",
      "‖w_svm‖₂       : 0.00017921279806768436\n",
      "‖alpha‖₁       : 0.8199999999999996\n",
      "scores min/max : -5.95491748827263e-06 7.5051270992757555e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6979  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.122e-17\n",
      "‖w_svm‖₂       : 8.215639327589927e-08\n",
      "‖alpha‖₁       : 0.5399999999999602\n",
      "scores min/max : -2.631325443017852e-08 -4.6883692343938e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.593e-20\n",
      "‖w_svm‖₂       : 7.507055712245703e-08\n",
      "‖alpha‖₁       : 0.13999999999999846\n",
      "scores min/max : -5.9030537470656725e-09 -1.7091074496284976e-10\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.002e-08\n",
      "‖w_svm‖₂       : 0.07234332347267446\n",
      "‖alpha‖₁       : 0.4702598546942116\n",
      "scores min/max : -2.010841318519493 3.0703466078212585\n",
      "Mask mean value:  tensor(0.5972, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.6651  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.985e-05\n",
      "‖w_svm‖₂       : 1.4270380502872001e-07\n",
      "‖alpha‖₁       : 0.37999999999999423\n",
      "scores min/max : -3.639014500014608e-10 1.4628464381174097e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.188e-07\n",
      "‖w_svm‖₂       : 0.07405986162616258\n",
      "‖alpha‖₁       : 0.5795973515546793\n",
      "scores min/max : -3.3239367150255923 1.8520144733072765\n",
      "Mask mean value:  tensor(0.1178, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2493  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.491e-03\n",
      "‖w_svm‖₂       : 1.1416870518945694e-06\n",
      "‖alpha‖₁       : 0.4999999999999939\n",
      "scores min/max : -1.5530091147855375e-07 3.746915601160839e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.271e-19\n",
      "‖w_svm‖₂       : 5.8334459499144964e-08\n",
      "‖alpha‖₁       : 0.11999999999998735\n",
      "scores min/max : -2.709594268407455e-08 3.760010044589573e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.364e-08\n",
      "‖w_svm‖₂       : 8.402560576953377e-08\n",
      "‖alpha‖₁       : 0.37999999999999984\n",
      "scores min/max : 9.806199336257974e-09 2.7953111291628237e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.606e-22\n",
      "‖w_svm‖₂       : 0.00012832123475516911\n",
      "‖alpha‖₁       : 0.4399999999999233\n",
      "scores min/max : -9.09523700329748e-05 2.9118768293897104e-05\n",
      "Mask mean value:  tensor(0.4999, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0283  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.406e-17\n",
      "‖w_svm‖₂       : 0.02324797320684093\n",
      "‖alpha‖₁       : 0.852119722128575\n",
      "scores min/max : -3.0117594349691723 1.4697947321405398\n",
      "Mask mean value:  tensor(0.1238, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8710  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.802e-04\n",
      "‖w_svm‖₂       : 0.020011296346006433\n",
      "‖alpha‖₁       : 0.5962283246002672\n",
      "scores min/max : -2.9345664259596114 2.200982139980301\n",
      "Mask mean value:  tensor(0.8664, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3935  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.103e-02\n",
      "‖w_svm‖₂       : 0.05827439496022909\n",
      "‖alpha‖₁       : 0.7293323518923399\n",
      "scores min/max : -0.2321425895762783 2.048247774475509\n",
      "Mask mean value:  tensor(0.7639, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.4072  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 7.769e-04\n",
      "‖w_svm‖₂       : 7.237731610199614e-08\n",
      "‖alpha‖₁       : 0.41999999999999205\n",
      "scores min/max : -5.518953709332583e-08 -7.935316143491689e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.268e-08\n",
      "‖w_svm‖₂       : 1.9551307511676047e-07\n",
      "‖alpha‖₁       : 0.3799999999999993\n",
      "scores min/max : -1.7622446603798924e-08 1.8063510254841494e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.526e-19\n",
      "‖w_svm‖₂       : 1.070431445910085e-06\n",
      "‖alpha‖₁       : 0.5999999999999533\n",
      "scores min/max : -1.188534965510915e-08 3.9186425720888034e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.506e-19\n",
      "‖w_svm‖₂       : 0.031801711890563925\n",
      "‖alpha‖₁       : 0.8983471110907286\n",
      "scores min/max : -0.816870251505126 1.9448243787858814\n",
      "Mask mean value:  tensor(0.7162, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5152  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.016e-06\n",
      "‖w_svm‖₂       : 1.1293273391308698e-07\n",
      "‖alpha‖₁       : 0.23999999999997126\n",
      "scores min/max : 4.213098231581532e-09 1.5243585051116643e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.679e-21\n",
      "‖w_svm‖₂       : 0.025652324459146163\n",
      "‖alpha‖₁       : 0.1964802661495229\n",
      "scores min/max : -2.108153851856016 0.11787100247187833\n",
      "Mask mean value:  tensor(0.2454, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.5961  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.638e-09\n",
      "‖w_svm‖₂       : 2.1219292583239785e-07\n",
      "‖alpha‖₁       : 0.4199999999999735\n",
      "scores min/max : -6.771247403632103e-08 -4.5395124570878875e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.496e-19\n",
      "‖w_svm‖₂       : 0.005450197299167258\n",
      "‖alpha‖₁       : 0.5599999999999994\n",
      "scores min/max : 0.001624333130036704 0.0031617354236340017\n",
      "Mask mean value:  tensor(0.5095, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0680  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.980e-04\n",
      "‖w_svm‖₂       : 9.220961313865315e-08\n",
      "‖alpha‖₁       : 0.179999999999998\n",
      "scores min/max : 1.7857307103803081e-09 2.3574225991517186e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.661e-21\n",
      "‖w_svm‖₂       : 0.00034387856755471735\n",
      "‖alpha‖₁       : 0.7399999999999997\n",
      "scores min/max : -4.068746612262406e-05 -2.5603711669987425e-05\n",
      "Mask mean value:  tensor(0.4999, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0640  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.946e-17\n",
      "‖w_svm‖₂       : 1.1983264477924537e-07\n",
      "‖alpha‖₁       : 0.2999999999998721\n",
      "scores min/max : 1.1860943105281457e-08 2.0929784743341616e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.190e-07\n",
      "‖w_svm‖₂       : 0.003026435313059566\n",
      "‖alpha‖₁       : 0.5799999999999905\n",
      "scores min/max : -0.0003800013582066482 0.0007893339665951474\n",
      "Mask mean value:  tensor(0.5027, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3197  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.127e-06\n",
      "‖w_svm‖₂       : 6.876970375014807e-07\n",
      "‖alpha‖₁       : 0.3999999999998959\n",
      "scores min/max : -1.3748463528241626e-07 1.7128353161440809e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.187e-09\n",
      "‖w_svm‖₂       : 0.056344130786124776\n",
      "‖alpha‖₁       : 0.5755858407766885\n",
      "scores min/max : -1.9446787507114636 0.8772420170524166\n",
      "Mask mean value:  tensor(0.7351, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.0113  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.024e-03\n",
      "‖w_svm‖₂       : 2.3480737062595614e-07\n",
      "‖alpha‖₁       : 0.26\n",
      "scores min/max : -2.8187824669099685e-08 -1.5475682757242687e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.920e-17\n",
      "‖w_svm‖₂       : 0.02516279496375669\n",
      "‖alpha‖₁       : 0.8150823347897601\n",
      "scores min/max : -11.513672412202286 2.0407961916156085\n",
      "Mask mean value:  tensor(0.7306, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9414  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.675e-07\n",
      "‖w_svm‖₂       : 0.0505603411870432\n",
      "‖alpha‖₁       : 0.8265399035469213\n",
      "scores min/max : -1.981684593541973 0.3779245895807976\n",
      "Mask mean value:  tensor(0.4076, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0231  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.693e-02\n",
      "‖w_svm‖₂       : 4.8802298477974695e-08\n",
      "‖alpha‖₁       : 0.1799999999999988\n",
      "scores min/max : 1.2374227290529844e-08 2.6992017465581205e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 4.607e-08\n",
      "‖w_svm‖₂       : 0.07733482623331257\n",
      "‖alpha‖₁       : 0.5763494383280334\n",
      "scores min/max : -1.9348079840471364 6.012032218822406\n",
      "Mask mean value:  tensor(0.7711, dtype=torch.float64)\n",
      "max feasible return = 0.3837  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.176842924445096e-07\n",
      "‖alpha‖₁       : 0.5799999999999976\n",
      "scores min/max : -1.2857413025592216e-07 -3.773802742375471e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.1104426854823008e-07\n",
      "‖alpha‖₁       : 0.29999999999996946\n",
      "scores min/max : 3.073187367616045e-09 8.191774527673566e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 7.515013107978504e-08\n",
      "‖alpha‖₁       : 0.5999999999999996\n",
      "scores min/max : -1.4914006727710826e-08 2.1822678300171582e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 5.0146267844533916e-08\n",
      "‖alpha‖₁       : 0.37999999999999884\n",
      "scores min/max : -3.5153162842440575e-10 9.032365151949583e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.591638325919955e-06\n",
      "‖alpha‖₁       : 0.3199999999886228\n",
      "scores min/max : -1.3713483042564915e-07 1.0552187936640615e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4257  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.06759236734524003\n",
      "‖alpha‖₁       : 0.7396456171828153\n",
      "scores min/max : -3.4861340964887386 2.1243208220932734\n",
      "Mask mean value:  tensor(0.8549, dtype=torch.float64)\n",
      "max feasible return = -0.9017  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03331335684919539\n",
      "‖alpha‖₁       : 0.6149297329685043\n",
      "scores min/max : -1.88486996020342 1.2452123292865054\n",
      "Mask mean value:  tensor(0.8398, dtype=torch.float64)\n",
      "max feasible return = 2.9599  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.865583860192638e-07\n",
      "‖alpha‖₁       : 0.45999999999999197\n",
      "scores min/max : -1.6020681663914763e-08 5.784695390399804e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.3982390119032593e-07\n",
      "‖alpha‖₁       : 0.5199999999999887\n",
      "scores min/max : 3.2364207963859405e-09 1.6137211259626866e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.6429153148882874e-07\n",
      "‖alpha‖₁       : 0.5799999999999988\n",
      "scores min/max : -7.519180330457532e-08 -3.905011235435246e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.04206552851508796\n",
      "‖alpha‖₁       : 0.6872644743750469\n",
      "scores min/max : -0.3555819269071895 2.036975481794209\n",
      "Mask mean value:  tensor(0.7038, dtype=torch.float64)\n",
      "max feasible return = -0.2124  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.035911231324233586\n",
      "‖alpha‖₁       : 0.42208353202817406\n",
      "scores min/max : -3.938850499643422 5.150595997196724\n",
      "Mask mean value:  tensor(0.0645, dtype=torch.float64)\n",
      "max feasible return = 0.4082  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04999671594872651\n",
      "‖alpha‖₁       : 0.8211629346650968\n",
      "scores min/max : -5.098653004470156 3.056663318466515\n",
      "Mask mean value:  tensor(0.9390, dtype=torch.float64)\n",
      "max feasible return = -3.5411  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.010640443076217742\n",
      "‖alpha‖₁       : 0.7999999999999614\n",
      "scores min/max : -0.05819828226552929 0.015026391224807456\n",
      "Mask mean value:  tensor(0.4827, dtype=torch.float64)\n",
      "max feasible return = -0.1756  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.2674337483751106e-07\n",
      "‖alpha‖₁       : 0.6199999999999819\n",
      "scores min/max : -1.2656563072499438e-08 3.81063933195758e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.781050849438555e-08\n",
      "‖alpha‖₁       : 0.4399999999999611\n",
      "scores min/max : 1.074320430420834e-09 1.0657063836672193e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.388408348438119e-07\n",
      "‖alpha‖₁       : 0.27999999999997055\n",
      "scores min/max : -1.0799209827534743e-07 -9.001849445542966e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch   5 | train 0.005575 | val 0.006807\n",
      "-----------------------------------------Epoch:  6 ----------------------------------------\n",
      "‖w_svm‖₂       : 0.01930173602143371\n",
      "‖alpha‖₁       : 0.6814817871360879\n",
      "scores min/max : -1.987559046075123 0.050114627829564724\n",
      "Mask mean value:  tensor(0.5394, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2033  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.259e-12\n",
      "‖w_svm‖₂       : 1.4657530876582076e-05\n",
      "‖alpha‖₁       : 0.3599999999955042\n",
      "scores min/max : 3.3848617187749697e-06 4.224188782757051e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3078  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.618e-17\n",
      "‖w_svm‖₂       : 0.07953574160501967\n",
      "‖alpha‖₁       : 0.4179631881539653\n",
      "scores min/max : -1.6742429264363894 3.6586075124340516\n",
      "Mask mean value:  tensor(0.8784, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8792  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.952e-11\n",
      "‖w_svm‖₂       : 0.054753727721016845\n",
      "‖alpha‖₁       : 0.8976149291739428\n",
      "scores min/max : -1.7612613325584179 3.6390022993867936\n",
      "Mask mean value:  tensor(0.3117, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4791  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.369e-03\n",
      "‖w_svm‖₂       : 0.047052291849685\n",
      "‖alpha‖₁       : 0.9388748138793268\n",
      "scores min/max : -2.4701419187164033 1.5663208575978593\n",
      "Mask mean value:  tensor(0.1355, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3686  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.504e-03\n",
      "‖w_svm‖₂       : 0.0002981528610151292\n",
      "‖alpha‖₁       : 0.4199999999994097\n",
      "scores min/max : -0.00017422358888544132 0.00012698463715695056\n",
      "Mask mean value:  tensor(0.5005, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9965  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.556e-15\n",
      "‖w_svm‖₂       : 0.00018018595022323579\n",
      "‖alpha‖₁       : 0.8199999999999803\n",
      "scores min/max : -5.914267357694534e-06 7.674225298065775e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6979  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.126e-17\n",
      "‖w_svm‖₂       : 5.7488056867564016e-08\n",
      "‖alpha‖₁       : 0.23999999999997362\n",
      "scores min/max : -1.8682956499528838e-08 -4.10338628275734e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.108e-20\n",
      "‖w_svm‖₂       : 1.786109938549008e-07\n",
      "‖alpha‖₁       : 0.23999999999999097\n",
      "scores min/max : 8.003312744119483e-08 9.542948396659576e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.643e-20\n",
      "‖w_svm‖₂       : 1.1660329392136997e-07\n",
      "‖alpha‖₁       : 0.5199999999999992\n",
      "scores min/max : 7.448141948344931e-08 9.588669241851814e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.200e-19\n",
      "‖w_svm‖₂       : 1.0649918996274853e-06\n",
      "‖alpha‖₁       : 0.5999999999999529\n",
      "scores min/max : 3.951521428752637e-09 5.5024272491743824e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.498e-19\n",
      "‖w_svm‖₂       : 2.3302745093512644e-08\n",
      "‖alpha‖₁       : 0.11999999999999765\n",
      "scores min/max : -6.9182603712439624e-09 7.580385302033135e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.878e-09\n",
      "‖w_svm‖₂       : 4.887550825359114e-08\n",
      "‖alpha‖₁       : 0.17999999999999855\n",
      "scores min/max : 1.2702881749558589e-08 2.7356350773894725e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 4.625e-08\n",
      "‖w_svm‖₂       : 0.00015433274384526232\n",
      "‖alpha‖₁       : 0.6399999999999999\n",
      "scores min/max : -1.8855235230237895e-05 -9.765514631509674e-06\n",
      "Mask mean value:  tensor(0.4999, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.028e-17\n",
      "‖w_svm‖₂       : 0.018967570860060963\n",
      "‖alpha‖₁       : 0.7800000000000021\n",
      "scores min/max : -0.002240818707228382 0.07065916935973549\n",
      "Mask mean value:  tensor(0.5199, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.2560  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.043e-15\n",
      "‖w_svm‖₂       : 0.12956142269618817\n",
      "‖alpha‖₁       : 0.7546709694608502\n",
      "scores min/max : -1.7036404037274537 2.274442908275263\n",
      "Mask mean value:  tensor(0.9161, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3469  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.390e-12\n",
      "‖w_svm‖₂       : 0.04446744883094351\n",
      "‖alpha‖₁       : 0.9401575383747405\n",
      "scores min/max : -1.8258803085544693 0.31128781673552935\n",
      "Mask mean value:  tensor(0.4403, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3530  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.228e-12\n",
      "‖w_svm‖₂       : 0.04223018827473174\n",
      "‖alpha‖₁       : 0.889660878191727\n",
      "scores min/max : -2.0128966402915283 0.3341375711710822\n",
      "Mask mean value:  tensor(0.2559, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0761  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.907e-05\n",
      "‖w_svm‖₂       : 0.005453459711828913\n",
      "‖alpha‖₁       : 0.5599999999999994\n",
      "scores min/max : 0.0016925146331739091 0.0032319768608451748\n",
      "Mask mean value:  tensor(0.5099, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0687  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.940e-04\n",
      "‖w_svm‖₂       : 4.0156070165307977e-07\n",
      "‖alpha‖₁       : 0.2799999999999952\n",
      "scores min/max : -4.814016503792302e-07 -4.196407965172205e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.050e-19\n",
      "‖w_svm‖₂       : 0.05654438443279206\n",
      "‖alpha‖₁       : 0.5756022166007996\n",
      "scores min/max : -1.947874258335829 0.8741005273708286\n",
      "Mask mean value:  tensor(0.7257, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9845  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.774e-04\n",
      "‖w_svm‖₂       : 6.812655159241315e-07\n",
      "‖alpha‖₁       : 0.3999999999999996\n",
      "scores min/max : -1.3000532917622582e-07 1.7595437800011518e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.173e-09\n",
      "‖w_svm‖₂       : 5.8537591151680086e-08\n",
      "‖alpha‖₁       : 0.43999999999999895\n",
      "scores min/max : -1.0657157850633651e-07 -2.4333875373885393e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.284e-20\n",
      "‖w_svm‖₂       : 0.0004907655905648931\n",
      "‖alpha‖₁       : 0.43999999999997763\n",
      "scores min/max : -0.0002772266085360892 -3.894606147221113e-05\n",
      "Mask mean value:  tensor(0.4988, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6514  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.718e-16\n",
      "‖w_svm‖₂       : 0.1602711223987686\n",
      "‖alpha‖₁       : 0.8799999999997589\n",
      "scores min/max : -1.8946046993298578 4.726720907570355\n",
      "Mask mean value:  tensor(0.1668, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2181  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.380e-03\n",
      "‖w_svm‖₂       : 2.3478648367032364e-07\n",
      "‖alpha‖₁       : 0.2599999999999999\n",
      "scores min/max : -3.010510687668919e-08 -1.738280673819095e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.910e-17\n",
      "‖w_svm‖₂       : 1.9546535689517436e-07\n",
      "‖alpha‖₁       : 0.3799999999999993\n",
      "scores min/max : -2.0829134408735094e-08 1.4857552600672233e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.542e-19\n",
      "‖w_svm‖₂       : 0.13407342270030623\n",
      "‖alpha‖₁       : 0.8729302152445148\n",
      "scores min/max : -12.22802329083361 2.1137577832115366\n",
      "Mask mean value:  tensor(0.4746, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.4007  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 9.791e-02\n",
      "‖w_svm‖₂       : 8.350742499462322e-08\n",
      "‖alpha‖₁       : 0.6599999999999943\n",
      "scores min/max : 2.0375681839729516e-08 1.6509837833325313e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.296e-08\n",
      "‖w_svm‖₂       : 0.025188112051743806\n",
      "‖alpha‖₁       : 0.8150833745178345\n",
      "scores min/max : -11.514599051852459 2.0399320572965407\n",
      "Mask mean value:  tensor(0.7288, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9371  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.498e-06\n",
      "‖w_svm‖₂       : 8.141525517277716e-08\n",
      "‖alpha‖₁       : 0.5399999999999631\n",
      "scores min/max : -2.7175781295225662e-08 -5.5512635220308745e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.562e-20\n",
      "‖w_svm‖₂       : 1.1264501521160428e-07\n",
      "‖alpha‖₁       : 0.2399999999999709\n",
      "scores min/max : 7.133964006107814e-09 1.8166436532239676e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.656e-21\n",
      "‖w_svm‖₂       : 3.7378820800583576e-06\n",
      "‖alpha‖₁       : 0.41999999999288506\n",
      "scores min/max : -4.846085035674714e-07 1.1405658238054806e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1230  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.610e-05\n",
      "‖w_svm‖₂       : 0.0003441492568593381\n",
      "‖alpha‖₁       : 0.7399999999999991\n",
      "scores min/max : -5.369599103349421e-05 -3.858848984833156e-05\n",
      "Mask mean value:  tensor(0.4998, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0638  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.950e-17\n",
      "‖w_svm‖₂       : 1.1345877655251785e-06\n",
      "‖alpha‖₁       : 0.4999999999999933\n",
      "scores min/max : -1.805599544659721e-07 1.2262212458524857e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.228e-19\n",
      "‖w_svm‖₂       : 2.4277602926040456e-07\n",
      "‖alpha‖₁       : 0.5799999999999959\n",
      "scores min/max : -9.004834265931696e-08 -7.44092412813185e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.706e-19\n",
      "‖w_svm‖₂       : 0.023468668729496163\n",
      "‖alpha‖₁       : 0.852127872556175\n",
      "scores min/max : -3.005380855031079 1.4762240439147827\n",
      "Mask mean value:  tensor(0.1253, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8789  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.671e-04\n",
      "‖w_svm‖₂       : 0.0008428339498381248\n",
      "‖alpha‖₁       : 0.8199999999999994\n",
      "scores min/max : -0.00021312840354292123 0.0014254514775110987\n",
      "Mask mean value:  tensor(0.5041, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5608  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.644e-17\n",
      "‖w_svm‖₂       : 1.4148497036784512e-07\n",
      "‖alpha‖₁       : 0.37999999999999634\n",
      "scores min/max : 1.1528119537958517e-09 1.6131721851725108e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.203e-06\n",
      "‖w_svm‖₂       : 0.004822804940794198\n",
      "‖alpha‖₁       : 0.700556978879278\n",
      "scores min/max : -2.007598276704648 0.015777901075245244\n",
      "Mask mean value:  tensor(0.4373, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.411e-05\n",
      "‖w_svm‖₂       : 0.05698414398614244\n",
      "‖alpha‖₁       : 0.9199999999999997\n",
      "scores min/max : -0.4167415663786534 0.8165633955211674\n",
      "Mask mean value:  tensor(0.5787, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.9352  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.666e-03\n",
      "‖w_svm‖₂       : 0.006072432555248709\n",
      "‖alpha‖₁       : 0.6076601863196253\n",
      "scores min/max : -1.9804395984238845 0.3065816586184134\n",
      "Mask mean value:  tensor(0.5994, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2424  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.465e-15\n",
      "‖w_svm‖₂       : 8.362752160102525e-08\n",
      "‖alpha‖₁       : 0.37999999999999984\n",
      "scores min/max : 1.6722623311770447e-08 3.486974569357774e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.579e-22\n",
      "‖w_svm‖₂       : 0.019914428681206662\n",
      "‖alpha‖₁       : 0.5962278990084633\n",
      "scores min/max : -2.93320992059414 2.204126085424072\n",
      "Mask mean value:  tensor(0.8694, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.4024  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.961e-02\n",
      "‖w_svm‖₂       : 0.0228685831560373\n",
      "‖alpha‖₁       : 0.86\n",
      "scores min/max : -0.047145965847672514 0.06514982362904516\n",
      "Mask mean value:  tensor(0.4902, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6478  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.323e-15\n",
      "‖w_svm‖₂       : 0.00012934748438693232\n",
      "‖alpha‖₁       : 0.43999999999991546\n",
      "scores min/max : -9.181288795024971e-05 3.0197326658445338e-05\n",
      "Mask mean value:  tensor(0.4999, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0283  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.417e-17\n",
      "‖w_svm‖₂       : 0.02570405846360566\n",
      "‖alpha‖₁       : 0.19648756954221228\n",
      "scores min/max : -2.1133354291422504 0.11247608284853071\n",
      "Mask mean value:  tensor(0.2289, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.4915  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.816e-07\n",
      "‖w_svm‖₂       : 0.0030569724932038617\n",
      "‖alpha‖₁       : 0.5799999999999973\n",
      "scores min/max : -0.0001528742374982458 0.0010406612157322498\n",
      "Mask mean value:  tensor(0.5040, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3204  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.325e-08\n",
      "‖w_svm‖₂       : 1.1896936902507846e-07\n",
      "‖alpha‖₁       : 0.29999999999990157\n",
      "scores min/max : 2.0194871361420457e-08 2.92300152085965e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.180e-07\n",
      "‖w_svm‖₂       : 7.188823014546596e-08\n",
      "‖alpha‖₁       : 0.42\n",
      "scores min/max : -5.911250308694831e-08 -1.1957219951035477e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.287e-08\n",
      "‖w_svm‖₂       : 0.12364329759196767\n",
      "‖alpha‖₁       : 0.6506075524102345\n",
      "scores min/max : -17.79772046518244 1.9964661298581072\n",
      "Mask mean value:  tensor(0.5378, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1403  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.221e-04\n",
      "‖w_svm‖₂       : 0.05794575537627239\n",
      "‖alpha‖₁       : 0.7293329247793312\n",
      "scores min/max : -0.23141637230084156 2.049120242411142\n",
      "Mask mean value:  tensor(0.7656, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.4164  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 7.715e-04\n",
      "‖w_svm‖₂       : 0.022809421025635504\n",
      "‖alpha‖₁       : 0.383334509150112\n",
      "scores min/max : -1.9764831741699362 0.22000601548808035\n",
      "Mask mean value:  tensor(0.6467, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.942e-14\n",
      "‖w_svm‖₂       : 9.097347146498627e-08\n",
      "‖alpha‖₁       : 0.17999999999999938\n",
      "scores min/max : 1.145203674808547e-09 2.3470382031820676e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.543e-21\n",
      "‖w_svm‖₂       : 0.059670182666418055\n",
      "‖alpha‖₁       : 0.6599999999999996\n",
      "scores min/max : -0.4799690705860452 0.36818730766845614\n",
      "Mask mean value:  tensor(0.5261, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8926  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.094e-05\n",
      "‖w_svm‖₂       : 0.07465287191741657\n",
      "‖alpha‖₁       : 0.5796873517837815\n",
      "scores min/max : -3.321505238765687 1.8543280905247452\n",
      "Mask mean value:  tensor(0.1205, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2536  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.539e-03\n",
      "‖w_svm‖₂       : 0.00020436532476398784\n",
      "‖alpha‖₁       : 0.6199999999991622\n",
      "scores min/max : 1.6039277667832886e-05 3.084059115923834e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.696e-17\n",
      "‖w_svm‖₂       : 0.004128808997188599\n",
      "‖alpha‖₁       : 0.4599999999999999\n",
      "scores min/max : -0.0036745189327476806 -0.0018981014601864438\n",
      "Mask mean value:  tensor(0.4884, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.1231  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.172e-16\n",
      "‖w_svm‖₂       : 1.1987827824947194e-06\n",
      "‖alpha‖₁       : 0.31999999999885304\n",
      "scores min/max : -5.244241115633696e-07 -4.097969032268023e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4899  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.565e-18\n",
      "‖w_svm‖₂       : 0.08213513255798778\n",
      "‖alpha‖₁       : 0.6577208222364144\n",
      "scores min/max : -1.978176813335191 3.7789317818275894\n",
      "Mask mean value:  tensor(0.4687, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.9738  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.445e-05\n",
      "‖w_svm‖₂       : 0.028088693065288127\n",
      "‖alpha‖₁       : 0.5509630581799279\n",
      "scores min/max : -3.595500429522301 0.978911392767683\n",
      "Mask mean value:  tensor(0.0998, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8484  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.037e-03\n",
      "‖w_svm‖₂       : 0.005010696735902956\n",
      "‖alpha‖₁       : 0.37999999999999967\n",
      "scores min/max : -0.008234523606999222 0.002854339740763866\n",
      "Mask mean value:  tensor(0.4998, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8899  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.640e-14\n",
      "‖w_svm‖₂       : 5.740049391740531e-08\n",
      "‖alpha‖₁       : 0.11999999999998863\n",
      "scores min/max : -3.5533395538450885e-08 -4.697381434298685e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.652e-08\n",
      "‖w_svm‖₂       : 0.03178670913518461\n",
      "‖alpha‖₁       : 0.898362032251461\n",
      "scores min/max : -0.8114610046725894 1.9499469575369335\n",
      "Mask mean value:  tensor(0.7282, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5075  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.379e-06\n",
      "‖w_svm‖₂       : 0.023947919584864856\n",
      "‖alpha‖₁       : 0.8233189770460694\n",
      "scores min/max : -1.8730194920796697 1.529079382703573\n",
      "Mask mean value:  tensor(0.7658, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.5526  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.536e-06\n",
      "‖w_svm‖₂       : 0.05548799692013003\n",
      "‖alpha‖₁       : 0.905654806988321\n",
      "scores min/max : -1.0341168172934476 1.7947393645903749\n",
      "Mask mean value:  tensor(0.2069, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3866  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.694e-03\n",
      "‖w_svm‖₂       : 0.029380040700328947\n",
      "‖alpha‖₁       : 0.6599999999999955\n",
      "scores min/max : -0.059891024481545235 0.1001881599799088\n",
      "Mask mean value:  tensor(0.4731, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.7829  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.279e-15\n",
      "‖w_svm‖₂       : 4.3371689352885026e-07\n",
      "‖alpha‖₁       : 0.7199999999999996\n",
      "scores min/max : 4.508759659235e-08 1.0226149261613201e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.739e-21\n",
      "‖w_svm‖₂       : 0.016895011219972507\n",
      "‖alpha‖₁       : 0.8599999999999999\n",
      "scores min/max : -0.03225983778738039 -0.013087316625097406\n",
      "Mask mean value:  tensor(0.4086, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0974  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.683e-15\n",
      "‖w_svm‖₂       : 7.430191937218903e-08\n",
      "‖alpha‖₁       : 0.13999999999999807\n",
      "scores min/max : -7.012021652811383e-09 -1.274024821130739e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.906e-08\n",
      "‖w_svm‖₂       : 0.175054856885244\n",
      "‖alpha‖₁       : 0.8777169008880756\n",
      "scores min/max : -3.5314146153548926 6.182962156361683\n",
      "Mask mean value:  tensor(0.1032, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1807  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.049e-05\n",
      "‖w_svm‖₂       : 2.101801419484661e-07\n",
      "‖alpha‖₁       : 0.4199999999999736\n",
      "scores min/max : -9.314992736028574e-08 -7.08300438212647e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.489e-19\n",
      "‖w_svm‖₂       : 0.050935002560592454\n",
      "‖alpha‖₁       : 0.8265869531828678\n",
      "scores min/max : -1.9773673016971915 0.3817187440307951\n",
      "Mask mean value:  tensor(0.4226, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0210  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.707e-02\n",
      "‖w_svm‖₂       : 2.4545263746266205e-07\n",
      "‖alpha‖₁       : 0.6399999999999956\n",
      "scores min/max : 7.572106391078195e-08 9.659630338130811e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.199e-20\n",
      "‖w_svm‖₂       : 2.9159229177248524e-07\n",
      "‖alpha‖₁       : 0.6599999999999968\n",
      "scores min/max : -2.4517857523972784e-07 -9.347703934221443e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.209e-18\n",
      "‖w_svm‖₂       : 0.07365115137778776\n",
      "‖alpha‖₁       : 0.470448371228672\n",
      "scores min/max : -2.0397115657195517 3.0398461429411117\n",
      "Mask mean value:  tensor(0.5060, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.8520  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.974e-05\n",
      "‖w_svm‖₂       : 0.07850099160957943\n",
      "‖alpha‖₁       : 0.5765502624409287\n",
      "scores min/max : -1.9563772566425373 5.988877680052175\n",
      "Mask mean value:  tensor(0.7155, dtype=torch.float64)\n",
      "max feasible return = 0.3320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.1416077465664874e-07\n",
      "‖alpha‖₁       : 0.5799999999999998\n",
      "scores min/max : -1.5664621060680935e-07 -6.591931038587182e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.1055013964823473e-07\n",
      "‖alpha‖₁       : 0.29999999999996824\n",
      "scores min/max : 2.137111858772461e-09 7.262513552962077e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 7.446450836353918e-08\n",
      "‖alpha‖₁       : 0.5999999999999994\n",
      "scores min/max : -1.2257941507881035e-08 2.447747541773692e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.975557801927549e-08\n",
      "‖alpha‖₁       : 0.37999999999999867\n",
      "scores min/max : 3.962661984405341e-10 9.781983765203442e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.2467487362538666e-06\n",
      "‖alpha‖₁       : 0.3199999999930168\n",
      "scores min/max : -4.784986905699107e-08 9.40021718445768e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4257  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.06726705436871495\n",
      "‖alpha‖₁       : 0.7396592633984956\n",
      "scores min/max : -3.4840023815413557 2.1272042178715584\n",
      "Mask mean value:  tensor(0.8579, dtype=torch.float64)\n",
      "max feasible return = -0.9046  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03357072234280739\n",
      "‖alpha‖₁       : 0.6149479737191499\n",
      "scores min/max : -1.8836035950446515 1.2465012072476083\n",
      "Mask mean value:  tensor(0.8422, dtype=torch.float64)\n",
      "max feasible return = 2.9692  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.835841844932836e-07\n",
      "‖alpha‖₁       : 0.4599999999999925\n",
      "scores min/max : -3.866304938615402e-08 5.558315785880808e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.3895780105687992e-07\n",
      "‖alpha‖₁       : 0.5199999999999882\n",
      "scores min/max : 1.24692943310018e-08 2.5369636846390027e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.62690242473429e-07\n",
      "‖alpha‖₁       : 0.5799999999999987\n",
      "scores min/max : -8.086641499453704e-08 -4.472471741768025e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.042536971407540605\n",
      "‖alpha‖₁       : 0.6873126295173559\n",
      "scores min/max : -0.36368077291514084 2.0286915126707834\n",
      "Mask mean value:  tensor(0.6746, dtype=torch.float64)\n",
      "max feasible return = -0.1906  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03587900045366037\n",
      "‖alpha‖₁       : 0.42209493696788514\n",
      "scores min/max : -3.9409248774452417 5.150717083416824\n",
      "Mask mean value:  tensor(0.0643, dtype=torch.float64)\n",
      "max feasible return = 0.4065  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.0497122544273473\n",
      "‖alpha‖₁       : 0.8211608341141046\n",
      "scores min/max : -5.110667451710647 3.0452653897537827\n",
      "Mask mean value:  tensor(0.9360, dtype=torch.float64)\n",
      "max feasible return = -3.5225  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.010556013795924583\n",
      "‖alpha‖₁       : 0.7999999999999896\n",
      "scores min/max : -0.05676470466019962 0.015388162281693402\n",
      "Mask mean value:  tensor(0.4858, dtype=torch.float64)\n",
      "max feasible return = -0.1768  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.2413871804645165e-07\n",
      "‖alpha‖₁       : 0.6199999999999813\n",
      "scores min/max : -2.5990664945626163e-08 -9.521023135286038e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.766209136123665e-08\n",
      "‖alpha‖₁       : 0.43999999999999995\n",
      "scores min/max : 1.9756147454765788e-09 1.1545328902392406e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.3738966550832704e-07\n",
      "‖alpha‖₁       : 0.2799999999999709\n",
      "scores min/max : -1.1795996271090927e-07 -9.99873570996435e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch   6 | train 0.005569 | val 0.006811\n",
      "-----------------------------------------Epoch:  7 ----------------------------------------\n",
      "‖w_svm‖₂       : 0.05571806487140686\n",
      "‖alpha‖₁       : 0.8977251673590717\n",
      "scores min/max : -1.754256952009976 3.6462480422398538\n",
      "Mask mean value:  tensor(0.3227, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4891  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.360e-03\n",
      "‖w_svm‖₂       : 0.04426400585966099\n",
      "‖alpha‖₁       : 0.9401646535674922\n",
      "scores min/max : -1.8243251560941047 0.31270030560957734\n",
      "Mask mean value:  tensor(0.4462, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3519  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.855e-10\n",
      "‖w_svm‖₂       : 5.736224064709948e-08\n",
      "‖alpha‖₁       : 0.11999999999998863\n",
      "scores min/max : -3.566931913128512e-08 -4.832527221816362e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.129e-07\n",
      "‖w_svm‖₂       : 0.019221259083461336\n",
      "‖alpha‖₁       : 0.6814832136667666\n",
      "scores min/max : -1.9870065145210594 0.05049107482924607\n",
      "Mask mean value:  tensor(0.5422, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2052  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.976e-14\n",
      "‖w_svm‖₂       : 0.054484049084583944\n",
      "‖alpha‖₁       : 0.9056535532036367\n",
      "scores min/max : -0.9968106383774451 1.805017443867218\n",
      "Mask mean value:  tensor(0.2188, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4082  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.099e-03\n",
      "‖w_svm‖₂       : 0.024049426281239426\n",
      "‖alpha‖₁       : 0.8233252036932145\n",
      "scores min/max : -1.873237205464825 1.5289082598719794\n",
      "Mask mean value:  tensor(0.7654, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.5518  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.579e-06\n",
      "‖w_svm‖₂       : 0.00020473119345396738\n",
      "‖alpha‖₁       : 0.619999999998566\n",
      "scores min/max : 1.5206976420746086e-05 3.006303322478333e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.720e-17\n",
      "‖w_svm‖₂       : 1.1593472753362793e-07\n",
      "‖alpha‖₁       : 0.5199999999999995\n",
      "scores min/max : 8.721105396389322e-08 1.0861587387778889e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.210e-19\n",
      "‖w_svm‖₂       : 0.12596462223103186\n",
      "‖alpha‖₁       : 0.6511780731836977\n",
      "scores min/max : -17.825659622096772 1.9671243773571956\n",
      "Mask mean value:  tensor(0.4766, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1748  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.198e-03\n",
      "‖w_svm‖₂       : 0.02375415733476038\n",
      "‖alpha‖₁       : 0.852140796552092\n",
      "scores min/max : -2.9966575600125425 1.4849324837695155\n",
      "Mask mean value:  tensor(0.1276, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8890  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.503e-04\n",
      "‖w_svm‖₂       : 0.02278391412794515\n",
      "‖alpha‖₁       : 0.3833342198312675\n",
      "scores min/max : -1.976843406923903 0.21966412825022763\n",
      "Mask mean value:  tensor(0.6452, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1340  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.008e-13\n",
      "‖w_svm‖₂       : 1.1863610867957564e-07\n",
      "‖alpha‖₁       : 0.2999999999998926\n",
      "scores min/max : 2.1239444842601835e-08 3.028514999429345e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.174e-07\n",
      "‖w_svm‖₂       : 0.0001304450703881676\n",
      "‖alpha‖₁       : 0.43999999999325756\n",
      "scores min/max : -9.575007265317213e-05 2.820709902890718e-05\n",
      "Mask mean value:  tensor(0.4999, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0282  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.428e-17\n",
      "‖w_svm‖₂       : 9.125453837877095e-08\n",
      "‖alpha‖₁       : 0.17999999999999886\n",
      "scores min/max : 4.380999742700449e-09 2.3826655605717166e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.566e-21\n",
      "‖w_svm‖₂       : 3.9895666073886427e-07\n",
      "‖alpha‖₁       : 0.27999999999999686\n",
      "scores min/max : -6.022766544668586e-07 -5.40561012178808e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.085e-19\n",
      "‖w_svm‖₂       : 0.03213343733864467\n",
      "‖alpha‖₁       : 0.8983838719209294\n",
      "scores min/max : -0.8092089409389928 1.9520839802753893\n",
      "Mask mean value:  tensor(0.7330, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5040  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.063e-06\n",
      "‖w_svm‖₂       : 0.004889808811014959\n",
      "‖alpha‖₁       : 0.7005576329423255\n",
      "scores min/max : -2.007670004663576 0.015704326362016383\n",
      "Mask mean value:  tensor(0.4370, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2457  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.604e-05\n",
      "‖w_svm‖₂       : 0.005001946358837225\n",
      "‖alpha‖₁       : 0.3799999999999981\n",
      "scores min/max : -0.007419505773188818 0.0036323372564871052\n",
      "Mask mean value:  tensor(0.5038, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8970  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.549e-14\n",
      "‖w_svm‖₂       : 0.07475496324223874\n",
      "‖alpha‖₁       : 0.4706239129143251\n",
      "scores min/max : -2.0689392100146446 3.00935445563396\n",
      "Mask mean value:  tensor(0.4186, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0832  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.621e-04\n",
      "‖w_svm‖₂       : 5.7037638026090284e-08\n",
      "‖alpha‖₁       : 0.2399999999999729\n",
      "scores min/max : -2.0227591976979375e-08 -5.643674439567559e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.107e-20\n",
      "‖w_svm‖₂       : 0.08145621471251357\n",
      "‖alpha‖₁       : 0.4183295543479107\n",
      "scores min/max : -1.6960043709426766 3.64145915830377\n",
      "Mask mean value:  tensor(0.8707, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8659  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.435e-11\n",
      "‖w_svm‖₂       : 1.3996664525684533e-07\n",
      "‖alpha‖₁       : 0.37999999999999856\n",
      "scores min/max : 7.926040428822593e-09 2.2888741501869616e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.331e-07\n",
      "‖w_svm‖₂       : 0.006082572452038904\n",
      "‖alpha‖₁       : 0.6076603123353204\n",
      "scores min/max : -1.9804885929487597 0.3065326264020431\n",
      "Mask mean value:  tensor(0.5992, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2423  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.502e-15\n",
      "‖w_svm‖₂       : 0.13334121001714058\n",
      "‖alpha‖₁       : 0.7556792778612363\n",
      "scores min/max : -1.707482424756215 2.2706509392005523\n",
      "Mask mean value:  tensor(0.9127, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3498  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.538e-12\n",
      "‖w_svm‖₂       : 8.331314749308397e-08\n",
      "‖alpha‖₁       : 0.3799999999999997\n",
      "scores min/max : 1.8465992677066024e-08 3.661721743440802e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.559e-22\n",
      "‖w_svm‖₂       : 2.3048675847700206e-08\n",
      "‖alpha‖₁       : 0.1199999999999973\n",
      "scores min/max : -1.0431943291619873e-08 4.081551402533564e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.869e-09\n",
      "‖w_svm‖₂       : 0.04759336890404825\n",
      "‖alpha‖₁       : 0.938910103713221\n",
      "scores min/max : -2.460744781399604 1.5755354353358944\n",
      "Mask mean value:  tensor(0.1407, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3817  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.701e-03\n",
      "‖w_svm‖₂       : 0.17772729565177806\n",
      "‖alpha‖₁       : 0.8786659070158336\n",
      "scores min/max : -3.5273649373260274 6.154335207478392\n",
      "Mask mean value:  tensor(0.1026, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1788  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.511e-03\n",
      "‖w_svm‖₂       : 4.331919027184637e-07\n",
      "‖alpha‖₁       : 0.7199999999999993\n",
      "scores min/max : 4.8271229714580234e-08 1.0544933911694221e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.745e-21\n",
      "‖w_svm‖₂       : 4.842420858884685e-08\n",
      "‖alpha‖₁       : 0.17999999999999852\n",
      "scores min/max : 2.341694897172734e-08 3.805238513614625e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 4.661e-08\n",
      "‖w_svm‖₂       : 0.05721359062659153\n",
      "‖alpha‖₁       : 0.5757036538014538\n",
      "scores min/max : -1.9488946882670852 0.8729496818361149\n",
      "Mask mean value:  tensor(0.7223, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9746  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.848e-04\n",
      "‖w_svm‖₂       : 0.07670941311325132\n",
      "‖alpha‖₁       : 0.5799776342834786\n",
      "scores min/max : -3.3207977013076726 1.8547049785995617\n",
      "Mask mean value:  tensor(0.1211, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2549  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.405e-03\n",
      "‖w_svm‖₂       : 8.079965269965582e-08\n",
      "‖alpha‖₁       : 0.5399999999999681\n",
      "scores min/max : -3.2515136588489495e-08 -1.089463044425859e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.555e-20\n",
      "‖w_svm‖₂       : 2.4156424272203607e-07\n",
      "‖alpha‖₁       : 0.5799999999999937\n",
      "scores min/max : -1.1314844148684209e-07 -9.749701570761983e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.714e-19\n",
      "‖w_svm‖₂       : 2.914488908440068e-07\n",
      "‖alpha‖₁       : 0.6599999999999993\n",
      "scores min/max : -2.411278775087675e-07 -8.94721636693703e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.209e-18\n",
      "‖w_svm‖₂       : 2.332149565785979e-07\n",
      "‖alpha‖₁       : 0.25999999999999995\n",
      "scores min/max : -1.2589718039318811e-08 1.2594415660993923e-10\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.875e-17\n",
      "‖w_svm‖₂       : 0.05174736129254502\n",
      "‖alpha‖₁       : 0.826679715836551\n",
      "scores min/max : -1.9703352611792166 0.38894854505028253\n",
      "Mask mean value:  tensor(0.4476, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0181  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.689e-02\n",
      "‖w_svm‖₂       : 0.016873397194822844\n",
      "‖alpha‖₁       : 0.8599999999999953\n",
      "scores min/max : -0.0341261857989415 -0.014969921550592333\n",
      "Mask mean value:  tensor(0.3995, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0948  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.136e-15\n",
      "‖w_svm‖₂       : 0.028145555115721487\n",
      "‖alpha‖₁       : 0.5509660816126543\n",
      "scores min/max : -3.5925325921357683 0.9818742793453713\n",
      "Mask mean value:  tensor(0.1008, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8575  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.114e-03\n",
      "‖w_svm‖₂       : 8.328201975500801e-08\n",
      "‖alpha‖₁       : 0.6599999999999933\n",
      "scores min/max : 2.4447950423494718e-08 1.6908593372315264e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.493e-09\n",
      "‖w_svm‖₂       : 0.0004962127644398207\n",
      "‖alpha‖₁       : 0.44\n",
      "scores min/max : -0.00035819307025379045 -0.00011459440060664788\n",
      "Mask mean value:  tensor(0.4984, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6501  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.757e-16\n",
      "‖w_svm‖₂       : 7.424963815173145e-08\n",
      "‖alpha‖₁       : 0.13999999999999804\n",
      "scores min/max : -6.765809437488942e-09 -1.0274648097566358e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.895e-08\n",
      "‖w_svm‖₂       : 1.4710366092048784e-05\n",
      "‖alpha‖₁       : 0.35999999999549226\n",
      "scores min/max : 4.29785843103619e-06 5.145386729548437e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3078  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.671e-17\n",
      "‖w_svm‖₂       : 0.08312642289223894\n",
      "‖alpha‖₁       : 0.6578597572252052\n",
      "scores min/max : -1.9964615633701972 3.750893135705069\n",
      "Mask mean value:  tensor(0.4257, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.6422  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.154e-04\n",
      "‖w_svm‖₂       : 3.732703372765279e-06\n",
      "‖alpha‖₁       : 0.41999999999281185\n",
      "scores min/max : -3.181174103872779e-07 1.3188991287880822e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1230  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.608e-05\n",
      "‖w_svm‖₂       : 0.056927653419844695\n",
      "‖alpha‖₁       : 0.9199999999999964\n",
      "scores min/max : -0.41706871283262203 0.8137238985236129\n",
      "Mask mean value:  tensor(0.5735, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.9069  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.522e-03\n",
      "‖w_svm‖₂       : 0.00018130110266365306\n",
      "‖alpha‖₁       : 0.8199999999999973\n",
      "scores min/max : -3.1684378356241144e-06 1.0591630451316567e-05\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6980  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.129e-17\n",
      "‖w_svm‖₂       : 0.04302841477370283\n",
      "‖alpha‖₁       : 0.8897502431455744\n",
      "scores min/max : -2.007797489650019 0.3391979630280643\n",
      "Mask mean value:  tensor(0.2705, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0826  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.007e-13\n",
      "‖w_svm‖₂       : 0.058086964792516155\n",
      "‖alpha‖₁       : 0.7293439520215232\n",
      "scores min/max : -0.2333613683164663 2.047133592346468\n",
      "Mask mean value:  tensor(0.7608, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3934  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 8.031e-04\n",
      "‖w_svm‖₂       : 1.0686890982157764e-07\n",
      "‖alpha‖₁       : 0.2399999999999996\n",
      "scores min/max : 1.1879549412534668e-08 2.2611454794116293e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.396e-21\n",
      "‖w_svm‖₂       : 0.004122521345045567\n",
      "‖alpha‖₁       : 0.45999999999999996\n",
      "scores min/max : -0.0038269282983277382 -0.002056776888752599\n",
      "Mask mean value:  tensor(0.4876, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.1197  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.203e-16\n",
      "‖w_svm‖₂       : 1.0572281477982732e-06\n",
      "‖alpha‖₁       : 0.5999999999999932\n",
      "scores min/max : -2.8987580920341802e-09 4.802214125291116e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.500e-19\n",
      "‖w_svm‖₂       : 0.025236330157114583\n",
      "‖alpha‖₁       : 0.8150847903826905\n",
      "scores min/max : -11.518547892766575 2.036432923789964\n",
      "Mask mean value:  tensor(0.7213, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9193  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.836e-06\n",
      "‖w_svm‖₂       : 0.022861206567442515\n",
      "‖alpha‖₁       : 0.8600000000000001\n",
      "scores min/max : -0.04705947270836941 0.06510537350385784\n",
      "Mask mean value:  tensor(0.4904, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6479  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.326e-15\n",
      "‖w_svm‖₂       : 1.778338227834775e-07\n",
      "‖alpha‖₁       : 0.23999999999998853\n",
      "scores min/max : 8.917535761238354e-08 1.0461211744968909e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.696e-20\n",
      "‖w_svm‖₂       : 2.4553673084662204e-07\n",
      "‖alpha‖₁       : 0.6399999999999956\n",
      "scores min/max : 7.640417373627197e-08 9.728066183815694e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.201e-20\n",
      "‖w_svm‖₂       : 0.000300291798394707\n",
      "‖alpha‖₁       : 0.41999999998308357\n",
      "scores min/max : -0.00016921884974320067 0.00013616075074273872\n",
      "Mask mean value:  tensor(0.5005, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9966  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.593e-15\n",
      "‖w_svm‖₂       : 1.9391958449674134e-07\n",
      "‖alpha‖₁       : 0.3799999999999993\n",
      "scores min/max : -4.192319322064579e-09 3.149416044669688e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.511e-19\n",
      "‖w_svm‖₂       : 0.00015524345571357837\n",
      "‖alpha‖₁       : 0.6399999999999998\n",
      "scores min/max : -3.4011906606209664e-05 -2.481471501703867e-05\n",
      "Mask mean value:  tensor(0.4999, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7642  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.049e-17\n",
      "‖w_svm‖₂       : 5.82086195636094e-08\n",
      "‖alpha‖₁       : 0.439999999999999\n",
      "scores min/max : -1.0687784579542557e-07 -2.7221660062055845e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.283e-20\n",
      "‖w_svm‖₂       : 0.059634548274707\n",
      "‖alpha‖₁       : 0.6599999999999995\n",
      "scores min/max : -0.48016476499173644 0.3670581153423456\n",
      "Mask mean value:  tensor(0.5229, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8865  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.205e-05\n",
      "‖w_svm‖₂       : 2.1029435691958872e-07\n",
      "‖alpha‖₁       : 0.4199999999999734\n",
      "scores min/max : -8.004197139234608e-08 -5.771918305511333e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.502e-19\n",
      "‖w_svm‖₂       : 0.019171708558400145\n",
      "‖alpha‖₁       : 0.7799999999999829\n",
      "scores min/max : -0.003568087746224583 0.07048627126388143\n",
      "Mask mean value:  tensor(0.5138, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.2323  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.227e-15\n",
      "‖w_svm‖₂       : 0.15405154836205162\n",
      "‖alpha‖₁       : 0.8799999999999998\n",
      "scores min/max : -1.7306588351759933 4.404328378954375\n",
      "Mask mean value:  tensor(0.2097, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2399  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.376e-03\n",
      "‖w_svm‖₂       : 1.1971079046522692e-06\n",
      "‖alpha‖₁       : 0.3199999999988544\n",
      "scores min/max : -5.123601692197968e-07 -3.977370882397952e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4899  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.576e-18\n",
      "‖w_svm‖₂       : 0.0008480753876698305\n",
      "‖alpha‖₁       : 0.8199999999999993\n",
      "scores min/max : -0.00012842074344908503 0.0015262017662596894\n",
      "Mask mean value:  tensor(0.5046, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5632  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.699e-17\n",
      "‖w_svm‖₂       : 1.1270483073512785e-06\n",
      "‖alpha‖₁       : 0.4999999999999923\n",
      "scores min/max : -2.0834350730488786e-07 -1.547366608383223e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.173e-19\n",
      "‖w_svm‖₂       : 7.168775422934932e-08\n",
      "‖alpha‖₁       : 0.41999999999999993\n",
      "scores min/max : -6.260158657800427e-08 -1.549904017246697e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.319e-08\n",
      "‖w_svm‖₂       : 0.003058674366390925\n",
      "‖alpha‖₁       : 0.5799999999999966\n",
      "scores min/max : 0.0005092032302583889 0.0017036495646175973\n",
      "Mask mean value:  tensor(0.5073, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3226  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.043e-12\n",
      "‖w_svm‖₂       : 0.019903213495243894\n",
      "‖alpha‖₁       : 0.5962273618773081\n",
      "scores min/max : -2.9290507791611122 2.208478084902421\n",
      "Mask mean value:  tensor(0.8734, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.4138  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.927e-02\n",
      "‖w_svm‖₂       : 0.0003469394660759953\n",
      "‖alpha‖₁       : 0.7399999999999992\n",
      "scores min/max : -6.961826149960159e-05 -5.426490551721528e-05\n",
      "Mask mean value:  tensor(0.4997, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0637  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.959e-17\n",
      "‖w_svm‖₂       : 0.13323442657541143\n",
      "‖alpha‖₁       : 0.8729666911107062\n",
      "scores min/max : -12.258847875566369 2.08662256113375\n",
      "Mask mean value:  tensor(0.4079, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2172  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 9.238e-02\n",
      "‖w_svm‖₂       : 0.02942069271113313\n",
      "‖alpha‖₁       : 0.6599999999999966\n",
      "scores min/max : -0.05834494024170199 0.10197204640278905\n",
      "Mask mean value:  tensor(0.4813, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.8447  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.260e-15\n",
      "‖w_svm‖₂       : 0.005503561317168212\n",
      "‖alpha‖₁       : 0.5599999999999992\n",
      "scores min/max : 0.002121666478241019 0.003689852180296944\n",
      "Mask mean value:  tensor(0.5120, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0733  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.677e-04\n",
      "‖w_svm‖₂       : 0.02622416609898079\n",
      "‖alpha‖₁       : 0.19652043082630502\n",
      "scores min/max : -2.128247161765677 0.09753048672108458\n",
      "Mask mean value:  tensor(0.1862, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.2201  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.537e-06\n",
      "‖w_svm‖₂       : 6.784497760786325e-07\n",
      "‖alpha‖₁       : 0.4000000000000001\n",
      "scores min/max : -1.251904611466359e-07 1.8122179323712076e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.179e-09\n",
      "‖w_svm‖₂       : 0.08025516571795369\n",
      "‖alpha‖₁       : 0.576867160706015\n",
      "scores min/max : -1.989302069433598 5.953379900497738\n",
      "Mask mean value:  tensor(0.6193, dtype=torch.float64)\n",
      "max feasible return = 0.2558  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.1272168846112144e-07\n",
      "‖alpha‖₁       : 0.5799999999999998\n",
      "scores min/max : -1.464875278311523e-07 -5.5764172110127793e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0593452825627001e-07\n",
      "‖alpha‖₁       : 0.2999999999999998\n",
      "scores min/max : 2.741188869032633e-09 7.704184616964e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 7.405447718214569e-08\n",
      "‖alpha‖₁       : 0.5999999999999996\n",
      "scores min/max : -1.507726398144032e-08 2.1657273419164026e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.950841013654651e-08\n",
      "‖alpha‖₁       : 0.3799999999999986\n",
      "scores min/max : -1.8195707645379478e-09 7.566412163844298e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.083439088509974e-06\n",
      "‖alpha‖₁       : 0.3199999999766093\n",
      "scores min/max : -1.2030545161169237e-07 1.611549991408549e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4257  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.0672232538487363\n",
      "‖alpha‖₁       : 0.7396696153969444\n",
      "scores min/max : -3.472835743292196 2.138696804333312\n",
      "Mask mean value:  tensor(0.8693, dtype=torch.float64)\n",
      "max feasible return = -0.9144  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03365798696287964\n",
      "‖alpha‖₁       : 0.6149527615079474\n",
      "scores min/max : -1.8879019051341894 1.2421760900142238\n",
      "Mask mean value:  tensor(0.8338, dtype=torch.float64)\n",
      "max feasible return = 2.9375  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.843654255336817e-07\n",
      "‖alpha‖₁       : 0.4599999999999422\n",
      "scores min/max : 1.6858038081226817e-09 5.978179075269279e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.391001136774736e-07\n",
      "‖alpha‖₁       : 0.5199999999999875\n",
      "scores min/max : 9.742346109857798e-09 2.264282199290223e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.625403649200004e-07\n",
      "‖alpha‖₁       : 0.579999999999999\n",
      "scores min/max : -9.297632806002112e-08 -5.683371907141355e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.04357347942821191\n",
      "‖alpha‖₁       : 0.6873873728196457\n",
      "scores min/max : -0.3760380937700965 2.016445979522014\n",
      "Mask mean value:  tensor(0.6283, dtype=torch.float64)\n",
      "max feasible return = -0.1525  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03592823338708298\n",
      "‖alpha‖₁       : 0.4221013032070643\n",
      "scores min/max : -3.936987111026469 5.15510656168741\n",
      "Mask mean value:  tensor(0.0653, dtype=torch.float64)\n",
      "max feasible return = 0.4130  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.049787432086252505\n",
      "‖alpha‖₁       : 0.8211625955610471\n",
      "scores min/max : -5.111377254244632 3.0445186975235643\n",
      "Mask mean value:  tensor(0.9359, dtype=torch.float64)\n",
      "max feasible return = -3.5215  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.010525113138448894\n",
      "‖alpha‖₁       : 0.7999999999999772\n",
      "scores min/max : -0.056822873691527366 0.01479152060476579\n",
      "Mask mean value:  tensor(0.4835, dtype=torch.float64)\n",
      "max feasible return = -0.1760  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.234172802712187e-07\n",
      "‖alpha‖₁       : 0.619999999999981\n",
      "scores min/max : -1.2503156022927911e-08 3.968038116871017e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.7672566844052195e-08\n",
      "‖alpha‖₁       : 0.4399999999999611\n",
      "scores min/max : 2.934334584030394e-09 1.251707653649626e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.3718031909674975e-07\n",
      "‖alpha‖₁       : 0.2799999999999717\n",
      "scores min/max : -1.391424924788287e-07 -1.211748111267423e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch   7 | train 0.005553 | val 0.006932\n",
      "-----------------------------------------Epoch:  8 ----------------------------------------\n",
      "‖w_svm‖₂       : 2.419205752475206e-07\n",
      "‖alpha‖₁       : 0.5799999999999879\n",
      "scores min/max : -1.4932168872482758e-07 -1.3367632812515778e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.751e-19\n",
      "‖w_svm‖₂       : 0.00030455341027559675\n",
      "‖alpha‖₁       : 0.4199999995599819\n",
      "scores min/max : -0.0001301183493192752 0.0001798396214820151\n",
      "Mask mean value:  tensor(0.5008, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9970  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.623e-15\n",
      "‖w_svm‖₂       : 0.08304244630939166\n",
      "‖alpha‖₁       : 0.6578609660069978\n",
      "scores min/max : -1.9989139459084733 3.7536118963486973\n",
      "Mask mean value:  tensor(0.4203, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.6006  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.142e-04\n",
      "‖w_svm‖₂       : 2.0933195994214024e-07\n",
      "‖alpha‖₁       : 0.41999999999997406\n",
      "scores min/max : -7.556817221010031e-08 -5.3248855467810135e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.487e-19\n",
      "‖w_svm‖₂       : 0.15371962075989798\n",
      "‖alpha‖₁       : 0.8799999999999999\n",
      "scores min/max : -1.7173038868847783 4.391522154937456\n",
      "Mask mean value:  tensor(0.2201, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2456  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.111e-03\n",
      "‖w_svm‖₂       : 3.9792376919038534e-07\n",
      "‖alpha‖₁       : 0.2799999999999947\n",
      "scores min/max : -6.369696606442162e-07 -5.752198330455618e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.099e-19\n",
      "‖w_svm‖₂       : 5.740966666314722e-08\n",
      "‖alpha‖₁       : 0.23999999999998373\n",
      "scores min/max : -2.7092608151129442e-08 -1.2575586719275153e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.139e-20\n",
      "‖w_svm‖₂       : 0.004955366272587076\n",
      "‖alpha‖₁       : 0.7005582318768945\n",
      "scores min/max : -2.0067880398737925 0.01658354581142447\n",
      "Mask mean value:  tensor(0.4412, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2480  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.836e-05\n",
      "‖w_svm‖₂       : 1.763450085600798e-07\n",
      "‖alpha‖₁       : 0.2399999999999941\n",
      "scores min/max : 1.1350450264752183e-07 1.2883007778819707e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.706e-20\n",
      "‖w_svm‖₂       : 0.0030611706368153663\n",
      "‖alpha‖₁       : 0.5799999999999997\n",
      "scores min/max : 0.00029379340713949615 0.0014870617898118388\n",
      "Mask mean value:  tensor(0.5062, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3219  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.543e-12\n",
      "‖w_svm‖₂       : 0.022742666226726077\n",
      "‖alpha‖₁       : 0.3833342261845741\n",
      "scores min/max : -1.975011420785208 0.22159260308974438\n",
      "Mask mean value:  tensor(0.6528, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1354  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.485e-14\n",
      "‖w_svm‖₂       : 0.0574777814620335\n",
      "‖alpha‖₁       : 0.5757253552655768\n",
      "scores min/max : -1.9527341400504312 0.8691640145223475\n",
      "Mask mean value:  tensor(0.7106, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9415  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.442e-04\n",
      "‖w_svm‖₂       : 0.13216434771338792\n",
      "‖alpha‖₁       : 0.8729849064079513\n",
      "scores min/max : -12.228945608490587 2.1237494343294814\n",
      "Mask mean value:  tensor(0.4907, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.4463  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.165e-01\n",
      "‖w_svm‖₂       : 1.1230687687443978e-06\n",
      "‖alpha‖₁       : 0.49999999999999256\n",
      "scores min/max : -2.05680338528448e-07 -1.2775708789409873e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.149e-19\n",
      "‖w_svm‖₂       : 0.00020480650481977694\n",
      "‖alpha‖₁       : 0.6199999999985086\n",
      "scores min/max : 2.079100090376289e-05 3.565640782568027e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.742e-17\n",
      "‖w_svm‖₂       : 0.07552182888370833\n",
      "‖alpha‖₁       : 0.47074422960472884\n",
      "scores min/max : -2.102500074465253 2.9755552596432104\n",
      "Mask mean value:  tensor(0.3319, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.3175  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.228e-04\n",
      "‖w_svm‖₂       : 0.13473711263821242\n",
      "‖alpha‖₁       : 0.756067619565208\n",
      "scores min/max : -1.7055069492659378 2.272893283769616\n",
      "Mask mean value:  tensor(0.9143, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3518  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.440e-12\n",
      "‖w_svm‖₂       : 5.761831375803589e-08\n",
      "‖alpha‖₁       : 0.43999999999999884\n",
      "scores min/max : -1.0527629977374863e-07 -1.1255086799399968e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.239e-20\n",
      "‖w_svm‖₂       : 0.00013090647368681823\n",
      "‖alpha‖₁       : 0.43999999999148254\n",
      "scores min/max : -0.00010237654673927912 2.2486611490841416e-05\n",
      "Mask mean value:  tensor(0.4999, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0280  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.436e-17\n",
      "‖w_svm‖₂       : 0.08142356754806215\n",
      "‖alpha‖₁       : 0.41849767002417987\n",
      "scores min/max : -1.710092772654759 3.4513416473236163\n",
      "Mask mean value:  tensor(0.8680, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8676  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.110e-09\n",
      "‖w_svm‖₂       : 0.05196737607002798\n",
      "‖alpha‖₁       : 0.826702002811143\n",
      "scores min/max : -1.9630135365885595 0.39610961362594277\n",
      "Mask mean value:  tensor(0.4738, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0147  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.628e-02\n",
      "‖w_svm‖₂       : 0.02496086756144892\n",
      "‖alpha‖₁       : 0.8150854220799375\n",
      "scores min/max : -11.51989401785666 2.0375573141215493\n",
      "Mask mean value:  tensor(0.7229, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9247  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.256e-06\n",
      "‖w_svm‖₂       : 0.005015147174584601\n",
      "‖alpha‖₁       : 0.38\n",
      "scores min/max : -0.007348369385740509 0.003274463619693772\n",
      "Mask mean value:  tensor(0.5026, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8952  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.587e-14\n",
      "‖w_svm‖₂       : 0.042939873425458346\n",
      "‖alpha‖₁       : 0.8897691943337418\n",
      "scores min/max : -2.0066481677087276 0.3404227461645019\n",
      "Mask mean value:  tensor(0.2737, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0832  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.303e-10\n",
      "‖w_svm‖₂       : 1.1241339340587391e-07\n",
      "‖alpha‖₁       : 0.23999999999996752\n",
      "scores min/max : 8.085151191072211e-09 1.9131937338621477e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.620e-21\n",
      "‖w_svm‖₂       : 0.016702365117196798\n",
      "‖alpha‖₁       : 0.8599999999999994\n",
      "scores min/max : -0.03317591602485093 -0.01472169446960947\n",
      "Mask mean value:  tensor(0.4018, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0968  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.930e-15\n",
      "‖w_svm‖₂       : 0.02620582587159455\n",
      "‖alpha‖₁       : 0.19652598706523128\n",
      "scores min/max : -2.1311083592580045 0.09440306501856997\n",
      "Mask mean value:  tensor(0.1787, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1721  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.987e-06\n",
      "‖w_svm‖₂       : 2.9191750832925287e-07\n",
      "‖alpha‖₁       : 0.6599999999999986\n",
      "scores min/max : -2.776646086706564e-07 -1.2597920557741246e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.231e-18\n",
      "‖w_svm‖₂       : 2.3403836445764933e-07\n",
      "‖alpha‖₁       : 0.25999999999999984\n",
      "scores min/max : -3.23385470842391e-08 -1.9615985232691922e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.869e-17\n",
      "‖w_svm‖₂       : 0.058856957000188885\n",
      "‖alpha‖₁       : 0.6599999999999898\n",
      "scores min/max : -0.4657350920716224 0.3589332398815517\n",
      "Mask mean value:  tensor(0.5293, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9031  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.961e-14\n",
      "‖w_svm‖₂       : 0.01974052172728746\n",
      "‖alpha‖₁       : 0.5962283435810345\n",
      "scores min/max : -2.9261769505289204 2.216009273008201\n",
      "Mask mean value:  tensor(0.8800, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.4320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.746e-02\n",
      "‖w_svm‖₂       : 1.2138430694988387e-06\n",
      "‖alpha‖₁       : 0.319999999998669\n",
      "scores min/max : -6.142581615200091e-07 -4.984271299489365e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4899  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.608e-18\n",
      "‖w_svm‖₂       : 1.9556644863815963e-07\n",
      "‖alpha‖₁       : 0.3799999999999993\n",
      "scores min/max : -1.4512699420365811e-08 2.1174170075589283e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.575e-19\n",
      "‖w_svm‖₂       : 0.05612863738048782\n",
      "‖alpha‖₁       : 0.9199999999999928\n",
      "scores min/max : -0.40717695626199557 0.7896241265684508\n",
      "Mask mean value:  tensor(0.5649, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.8617  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.643e-02\n",
      "‖w_svm‖₂       : 1.054845719229419e-06\n",
      "‖alpha‖₁       : 0.599999999999993\n",
      "scores min/max : 7.199570159628296e-09 5.8126270930546055e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.489e-19\n",
      "‖w_svm‖₂       : 0.028298921885140972\n",
      "‖alpha‖₁       : 0.550981929896725\n",
      "scores min/max : -3.5884301978194744 0.9856954029637235\n",
      "Mask mean value:  tensor(0.1021, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8687  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.222e-03\n",
      "‖w_svm‖₂       : 0.05274139403133922\n",
      "‖alpha‖₁       : 0.9056503766424602\n",
      "scores min/max : -0.9295488414207813 1.8238251508517918\n",
      "Mask mean value:  tensor(0.2436, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4542  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.642e-03\n",
      "‖w_svm‖₂       : 3.756723295553387e-06\n",
      "‖alpha‖₁       : 0.4199999999926288\n",
      "scores min/max : -9.117896201148343e-08 1.5526455989376066e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1230  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.622e-05\n",
      "‖w_svm‖₂       : 0.057838544086601884\n",
      "‖alpha‖₁       : 0.8979689747359589\n",
      "scores min/max : -1.7491686140584182 3.650536949029205\n",
      "Mask mean value:  tensor(0.3303, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4956  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.362e-03\n",
      "‖w_svm‖₂       : 1.4247628955268986e-05\n",
      "‖alpha‖₁       : 0.3599999999969288\n",
      "scores min/max : 5.63374864447538e-06 6.4527210499815635e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3078  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.582e-17\n",
      "‖w_svm‖₂       : 0.0001814641123979741\n",
      "‖alpha‖₁       : 0.8199999999999982\n",
      "scores min/max : -3.4233958723033873e-06 1.032105459984652e-05\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6980  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.131e-17\n",
      "‖w_svm‖₂       : 7.237673714764352e-08\n",
      "‖alpha‖₁       : 0.42000000000000004\n",
      "scores min/max : -6.575871603529057e-08 -1.8649336920731013e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.476e-08\n",
      "‖w_svm‖₂       : 2.437796575534751e-07\n",
      "‖alpha‖₁       : 0.6399999999999946\n",
      "scores min/max : 1.10102796321631e-07 1.3099041509549596e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.191e-20\n",
      "‖w_svm‖₂       : 5.6721982034764056e-08\n",
      "‖alpha‖₁       : 0.11999999999999014\n",
      "scores min/max : -4.328297982314845e-08 -1.2466991991602541e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.795e-08\n",
      "‖w_svm‖₂       : 2.2810147831004547e-08\n",
      "‖alpha‖₁       : 0.11999999999999557\n",
      "scores min/max : -1.106133622049194e-08 3.564983182991711e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.842e-09\n",
      "‖w_svm‖₂       : 0.01925803709755875\n",
      "‖alpha‖₁       : 0.779999999999999\n",
      "scores min/max : -0.002380527318667648 0.07017878671691993\n",
      "Mask mean value:  tensor(0.5190, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.2526  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.359e-15\n",
      "‖w_svm‖₂       : 0.0001558921951782388\n",
      "‖alpha‖₁       : 0.6399999999999998\n",
      "scores min/max : -3.2482707230571434e-05 -2.3208740485901178e-05\n",
      "Mask mean value:  tensor(0.4999, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7642  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.048e-17\n",
      "‖w_svm‖₂       : 0.04414406402217602\n",
      "‖alpha‖₁       : 0.9401853873977535\n",
      "scores min/max : -1.821408997583422 0.3154131242496355\n",
      "Mask mean value:  tensor(0.4574, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3504  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.819e-10\n",
      "‖w_svm‖₂       : 4.3414293137406964e-07\n",
      "‖alpha‖₁       : 0.72\n",
      "scores min/max : 1.1333635841218396e-07 1.7058294088311222e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.833e-21\n",
      "‖w_svm‖₂       : 8.962975495382004e-08\n",
      "‖alpha‖₁       : 0.17999999999999727\n",
      "scores min/max : 2.942626391335176e-09 2.3719439303429824e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.399e-21\n",
      "‖w_svm‖₂       : 4.86401293766698e-08\n",
      "‖alpha‖₁       : 0.17999999999999866\n",
      "scores min/max : 2.798712587671592e-08 4.261246758083377e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 4.718e-08\n",
      "‖w_svm‖₂       : 0.04768421285135164\n",
      "‖alpha‖₁       : 0.9389206735242464\n",
      "scores min/max : -2.4435237226633078 1.593044976247745\n",
      "Mask mean value:  tensor(0.1518, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4050  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.305e-03\n",
      "‖w_svm‖₂       : 0.13020749318646285\n",
      "‖alpha‖₁       : 0.6522163607210559\n",
      "scores min/max : -17.898977155518587 1.885634954071299\n",
      "Mask mean value:  tensor(0.3356, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2734  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.987e-03\n",
      "‖w_svm‖₂       : 8.366837746476075e-08\n",
      "‖alpha‖₁       : 0.3799999999999997\n",
      "scores min/max : 1.7790657278308067e-08 3.594260719567488e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.580e-22\n",
      "‖w_svm‖₂       : 0.00034987864955035944\n",
      "‖alpha‖₁       : 0.7399999999999989\n",
      "scores min/max : -6.717581465981421e-05 -5.1561319566051745e-05\n",
      "Mask mean value:  tensor(0.4997, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0637  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.974e-17\n",
      "‖w_svm‖₂       : 0.03234482717117403\n",
      "‖alpha‖₁       : 0.898430508166638\n",
      "scores min/max : -0.7978853298694139 1.9626277432116064\n",
      "Mask mean value:  tensor(0.7563, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4835  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.145e-06\n",
      "‖w_svm‖₂       : 0.022515422479019727\n",
      "‖alpha‖₁       : 0.86\n",
      "scores min/max : -0.04628424770646705 0.06235275569123173\n",
      "Mask mean value:  tensor(0.4870, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6418  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.592e-15\n",
      "‖w_svm‖₂       : 0.02910234769942136\n",
      "‖alpha‖₁       : 0.659999999999997\n",
      "scores min/max : -0.059654468053271376 0.09602586851408593\n",
      "Mask mean value:  tensor(0.4681, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.7419  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.311e-15\n",
      "‖w_svm‖₂       : 0.024236615792298775\n",
      "‖alpha‖₁       : 0.823335969125774\n",
      "scores min/max : -1.875716072928315 1.5263044559518624\n",
      "Mask mean value:  tensor(0.7598, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.5385  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.338e-06\n",
      "‖w_svm‖₂       : 0.005545821155795318\n",
      "‖alpha‖₁       : 0.5599999999999861\n",
      "scores min/max : 0.0026526059889516902 0.004246235172424147\n",
      "Mask mean value:  tensor(0.5147, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0789  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.357e-04\n",
      "‖w_svm‖₂       : 0.024434591567397676\n",
      "‖alpha‖₁       : 0.8521704649632382\n",
      "scores min/max : -2.9737891842504403 1.5076569505877617\n",
      "Mask mean value:  tensor(0.1340, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9118  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.461e-04\n",
      "‖w_svm‖₂       : 6.754122885514716e-07\n",
      "‖alpha‖₁       : 0.3999999999999997\n",
      "scores min/max : -1.381406068964511e-07 1.6798673019058396e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.183e-09\n",
      "‖w_svm‖₂       : 8.36586877809081e-08\n",
      "‖alpha‖₁       : 0.6599999999999728\n",
      "scores min/max : 4.30490289529121e-08 1.875569080543753e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.583e-09\n",
      "‖w_svm‖₂       : 0.0008678452803292871\n",
      "‖alpha‖₁       : 0.8199999999999994\n",
      "scores min/max : 0.0003295842589631434 0.0020522897934901847\n",
      "Mask mean value:  tensor(0.5071, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5759  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.750e-17\n",
      "‖w_svm‖₂       : 0.057285563627432486\n",
      "‖alpha‖₁       : 0.7293396208190577\n",
      "scores min/max : -0.23374249034048952 2.047128407784572\n",
      "Mask mean value:  tensor(0.7599, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3910  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 8.155e-04\n",
      "‖w_svm‖₂       : 0.019499665191941715\n",
      "‖alpha‖₁       : 0.681492110314914\n",
      "scores min/max : -1.9839870187621216 0.05355093530501431\n",
      "Mask mean value:  tensor(0.5565, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2125  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.106e-12\n",
      "‖w_svm‖₂       : 7.973600391932053e-08\n",
      "‖alpha‖₁       : 0.5399999999999665\n",
      "scores min/max : -3.715992395241616e-08 -1.5538195451493134e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.509e-20\n",
      "‖w_svm‖₂       : 0.18078587312339675\n",
      "‖alpha‖₁       : 0.8797074479760505\n",
      "scores min/max : -3.5299772132594267 6.1382690919601925\n",
      "Mask mean value:  tensor(0.1009, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1768  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.178e-03\n",
      "‖w_svm‖₂       : 0.0004997225336911187\n",
      "‖alpha‖₁       : 0.4399999999999998\n",
      "scores min/max : -0.00046683485600730005 -0.00021795382027476538\n",
      "Mask mean value:  tensor(0.4979, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6483  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.846e-16\n",
      "‖w_svm‖₂       : 1.184315877377873e-07\n",
      "‖alpha‖₁       : 0.2999999999998807\n",
      "scores min/max : 2.7065609860727617e-08 3.6125182713957744e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.174e-07\n",
      "‖w_svm‖₂       : 7.425216757925728e-08\n",
      "‖alpha‖₁       : 0.13999999999999727\n",
      "scores min/max : -6.550463531004751e-09 -7.810997155640716e-10\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.887e-08\n",
      "‖w_svm‖₂       : 1.1514239285892796e-07\n",
      "‖alpha‖₁       : 0.5199999999999996\n",
      "scores min/max : 1.3589251791541061e-07 1.5729713491705445e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.276e-19\n",
      "‖w_svm‖₂       : 0.0751486616913883\n",
      "‖alpha‖₁       : 0.5799999999999901\n",
      "scores min/max : -3.1769502963729357 1.7620479360982748\n",
      "Mask mean value:  tensor(0.1167, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2435  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.214e-02\n",
      "‖w_svm‖₂       : 0.004146543014145535\n",
      "‖alpha‖₁       : 0.4600000000000001\n",
      "scores min/max : -0.004348595932783539 -0.0025479406815231836\n",
      "Mask mean value:  tensor(0.4852, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.1089  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.159e-16\n",
      "‖w_svm‖₂       : 1.3855233444968283e-07\n",
      "‖alpha‖₁       : 0.379999999999999\n",
      "scores min/max : 2.1857456260021183e-08 3.6869328797148566e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.265e-07\n",
      "‖w_svm‖₂       : 0.006181670133794926\n",
      "‖alpha‖₁       : 0.607661509344072\n",
      "scores min/max : -1.9806912806393102 0.3063301892347105\n",
      "Mask mean value:  tensor(0.5983, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2419  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.592e-15\n",
      "‖w_svm‖₂       : 0.08168902700199254\n",
      "‖alpha‖₁       : 0.5771326048777549\n",
      "scores min/max : -2.0130476001242696 5.926815143632132\n",
      "Mask mean value:  tensor(0.5466, dtype=torch.float64)\n",
      "max feasible return = 0.2092  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.109433228064524e-07\n",
      "‖alpha‖₁       : 0.5799999999999942\n",
      "scores min/max : -1.9894326203593284e-07 -1.0804427352503612e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0987443751099991e-07\n",
      "‖alpha‖₁       : 0.29999999999996835\n",
      "scores min/max : 7.690322705581894e-09 1.2815425786316824e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 7.29304929663771e-08\n",
      "‖alpha‖₁       : 0.5999999999999995\n",
      "scores min/max : -6.9213985424509116e-09 2.981460671798378e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.8885012522439587e-08\n",
      "‖alpha‖₁       : 0.37999999999999995\n",
      "scores min/max : 3.1611413869509013e-09 1.252915063636252e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.93507680978468e-06\n",
      "‖alpha‖₁       : 0.31999999996132644\n",
      "scores min/max : 2.009301810789953e-07 2.5301364415967505e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4258  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.06652184234401644\n",
      "‖alpha‖₁       : 0.7396775622627251\n",
      "scores min/max : -3.476370101382785 2.1367635074457656\n",
      "Mask mean value:  tensor(0.8672, dtype=torch.float64)\n",
      "max feasible return = -0.9129  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.034090184594801426\n",
      "‖alpha‖₁       : 0.6149822770847432\n",
      "scores min/max : -1.8861332237029766 1.2439432639332961\n",
      "Mask mean value:  tensor(0.8373, dtype=torch.float64)\n",
      "max feasible return = 2.9506  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.808333934945484e-07\n",
      "‖alpha‖₁       : 0.45999999999997737\n",
      "scores min/max : 1.5282115307538556e-08 6.101859454121813e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.38965627767527e-07\n",
      "‖alpha‖₁       : 0.5199999999999868\n",
      "scores min/max : 1.7131756842028186e-08 3.003358923150877e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.6171088153914706e-07\n",
      "‖alpha‖₁       : 0.579999999999999\n",
      "scores min/max : -1.1811529129616038e-07 -8.197428253318541e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.043903553136390404\n",
      "‖alpha‖₁       : 0.6874459535344992\n",
      "scores min/max : -0.38636467991300444 2.005752091885244\n",
      "Mask mean value:  tensor(0.5872, dtype=torch.float64)\n",
      "max feasible return = -0.1189  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.035833646723283354\n",
      "‖alpha‖₁       : 0.42211546108306264\n",
      "scores min/max : -3.9379686684689714 5.1573940701196355\n",
      "Mask mean value:  tensor(0.0654, dtype=torch.float64)\n",
      "max feasible return = 0.4137  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04955568003483634\n",
      "‖alpha‖₁       : 0.8211768882881283\n",
      "scores min/max : -5.122528184446415 3.0342477346948895\n",
      "Mask mean value:  tensor(0.9329, dtype=torch.float64)\n",
      "max feasible return = -3.5030  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.01037093833578724\n",
      "‖alpha‖₁       : 0.799999999999993\n",
      "scores min/max : -0.05453994974858006 0.01504460099218264\n",
      "Mask mean value:  tensor(0.4873, dtype=torch.float64)\n",
      "max feasible return = -0.1773  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.214402229614472e-07\n",
      "‖alpha‖₁       : 0.6199999999999797\n",
      "scores min/max : -1.74914862530301e-08 -1.0145963634435633e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.768779695518716e-08\n",
      "‖alpha‖₁       : 0.43999999999996137\n",
      "scores min/max : 4.580095549052387e-09 1.4163233878162616e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.3635773091955713e-07\n",
      "‖alpha‖₁       : 0.27999999999997294\n",
      "scores min/max : -1.8083872065191704e-07 -1.6287656308346722e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch   8 | train 0.005543 | val 0.007108\n",
      "-----------------------------------------Epoch:  9 ----------------------------------------\n",
      "‖w_svm‖₂       : 3.7807625905025577e-06\n",
      "‖alpha‖₁       : 0.4199999999925015\n",
      "scores min/max : -4.499939554950587e-09 1.6379654941811285e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1230  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.629e-05\n",
      "‖w_svm‖₂       : 0.13044134215831857\n",
      "‖alpha‖₁       : 0.6522867096205575\n",
      "scores min/max : -17.87922498467258 1.907000267024384\n",
      "Mask mean value:  tensor(0.3676, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2531  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.877e-03\n",
      "‖w_svm‖₂       : 0.005547287008943889\n",
      "‖alpha‖₁       : 0.5599999999999857\n",
      "scores min/max : 0.0026624461113849184 0.004256912689813737\n",
      "Mask mean value:  tensor(0.5148, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0790  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.353e-04\n",
      "‖w_svm‖₂       : 0.18099474963034007\n",
      "‖alpha‖₁       : 0.8798047907807067\n",
      "scores min/max : -3.5245456969459594 6.146759681903809\n",
      "Mask mean value:  tensor(0.1024, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1778  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.867e-03\n",
      "‖w_svm‖₂       : 2.0780884781883568e-07\n",
      "‖alpha‖₁       : 0.4199999999999746\n",
      "scores min/max : -1.12104071829459e-07 -8.978467256659363e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.491e-19\n",
      "‖w_svm‖₂       : 0.02248539808009958\n",
      "‖alpha‖₁       : 0.3833341751309137\n",
      "scores min/max : -1.975892105829304 0.22107539243032182\n",
      "Mask mean value:  tensor(0.6498, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1355  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.567e-14\n",
      "‖w_svm‖₂       : 2.891109877923687e-07\n",
      "‖alpha‖₁       : 0.6599999999999991\n",
      "scores min/max : -3.1789516905774844e-07 -1.6623150735449196e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.229e-18\n",
      "‖w_svm‖₂       : 0.01676168251250525\n",
      "‖alpha‖₁       : 0.8599999999999998\n",
      "scores min/max : -0.03677065287188741 -0.018380600894226384\n",
      "Mask mean value:  tensor(0.3846, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0917  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.034e-15\n",
      "‖w_svm‖₂       : 0.03229549400406688\n",
      "‖alpha‖₁       : 0.898425455282807\n",
      "scores min/max : -0.7960231146738054 1.9644934461862709\n",
      "Mask mean value:  tensor(0.7602, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4801  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.984e-07\n",
      "‖w_svm‖₂       : 5.654536264610487e-08\n",
      "‖alpha‖₁       : 0.11999999999999011\n",
      "scores min/max : -4.5172769019143414e-08 -1.4358010238777726e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.665e-08\n",
      "‖w_svm‖₂       : 5.6998849148985824e-08\n",
      "‖alpha‖₁       : 0.43999999999999895\n",
      "scores min/max : -1.0745479463656384e-07 -3.3365806156781107e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.229e-20\n",
      "‖w_svm‖₂       : 0.0576913522873841\n",
      "‖alpha‖₁       : 0.5757828657492656\n",
      "scores min/max : -1.9520192223773918 0.8696537155420276\n",
      "Mask mean value:  tensor(0.7125, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9463  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.644e-04\n",
      "‖w_svm‖₂       : 0.08093810163520145\n",
      "‖alpha‖₁       : 0.4185905162534359\n",
      "scores min/max : -1.7477290547749198 3.143605615787235\n",
      "Mask mean value:  tensor(0.8563, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8664  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.194e-10\n",
      "‖w_svm‖₂       : 1.756350130224586e-07\n",
      "‖alpha‖₁       : 0.23999999999999092\n",
      "scores min/max : 1.4323255353007694e-07 1.5862802837108427e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.740e-20\n",
      "‖w_svm‖₂       : 0.0001320276227317904\n",
      "‖alpha‖₁       : 0.4399999999917715\n",
      "scores min/max : -0.00010449015033618409 2.2540970218264925e-05\n",
      "Mask mean value:  tensor(0.4999, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0279  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.451e-17\n",
      "‖w_svm‖₂       : 0.0195073657600815\n",
      "‖alpha‖₁       : 0.681492891513534\n",
      "scores min/max : -1.9842847873471727 0.053242714617556\n",
      "Mask mean value:  tensor(0.5551, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2118  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.542e-12\n",
      "‖w_svm‖₂       : 1.4251671173509822e-05\n",
      "‖alpha‖₁       : 0.3599999999969885\n",
      "scores min/max : 6.05118675984309e-06 6.873032636647046e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3078  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.600e-17\n",
      "‖w_svm‖₂       : 0.04782353535167907\n",
      "‖alpha‖₁       : 0.9389310362965543\n",
      "scores min/max : -2.4518976532756938 1.5847266260778445\n",
      "Mask mean value:  tensor(0.1463, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3943  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.803e-03\n",
      "‖w_svm‖₂       : 0.024389137836550626\n",
      "‖alpha‖₁       : 0.8521683275608922\n",
      "scores min/max : -2.9776212765582204 1.503818161773582\n",
      "Mask mean value:  tensor(0.1329, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9082  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.208e-04\n",
      "‖w_svm‖₂       : 0.05253697963089924\n",
      "‖alpha‖₁       : 0.8267716701923454\n",
      "scores min/max : -1.9646238559554405 0.3943610819670927\n",
      "Mask mean value:  tensor(0.4680, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0152  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.700e-02\n",
      "‖w_svm‖₂       : 1.2048211943594008e-06\n",
      "‖alpha‖₁       : 0.3199999999986579\n",
      "scores min/max : -7.403724023835858e-07 -6.24456926734926e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4898  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.608e-18\n",
      "‖w_svm‖₂       : 2.411830660350851e-07\n",
      "‖alpha‖₁       : 0.5799999999999998\n",
      "scores min/max : -1.9371497710290297e-07 -1.7809214437002404e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.768e-19\n",
      "‖w_svm‖₂       : 7.231425126286988e-08\n",
      "‖alpha‖₁       : 0.41999999999999177\n",
      "scores min/max : -6.865050239530108e-08 -2.1379425137206515e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.433e-08\n",
      "‖w_svm‖₂       : 0.0031017265277332134\n",
      "‖alpha‖₁       : 0.58\n",
      "scores min/max : 0.0005478420519504405 0.0017723158473523168\n",
      "Mask mean value:  tensor(0.5076, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3228  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.677e-13\n",
      "‖w_svm‖₂       : 8.363343610876594e-08\n",
      "‖alpha‖₁       : 0.37999999999999967\n",
      "scores min/max : 1.925023821873897e-08 3.7402526790047365e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.586e-22\n",
      "‖w_svm‖₂       : 8.398704078720153e-08\n",
      "‖alpha‖₁       : 0.6599999999999624\n",
      "scores min/max : 4.328540686734902e-08 1.878098621255789e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.609e-09\n",
      "‖w_svm‖₂       : 0.08298417428005031\n",
      "‖alpha‖₁       : 0.6579431981297463\n",
      "scores min/max : -1.994757289444612 3.7800088506449896\n",
      "Mask mean value:  tensor(0.4299, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.6794  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.147e-04\n",
      "‖w_svm‖₂       : 1.1154466947232362e-07\n",
      "‖alpha‖₁       : 0.23999999999996663\n",
      "scores min/max : 9.713601722901922e-09 2.0764159705358575e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.571e-21\n",
      "‖w_svm‖₂       : 0.006180526120085006\n",
      "‖alpha‖₁       : 0.607661495148776\n",
      "scores min/max : -1.980659419073458 0.30636205149989953\n",
      "Mask mean value:  tensor(0.5984, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2420  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.575e-15\n",
      "‖w_svm‖₂       : 1.048323691183146e-06\n",
      "‖alpha‖₁       : 0.5999999999999921\n",
      "scores min/max : 1.4623877747126565e-09 5.240520801888587e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.480e-19\n",
      "‖w_svm‖₂       : 3.9573098483497864e-07\n",
      "‖alpha‖₁       : 0.27999999999999564\n",
      "scores min/max : -8.13262013948586e-07 -7.515049487716893e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.140e-19\n",
      "‖w_svm‖₂       : 1.393866149825547e-07\n",
      "‖alpha‖₁       : 0.37999999999999384\n",
      "scores min/max : 2.0002186606109837e-08 3.50850451115706e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.236e-07\n",
      "‖w_svm‖₂       : 0.051466263022885504\n",
      "‖alpha‖₁       : 0.9056497660391936\n",
      "scores min/max : -0.88225738906753 1.837574044315164\n",
      "Mask mean value:  tensor(0.2643, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4918  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.337e-03\n",
      "‖w_svm‖₂       : 0.004144487472156895\n",
      "‖alpha‖₁       : 0.45999999999999996\n",
      "scores min/max : -0.004350035101984398 -0.0025518207340863228\n",
      "Mask mean value:  tensor(0.4851, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.1088  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.406e-16\n",
      "‖w_svm‖₂       : 1.1871629738245044e-07\n",
      "‖alpha‖₁       : 0.29999999999985894\n",
      "scores min/max : 2.7042085223225646e-08 3.61248292591892e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.188e-07\n",
      "‖w_svm‖₂       : 0.0001561245280594497\n",
      "‖alpha‖₁       : 0.6399999999999997\n",
      "scores min/max : -3.414061868908034e-05 -2.4839023760935667e-05\n",
      "Mask mean value:  tensor(0.4999, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7642  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.055e-17\n",
      "‖w_svm‖₂       : 2.2792482180890958e-08\n",
      "‖alpha‖₁       : 0.1199999999999955\n",
      "scores min/max : -1.2817380137838109e-08 1.8139321342702239e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.834e-09\n",
      "‖w_svm‖₂       : 0.00020594131173237883\n",
      "‖alpha‖₁       : 0.6199999999983513\n",
      "scores min/max : 2.5522678709112597e-05 4.055296780161116e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.752e-17\n",
      "‖w_svm‖₂       : 0.057317098333285264\n",
      "‖alpha‖₁       : 0.7293466184776786\n",
      "scores min/max : -0.2330826322483398 2.047766937137535\n",
      "Mask mean value:  tensor(0.7614, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3986  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 8.027e-04\n",
      "‖w_svm‖₂       : 0.13119031237123685\n",
      "‖alpha‖₁       : 0.8730107296172211\n",
      "scores min/max : -12.264523261028327 2.0917888802547555\n",
      "Mask mean value:  tensor(0.4127, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2308  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 8.754e-02\n",
      "‖w_svm‖₂       : 7.428615880101009e-08\n",
      "‖alpha‖₁       : 0.13999999999999713\n",
      "scores min/max : -6.47722403175204e-09 -7.061164226485573e-10\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.899e-08\n",
      "‖w_svm‖₂       : 0.022488923929629007\n",
      "‖alpha‖₁       : 0.8599999999999922\n",
      "scores min/max : -0.046093812417647026 0.062282372417917245\n",
      "Mask mean value:  tensor(0.4875, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6421  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.586e-15\n",
      "‖w_svm‖₂       : 0.0002997803192164884\n",
      "‖alpha‖₁       : 0.419999999997873\n",
      "scores min/max : -7.522185080556707e-05 0.0002292648376095393\n",
      "Mask mean value:  tensor(0.5010, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9975  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.622e-15\n",
      "‖w_svm‖₂       : 0.05921909834196151\n",
      "‖alpha‖₁       : 0.898159192665126\n",
      "scores min/max : -1.7494913462505275 3.649732590224811\n",
      "Mask mean value:  tensor(0.3295, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4948  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.678e-03\n",
      "‖w_svm‖₂       : 4.3461067732322573e-07\n",
      "‖alpha‖₁       : 0.7200000000000001\n",
      "scores min/max : 1.4541781443619984e-07 2.0265219930784022e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.861e-21\n",
      "‖w_svm‖₂       : 0.058467339056981864\n",
      "‖alpha‖₁       : 0.6599999999999995\n",
      "scores min/max : -0.4299293511293031 0.3318858840719757\n",
      "Mask mean value:  tensor(0.5307, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9083  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.529e-05\n",
      "‖w_svm‖₂       : 2.326722984346731e-07\n",
      "‖alpha‖₁       : 0.2599999999999994\n",
      "scores min/max : -5.5710885278757955e-08 -4.2979854466168875e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.815e-17\n",
      "‖w_svm‖₂       : 8.984046836687725e-08\n",
      "‖alpha‖₁       : 0.17999999999999614\n",
      "scores min/max : 6.819472642250614e-09 2.413508291497836e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.446e-21\n",
      "‖w_svm‖₂       : 4.872821634348153e-08\n",
      "‖alpha‖₁       : 0.17999999999999997\n",
      "scores min/max : 2.6610793768508403e-08 4.109074552018197e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 4.706e-08\n",
      "‖w_svm‖₂       : 5.7413110868875195e-08\n",
      "‖alpha‖₁       : 0.2399999999999917\n",
      "scores min/max : -3.467561908250956e-08 -2.0224424831447957e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.172e-20\n",
      "‖w_svm‖₂       : 0.00034937358969459544\n",
      "‖alpha‖₁       : 0.7399999999999993\n",
      "scores min/max : -6.705020321594357e-05 -5.1480733921489807e-05\n",
      "Mask mean value:  tensor(0.4997, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0637  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.970e-17\n",
      "‖w_svm‖₂       : 0.0740591504143695\n",
      "‖alpha‖₁       : 0.5799999999999772\n",
      "scores min/max : -3.062073639690307 1.7164346934994092\n",
      "Mask mean value:  tensor(0.1352, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2707  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.904e-04\n",
      "‖w_svm‖₂       : 1.1547834396097682e-07\n",
      "‖alpha‖₁       : 0.5199999999999996\n",
      "scores min/max : 1.3985890329876954e-07 1.6126185867008003e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.315e-19\n",
      "‖w_svm‖₂       : 0.0004974216554537711\n",
      "‖alpha‖₁       : 0.4399999999999997\n",
      "scores min/max : -0.000369618631418504 -0.0001225895285818301\n",
      "Mask mean value:  tensor(0.4984, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6499  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.855e-16\n",
      "‖w_svm‖₂       : 0.019111557147774207\n",
      "‖alpha‖₁       : 0.78\n",
      "scores min/max : -0.006901495186877754 0.06348711693185594\n",
      "Mask mean value:  tensor(0.4955, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.1540  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.673e-15\n",
      "‖w_svm‖₂       : 0.05566743385661103\n",
      "‖alpha‖₁       : 0.919999999999997\n",
      "scores min/max : -0.4016703371853289 0.7756576871176912\n",
      "Mask mean value:  tensor(0.5584, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.8277  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.877e-02\n",
      "‖w_svm‖₂       : 0.027190537575538617\n",
      "‖alpha‖₁       : 0.1965755715756763\n",
      "scores min/max : -2.1486116545371656 0.0771599999643354\n",
      "Mask mean value:  tensor(0.1386, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9161  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.928e-05\n",
      "‖w_svm‖₂       : 0.000865938586332265\n",
      "‖alpha‖₁       : 0.8199999999999988\n",
      "scores min/max : 0.00015278807480957352 0.0018655251846965825\n",
      "Mask mean value:  tensor(0.5062, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5713  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.823e-17\n",
      "‖w_svm‖₂       : 0.14713089046080235\n",
      "‖alpha‖₁       : 0.88\n",
      "scores min/max : -1.5547753171640255 4.057792196607043\n",
      "Mask mean value:  tensor(0.2675, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2587  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.430e-03\n",
      "‖w_svm‖₂       : 7.983033249258106e-08\n",
      "‖alpha‖₁       : 0.5399999999999647\n",
      "scores min/max : -3.337290164740315e-08 -1.1749223280229068e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.512e-20\n",
      "‖w_svm‖₂       : 0.1400904021640855\n",
      "‖alpha‖₁       : 0.7575715308854103\n",
      "scores min/max : -1.7197237686642348 2.258933966907253\n",
      "Mask mean value:  tensor(0.9007, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3592  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.567e-12\n",
      "‖w_svm‖₂       : 0.04377616386211756\n",
      "‖alpha‖₁       : 0.8898852330699564\n",
      "scores min/max : -2.007820415616752 0.3392010617491636\n",
      "Mask mean value:  tensor(0.2697, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0791  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.272e-11\n",
      "‖w_svm‖₂       : 0.028389710138248325\n",
      "‖alpha‖₁       : 0.550986300510612\n",
      "scores min/max : -3.5852675677945673 0.9888794680248538\n",
      "Mask mean value:  tensor(0.1033, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8784  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.293e-03\n",
      "‖w_svm‖₂       : 2.4321817266456883e-07\n",
      "‖alpha‖₁       : 0.6399999999999958\n",
      "scores min/max : 1.3515967680603003e-07 1.5603240803124615e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.279e-20\n",
      "‖w_svm‖₂       : 0.0783115350616656\n",
      "‖alpha‖₁       : 0.471201511940721\n",
      "scores min/max : -2.183556518622698 2.8936069393311192\n",
      "Mask mean value:  tensor(0.1918, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.0100  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.465e-03\n",
      "‖w_svm‖₂       : 0.0051509914440383085\n",
      "‖alpha‖₁       : 0.7005600712224775\n",
      "scores min/max : -2.0058981924962973 0.017472949902817267\n",
      "Mask mean value:  tensor(0.4455, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2503  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.155e-05\n",
      "‖w_svm‖₂       : 0.005025473539835078\n",
      "‖alpha‖₁       : 0.38\n",
      "scores min/max : -0.006616457469224055 0.0038592275056416707\n",
      "Mask mean value:  tensor(0.5058, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9010  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.556e-14\n",
      "‖w_svm‖₂       : 0.00018188267030198464\n",
      "‖alpha‖₁       : 0.8199999999999998\n",
      "scores min/max : -1.1522000679656509e-05 2.274163866645946e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6978  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.139e-17\n",
      "‖w_svm‖₂       : 1.1162285807693811e-06\n",
      "‖alpha‖₁       : 0.4999999999999929\n",
      "scores min/max : -2.26848921422927e-07 -3.396041677643616e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.113e-19\n",
      "‖w_svm‖₂       : 0.019518046769070478\n",
      "‖alpha‖₁       : 0.5962323473972229\n",
      "scores min/max : -2.9203973219831605 2.2289091564166026\n",
      "Mask mean value:  tensor(0.8900, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.4576  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.222e-02\n",
      "‖w_svm‖₂       : 1.953635156465396e-07\n",
      "‖alpha‖₁       : 0.3799999999999991\n",
      "scores min/max : -3.955944755633401e-08 -3.868582508775871e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.601e-19\n",
      "‖w_svm‖₂       : 0.024131103729440377\n",
      "‖alpha‖₁       : 0.823341230572897\n",
      "scores min/max : -1.8758616885494568 1.52611587537817\n",
      "Mask mean value:  tensor(0.7595, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.5374  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.382e-06\n",
      "‖w_svm‖₂       : 0.028743029508639252\n",
      "‖alpha‖₁       : 0.6599999999999974\n",
      "scores min/max : -0.060851770573515346 0.09074832437731842\n",
      "Mask mean value:  tensor(0.4557, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.6452  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.348e-15\n",
      "‖w_svm‖₂       : 6.85297676015714e-07\n",
      "‖alpha‖₁       : 0.39999999999989455\n",
      "scores min/max : -1.049659463516294e-07 2.0354132193552344e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.239e-09\n",
      "‖w_svm‖₂       : 0.02480332187393029\n",
      "‖alpha‖₁       : 0.8150830086178025\n",
      "scores min/max : -11.521341395021821 2.0375444569665406\n",
      "Mask mean value:  tensor(0.7224, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9244  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.303e-06\n",
      "‖w_svm‖₂       : 0.04392606400955601\n",
      "‖alpha‖₁       : 0.9401899894137249\n",
      "scores min/max : -1.8253744578273787 0.3113373774000208\n",
      "Mask mean value:  tensor(0.4414, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3498  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.258e-09\n",
      "‖w_svm‖₂       : 0.08307374318372025\n",
      "‖alpha‖₁       : 0.5774271440961467\n",
      "scores min/max : -2.0401889725258107 5.894514883279243\n",
      "Mask mean value:  tensor(0.4653, dtype=torch.float64)\n",
      "max feasible return = 0.1675  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.0832234666540863e-07\n",
      "‖alpha‖₁       : 0.5799999999999961\n",
      "scores min/max : -2.2085345343601773e-07 -1.2996448946626698e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0498683925068329e-07\n",
      "‖alpha‖₁       : 0.2999999999999997\n",
      "scores min/max : 1.578622658901092e-08 2.075019912439803e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 7.25151484498677e-08\n",
      "‖alpha‖₁       : 0.5999999999999996\n",
      "scores min/max : -3.055798530287815e-09 3.368192067654406e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.8555242778004174e-08\n",
      "‖alpha‖₁       : 0.38\n",
      "scores min/max : 5.196655930330124e-09 1.4565177487985427e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.0257360251866795e-06\n",
      "‖alpha‖₁       : 0.3199999999764064\n",
      "scores min/max : 7.520294400919266e-07 2.4736149566490987e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4258  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.06586493521638566\n",
      "‖alpha‖₁       : 0.7396554536721962\n",
      "scores min/max : -3.485882969916745 2.128198121307383\n",
      "Mask mean value:  tensor(0.8585, dtype=torch.float64)\n",
      "max feasible return = -0.9056  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03433539253165961\n",
      "‖alpha‖₁       : 0.6149997278382322\n",
      "scores min/max : -1.8869125815658379 1.2431802460040864\n",
      "Mask mean value:  tensor(0.8358, dtype=torch.float64)\n",
      "max feasible return = 2.9449  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.780134199343603e-07\n",
      "‖alpha‖₁       : 0.4599999999999912\n",
      "scores min/max : 8.695969088243634e-08 6.814905844049638e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.386845273624168e-07\n",
      "‖alpha‖₁       : 0.5199999999999877\n",
      "scores min/max : 1.6408612690337007e-08 2.9311634149067982e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.606452264746979e-07\n",
      "‖alpha‖₁       : 0.5799999999999994\n",
      "scores min/max : -1.5212286493281285e-07 -1.1600347125409974e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.04447995461364977\n",
      "‖alpha‖₁       : 0.6875242642974093\n",
      "scores min/max : -0.399063410713193 1.9926754938972713\n",
      "Mask mean value:  tensor(0.5358, dtype=torch.float64)\n",
      "max feasible return = -0.0746  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03569164540360521\n",
      "‖alpha‖₁       : 0.4221099991621043\n",
      "scores min/max : -3.9355924218033986 5.160543148704463\n",
      "Mask mean value:  tensor(0.0661, dtype=torch.float64)\n",
      "max feasible return = 0.4182  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.049428495100246536\n",
      "‖alpha‖₁       : 0.8211985507652395\n",
      "scores min/max : -5.122188478344755 3.035395048805667\n",
      "Mask mean value:  tensor(0.9332, dtype=torch.float64)\n",
      "max feasible return = -3.5052  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.010231335682116623\n",
      "‖alpha‖₁       : 0.799999999999999\n",
      "scores min/max : -0.053667187659573096 0.014044531460247233\n",
      "Mask mean value:  tensor(0.4847, dtype=torch.float64)\n",
      "max feasible return = -0.1764  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.194110516994169e-07\n",
      "‖alpha‖₁       : 0.6199999999999779\n",
      "scores min/max : 6.036508116717122e-09 2.2520299671400525e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.7680688416315296e-08\n",
      "‖alpha‖₁       : 0.43999999999996153\n",
      "scores min/max : 6.142077627274568e-09 1.5724429544976257e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.353509035475186e-07\n",
      "‖alpha‖₁       : 0.2799999999999738\n",
      "scores min/max : -2.2948547121814098e-07 -2.1152853860338528e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch   9 | train 0.005521 | val 0.006992\n",
      "-----------------------------------------Epoch:  10 ----------------------------------------\n",
      "‖w_svm‖₂       : 5.666426586874764e-08\n",
      "‖alpha‖₁       : 0.43999999999999895\n",
      "scores min/max : -1.1146503025363089e-07 -7.348575284764905e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.212e-20\n",
      "‖w_svm‖₂       : 0.005175343017005125\n",
      "‖alpha‖₁       : 0.7005602647507323\n",
      "scores min/max : -2.0058044947787437 0.017568106139481482\n",
      "Mask mean value:  tensor(0.4460, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2505  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.082e-05\n",
      "‖w_svm‖₂       : 2.406813544812666e-07\n",
      "‖alpha‖₁       : 0.5799999999999994\n",
      "scores min/max : -2.5873031258837625e-07 -2.431009129546543e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.810e-19\n",
      "‖w_svm‖₂       : 0.02234283313256022\n",
      "‖alpha‖₁       : 0.3833347545625425\n",
      "scores min/max : -1.9732974308412903 0.22389210460926234\n",
      "Mask mean value:  tensor(0.6607, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1376  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.733e-14\n",
      "‖w_svm‖₂       : 0.0030884293810900226\n",
      "‖alpha‖₁       : 0.5799999999999996\n",
      "scores min/max : 0.000602687613865195 0.0018150216534162655\n",
      "Mask mean value:  tensor(0.5078, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3229  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.214e-13\n",
      "‖w_svm‖₂       : 0.05782004183714572\n",
      "‖alpha‖₁       : 0.6599999999999998\n",
      "scores min/max : -0.4187332960871916 0.3268725651294981\n",
      "Mask mean value:  tensor(0.5389, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9281  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.307e-02\n",
      "‖w_svm‖₂       : 0.005041163478360915\n",
      "‖alpha‖₁       : 0.38\n",
      "scores min/max : -0.007100452555345506 0.0033739566821368618\n",
      "Mask mean value:  tensor(0.5034, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8967  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.626e-14\n",
      "‖w_svm‖₂       : 0.024799303516547057\n",
      "‖alpha‖₁       : 0.8521892401771385\n",
      "scores min/max : -2.958564098376266 1.522471449915807\n",
      "Mask mean value:  tensor(0.1387, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.995e-04\n",
      "‖w_svm‖₂       : 0.053207175690641526\n",
      "‖alpha‖₁       : 0.826834784753063\n",
      "scores min/max : -1.9555516472961794 0.40441119436224127\n",
      "Mask mean value:  tensor(0.5006, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0115  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.606e-02\n",
      "‖w_svm‖₂       : 7.895655576226929e-08\n",
      "‖alpha‖₁       : 0.5399999999999635\n",
      "scores min/max : -4.129883025796854e-08 -1.967310661996608e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.481e-20\n",
      "‖w_svm‖₂       : 7.3926444063566e-08\n",
      "‖alpha‖₁       : 0.13999999999999405\n",
      "scores min/max : -4.044389659030704e-09 1.7461681670622373e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.874e-08\n",
      "‖w_svm‖₂       : 0.07987519203016458\n",
      "‖alpha‖₁       : 0.41879665564478613\n",
      "scores min/max : -1.79217305010055 2.6498759564485788\n",
      "Mask mean value:  tensor(0.8415, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8764  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.860e-10\n",
      "‖w_svm‖₂       : 2.3031692135693033e-07\n",
      "‖alpha‖₁       : 0.2599999999999997\n",
      "scores min/max : -6.130149035397534e-08 -4.857312475592031e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.708e-17\n",
      "‖w_svm‖₂       : 0.00020760601787941528\n",
      "‖alpha‖₁       : 0.6199999999988881\n",
      "scores min/max : 3.3344641470712815e-05 4.861839529273093e-05\n",
      "Mask mean value:  tensor(0.5002, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.798e-17\n",
      "‖w_svm‖₂       : 0.14049984936788887\n",
      "‖alpha‖₁       : 0.7577050406840873\n",
      "scores min/max : -1.7195747420084753 2.2597641932191452\n",
      "Mask mean value:  tensor(0.9007, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3623  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.303e-11\n",
      "‖w_svm‖₂       : 2.2799929463316126e-08\n",
      "‖alpha‖₁       : 0.11999999999999506\n",
      "scores min/max : -1.2565940652608832e-08 2.08575241350288e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.822e-09\n",
      "‖w_svm‖₂       : 4.3127787510038463e-07\n",
      "‖alpha‖₁       : 0.7199999999999995\n",
      "scores min/max : 1.978050028877929e-07 2.5502579501739474e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.845e-21\n",
      "‖w_svm‖₂       : 0.024133276438149477\n",
      "‖alpha‖₁       : 0.8233419950265609\n",
      "scores min/max : -1.8760487507312555 1.5259322187025686\n",
      "Mask mean value:  tensor(0.7591, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.5364  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.393e-06\n",
      "‖w_svm‖₂       : 2.0577937166530934e-07\n",
      "‖alpha‖₁       : 0.41999999999997495\n",
      "scores min/max : -1.3279606833997838e-07 -1.1047481942342131e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.485e-19\n",
      "‖w_svm‖₂       : 0.07355535927279938\n",
      "‖alpha‖₁       : 0.5799999999999701\n",
      "scores min/max : -3.024232505561846 1.6902899662581847\n",
      "Mask mean value:  tensor(0.1323, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2646  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.953e-04\n",
      "‖w_svm‖₂       : 0.005508751850713778\n",
      "‖alpha‖₁       : 0.5599999999999999\n",
      "scores min/max : 0.0024713801821456913 0.004043999399684315\n",
      "Mask mean value:  tensor(0.5138, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0769  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.497e-04\n",
      "‖w_svm‖₂       : 2.8638894071731846e-07\n",
      "‖alpha‖₁       : 0.6599999999999999\n",
      "scores min/max : -3.750887503633301e-07 -2.2345306305836258e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.245e-18\n",
      "‖w_svm‖₂       : 1.1757877114251943e-07\n",
      "‖alpha‖₁       : 0.29999999999987786\n",
      "scores min/max : 3.2353362882192744e-08 4.141724494679614e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.177e-07\n",
      "‖w_svm‖₂       : 0.14611881609811472\n",
      "‖alpha‖₁       : 0.8799999999999994\n",
      "scores min/max : -1.5190647932380432 4.012461213592951\n",
      "Mask mean value:  tensor(0.2943, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.883e-04\n",
      "‖w_svm‖₂       : 0.043970390845937665\n",
      "‖alpha‖₁       : 0.9401925527507051\n",
      "scores min/max : -1.8224813168723628 0.3142567765707206\n",
      "Mask mean value:  tensor(0.4530, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3497  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.972e-15\n",
      "‖w_svm‖₂       : 0.00035373258103625594\n",
      "‖alpha‖₁       : 0.74\n",
      "scores min/max : -8.131218786657226e-05 -6.535194433539785e-05\n",
      "Mask mean value:  tensor(0.4997, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0636  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.991e-17\n",
      "‖w_svm‖₂       : 1.738046602825709e-07\n",
      "‖alpha‖₁       : 0.23999999999999222\n",
      "scores min/max : 1.6450590146070174e-07 1.7988015231832474e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.811e-20\n",
      "‖w_svm‖₂       : 3.686355780608775e-06\n",
      "‖alpha‖₁       : 0.41999999999276627\n",
      "scores min/max : -3.262859278557371e-07 1.3159129288015797e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1230  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.566e-05\n",
      "‖w_svm‖₂       : 0.019616792229071003\n",
      "‖alpha‖₁       : 0.68149938106439\n",
      "scores min/max : -1.982095664343775 0.05538267223801926\n",
      "Mask mean value:  tensor(0.5655, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2174  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.423e-14\n",
      "‖w_svm‖₂       : 0.055191674357534366\n",
      "‖alpha‖₁       : 0.919999999999594\n",
      "scores min/max : -0.3988520277193207 0.794569086891757\n",
      "Mask mean value:  tensor(0.5211, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.6109  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 2.768e-01\n",
      "‖w_svm‖₂       : 0.07853700177627158\n",
      "‖alpha‖₁       : 0.4712385650311518\n",
      "scores min/max : -2.201627540451808 2.8754943899962653\n",
      "Mask mean value:  tensor(0.1726, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.8217  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.007e-02\n",
      "‖w_svm‖₂       : 0.00049586053599043\n",
      "‖alpha‖₁       : 0.4399999999999999\n",
      "scores min/max : -0.0004035466318515112 -0.00015784934009738282\n",
      "Mask mean value:  tensor(0.4982, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6493  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.906e-16\n",
      "‖w_svm‖₂       : 0.02873434677774813\n",
      "‖alpha‖₁       : 0.6599999999999974\n",
      "scores min/max : -0.06182474001401449 0.08982993269560481\n",
      "Mask mean value:  tensor(0.4510, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.6095  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.393e-15\n",
      "‖w_svm‖₂       : 1.1645097300312019e-07\n",
      "‖alpha‖₁       : 0.5199999999999609\n",
      "scores min/max : 1.8606315595314148e-07 2.074890267047148e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.381e-19\n",
      "‖w_svm‖₂       : 0.02721654809718035\n",
      "‖alpha‖₁       : 0.19657727402341263\n",
      "scores min/max : -2.161258557512006 0.06449707266476071\n",
      "Mask mean value:  tensor(0.1149, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7628  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.509e-05\n",
      "‖w_svm‖₂       : 0.00013435878901642699\n",
      "‖alpha‖₁       : 0.4399999999802207\n",
      "scores min/max : -0.00013065464396287067 9.594892643737278e-07\n",
      "Mask mean value:  tensor(0.4998, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0270  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.493e-17\n",
      "‖w_svm‖₂       : 0.01663983458192111\n",
      "‖alpha‖₁       : 0.8599999999999992\n",
      "scores min/max : -0.03456700486495185 -0.016473272364885966\n",
      "Mask mean value:  tensor(0.3940, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0950  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.284e-15\n",
      "‖w_svm‖₂       : 0.13177415905383189\n",
      "‖alpha‖₁       : 0.6527155586646833\n",
      "scores min/max : -17.94567045826959 1.8553831654335282\n",
      "Mask mean value:  tensor(0.2960, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2880  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.449e-03\n",
      "‖w_svm‖₂       : 0.056735872809397535\n",
      "‖alpha‖₁       : 0.7293353542028224\n",
      "scores min/max : -0.23650251628703023 2.0445005841141697\n",
      "Mask mean value:  tensor(0.7527, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3589  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 8.563e-04\n",
      "‖w_svm‖₂       : 4.826589781624565e-08\n",
      "‖alpha‖₁       : 0.17999999999999958\n",
      "scores min/max : 2.8862462135337064e-08 4.341827470448192e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 4.641e-08\n",
      "‖w_svm‖₂       : 3.8941380931538413e-07\n",
      "‖alpha‖₁       : 0.2799999999999877\n",
      "scores min/max : -9.705466916817924e-07 -9.086396385855282e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.136e-19\n",
      "‖w_svm‖₂       : 7.216491420818645e-08\n",
      "‖alpha‖₁       : 0.42\n",
      "scores min/max : -6.902619070994593e-08 -2.1936399317057452e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.418e-08\n",
      "‖w_svm‖₂       : 6.826855493424176e-07\n",
      "‖alpha‖₁       : 0.3999999999999993\n",
      "scores min/max : -1.0048724487311355e-07 2.0572542041363487e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.245e-09\n",
      "‖w_svm‖₂       : 0.05072382340825213\n",
      "‖alpha‖₁       : 0.9056489249922977\n",
      "scores min/max : -0.8620635803013245 1.8450118979487025\n",
      "Mask mean value:  tensor(0.2787, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5173  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.516e-03\n",
      "‖w_svm‖₂       : 0.019701583594017226\n",
      "‖alpha‖₁       : 0.5962475397490012\n",
      "scores min/max : -2.9155641058953226 2.2373031518929842\n",
      "Mask mean value:  tensor(0.8958, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.4709  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.783e-02\n",
      "‖w_svm‖₂       : 1.0273482435227918e-06\n",
      "‖alpha‖₁       : 0.5999999999999908\n",
      "scores min/max : -5.763399152289815e-09 4.519566799430916e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.434e-19\n",
      "‖w_svm‖₂       : 2.3769812743917832e-07\n",
      "‖alpha‖₁       : 0.6399999999999955\n",
      "scores min/max : 1.695833676136317e-07 1.9045719513740202e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.122e-20\n",
      "‖w_svm‖₂       : 0.0041064162018498475\n",
      "‖alpha‖₁       : 0.4599999999999998\n",
      "scores min/max : -0.0043870935614767825 -0.0026186153551055885\n",
      "Mask mean value:  tensor(0.4848, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.1075  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.496e-16\n",
      "‖w_svm‖₂       : 1.0927451656648213e-06\n",
      "‖alpha‖₁       : 0.49999999999999234\n",
      "scores min/max : -2.316825017205285e-07 -3.867599910381435e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.937e-19\n",
      "‖w_svm‖₂       : 1.1810342171312053e-06\n",
      "‖alpha‖₁       : 0.31999999999863693\n",
      "scores min/max : -1.0729766417580403e-06 -9.569413947252446e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4898  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.606e-18\n",
      "‖w_svm‖₂       : 0.0001855813750207187\n",
      "‖alpha‖₁       : 0.8199999999999597\n",
      "scores min/max : -1.8794020528864805e-05 -4.451611908820965e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6977  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.155e-17\n",
      "‖w_svm‖₂       : 8.300511756019442e-08\n",
      "‖alpha‖₁       : 0.3799999999999997\n",
      "scores min/max : 6.944130569564759e-09 2.5096560195413606e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.562e-22\n",
      "‖w_svm‖₂       : 0.022296015606813272\n",
      "‖alpha‖₁       : 0.8599999999999964\n",
      "scores min/max : -0.05162422871924394 0.054940497706093445\n",
      "Mask mean value:  tensor(0.4567, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6041  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.740e-14\n",
      "‖w_svm‖₂       : 5.5136039027752294e-08\n",
      "‖alpha‖₁       : 0.11999999999998869\n",
      "scores min/max : -5.4104478464067706e-08 -2.3267706439234188e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.776e-08\n",
      "‖w_svm‖₂       : 0.05830463687816318\n",
      "‖alpha‖₁       : 0.5759064072901678\n",
      "scores min/max : -1.9546979975342407 0.8669521384316516\n",
      "Mask mean value:  tensor(0.7039, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9213  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.636e-04\n",
      "‖w_svm‖₂       : 0.029135168575757577\n",
      "‖alpha‖₁       : 0.5510040804610132\n",
      "scores min/max : -3.586893507119275 0.9883350331598448\n",
      "Mask mean value:  tensor(0.1033, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8785  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.361e-03\n",
      "‖w_svm‖₂       : 0.006171422492962849\n",
      "‖alpha‖₁       : 0.6076614108694534\n",
      "scores min/max : -1.9818597488254688 0.30516133760313685\n",
      "Mask mean value:  tensor(0.5933, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2398  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.755e-15\n",
      "‖w_svm‖₂       : 0.047620604637480655\n",
      "‖alpha‖₁       : 0.9389325944400354\n",
      "scores min/max : -2.445128475776758 1.5933040296958438\n",
      "Mask mean value:  tensor(0.1518, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4061  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.502e-03\n",
      "‖w_svm‖₂       : 8.797382251692551e-08\n",
      "‖alpha‖₁       : 0.17999999999999516\n",
      "scores min/max : -1.8700867636739102e-08 2.1605983301884086e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.350e-21\n",
      "‖w_svm‖₂       : 1.4383907246777573e-05\n",
      "‖alpha‖₁       : 0.3599999999970479\n",
      "scores min/max : 8.069185130113554e-06 8.916464614032442e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.760e-17\n",
      "‖w_svm‖₂       : 0.0003055791921681631\n",
      "‖alpha‖₁       : 0.41999999997300586\n",
      "scores min/max : 2.9890724881014995e-05 0.0003460539752461816\n",
      "Mask mean value:  tensor(0.5016, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9987  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.664e-15\n",
      "‖w_svm‖₂       : 0.0191894175180935\n",
      "‖alpha‖₁       : 0.7799999999999905\n",
      "scores min/max : -0.00714564126892841 0.06436100150733065\n",
      "Mask mean value:  tensor(0.4948, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.1518  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.706e-15\n",
      "‖w_svm‖₂       : 1.0882768940618591e-07\n",
      "‖alpha‖₁       : 0.23999999999996502\n",
      "scores min/max : 3.59599436720025e-09 1.4652990442403921e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.398e-21\n",
      "‖w_svm‖₂       : 0.18109105341368736\n",
      "‖alpha‖₁       : 0.8804195105723873\n",
      "scores min/max : -3.508005722422155 6.105309512584757\n",
      "Mask mean value:  tensor(0.1025, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1832  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.115e-05\n",
      "‖w_svm‖₂       : 1.3630515358078818e-07\n",
      "‖alpha‖₁       : 0.37999999999998096\n",
      "scores min/max : 2.62589950368599e-08 4.145255129774466e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.707e-08\n",
      "‖w_svm‖₂       : 0.08281081451722452\n",
      "‖alpha‖₁       : 0.6580493185576216\n",
      "scores min/max : -1.989296375763848 3.8171159879460212\n",
      "Mask mean value:  tensor(0.4426, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.7797  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.097e-04\n",
      "‖w_svm‖₂       : 0.059119987008964175\n",
      "‖alpha‖₁       : 0.8981885121008326\n",
      "scores min/max : -1.7504115249561547 3.649676925779273\n",
      "Mask mean value:  tensor(0.3285, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4942  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.636e-03\n",
      "‖w_svm‖₂       : 0.02488739828038835\n",
      "‖alpha‖₁       : 0.8150829339346415\n",
      "scores min/max : -11.523568277943358 2.034468514062049\n",
      "Mask mean value:  tensor(0.7162, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9090  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.997e-06\n",
      "‖w_svm‖₂       : 1.9370786391489544e-07\n",
      "‖alpha‖₁       : 0.3799999999999991\n",
      "scores min/max : -4.712642055467817e-08 -1.1436657337703359e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.595e-19\n",
      "‖w_svm‖₂       : 0.0008790427798140635\n",
      "‖alpha‖₁       : 0.8199999999999996\n",
      "scores min/max : 0.0005548255746848068 0.0022869946965863915\n",
      "Mask mean value:  tensor(0.5082, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5818  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.972e-17\n",
      "‖w_svm‖₂       : 0.043555049043359016\n",
      "‖alpha‖₁       : 0.8898339070796668\n",
      "scores min/max : -2.0101450225094575 0.3366590169613858\n",
      "Mask mean value:  tensor(0.2629, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0767  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.097e-07\n",
      "‖w_svm‖₂       : 0.1301487028101993\n",
      "‖alpha‖₁       : 0.8730380922522646\n",
      "scores min/max : -12.240065856546698 2.121122270671263\n",
      "Mask mean value:  tensor(0.4786, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.4131  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 7.273e-02\n",
      "‖w_svm‖₂       : 0.03243005493046976\n",
      "‖alpha‖₁       : 0.8984380257263247\n",
      "scores min/max : -0.7807678274776209 1.9795566099959339\n",
      "Mask mean value:  tensor(0.7901, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4483  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.248e-14\n",
      "‖w_svm‖₂       : 8.043520847026537e-08\n",
      "‖alpha‖₁       : 0.6599999999999895\n",
      "scores min/max : 6.47620846196123e-08 2.0923357582818607e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.567e-09\n",
      "‖w_svm‖₂       : 5.70618559588476e-08\n",
      "‖alpha‖₁       : 0.23999999999998178\n",
      "scores min/max : -4.469867294097626e-08 -3.0168568750680585e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.201e-20\n",
      "‖w_svm‖₂       : 0.00015974135919600215\n",
      "‖alpha‖₁       : 0.6399999999998843\n",
      "scores min/max : -4.713134061692421e-05 -3.7394574283835874e-05\n",
      "Mask mean value:  tensor(0.4998, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7641  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.094e-17\n",
      "‖w_svm‖₂       : 0.08252075383695928\n",
      "‖alpha‖₁       : 0.5772647817449362\n",
      "scores min/max : -2.061501641203472 5.879976321077907\n",
      "Mask mean value:  tensor(0.4084, dtype=torch.float64)\n",
      "max feasible return = 0.1475  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.9810049379272033e-07\n",
      "‖alpha‖₁       : 0.5800000000000001\n",
      "scores min/max : -2.717536850444463e-07 -1.810327207623217e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0312003238898037e-07\n",
      "‖alpha‖₁       : 0.2999999999999997\n",
      "scores min/max : 2.1054601861794204e-08 2.6019618316701362e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 7.086555013593677e-08\n",
      "‖alpha‖₁       : 0.5999999999999993\n",
      "scores min/max : 4.859706517511611e-09 4.159840427565611e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.747535090194434e-08\n",
      "‖alpha‖₁       : 0.37999999999999257\n",
      "scores min/max : 9.149485971891093e-09 1.8561367945870162e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.3195155159395997e-06\n",
      "‖alpha‖₁       : 0.3199999999911382\n",
      "scores min/max : 1.0356474548743551e-06 2.1282982590117595e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4258  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.06599889949273448\n",
      "‖alpha‖₁       : 0.739668801145738\n",
      "scores min/max : -3.4976556956378446 2.116170876229042\n",
      "Mask mean value:  tensor(0.8452, dtype=torch.float64)\n",
      "max feasible return = -0.8935  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03412161353252418\n",
      "‖alpha‖₁       : 0.614985042771636\n",
      "scores min/max : -1.8849506347383973 1.2451679793998545\n",
      "Mask mean value:  tensor(0.8397, dtype=torch.float64)\n",
      "max feasible return = 2.9594  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.738478523548534e-07\n",
      "‖alpha‖₁       : 0.45999999999980845\n",
      "scores min/max : 9.172703362140028e-08 6.913654775242898e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.367827501273081e-07\n",
      "‖alpha‖₁       : 0.5199999999999875\n",
      "scores min/max : 2.9219027329625926e-08 4.212131480257353e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.565276749479407e-07\n",
      "‖alpha‖₁       : 0.5799999999999991\n",
      "scores min/max : -1.7333330300287874e-07 -1.3719772398244129e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.043934515614881785\n",
      "‖alpha‖₁       : 0.6874375333047981\n",
      "scores min/max : -0.40969918062173827 1.9824588761338275\n",
      "Mask mean value:  tensor(0.4947, dtype=torch.float64)\n",
      "max feasible return = -0.0343  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.036106116417466555\n",
      "‖alpha‖₁       : 0.42211098826370863\n",
      "scores min/max : -3.93312023919382 5.158206898079091\n",
      "Mask mean value:  tensor(0.0662, dtype=torch.float64)\n",
      "max feasible return = 0.4188  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04947877845698035\n",
      "‖alpha‖₁       : 0.8210991316739763\n",
      "scores min/max : -5.129745423444612 3.02566100658872\n",
      "Mask mean value:  tensor(0.9304, dtype=torch.float64)\n",
      "max feasible return = -3.4868  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.010253339493252012\n",
      "‖alpha‖₁       : 0.7999999999999993\n",
      "scores min/max : -0.05394125657417318 0.014057425382538433\n",
      "Mask mean value:  tensor(0.4844, dtype=torch.float64)\n",
      "max feasible return = -0.1763  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.1189810600309007e-07\n",
      "‖alpha‖₁       : 0.6199999999999797\n",
      "scores min/max : 1.6760723361911266e-09 1.8155299093591362e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.724516348777366e-08\n",
      "‖alpha‖₁       : 0.43999999999996237\n",
      "scores min/max : 7.000917113357057e-09 1.6583864051279816e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.3146952576019345e-07\n",
      "‖alpha‖₁       : 0.27999999999997416\n",
      "scores min/max : -2.5510607435351197e-07 -2.3715129237873665e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  10 | train 0.005619 | val 0.006902\n",
      "-----------------------------------------Epoch:  11 ----------------------------------------\n",
      "‖w_svm‖₂       : 8.063638771437807e-08\n",
      "‖alpha‖₁       : 0.6599999999999884\n",
      "scores min/max : 6.4662966269816e-08 2.0910076283320696e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.447e-08\n",
      "‖w_svm‖₂       : 0.00018575321619097285\n",
      "‖alpha‖₁       : 0.8199999999999549\n",
      "scores min/max : -1.766830372475488e-05 -3.304878331709482e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6977  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.155e-17\n",
      "‖w_svm‖₂       : 0.032357200673058126\n",
      "‖alpha‖₁       : 0.8984376608873156\n",
      "scores min/max : -0.780400390772054 1.9798484172978723\n",
      "Mask mean value:  tensor(0.7907, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4474  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.208e-14\n",
      "‖w_svm‖₂       : 0.026772263280632466\n",
      "‖alpha‖₁       : 0.19655184291649305\n",
      "scores min/max : -2.1645017051903475 0.06139208751308709\n",
      "Mask mean value:  tensor(0.1094, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7278  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.839e-05\n",
      "‖w_svm‖₂       : 1.7123016785953894e-07\n",
      "‖alpha‖₁       : 0.23999999999999144\n",
      "scores min/max : 1.9335319746983372e-07 2.0874715744194185e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.782e-20\n",
      "‖w_svm‖₂       : 0.00508616842199923\n",
      "‖alpha‖₁       : 0.37999999999999495\n",
      "scores min/max : -0.0070794977487071 0.0035949473998868713\n",
      "Mask mean value:  tensor(0.5042, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8980  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.675e-14\n",
      "‖w_svm‖₂       : 0.0001355992819680193\n",
      "‖alpha‖₁       : 0.4399999999848311\n",
      "scores min/max : -0.0001323680140605239 1.5810861721091184e-06\n",
      "Mask mean value:  tensor(0.4998, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0270  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.508e-17\n",
      "‖w_svm‖₂       : 0.0003593193963369798\n",
      "‖alpha‖₁       : 0.7399999999999987\n",
      "scores min/max : -8.83644483376462e-05 -7.189624883509443e-05\n",
      "Mask mean value:  tensor(0.4996, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0635  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.022e-17\n",
      "‖w_svm‖₂       : 0.019654478174693575\n",
      "‖alpha‖₁       : 0.5962511531944267\n",
      "scores min/max : -2.9149788973821984 2.240632481674586\n",
      "Mask mean value:  tensor(0.8980, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.4756  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.561e-02\n",
      "‖w_svm‖₂       : 0.0031124416364274794\n",
      "‖alpha‖₁       : 0.58\n",
      "scores min/max : 0.0005971469026334994 0.0018273705421556624\n",
      "Mask mean value:  tensor(0.5079, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3229  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.353e-13\n",
      "‖w_svm‖₂       : 3.874948901236709e-07\n",
      "‖alpha‖₁       : 0.2799999999999705\n",
      "scores min/max : -1.1309848168876567e-06 -1.0688659496551722e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.169e-19\n",
      "‖w_svm‖₂       : 8.294680211330249e-08\n",
      "‖alpha‖₁       : 0.3799999999999998\n",
      "scores min/max : 1.5379696748033214e-08 3.3532270599607025e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.548e-22\n",
      "‖w_svm‖₂       : 0.12956129647903264\n",
      "‖alpha‖₁       : 0.8730335940156675\n",
      "scores min/max : -12.26723968752536 2.0956738403374326\n",
      "Mask mean value:  tensor(0.4169, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2429  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 8.283e-02\n",
      "‖w_svm‖₂       : 0.00021070061022912995\n",
      "‖alpha‖₁       : 0.6199999999984993\n",
      "scores min/max : 4.161597368749107e-05 5.734739723905507e-05\n",
      "Mask mean value:  tensor(0.5002, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.841e-17\n",
      "‖w_svm‖₂       : 0.050568904166357426\n",
      "‖alpha‖₁       : 0.9056491690630566\n",
      "scores min/max : -0.8496931672237276 1.8507028595548194\n",
      "Mask mean value:  tensor(0.2915, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5400  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.414e-03\n",
      "‖w_svm‖₂       : 0.005078951751741835\n",
      "‖alpha‖₁       : 0.7005594909362518\n",
      "scores min/max : -2.005501814287581 0.01786935212693825\n",
      "Mask mean value:  tensor(0.4475, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2513  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.708e-05\n",
      "‖w_svm‖₂       : 0.00015970325542109177\n",
      "‖alpha‖₁       : 0.6399999999999999\n",
      "scores min/max : -5.0471791483575754e-05 -4.073965238517234e-05\n",
      "Mask mean value:  tensor(0.4998, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7641  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.097e-17\n",
      "‖w_svm‖₂       : 0.000886161224583055\n",
      "‖alpha‖₁       : 0.8199999999999983\n",
      "scores min/max : 0.0006924411939172748 0.0024568425906759785\n",
      "Mask mean value:  tensor(0.5090, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5858  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.955e-17\n",
      "‖w_svm‖₂       : 0.029086177322369487\n",
      "‖alpha‖₁       : 0.5510058668451807\n",
      "scores min/max : -3.582521401575936 0.9925267297443462\n",
      "Mask mean value:  tensor(0.1048, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8907  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.430e-03\n",
      "‖w_svm‖₂       : 2.361927086269148e-07\n",
      "‖alpha‖₁       : 0.6399999999999912\n",
      "scores min/max : 1.8531048722806804e-07 2.06197325680396e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.083e-20\n",
      "‖w_svm‖₂       : 1.1769144563343625e-06\n",
      "‖alpha‖₁       : 0.31999999999865403\n",
      "scores min/max : -1.166175315625114e-06 -1.0502916879568304e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4898  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.613e-18\n",
      "‖w_svm‖₂       : 1.0214169296125882e-06\n",
      "‖alpha‖₁       : 0.5999999999999591\n",
      "scores min/max : -1.4083259319321436e-08 3.694837558465553e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.433e-19\n",
      "‖w_svm‖₂       : 0.02188788423807349\n",
      "‖alpha‖₁       : 0.3833360078972113\n",
      "scores min/max : -1.9716796403827002 0.22620394500134278\n",
      "Mask mean value:  tensor(0.6683, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1399  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.063e-14\n",
      "‖w_svm‖₂       : 4.273231832180509e-07\n",
      "‖alpha‖₁       : 0.72\n",
      "scores min/max : 2.3341385260461235e-07 2.905894899940803e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.823e-21\n",
      "‖w_svm‖₂       : 0.07841412514706493\n",
      "‖alpha‖₁       : 0.4712526378965801\n",
      "scores min/max : -2.1970590703898067 2.881543352795383\n",
      "Mask mean value:  tensor(0.1780, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.8738  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.180e-03\n",
      "‖w_svm‖₂       : 0.024107375707314235\n",
      "‖alpha‖₁       : 0.8233451928505019\n",
      "scores min/max : -1.8788627456607896 1.5229641492842143\n",
      "Mask mean value:  tensor(0.7526, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.5209  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.053e-06\n",
      "‖w_svm‖₂       : 0.022048298208233116\n",
      "‖alpha‖₁       : 0.859999999999999\n",
      "scores min/max : -0.050805553943166645 0.053339142952345264\n",
      "Mask mean value:  tensor(0.4558, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6016  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.773e-14\n",
      "‖w_svm‖₂       : 0.0004983040640512401\n",
      "‖alpha‖₁       : 0.43999999999999995\n",
      "scores min/max : -0.0005321802891806501 -0.00028099443096591434\n",
      "Mask mean value:  tensor(0.4975, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6472  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.971e-16\n",
      "‖w_svm‖₂       : 0.07941007872347847\n",
      "‖alpha‖₁       : 0.41870007709068474\n",
      "scores min/max : -1.8001659214328838 2.6479317104961146\n",
      "Mask mean value:  tensor(0.8369, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8770  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.499e-10\n",
      "‖w_svm‖₂       : 7.248275794222748e-08\n",
      "‖alpha‖₁       : 0.41999999999999993\n",
      "scores min/max : -7.120514243238178e-08 -2.4117986101112752e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.508e-08\n",
      "‖w_svm‖₂       : 0.1820402408081728\n",
      "‖alpha‖₁       : 0.8808647182795701\n",
      "scores min/max : -3.518714188000066 6.100098930851054\n",
      "Mask mean value:  tensor(0.1010, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1842  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.670e-05\n",
      "‖w_svm‖₂       : 0.05836661269648709\n",
      "‖alpha‖₁       : 0.575948320510072\n",
      "scores min/max : -1.9522867545346456 0.8694507603104781\n",
      "Mask mean value:  tensor(0.7119, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9436  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.020e-03\n",
      "‖w_svm‖₂       : 5.508423405279951e-08\n",
      "‖alpha‖₁       : 0.11999999999998079\n",
      "scores min/max : -5.845076733860477e-08 -2.7511634179451988e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.156e-07\n",
      "‖w_svm‖₂       : 1.1610637081968887e-07\n",
      "‖alpha‖₁       : 0.29999999999989096\n",
      "scores min/max : 4.444968516660471e-08 5.3498086157001105e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.170e-07\n",
      "‖w_svm‖₂       : 1.1251920652629771e-07\n",
      "‖alpha‖₁       : 0.5199999999999996\n",
      "scores min/max : 2.2317724976830975e-07 2.445780525868775e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.335e-19\n",
      "‖w_svm‖₂       : 0.016595458713064642\n",
      "‖alpha‖₁       : 0.8599999999999994\n",
      "scores min/max : -0.036103050291189376 -0.01840379781529755\n",
      "Mask mean value:  tensor(0.3855, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0932  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.881e-15\n",
      "‖w_svm‖₂       : 0.08238374650227083\n",
      "‖alpha‖₁       : 0.6580888104986663\n",
      "scores min/max : -2.0027829382473694 3.8326368008638605\n",
      "Mask mean value:  tensor(0.4127, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.5459  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.178e-04\n",
      "‖w_svm‖₂       : 7.281126580903338e-08\n",
      "‖alpha‖₁       : 0.13999999999999682\n",
      "scores min/max : -2.695687080595439e-09 3.077861485944398e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.772e-08\n",
      "‖w_svm‖₂       : 2.241634480184509e-08\n",
      "‖alpha‖₁       : 0.11999999999999529\n",
      "scores min/max : -1.2387837393197014e-08 2.2521012711631936e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.793e-09\n",
      "‖w_svm‖₂       : 4.860109552128851e-08\n",
      "‖alpha‖₁       : 0.17999999999999355\n",
      "scores min/max : 3.261536149664214e-08 4.745926459468835e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 4.702e-08\n",
      "‖w_svm‖₂       : 0.024676109507155708\n",
      "‖alpha‖₁       : 0.8150835500736171\n",
      "scores min/max : -11.524823639825273 2.035461463612218\n",
      "Mask mean value:  tensor(0.7176, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9138  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.568e-06\n",
      "‖w_svm‖₂       : 1.0869148208763059e-06\n",
      "‖alpha‖₁       : 0.49999999999999245\n",
      "scores min/max : -2.4327897061329014e-07 -5.026386099792153e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.902e-19\n",
      "‖w_svm‖₂       : 0.13973886845465175\n",
      "‖alpha‖₁       : 0.7574416519649861\n",
      "scores min/max : -1.7239441768014958 2.2531743002598366\n",
      "Mask mean value:  tensor(0.8966, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3542  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.008e-11\n",
      "‖w_svm‖₂       : 0.14708829104484966\n",
      "‖alpha‖₁       : 0.8799999999999983\n",
      "scores min/max : -1.5487018505201662 4.072569036256221\n",
      "Mask mean value:  tensor(0.2824, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2607  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.304e-03\n",
      "‖w_svm‖₂       : 0.05460011013361339\n",
      "‖alpha‖₁       : 0.9199999999999979\n",
      "scores min/max : -0.41220116079670943 0.6984837139267648\n",
      "Mask mean value:  tensor(0.4680, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3459  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 3.887e-02\n",
      "‖w_svm‖₂       : 1.3488561283439874e-07\n",
      "‖alpha‖₁       : 0.3799999999999917\n",
      "scores min/max : 3.315216832137681e-08 4.826892898055919e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.648e-07\n",
      "‖w_svm‖₂       : 0.04761817532349389\n",
      "‖alpha‖₁       : 0.938950345823786\n",
      "scores min/max : -2.4490662282166857 1.5903057080386507\n",
      "Mask mean value:  tensor(0.1496, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4025  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.768e-03\n",
      "‖w_svm‖₂       : 0.07421571622344046\n",
      "‖alpha‖₁       : 0.5799999999999803\n",
      "scores min/max : -3.0607736290572523 1.723397504201182\n",
      "Mask mean value:  tensor(0.1422, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2824  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.766e-04\n",
      "‖w_svm‖₂       : 7.698611511002249e-08\n",
      "‖alpha‖₁       : 0.5399999999999621\n",
      "scores min/max : -5.334203040400187e-08 -3.171773231033159e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.426e-20\n",
      "‖w_svm‖₂       : 0.004132018144390087\n",
      "‖alpha‖₁       : 0.4599999999999994\n",
      "scores min/max : -0.004813885783548345 -0.0030158783084242927\n",
      "Mask mean value:  tensor(0.4828, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.0987  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.656e-16\n",
      "‖w_svm‖₂       : 0.05947958585006548\n",
      "‖alpha‖₁       : 0.8982412711542389\n",
      "scores min/max : -1.7436817520891643 3.6572536818162447\n",
      "Mask mean value:  tensor(0.3400, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.051e-02\n",
      "‖w_svm‖₂       : 0.028437673332788974\n",
      "‖alpha‖₁       : 0.6599999999999991\n",
      "scores min/max : -0.06545535859735935 0.08230134881323546\n",
      "Mask mean value:  tensor(0.4277, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.4294  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.525e-15\n",
      "‖w_svm‖₂       : 1.035168733415777e-07\n",
      "‖alpha‖₁       : 0.2399999999999995\n",
      "scores min/max : 2.960472178705833e-09 1.3695216767839285e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.182e-21\n",
      "‖w_svm‖₂       : 5.715741001753515e-08\n",
      "‖alpha‖₁       : 0.23999999999998872\n",
      "scores min/max : -4.8438859378782036e-08 -3.395627955968669e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.219e-20\n",
      "‖w_svm‖₂       : 2.805561379769571e-07\n",
      "‖alpha‖₁       : 0.6599999999999998\n",
      "scores min/max : -4.583638950463583e-07 -3.0672221896047775e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.236e-18\n",
      "‖w_svm‖₂       : 0.04358593510531724\n",
      "‖alpha‖₁       : 0.8898576310581454\n",
      "scores min/max : -2.008614233713973 0.33829120070268526\n",
      "Mask mean value:  tensor(0.2673, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0782  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.784e-11\n",
      "‖w_svm‖₂       : 6.901415814534876e-07\n",
      "‖alpha‖₁       : 0.39999999999987257\n",
      "scores min/max : -1.1671299071185664e-07 1.9281018952884977e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.279e-09\n",
      "‖w_svm‖₂       : 2.0124834816697735e-07\n",
      "‖alpha‖₁       : 0.4199999999999756\n",
      "scores min/max : -2.0240435460352366e-07 -1.8008751907102066e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.472e-19\n",
      "‖w_svm‖₂       : 0.13305391895247726\n",
      "‖alpha‖₁       : 0.6531412857839161\n",
      "scores min/max : -17.935594197191154 1.8790618500584562\n",
      "Mask mean value:  tensor(0.3259, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2753  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.494e-03\n",
      "‖w_svm‖₂       : 0.01937404078152532\n",
      "‖alpha‖₁       : 0.78\n",
      "scores min/max : -0.006064249352501903 0.06458065411478642\n",
      "Mask mean value:  tensor(0.4997, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.1718  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.688e-15\n",
      "‖w_svm‖₂       : 0.04342062072331005\n",
      "‖alpha‖₁       : 0.9402300652011908\n",
      "scores min/max : -1.831312036673319 0.3049010923472406\n",
      "Mask mean value:  tensor(0.4171, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3459  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.153e-10\n",
      "‖w_svm‖₂       : 1.9393265765558343e-07\n",
      "‖alpha‖₁       : 0.379999999999999\n",
      "scores min/max : -4.631180470112615e-08 -1.061928614339286e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.600e-19\n",
      "‖w_svm‖₂       : 0.019757181861691443\n",
      "‖alpha‖₁       : 0.6815089408733462\n",
      "scores min/max : -1.979788055806039 0.05739041632022592\n",
      "Mask mean value:  tensor(0.5764, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2236  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.303e-13\n",
      "‖w_svm‖₂       : 0.05697621165391334\n",
      "‖alpha‖₁       : 0.6599999999999997\n",
      "scores min/max : -0.4080636618154716 0.3172935618728292\n",
      "Mask mean value:  tensor(0.5348, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9242  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.359e-02\n",
      "‖w_svm‖₂       : 5.484218863812213e-08\n",
      "‖alpha‖₁       : 0.43999999999999884\n",
      "scores min/max : -1.2011382855723814e-07 -1.598691827410359e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.156e-20\n",
      "‖w_svm‖₂       : 3.927375429005013e-06\n",
      "‖alpha‖₁       : 0.4199999999907167\n",
      "scores min/max : -3.419007984004037e-07 1.392161585802114e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1230  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.650e-05\n",
      "‖w_svm‖₂       : 0.05303631559179438\n",
      "‖alpha‖₁       : 0.8269154891172901\n",
      "scores min/max : -1.9604943124774041 0.39457636917063893\n",
      "Mask mean value:  tensor(0.4824, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0090  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.762e-02\n",
      "‖w_svm‖₂       : 0.006270916489377843\n",
      "‖alpha‖₁       : 0.6076626357435998\n",
      "scores min/max : -1.982343987510724 0.30467727623725976\n",
      "Mask mean value:  tensor(0.5912, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2390  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.858e-15\n",
      "‖w_svm‖₂       : 0.005522923663249703\n",
      "‖alpha‖₁       : 0.5599999999999999\n",
      "scores min/max : 0.0031327502317242528 0.004714707648542708\n",
      "Mask mean value:  tensor(0.5171, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0839  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.108e-04\n",
      "‖w_svm‖₂       : 8.798484597064022e-08\n",
      "‖alpha‖₁       : 0.179999999999991\n",
      "scores min/max : -2.993098301833899e-08 2.0575556129921656e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.367e-21\n",
      "‖w_svm‖₂       : 0.025583446259569882\n",
      "‖alpha‖₁       : 0.8522193403586605\n",
      "scores min/max : -2.9492548649726285 1.5312449516574858\n",
      "Mask mean value:  tensor(0.1416, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9301  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.028e-03\n",
      "‖w_svm‖₂       : 1.4159924337121389e-05\n",
      "‖alpha‖₁       : 0.3599999999977525\n",
      "scores min/max : 9.016742212222698e-06 9.850724025549378e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.731e-17\n",
      "‖w_svm‖₂       : 0.0003056760172964429\n",
      "‖alpha‖₁       : 0.4199999999974696\n",
      "scores min/max : 6.633656803931774e-05 0.0003829015916923383\n",
      "Mask mean value:  tensor(0.5018, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9990  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.682e-15\n",
      "‖w_svm‖₂       : 2.3720951195306913e-07\n",
      "‖alpha‖₁       : 0.5799999999999998\n",
      "scores min/max : -3.0879838635634484e-07 -2.93172539051356e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.819e-19\n",
      "‖w_svm‖₂       : 0.05570196851422366\n",
      "‖alpha‖₁       : 0.7293409767556349\n",
      "scores min/max : -0.2384833643837248 2.042911796299183\n",
      "Mask mean value:  tensor(0.7474, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3371  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 8.991e-04\n",
      "‖w_svm‖₂       : 2.2797241415221017e-07\n",
      "‖alpha‖₁       : 0.259999999999989\n",
      "scores min/max : -4.3204956527417765e-08 -3.0375999826999536e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.603e-17\n",
      "‖w_svm‖₂       : 0.08340634255193635\n",
      "‖alpha‖₁       : 0.5774282791901988\n",
      "scores min/max : -2.0734873638757296 5.86653395870146\n",
      "Mask mean value:  tensor(0.3776, dtype=torch.float64)\n",
      "max feasible return = 0.1372  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.9611281026396664e-07\n",
      "‖alpha‖₁       : 0.5799999999999998\n",
      "scores min/max : -2.9436305515838416e-07 -2.036418836394998e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0736564051123222e-07\n",
      "‖alpha‖₁       : 0.29999999999996707\n",
      "scores min/max : 2.637373689541318e-08 3.151422043026966e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 7.036907947895259e-08\n",
      "‖alpha‖₁       : 0.5999999999999993\n",
      "scores min/max : 1.0687195712108633e-08 4.742876884138173e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.7133312155710175e-08\n",
      "‖alpha‖₁       : 0.3799999999999975\n",
      "scores min/max : 1.1364059235282642e-08 2.0755565851269264e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.5837324845773944e-06\n",
      "‖alpha‖₁       : 0.31999999998789974\n",
      "scores min/max : 1.5425685914530512e-06 2.8114169478549185e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4258  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.06506103058101675\n",
      "‖alpha‖₁       : 0.7396757649082244\n",
      "scores min/max : -3.494591426078663 2.1211492622710986\n",
      "Mask mean value:  tensor(0.8506, dtype=torch.float64)\n",
      "max feasible return = -0.8989  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03383159159796947\n",
      "‖alpha‖₁       : 0.6149954504166111\n",
      "scores min/max : -1.8900247500702336 1.2421361329580638\n",
      "Mask mean value:  tensor(0.8333, dtype=torch.float64)\n",
      "max feasible return = 2.9344  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.6707508341668026e-07\n",
      "‖alpha‖₁       : 0.45999999999999885\n",
      "scores min/max : 1.4004337122641485e-07 7.342412397395855e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.365318031447141e-07\n",
      "‖alpha‖₁       : 0.519999999999989\n",
      "scores min/max : 3.223265276209998e-08 4.5136215410471604e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.556984966430627e-07\n",
      "‖alpha‖₁       : 0.5799999999999994\n",
      "scores min/max : -1.9538117900065124e-07 -1.5926380059791776e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.043945193892021936\n",
      "‖alpha‖₁       : 0.6874694110943733\n",
      "scores min/max : -0.41469140223428747 1.9771031850344583\n",
      "Mask mean value:  tensor(0.4744, dtype=torch.float64)\n",
      "max feasible return = -0.0170  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03579043982806176\n",
      "‖alpha‖₁       : 0.4221142761914507\n",
      "scores min/max : -3.930571449398951 5.1649245415913\n",
      "Mask mean value:  tensor(0.0674, dtype=torch.float64)\n",
      "max feasible return = 0.4261  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.049100773281497395\n",
      "‖alpha‖₁       : 0.8211098525237459\n",
      "scores min/max : -5.13126445044791 3.0251971384616043\n",
      "Mask mean value:  tensor(0.9301, dtype=torch.float64)\n",
      "max feasible return = -3.4856  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.010070727917264499\n",
      "‖alpha‖₁       : 0.7999999999999996\n",
      "scores min/max : -0.05278836381217402 0.012918643546461389\n",
      "Mask mean value:  tensor(0.4816, dtype=torch.float64)\n",
      "max feasible return = -0.1752  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.10347979020053e-07\n",
      "‖alpha‖₁       : 0.6199999999999776\n",
      "scores min/max : 1.6806263773230066e-08 3.329369411218735e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.720960826224842e-08\n",
      "‖alpha‖₁       : 0.43999999999996303\n",
      "scores min/max : 7.533673802208147e-09 1.7116521517209342e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.3068976113961752e-07\n",
      "‖alpha‖₁       : 0.2799999999999751\n",
      "scores min/max : -2.851635435780868e-07 -2.6721408254628665e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  11 | train 0.005583 | val 0.006880\n",
      "-----------------------------------------Epoch:  12 ----------------------------------------\n",
      "‖w_svm‖₂       : 1.7030324872113022e-07\n",
      "‖alpha‖₁       : 0.2399999999999905\n",
      "scores min/max : 2.1590845450528653e-07 2.3132203236705886e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.822e-20\n",
      "‖w_svm‖₂       : 5.699845248672251e-08\n",
      "‖alpha‖₁       : 0.239999999999986\n",
      "scores min/max : -4.9468125096814145e-08 -3.4965261322910535e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.220e-20\n",
      "‖w_svm‖₂       : 0.13346193116443736\n",
      "‖alpha‖₁       : 0.6532521840384237\n",
      "scores min/max : -17.957035569880244 1.8580056623396686\n",
      "Mask mean value:  tensor(0.2990, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2858  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.595e-03\n",
      "‖w_svm‖₂       : 0.0001603068888926206\n",
      "‖alpha‖₁       : 0.6399999999998705\n",
      "scores min/max : -4.93424935097944e-05 -3.9536787499587197e-05\n",
      "Mask mean value:  tensor(0.4998, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7641  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.098e-17\n",
      "‖w_svm‖₂       : 1.418339634683926e-05\n",
      "‖alpha‖₁       : 0.35999999999770926\n",
      "scores min/max : 9.045866855732671e-06 9.881542069508832e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.742e-17\n",
      "‖w_svm‖₂       : 0.07883860087388313\n",
      "‖alpha‖₁       : 0.47132274536516916\n",
      "scores min/max : -2.2087178508631107 2.870261145683918\n",
      "Mask mean value:  tensor(0.1669, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.7648  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.178e-02\n",
      "‖w_svm‖₂       : 7.219500494524132e-08\n",
      "‖alpha‖₁       : 0.13999999999999788\n",
      "scores min/max : -1.4542260344525426e-09 4.284809393344247e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.722e-08\n",
      "‖w_svm‖₂       : 3.841300302930936e-07\n",
      "‖alpha‖₁       : 0.27999999999999503\n",
      "scores min/max : -1.1749552129340772e-06 -1.1131914571187939e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.162e-19\n",
      "‖w_svm‖₂       : 0.0820636278048575\n",
      "‖alpha‖₁       : 0.6581073535816143\n",
      "scores min/max : -2.0083947623704814 3.8438319446335427\n",
      "Mask mean value:  tensor(0.4007, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.4456  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.158e-04\n",
      "‖w_svm‖₂       : 0.0004975218486144277\n",
      "‖alpha‖₁       : 0.43999999999999995\n",
      "scores min/max : -0.0005430598507208737 -0.00029342466167312994\n",
      "Mask mean value:  tensor(0.4975, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6470  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.972e-16\n",
      "‖w_svm‖₂       : 0.0003617274944198071\n",
      "‖alpha‖₁       : 0.7399999999997878\n",
      "scores min/max : -0.00010037011018501196 -8.368054536679899e-05\n",
      "Mask mean value:  tensor(0.4996, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0634  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.036e-17\n",
      "‖w_svm‖₂       : 0.05955525206531984\n",
      "‖alpha‖₁       : 0.8982580787973661\n",
      "scores min/max : -1.7395522517473672 3.6616818860647933\n",
      "Mask mean value:  tensor(0.3472, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5106  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.101e-02\n",
      "‖w_svm‖₂       : 6.80691642792917e-07\n",
      "‖alpha‖₁       : 0.3999999999999998\n",
      "scores min/max : -1.107810030929971e-07 1.9517630442153597e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.247e-09\n",
      "‖w_svm‖₂       : 0.055681124467648106\n",
      "‖alpha‖₁       : 0.729340015286214\n",
      "scores min/max : -0.23873152687141797 2.0426615433230855\n",
      "Mask mean value:  tensor(0.7467, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3341  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 9.034e-04\n",
      "‖w_svm‖₂       : 1.0174567688740792e-06\n",
      "‖alpha‖₁       : 0.5999999999999569\n",
      "scores min/max : -1.7916796296983822e-08 3.312179291989615e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.425e-19\n",
      "‖w_svm‖₂       : 0.05692866866934707\n",
      "‖alpha‖₁       : 0.6599999999999998\n",
      "scores min/max : -0.40919824273386046 0.3153128865889345\n",
      "Mask mean value:  tensor(0.5277, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9103  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.473e-02\n",
      "‖w_svm‖₂       : 0.1466725896237394\n",
      "‖alpha‖₁       : 0.8799999999999988\n",
      "scores min/max : -1.5389212142810744 4.048045218184892\n",
      "Mask mean value:  tensor(0.2833, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2604  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.271e-03\n",
      "‖w_svm‖₂       : 0.04333955086268993\n",
      "‖alpha‖₁       : 0.9402294059526874\n",
      "scores min/max : -1.8299134407057593 0.30627780444475616\n",
      "Mask mean value:  tensor(0.4225, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.780e-14\n",
      "‖w_svm‖₂       : 0.0031117330505763385\n",
      "‖alpha‖₁       : 0.5799999999999914\n",
      "scores min/max : 0.0006639390842455178 0.0018926163308616295\n",
      "Mask mean value:  tensor(0.5082, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3231  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.692e-13\n",
      "‖w_svm‖₂       : 0.07909815214295299\n",
      "‖alpha‖₁       : 0.4187251338033968\n",
      "scores min/max : -1.8075717483244294 2.6240463042037883\n",
      "Mask mean value:  tensor(0.8329, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8783  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.069e-13\n",
      "‖w_svm‖₂       : 0.025577769685593176\n",
      "‖alpha‖₁       : 0.8522190419194375\n",
      "scores min/max : -2.950930191657194 1.529591479164142\n",
      "Mask mean value:  tensor(0.1411, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9289  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.009e-03\n",
      "‖w_svm‖₂       : 0.00021157854519240877\n",
      "‖alpha‖₁       : 0.6199999999984224\n",
      "scores min/max : 4.4036904436525385e-05 5.9899460259412726e-05\n",
      "Mask mean value:  tensor(0.5002, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.866e-17\n",
      "‖w_svm‖₂       : 0.019654452471595214\n",
      "‖alpha‖₁       : 0.6815088942319648\n",
      "scores min/max : -1.979668023953072 0.057265356956194585\n",
      "Mask mean value:  tensor(0.5770, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2243  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.724e-14\n",
      "‖w_svm‖₂       : 1.172669539966791e-06\n",
      "‖alpha‖₁       : 0.3199999999986475\n",
      "scores min/max : -1.2986063279093215e-06 -1.1826572767992407e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4898  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.617e-18\n",
      "‖w_svm‖₂       : 1.3336061042838552e-07\n",
      "‖alpha‖₁       : 0.3799999999999979\n",
      "scores min/max : 2.0186678177412524e-08 3.516466240291497e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.244e-07\n",
      "‖w_svm‖₂       : 8.818406013132863e-08\n",
      "‖alpha‖₁       : 0.17999999999999083\n",
      "scores min/max : -3.3196025726932304e-08 2.0251465230525677e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.393e-21\n",
      "‖w_svm‖₂       : 4.8096558708722064e-08\n",
      "‖alpha‖₁       : 0.17999999999999863\n",
      "scores min/max : 3.529145664442524e-08 4.993070287366503e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 4.677e-08\n",
      "‖w_svm‖₂       : 7.696910472531639e-08\n",
      "‖alpha‖₁       : 0.5399999999999585\n",
      "scores min/max : -6.205658076121633e-08 -4.043002425544558e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.444e-20\n",
      "‖w_svm‖₂       : 3.898655966774463e-06\n",
      "‖alpha‖₁       : 0.4199999999909615\n",
      "scores min/max : -4.191053152418379e-07 1.3035962240849076e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1230  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.637e-05\n",
      "‖w_svm‖₂       : 0.1822723570796172\n",
      "‖alpha‖₁       : 0.8809969439453859\n",
      "scores min/max : -3.5328410889667405 6.08811060460621\n",
      "Mask mean value:  tensor(0.0983, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1834  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.503e-03\n",
      "‖w_svm‖₂       : 5.4980817151108606e-08\n",
      "‖alpha‖₁       : 0.43999999999999884\n",
      "scores min/max : -1.2177978261785407e-07 -1.764461186931656e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.170e-20\n",
      "‖w_svm‖₂       : 0.019482509133564044\n",
      "‖alpha‖₁       : 0.7799999999999938\n",
      "scores min/max : -0.006831819813710609 0.06522673783015898\n",
      "Mask mean value:  tensor(0.4965, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.1598  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.739e-15\n",
      "‖w_svm‖₂       : 0.028127450021679375\n",
      "‖alpha‖₁       : 0.6599999999999979\n",
      "scores min/max : -0.06273460325162925 0.08224645555666762\n",
      "Mask mean value:  tensor(0.4358, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.4904  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.414e-15\n",
      "‖w_svm‖₂       : 5.493757243747408e-08\n",
      "‖alpha‖₁       : 0.1199999999999822\n",
      "scores min/max : -6.169233585982654e-08 -3.07710811706637e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.561e-08\n",
      "‖w_svm‖₂       : 1.1577757439138818e-07\n",
      "‖alpha‖₁       : 0.29999999999997784\n",
      "scores min/max : 5.411639816849723e-08 6.318418302213365e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.172e-07\n",
      "‖w_svm‖₂       : 1.0833680304903752e-07\n",
      "‖alpha‖₁       : 0.23999999999996754\n",
      "scores min/max : 5.415264816588125e-09 1.6460143335033975e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.372e-21\n",
      "‖w_svm‖₂       : 0.07404839321197695\n",
      "‖alpha‖₁       : 0.58\n",
      "scores min/max : -3.0460317751513144 1.717074448909798\n",
      "Mask mean value:  tensor(0.1446, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2854  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.741e-04\n",
      "‖w_svm‖₂       : 0.1400951200828093\n",
      "‖alpha‖₁       : 0.7575438564762588\n",
      "scores min/max : -1.7261743268305543 2.2511034929303375\n",
      "Mask mean value:  tensor(0.8942, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3546  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.050e-11\n",
      "‖w_svm‖₂       : 0.023754841977149863\n",
      "‖alpha‖₁       : 0.8233447932089031\n",
      "scores min/max : -1.8814782042377731 1.5200477755627093\n",
      "Mask mean value:  tensor(0.7464, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.5052  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.589e-06\n",
      "‖w_svm‖₂       : 0.005123846726148454\n",
      "‖alpha‖₁       : 0.37999999999999984\n",
      "scores min/max : -0.007577135726329856 0.0034172098602328477\n",
      "Mask mean value:  tensor(0.5029, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8954  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.725e-14\n",
      "‖w_svm‖₂       : 0.02899339004427434\n",
      "‖alpha‖₁       : 0.5510069734899581\n",
      "scores min/max : -3.5781697477606658 0.9966024039487416\n",
      "Mask mean value:  tensor(0.1062, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9022  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.467e-03\n",
      "‖w_svm‖₂       : 8.277974836238207e-08\n",
      "‖alpha‖₁       : 0.37999999999999967\n",
      "scores min/max : 7.642318108183708e-09 2.5794726847244466e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.549e-22\n",
      "‖w_svm‖₂       : 0.006272000454160981\n",
      "‖alpha‖₁       : 0.6076626490191447\n",
      "scores min/max : -1.982500749153684 0.3045205183067158\n",
      "Mask mean value:  tensor(0.5906, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2387  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.877e-15\n",
      "‖w_svm‖₂       : 0.021605462863255116\n",
      "‖alpha‖₁       : 0.3833361219427118\n",
      "scores min/max : -1.9712461203028817 0.22700918850374996\n",
      "Mask mean value:  tensor(0.6706, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1407  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.715e-14\n",
      "‖w_svm‖₂       : 4.254601281531405e-07\n",
      "‖alpha‖₁       : 0.7199999999999999\n",
      "scores min/max : 2.3189564292242764e-07 2.891107030879473e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.802e-21\n",
      "‖w_svm‖₂       : 0.047734288164531076\n",
      "‖alpha‖₁       : 0.9389539837495499\n",
      "scores min/max : -2.4490975804972597 1.5901356920915173\n",
      "Mask mean value:  tensor(0.1495, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4023  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.764e-03\n",
      "‖w_svm‖₂       : 0.127894928525954\n",
      "‖alpha‖₁       : 0.8730439320701842\n",
      "scores min/max : -12.253167093477208 2.1160887982263104\n",
      "Mask mean value:  tensor(0.4608, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3641  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.531e-02\n",
      "‖w_svm‖₂       : 2.2435351633793473e-08\n",
      "‖alpha‖₁       : 0.11999999999999468\n",
      "scores min/max : -1.4387239703288953e-08 2.815853242542762e-10\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.796e-09\n",
      "‖w_svm‖₂       : 2.3549638935530246e-07\n",
      "‖alpha‖₁       : 0.6399999999999995\n",
      "scores min/max : 2.0913287164933859e-07 2.299590231470203e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.090e-20\n",
      "‖w_svm‖₂       : 2.015294858677094e-07\n",
      "‖alpha‖₁       : 0.4199999999999754\n",
      "scores min/max : -2.1513473248332609e-07 -1.9281591806497877e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.485e-19\n",
      "‖w_svm‖₂       : 1.0850465042730444e-06\n",
      "‖alpha‖₁       : 0.4999999999999932\n",
      "scores min/max : -2.442086978164473e-07 -5.122124030936202e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.880e-19\n",
      "‖w_svm‖₂       : 2.8087281180630636e-07\n",
      "‖alpha‖₁       : 0.6599999999999998\n",
      "scores min/max : -4.5648568726136414e-07 -3.0484560984820327e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.234e-18\n",
      "‖w_svm‖₂       : 1.9335205475786932e-07\n",
      "‖alpha‖₁       : 0.37999999999999906\n",
      "scores min/max : -3.731180291958762e-08 -1.6187535992932142e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.568e-19\n",
      "‖w_svm‖₂       : 0.058152007103612034\n",
      "‖alpha‖₁       : 0.5759670548051685\n",
      "scores min/max : -1.9518179138846032 0.8699853938185673\n",
      "Mask mean value:  tensor(0.7136, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9479  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.017e-03\n",
      "‖w_svm‖₂       : 0.052936329974982994\n",
      "‖alpha‖₁       : 0.8269206615451078\n",
      "scores min/max : -1.9581463192352868 0.39621511765412953\n",
      "Mask mean value:  tensor(0.4908, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0069  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.714e-02\n",
      "‖w_svm‖₂       : 1.1233570805732926e-07\n",
      "‖alpha‖₁       : 0.5199999999999996\n",
      "scores min/max : 2.2995394972363313e-07 2.513543209284676e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.340e-19\n",
      "‖w_svm‖₂       : 0.0003055079605647386\n",
      "‖alpha‖₁       : 0.41999999999167753\n",
      "scores min/max : 5.655964357061767e-05 0.00037269707705692005\n",
      "Mask mean value:  tensor(0.5017, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9989  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.679e-15\n",
      "‖w_svm‖₂       : 0.04998408867851547\n",
      "‖alpha‖₁       : 0.9056487088020564\n",
      "scores min/max : -0.8257568735488139 1.8568982131135394\n",
      "Mask mean value:  tensor(0.3012, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5575  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.023e-03\n",
      "‖w_svm‖₂       : 2.2645266488975366e-07\n",
      "‖alpha‖₁       : 0.25999999999999995\n",
      "scores min/max : -2.8336323996711772e-08 -1.5616283702747257e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.579e-17\n",
      "‖w_svm‖₂       : 0.019264054497021413\n",
      "‖alpha‖₁       : 0.5962514638731842\n",
      "scores min/max : -2.9183463860505885 2.2453945624252656\n",
      "Mask mean value:  tensor(0.9010, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.4819  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.212e-02\n",
      "‖w_svm‖₂       : 0.021662662688804016\n",
      "‖alpha‖₁       : 0.8599999999999992\n",
      "scores min/max : -0.05160443590853327 0.04938088256136719\n",
      "Mask mean value:  tensor(0.4459, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5878  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.173e-14\n",
      "‖w_svm‖₂       : 7.887576243801377e-08\n",
      "‖alpha‖₁       : 0.6599999999999945\n",
      "scores min/max : 7.31309053347779e-08 2.1796908935572881e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.546e-09\n",
      "‖w_svm‖₂       : 0.00018618170112501397\n",
      "‖alpha‖₁       : 0.8199999999999408\n",
      "scores min/max : -2.6190791845705154e-05 -1.1768794071140547e-05\n",
      "Mask mean value:  tensor(0.4999, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6976  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.163e-17\n",
      "‖w_svm‖₂       : 0.02731200345045653\n",
      "‖alpha‖₁       : 0.19658212349110332\n",
      "scores min/max : -2.17458008945921 0.051215122914304766\n",
      "Mask mean value:  tensor(0.0939, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6272  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.340e-04\n",
      "‖w_svm‖₂       : 0.005553072067976635\n",
      "‖alpha‖₁       : 0.5599999999999957\n",
      "scores min/max : 0.0032026151478059817 0.004801645013605819\n",
      "Mask mean value:  tensor(0.5175, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0846  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.068e-04\n",
      "‖w_svm‖₂       : 0.005170075121219571\n",
      "‖alpha‖₁       : 0.7005603283696743\n",
      "scores min/max : -2.0050904861922203 0.018283291844005657\n",
      "Mask mean value:  tensor(0.4495, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2524  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.332e-05\n",
      "‖w_svm‖₂       : 2.372124276362852e-07\n",
      "‖alpha‖₁       : 0.5800000000000001\n",
      "scores min/max : -3.023968066302024e-07 -2.867611317901917e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.824e-19\n",
      "‖w_svm‖₂       : 0.0438373255177916\n",
      "‖alpha‖₁       : 0.8898889634343743\n",
      "scores min/max : -2.0086219086603196 0.33833793148810176\n",
      "Mask mean value:  tensor(0.2673, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0778  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.033e-12\n",
      "‖w_svm‖₂       : 0.03319106726730629\n",
      "‖alpha‖₁       : 0.8984932570150768\n",
      "scores min/max : -0.7745212154153206 1.9857295228873297\n",
      "Mask mean value:  tensor(0.8016, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.619e-14\n",
      "‖w_svm‖₂       : 0.00013644738433272677\n",
      "‖alpha‖₁       : 0.4399999999876005\n",
      "scores min/max : -0.00015158262694105564 -1.5901731297376736e-05\n",
      "Mask mean value:  tensor(0.4997, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0263  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.529e-17\n",
      "‖w_svm‖₂       : 0.016783402154087843\n",
      "‖alpha‖₁       : 0.8599999999999637\n",
      "scores min/max : -0.038973126888747725 -0.02092272062256073\n",
      "Mask mean value:  tensor(0.3733, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0888  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.012e-14\n",
      "‖w_svm‖₂       : 0.0008862924075276078\n",
      "‖alpha‖₁       : 0.8199999999999995\n",
      "scores min/max : 0.0007589420905658971 0.002522001696602047\n",
      "Mask mean value:  tensor(0.5093, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5875  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.991e-17\n",
      "‖w_svm‖₂       : 7.1942277912161e-08\n",
      "‖alpha‖₁       : 0.4199999999999969\n",
      "scores min/max : -7.244869208108555e-08 -2.5239319107088864e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.468e-08\n",
      "‖w_svm‖₂       : 0.004175344282852696\n",
      "‖alpha‖₁       : 0.45999999999999996\n",
      "scores min/max : -0.005200538252686305 -0.0033661411976929276\n",
      "Mask mean value:  tensor(0.4810, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.0909  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.776e-16\n",
      "‖w_svm‖₂       : 0.02473181917266107\n",
      "‖alpha‖₁       : 0.815084705498209\n",
      "scores min/max : -11.527268544955536 2.0336755698158284\n",
      "Mask mean value:  tensor(0.7135, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9046  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.544e-06\n",
      "‖w_svm‖₂       : 0.053781445726844616\n",
      "‖alpha‖₁       : 0.92\n",
      "scores min/max : -0.40880483514846405 0.6666599247955733\n",
      "Mask mean value:  tensor(0.4380, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.1866  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 4.826e-02\n",
      "‖w_svm‖₂       : 0.08421342309072756\n",
      "‖alpha‖₁       : 0.5775745970216564\n",
      "scores min/max : -2.0875434752780273 5.851670701593184\n",
      "Mask mean value:  tensor(0.3439, dtype=torch.float64)\n",
      "max feasible return = 0.1276  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.957144008452434e-07\n",
      "‖alpha‖₁       : 0.5800000000000001\n",
      "scores min/max : -2.9917987391191977e-07 -2.0845939043505953e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0264305978240582e-07\n",
      "‖alpha‖₁       : 0.29999999999999966\n",
      "scores min/max : 2.501153293874355e-08 2.997829089593602e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 7.022683737892183e-08\n",
      "‖alpha‖₁       : 0.5999999999999994\n",
      "scores min/max : 9.08403061167143e-09 4.582575717077089e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.7061418641499266e-08\n",
      "‖alpha‖₁       : 0.3799999999999974\n",
      "scores min/max : 9.707929501065467e-09 1.910011751701526e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.4018654329203184e-06\n",
      "‖alpha‖₁       : 0.3199999999903071\n",
      "scores min/max : 1.4844219284949112e-06 2.641117637749291e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4258  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.06474296206923046\n",
      "‖alpha‖₁       : 0.7396832520210693\n",
      "scores min/max : -3.487105900951526 2.1291933240028054\n",
      "Mask mean value:  tensor(0.8592, dtype=torch.float64)\n",
      "max feasible return = -0.9067  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.033582129056526924\n",
      "‖alpha‖₁       : 0.614995974323829\n",
      "scores min/max : -1.8935309932588824 1.2397759621359705\n",
      "Mask mean value:  tensor(0.8283, dtype=torch.float64)\n",
      "max feasible return = 2.9150  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.6685490941525705e-07\n",
      "‖alpha‖₁       : 0.45999999999999225\n",
      "scores min/max : 1.3311205044670125e-07 7.276352329025782e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.3627499991527865e-07\n",
      "‖alpha‖₁       : 0.5199999999999896\n",
      "scores min/max : 3.695267169225983e-08 4.9856414590780096e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.552871851299047e-07\n",
      "‖alpha‖₁       : 0.5799999999999996\n",
      "scores min/max : -1.9687511931907072e-07 -1.6076379137388543e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.044369430176767546\n",
      "‖alpha‖₁       : 0.6875072202429056\n",
      "scores min/max : -0.41862978368011466 1.97316237057125\n",
      "Mask mean value:  tensor(0.4592, dtype=torch.float64)\n",
      "max feasible return = -0.0026  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.0357077997858392\n",
      "‖alpha‖₁       : 0.4221180341703029\n",
      "scores min/max : -3.929070999090633 5.168059396358921\n",
      "Mask mean value:  tensor(0.0680, dtype=torch.float64)\n",
      "max feasible return = 0.4299  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.048956431539383935\n",
      "‖alpha‖₁       : 0.8211163192325264\n",
      "scores min/max : -5.128857036834276 3.0280722523621226\n",
      "Mask mean value:  tensor(0.9310, dtype=torch.float64)\n",
      "max feasible return = -3.4914  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.009995520933433122\n",
      "‖alpha‖₁       : 0.7999999999999996\n",
      "scores min/max : -0.0524031716905891 0.012425660259677708\n",
      "Mask mean value:  tensor(0.4803, dtype=torch.float64)\n",
      "max feasible return = -0.1747  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.0980511844435403e-07\n",
      "‖alpha‖₁       : 0.6199999999999767\n",
      "scores min/max : 1.2964322930976122e-08 2.9454267134139482e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.71213715755016e-08\n",
      "‖alpha‖₁       : 0.4399999999999634\n",
      "scores min/max : 7.552034157872057e-09 1.7134410433104736e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.3035603218635208e-07\n",
      "‖alpha‖₁       : 0.27999999999997544\n",
      "scores min/max : -2.852831863394311e-07 -2.6733297990935716e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  12 | train 0.005560 | val 0.006834\n",
      "-----------------------------------------Epoch:  13 ----------------------------------------\n",
      "‖w_svm‖₂       : 0.005145759844388963\n",
      "‖alpha‖₁       : 0.3799999999998675\n",
      "scores min/max : -0.007312245300649611 0.0037193784922931704\n",
      "Mask mean value:  tensor(0.5044, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8980  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.724e-14\n",
      "‖w_svm‖₂       : 0.060259347406173405\n",
      "‖alpha‖₁       : 0.8983582962937234\n",
      "scores min/max : -1.7345204117346573 3.6670802355309142\n",
      "Mask mean value:  tensor(0.3561, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5182  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.179e-02\n",
      "‖w_svm‖₂       : 0.027436410074186478\n",
      "‖alpha‖₁       : 0.1965889945895525\n",
      "scores min/max : -2.179602027254122 0.04618961322963954\n",
      "Mask mean value:  tensor(0.0870, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5821  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.777e-04\n",
      "‖w_svm‖₂       : 1.0807993337025839e-06\n",
      "‖alpha‖₁       : 0.4999999999999929\n",
      "scores min/max : -2.551139640953757e-07 -6.207156418320009e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.849e-19\n",
      "‖w_svm‖₂       : 0.07971183423605557\n",
      "‖alpha‖₁       : 0.47146917803651633\n",
      "scores min/max : -2.24141933991211 2.8376509834270207\n",
      "Mask mean value:  tensor(0.1415, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.5144  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.306e-02\n",
      "‖w_svm‖₂       : 2.235421297314324e-08\n",
      "‖alpha‖₁       : 0.11999999999999467\n",
      "scores min/max : -1.5555552839756878e-08 -8.851773864835756e-10\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.794e-09\n",
      "‖w_svm‖₂       : 1.4229337982652913e-05\n",
      "‖alpha‖₁       : 0.3599999999976554\n",
      "scores min/max : 9.685997714546576e-06 1.052824724112244e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.789e-17\n",
      "‖w_svm‖₂       : 4.233500361611347e-07\n",
      "‖alpha‖₁       : 0.7199999999999995\n",
      "scores min/max : 2.3050714964414706e-07 2.8771941485969596e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.809e-21\n",
      "‖w_svm‖₂       : 2.2711462308698717e-07\n",
      "‖alpha‖₁       : 0.2599999999999849\n",
      "scores min/max : -2.0377216660497327e-08 -7.527386348163492e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.593e-17\n",
      "‖w_svm‖₂       : 0.00016124309463741101\n",
      "‖alpha‖₁       : 0.6399999999999997\n",
      "scores min/max : -6.469545259220451e-05 -5.4774953236140414e-05\n",
      "Mask mean value:  tensor(0.4997, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7640  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.125e-17\n",
      "‖w_svm‖₂       : 0.058803502552240115\n",
      "‖alpha‖₁       : 0.57604000505147\n",
      "scores min/max : -1.9535332569430186 0.8682474366673187\n",
      "Mask mean value:  tensor(0.7082, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9324  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.004e-03\n",
      "‖w_svm‖₂       : 0.027751549753207403\n",
      "‖alpha‖₁       : 0.6599999999999956\n",
      "scores min/max : -0.06199997918018188 0.07914066067489653\n",
      "Mask mean value:  tensor(0.4330, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.4667  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.382e-15\n",
      "‖w_svm‖₂       : 0.03333818731238146\n",
      "‖alpha‖₁       : 0.8985049857946406\n",
      "scores min/max : -0.7724494301271732 1.9878250765788772\n",
      "Mask mean value:  tensor(0.8054, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4295  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.523e-14\n",
      "‖w_svm‖₂       : 0.0002129389907003253\n",
      "‖alpha‖₁       : 0.6199999999984186\n",
      "scores min/max : 4.519975880525494e-05 6.126683418978538e-05\n",
      "Mask mean value:  tensor(0.5002, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.911e-17\n",
      "‖w_svm‖₂       : 5.663202977114602e-08\n",
      "‖alpha‖₁       : 0.23999999999997837\n",
      "scores min/max : -5.111733105753642e-08 -3.656629673159015e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.222e-20\n",
      "‖w_svm‖₂       : 8.225059846460146e-08\n",
      "‖alpha‖₁       : 0.3799999999999997\n",
      "scores min/max : 8.493106105539396e-09 2.6644269342288853e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.513e-22\n",
      "‖w_svm‖₂       : 0.12699614344959434\n",
      "‖alpha‖₁       : 0.8730542147808631\n",
      "scores min/max : -12.264166948218653 2.107821804834381\n",
      "Mask mean value:  tensor(0.4389, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3037  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 4.175e-02\n",
      "‖w_svm‖₂       : 0.00036417300514723515\n",
      "‖alpha‖₁       : 0.7399999999998619\n",
      "scores min/max : -0.00012173978124073486 -0.00010482387561470201\n",
      "Mask mean value:  tensor(0.4995, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0631  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.055e-17\n",
      "‖w_svm‖₂       : 0.14484752176847268\n",
      "‖alpha‖₁       : 0.879999999999998\n",
      "scores min/max : -1.4981661320131718 3.951779160926608\n",
      "Mask mean value:  tensor(0.2904, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2604  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.059e-03\n",
      "‖w_svm‖₂       : 1.0734283043215907e-07\n",
      "‖alpha‖₁       : 0.2399999999999705\n",
      "scores min/max : 9.950420161018787e-09 2.0981270474784997e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.317e-21\n",
      "‖w_svm‖₂       : 2.3378435346310143e-07\n",
      "‖alpha‖₁       : 0.6399999999999899\n",
      "scores min/max : 2.325651982608894e-07 2.5349665190559195e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.063e-20\n",
      "‖w_svm‖₂       : 0.000187818635951168\n",
      "‖alpha‖₁       : 0.8199999999999218\n",
      "scores min/max : -3.227473667827223e-05 -1.7596690769613966e-05\n",
      "Mask mean value:  tensor(0.4999, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6975  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.173e-17\n",
      "‖w_svm‖₂       : 0.05328913147382262\n",
      "‖alpha‖₁       : 0.9199999999999995\n",
      "scores min/max : -0.40887440903958583 0.6456354317599559\n",
      "Mask mean value:  tensor(0.4107, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.0417  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 4.289e-02\n",
      "‖w_svm‖₂       : 2.1197030772031297e-07\n",
      "‖alpha‖₁       : 0.3799999999947056\n",
      "scores min/max : 1.4491668978234243e-08 3.346084520176974e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.529e-07\n",
      "‖w_svm‖₂       : 0.05317972871073392\n",
      "‖alpha‖₁       : 0.8269674180598084\n",
      "scores min/max : -1.954617907399271 0.3988001274443124\n",
      "Mask mean value:  tensor(0.5034, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0038  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.681e-02\n",
      "‖w_svm‖₂       : 0.016847490235225537\n",
      "‖alpha‖₁       : 0.8599999999999973\n",
      "scores min/max : -0.03914064823048023 -0.021137624629685464\n",
      "Mask mean value:  tensor(0.3724, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0886  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.023e-14\n",
      "‖w_svm‖₂       : 0.04380060521947234\n",
      "‖alpha‖₁       : 0.8898971246269809\n",
      "scores min/max : -2.0081539983103314 0.33891033924970165\n",
      "Mask mean value:  tensor(0.2687, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0782  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.774e-11\n",
      "‖w_svm‖₂       : 0.07309957868884621\n",
      "‖alpha‖₁       : 0.5800000000000001\n",
      "scores min/max : -2.9704045119204805 1.6667980056620941\n",
      "Mask mean value:  tensor(0.1405, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2767  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.809e-04\n",
      "‖w_svm‖₂       : 0.01962561655453232\n",
      "‖alpha‖₁       : 0.7799999999999989\n",
      "scores min/max : -0.008956243782917426 0.06378890646872726\n",
      "Mask mean value:  tensor(0.4862, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.1182  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.937e-15\n",
      "‖w_svm‖₂       : 5.462446646891159e-08\n",
      "‖alpha‖₁       : 0.4399999999999989\n",
      "scores min/max : -1.3083463873659754e-07 -2.6698233938580735e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.179e-20\n",
      "‖w_svm‖₂       : 0.0004989579969918994\n",
      "‖alpha‖₁       : 0.43999999999999995\n",
      "scores min/max : -0.0005740795240227926 -0.00032396942272562936\n",
      "Mask mean value:  tensor(0.4973, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6465  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.011e-16\n",
      "‖w_svm‖₂       : 1.0079393070860478e-06\n",
      "‖alpha‖₁       : 0.5999999999999143\n",
      "scores min/max : -3.6935592446473644e-08 1.416353198377339e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.413e-19\n",
      "‖w_svm‖₂       : 0.08137933674184551\n",
      "‖alpha‖₁       : 0.6581686220262961\n",
      "scores min/max : -2.0189447864145365 3.875359317615562\n",
      "Mask mean value:  tensor(0.3792, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.2596  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.243e-04\n",
      "‖w_svm‖₂       : 0.0008808717195033168\n",
      "‖alpha‖₁       : 0.819999999999999\n",
      "scores min/max : 0.0007932047746233659 0.0025251563288795145\n",
      "Mask mean value:  tensor(0.5094, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5879  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.975e-17\n",
      "‖w_svm‖₂       : 0.14238454378819335\n",
      "‖alpha‖₁       : 0.7581999051171379\n",
      "scores min/max : -1.7247654207851824 2.2526874220660216\n",
      "Mask mean value:  tensor(0.8956, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3562  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.066e-11\n",
      "‖w_svm‖₂       : 1.9087792046964303e-07\n",
      "‖alpha‖₁       : 0.37999999999999895\n",
      "scores min/max : -5.1817389937381685e-08 -1.61212917721391e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.523e-19\n",
      "‖w_svm‖₂       : 0.005540511108160341\n",
      "‖alpha‖₁       : 0.5599999999999997\n",
      "scores min/max : 0.00323450180383995 0.004825612776191163\n",
      "Mask mean value:  tensor(0.5176, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0850  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.057e-04\n",
      "‖w_svm‖₂       : 7.627265504637577e-08\n",
      "‖alpha‖₁       : 0.539999999999943\n",
      "scores min/max : -7.977428332730473e-08 -5.813966462328794e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.444e-20\n",
      "‖w_svm‖₂       : 0.01894790584556223\n",
      "‖alpha‖₁       : 0.5962518023399845\n",
      "scores min/max : -2.9170608288143427 2.2528482933118954\n",
      "Mask mean value:  tensor(0.9054, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.4901  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.275e-03\n",
      "‖w_svm‖₂       : 0.13509133471456258\n",
      "‖alpha‖₁       : 0.6537242699555377\n",
      "scores min/max : -18.003597099350426 1.8173386799886162\n",
      "Mask mean value:  tensor(0.2556, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2878  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.534e-03\n",
      "‖w_svm‖₂       : 0.18415150737001973\n",
      "‖alpha‖₁       : 0.8818014582201621\n",
      "scores min/max : -3.536857489863868 6.088296373776927\n",
      "Mask mean value:  tensor(0.0982, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1851  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.363e-03\n",
      "‖w_svm‖₂       : 0.055681835956162114\n",
      "‖alpha‖₁       : 0.6599999999999975\n",
      "scores min/max : -0.3949477460341061 0.3002806557331386\n",
      "Mask mean value:  tensor(0.5164, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.019e-14\n",
      "‖w_svm‖₂       : 1.1059698722155262e-07\n",
      "‖alpha‖₁       : 0.5199999999999996\n",
      "scores min/max : 2.6646601300381987e-07 2.878668207099677e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.361e-19\n",
      "‖w_svm‖₂       : 0.025767771721764067\n",
      "‖alpha‖₁       : 0.8522289239262258\n",
      "scores min/max : -2.9393966065099866 1.5412187480717905\n",
      "Mask mean value:  tensor(0.1454, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9370  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.153e-03\n",
      "‖w_svm‖₂       : 0.02111322495524175\n",
      "‖alpha‖₁       : 0.3833361230537006\n",
      "scores min/max : -1.9708241218400995 0.22808440322293397\n",
      "Mask mean value:  tensor(0.6733, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1419  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.453e-14\n",
      "‖w_svm‖₂       : 2.769535669779175e-07\n",
      "‖alpha‖₁       : 0.6599999999999671\n",
      "scores min/max : -5.21114199090687e-07 -3.6900907441388277e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.238e-18\n",
      "‖w_svm‖₂       : 0.047892283983201604\n",
      "‖alpha‖₁       : 0.9389714430028031\n",
      "scores min/max : -2.4399002052418437 1.5997367749063776\n",
      "Mask mean value:  tensor(0.1562, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.840e-03\n",
      "‖w_svm‖₂       : 0.00013824032877096337\n",
      "‖alpha‖₁       : 0.4399999999890516\n",
      "scores min/max : -0.00017716161096934576 -3.7901306622338104e-05\n",
      "Mask mean value:  tensor(0.4996, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0253  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.572e-17\n",
      "‖w_svm‖₂       : 0.049274642894942974\n",
      "‖alpha‖₁       : 0.9056476155826295\n",
      "scores min/max : -0.7971132619239553 1.8656029804203413\n",
      "Mask mean value:  tensor(0.3178, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5880  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.843e-03\n",
      "‖w_svm‖₂       : 0.07895315137077039\n",
      "‖alpha‖₁       : 0.41888276133414587\n",
      "scores min/max : -1.8155312870704527 2.5786082874199874\n",
      "Mask mean value:  tensor(0.8289, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8804  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.139e-13\n",
      "‖w_svm‖₂       : 4.746119892546251e-08\n",
      "‖alpha‖₁       : 0.1799999999999975\n",
      "scores min/max : 3.7337729235368654e-08 5.204827135835462e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 4.620e-08\n",
      "‖w_svm‖₂       : 1.1571809618316688e-06\n",
      "‖alpha‖₁       : 0.31999999999862194\n",
      "scores min/max : -1.5917367967168114e-06 -1.4755764165061442e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4898  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.631e-18\n",
      "‖w_svm‖₂       : 5.4208587224876855e-08\n",
      "‖alpha‖₁       : 0.11999999999998227\n",
      "scores min/max : -7.013283099926465e-08 -3.92149153624278e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.136e-07\n",
      "‖w_svm‖₂       : 0.005246173366722841\n",
      "‖alpha‖₁       : 0.7005610965068254\n",
      "scores min/max : -2.004402702791957 0.018974385642133745\n",
      "Mask mean value:  tensor(0.4528, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2541  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.600e-05\n",
      "‖w_svm‖₂       : 0.0003108633201910561\n",
      "‖alpha‖₁       : 0.4199999999971945\n",
      "scores min/max : 0.00012464007965459593 0.0004520393988979364\n",
      "Mask mean value:  tensor(0.5021, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9997  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.724e-15\n",
      "‖w_svm‖₂       : 8.717400750862756e-08\n",
      "‖alpha‖₁       : 0.1799999999999944\n",
      "scores min/max : -5.573676408113183e-08 1.7927296001423103e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.432e-21\n",
      "‖w_svm‖₂       : 0.02465547455619301\n",
      "‖alpha‖₁       : 0.815084960161098\n",
      "scores min/max : -11.529924329284448 2.0322999458954816\n",
      "Mask mean value:  tensor(0.7100, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.8974  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.221e-06\n",
      "‖w_svm‖₂       : 0.021257229019995834\n",
      "‖alpha‖₁       : 0.8599999999999999\n",
      "scores min/max : -0.05459318420070322 0.04301087739908312\n",
      "Mask mean value:  tensor(0.4250, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5605  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.335e-14\n",
      "‖w_svm‖₂       : 3.5405192055926227e-06\n",
      "‖alpha‖₁       : 0.41999999999306514\n",
      "scores min/max : -7.241463166962762e-07 9.125281344957535e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1230  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.473e-05\n",
      "‖w_svm‖₂       : 0.04269686287576507\n",
      "‖alpha‖₁       : 0.9402471388896246\n",
      "scores min/max : -1.8261126943219557 0.30968852679335745\n",
      "Mask mean value:  tensor(0.4368, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3429  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.747e-12\n",
      "‖w_svm‖₂       : 0.01904354526822661\n",
      "‖alpha‖₁       : 0.6815090833672857\n",
      "scores min/max : -1.9791206799688417 0.05602980671840274\n",
      "Mask mean value:  tensor(0.5797, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2285  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.393e-13\n",
      "‖w_svm‖₂       : 7.710558619278434e-08\n",
      "‖alpha‖₁       : 0.6599999999999973\n",
      "scores min/max : 8.632295259030275e-08 2.3130646399151322e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.548e-09\n",
      "‖w_svm‖₂       : 0.006379223952345367\n",
      "‖alpha‖₁       : 0.6076640137397169\n",
      "scores min/max : -1.983536868676735 0.3034842849515249\n",
      "Mask mean value:  tensor(0.5861, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2368  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.149e-15\n",
      "‖w_svm‖₂       : 1.9844213379861769e-07\n",
      "‖alpha‖₁       : 0.4199999999999755\n",
      "scores min/max : -2.705557153901365e-07 -2.4823503919763847e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.489e-19\n",
      "‖w_svm‖₂       : 7.073191056513386e-08\n",
      "‖alpha‖₁       : 0.42000000000000004\n",
      "scores min/max : -7.220102978638907e-08 -2.511972869198376e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.336e-08\n",
      "‖w_svm‖₂       : 0.02894966649375948\n",
      "‖alpha‖₁       : 0.5510145600255285\n",
      "scores min/max : -3.573073153344455 1.0012656075637487\n",
      "Mask mean value:  tensor(0.1078, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9151  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.704e-03\n",
      "‖w_svm‖₂       : 0.0031061311585174898\n",
      "‖alpha‖₁       : 0.5799999999999964\n",
      "scores min/max : 0.0009093242647155819 0.0021342020910837117\n",
      "Mask mean value:  tensor(0.5094, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3239  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.918e-13\n",
      "‖w_svm‖₂       : 0.004180503486957289\n",
      "‖alpha‖₁       : 0.45999999999999935\n",
      "scores min/max : -0.005350887810079795 -0.003516237836279898\n",
      "Mask mean value:  tensor(0.4803, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.0876  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.843e-16\n",
      "‖w_svm‖₂       : 0.05421833968346375\n",
      "‖alpha‖₁       : 0.72934578256035\n",
      "scores min/max : -0.24226713781731213 2.0395540506989223\n",
      "Mask mean value:  tensor(0.7369, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2930  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 9.806e-04\n",
      "‖w_svm‖₂       : 3.7819436433614167e-07\n",
      "‖alpha‖₁       : 0.2799999999999975\n",
      "scores min/max : -1.3165101403822837e-06 -1.2548008205747881e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.185e-19\n",
      "‖w_svm‖₂       : 1.124757388781763e-07\n",
      "‖alpha‖₁       : 0.299999999999996\n",
      "scores min/max : 7.288743319100987e-08 8.184111735455875e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.152e-07\n",
      "‖w_svm‖₂       : 0.023356600723115056\n",
      "‖alpha‖₁       : 0.8233531133225543\n",
      "scores min/max : -1.882239379171264 1.5188260260160942\n",
      "Mask mean value:  tensor(0.7445, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.4984  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.554e-06\n",
      "‖w_svm‖₂       : 7.137124513257735e-08\n",
      "‖alpha‖₁       : 0.13999999999999438\n",
      "scores min/max : 1.4283283030414588e-09 7.219655143859173e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.664e-08\n",
      "‖w_svm‖₂       : 1.695949222487133e-07\n",
      "‖alpha‖₁       : 0.239999999999976\n",
      "scores min/max : 2.4778512055414235e-07 2.6353249630080324e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.901e-20\n",
      "‖w_svm‖₂       : 6.77223551386287e-07\n",
      "‖alpha‖₁       : 0.3999999999999999\n",
      "scores min/max : -9.74194173453088e-08 2.0899803183975713e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.247e-09\n",
      "‖w_svm‖₂       : 2.3314498619191097e-07\n",
      "‖alpha‖₁       : 0.5799999999999992\n",
      "scores min/max : -3.424788194601272e-07 -3.2685312393155036e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.814e-19\n",
      "‖w_svm‖₂       : 0.08496491550525696\n",
      "‖alpha‖₁       : 0.5777190555898688\n",
      "scores min/max : -2.1053307945501953 5.832431670260996\n",
      "Mask mean value:  tensor(0.3053, dtype=torch.float64)\n",
      "max feasible return = 0.1184  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.9147780846612514e-07\n",
      "‖alpha‖₁       : 0.5799999999999956\n",
      "scores min/max : -3.347042056973408e-07 -2.4381207286224466e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0601255748234202e-07\n",
      "‖alpha‖₁       : 0.29999999999996646\n",
      "scores min/max : 3.2459462139164557e-08 3.7614622772417525e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 6.99117423372828e-08\n",
      "‖alpha‖₁       : 0.5999999999999994\n",
      "scores min/max : 1.5883745212298578e-08 5.262728277843906e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.6589015678360334e-08\n",
      "‖alpha‖₁       : 0.3799999999999998\n",
      "scores min/max : 1.1811082259660602e-08 2.1182836761487648e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.2605717406199904e-06\n",
      "‖alpha‖₁       : 0.31999999999128953\n",
      "scores min/max : 1.8479619153362284e-06 2.9162717551228745e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4258  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.0638902043921602\n",
      "‖alpha‖₁       : 0.7396876359215161\n",
      "scores min/max : -3.4979189612677137 2.119744782677624\n",
      "Mask mean value:  tensor(0.8487, dtype=torch.float64)\n",
      "max feasible return = -0.8977  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03371298133647898\n",
      "‖alpha‖₁       : 0.6150208880901565\n",
      "scores min/max : -1.893938602297128 1.2403234682867832\n",
      "Mask mean value:  tensor(0.8291, dtype=torch.float64)\n",
      "max feasible return = 2.9178  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.5986252798219696e-07\n",
      "‖alpha‖₁       : 0.45999999999999136\n",
      "scores min/max : 1.7514149701259162e-07 7.697140728795978e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.3415238238332987e-07\n",
      "‖alpha‖₁       : 0.5199999999999896\n",
      "scores min/max : 4.4114243908737526e-08 5.701806278806378e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.5312247498568574e-07\n",
      "‖alpha‖₁       : 0.5799999999999634\n",
      "scores min/max : -2.2452777172081485e-07 -1.8813991077786125e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.04492713799122691\n",
      "‖alpha‖₁       : 0.687563612354647\n",
      "scores min/max : -0.4281225688019789 1.9636350088459984\n",
      "Mask mean value:  tensor(0.4235, dtype=torch.float64)\n",
      "max feasible return = 0.0312  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03546632662119901\n",
      "‖alpha‖₁       : 0.42212192852758695\n",
      "scores min/max : -3.928613797749388 5.1719547556179615\n",
      "Mask mean value:  tensor(0.0686, dtype=torch.float64)\n",
      "max feasible return = 0.4333  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04842864291201958\n",
      "‖alpha‖₁       : 0.8211217142587991\n",
      "scores min/max : -5.131990504872535 3.026155065993365\n",
      "Mask mean value:  tensor(0.9303, dtype=torch.float64)\n",
      "max feasible return = -3.4872  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.00982647975465949\n",
      "‖alpha‖₁       : 0.7999999999999996\n",
      "scores min/max : -0.05146931926156684 0.011379998774829578\n",
      "Mask mean value:  tensor(0.4777, dtype=torch.float64)\n",
      "max feasible return = -0.1737  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.039020115605331e-07\n",
      "‖alpha‖₁       : 0.6199999999999752\n",
      "scores min/max : 2.9182280611683744e-08 4.567965201749894e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.6540499009760895e-08\n",
      "‖alpha‖₁       : 0.43999999999996486\n",
      "scores min/max : 8.279083252768714e-09 1.7861642666003573e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.268383576232033e-07\n",
      "‖alpha‖₁       : 0.27999999999997577\n",
      "scores min/max : -3.1386990156460464e-07 -2.959203412533502e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  13 | train 0.005436 | val 0.006808\n",
      "-----------------------------------------Epoch:  14 ----------------------------------------\n",
      "‖w_svm‖₂       : 0.14300431034228553\n",
      "‖alpha‖₁       : 0.8799999999999948\n",
      "scores min/max : -1.4469312204568778 3.8705920345225353\n",
      "Mask mean value:  tensor(0.3226, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2652  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.053e-06\n",
      "‖w_svm‖₂       : 0.000367195097519519\n",
      "‖alpha‖₁       : 0.7399999999996987\n",
      "scores min/max : -0.00014172295254356147 -0.00012452526716565598\n",
      "Mask mean value:  tensor(0.4994, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0629  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.069e-17\n",
      "‖w_svm‖₂       : 8.16181909499109e-08\n",
      "‖alpha‖₁       : 0.3799999999999998\n",
      "scores min/max : 1.0887704646993877e-09 1.924041300599589e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.482e-22\n",
      "‖w_svm‖₂       : 8.711535853982856e-08\n",
      "‖alpha‖₁       : 0.1799999999999945\n",
      "scores min/max : -5.436830407691268e-08 1.80595937265522e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.434e-21\n",
      "‖w_svm‖₂       : 5.417503043161565e-08\n",
      "‖alpha‖₁       : 0.11999999999998243\n",
      "scores min/max : -7.012631873403661e-08 -3.921021213719839e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.684e-08\n",
      "‖w_svm‖₂       : 3.7820022167430063e-07\n",
      "‖alpha‖₁       : 0.27999999999999753\n",
      "scores min/max : -1.3142751106266794e-06 -1.2525659025972752e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.188e-19\n",
      "‖w_svm‖₂       : 3.524202294916616e-06\n",
      "‖alpha‖₁       : 0.419999999993144\n",
      "scores min/max : -7.138736228518884e-07 9.124383518409612e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1230  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.466e-05\n",
      "‖w_svm‖₂       : 0.1435024288339166\n",
      "‖alpha‖₁       : 0.7585220789259766\n",
      "scores min/max : -1.7290693120676472 2.24827558810585\n",
      "Mask mean value:  tensor(0.8909, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3567  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.297e-12\n",
      "‖w_svm‖₂       : 0.02466007607188299\n",
      "‖alpha‖₁       : 0.8150851506516729\n",
      "scores min/max : -11.529792600832575 2.0324541633307733\n",
      "Mask mean value:  tensor(0.7103, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.8982  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.160e-06\n",
      "‖w_svm‖₂       : 0.12723543093472403\n",
      "‖alpha‖₁       : 0.8732518487444474\n",
      "scores min/max : -12.249362981871915 2.123217041923441\n",
      "Mask mean value:  tensor(0.4746, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.4029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 3.323e-02\n",
      "‖w_svm‖₂       : 0.027771421763313688\n",
      "‖alpha‖₁       : 0.19660871280781567\n",
      "scores min/max : -2.191586679135404 0.03410129632501728\n",
      "Mask mean value:  tensor(0.0723, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4863  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.542e-04\n",
      "‖w_svm‖₂       : 0.000310798780001503\n",
      "‖alpha‖₁       : 0.4199999999987081\n",
      "scores min/max : 0.00012268602125137473 0.00044997232493049026\n",
      "Mask mean value:  tensor(0.5021, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9997  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.729e-15\n",
      "‖w_svm‖₂       : 0.0001628036374264338\n",
      "‖alpha‖₁       : 0.6399999999999998\n",
      "scores min/max : -7.172370912544167e-05 -6.161045914850521e-05\n",
      "Mask mean value:  tensor(0.4997, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7639  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.142e-17\n",
      "‖w_svm‖₂       : 2.332576089729079e-07\n",
      "‖alpha‖₁       : 0.5799999999999994\n",
      "scores min/max : -3.3676073233769474e-07 -3.211378697635176e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.813e-19\n",
      "‖w_svm‖₂       : 0.13594782663615107\n",
      "‖alpha‖₁       : 0.6539738086990932\n",
      "scores min/max : -18.007575379423162 1.816345991981146\n",
      "Mask mean value:  tensor(0.2546, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2873  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.673e-03\n",
      "‖w_svm‖₂       : 5.450503588817861e-08\n",
      "‖alpha‖₁       : 0.43999999999999895\n",
      "scores min/max : -1.3340808351011793e-07 -2.9258055536064022e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.194e-20\n",
      "‖w_svm‖₂       : 0.03386990489191547\n",
      "‖alpha‖₁       : 0.8985417820143791\n",
      "scores min/max : -0.765727517517111 1.994560444108059\n",
      "Mask mean value:  tensor(0.8171, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4139  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.279e-14\n",
      "‖w_svm‖₂       : 7.098556081737264e-08\n",
      "‖alpha‖₁       : 0.41999999999999826\n",
      "scores min/max : -7.268964495479868e-08 -2.5542838453384523e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.363e-08\n",
      "‖w_svm‖₂       : 0.07903286808282725\n",
      "‖alpha‖₁       : 0.41892121682420946\n",
      "scores min/max : -1.8215545274819611 2.567297703189019\n",
      "Mask mean value:  tensor(0.8253, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8816  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.602e-11\n",
      "‖w_svm‖₂       : 2.2363538069375045e-08\n",
      "‖alpha‖₁       : 0.11999999999999475\n",
      "scores min/max : -1.6135717424363654e-08 -1.471373547317701e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.794e-09\n",
      "‖w_svm‖₂       : 0.07254729727226436\n",
      "‖alpha‖₁       : 0.5799999999999549\n",
      "scores min/max : -2.9176269221074596 1.6445024126415995\n",
      "Mask mean value:  tensor(0.1496, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2887  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.367e-04\n",
      "‖w_svm‖₂       : 0.00013815513516009053\n",
      "‖alpha‖₁       : 0.4399999999865484\n",
      "scores min/max : -0.00017439270507313572 -3.533129677678424e-05\n",
      "Mask mean value:  tensor(0.4996, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0254  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.570e-17\n",
      "‖w_svm‖₂       : 0.021145036848222453\n",
      "‖alpha‖₁       : 0.86\n",
      "scores min/max : -0.05407226814681356 0.04258503405241926\n",
      "Mask mean value:  tensor(0.4257, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5608  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.100e-14\n",
      "‖w_svm‖₂       : 2.7721748617976763e-07\n",
      "‖alpha‖₁       : 0.6599999999999697\n",
      "scores min/max : -5.23502203752575e-07 -3.7144058403627524e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.242e-18\n",
      "‖w_svm‖₂       : 0.05537360391201182\n",
      "‖alpha‖₁       : 0.659999999999962\n",
      "scores min/max : -0.3918187665511043 0.2959581751049042\n",
      "Mask mean value:  tensor(0.5113, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8859  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.545e-14\n",
      "‖w_svm‖₂       : 0.028893370864186266\n",
      "‖alpha‖₁       : 0.5510146741970321\n",
      "scores min/max : -3.5714653978101287 1.0027355586838718\n",
      "Mask mean value:  tensor(0.1083, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9191  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.764e-03\n",
      "‖w_svm‖₂       : 0.00021444662548421886\n",
      "‖alpha‖₁       : 0.6199999999983785\n",
      "scores min/max : 4.6495238880161225e-05 6.279059243833972e-05\n",
      "Mask mean value:  tensor(0.5003, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.931e-17\n",
      "‖w_svm‖₂       : 1.989099693838107e-07\n",
      "‖alpha‖₁       : 0.4199999999999758\n",
      "scores min/max : -2.864618171278749e-07 -2.641424167473334e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.505e-19\n",
      "‖w_svm‖₂       : 4.1948050538699096e-07\n",
      "‖alpha‖₁       : 0.7199999999999991\n",
      "scores min/max : 2.401184598925608e-07 2.973418215010983e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.773e-21\n",
      "‖w_svm‖₂       : 0.018925897557162077\n",
      "‖alpha‖₁       : 0.681509070268637\n",
      "scores min/max : -1.9797801069973684 0.054970064289568125\n",
      "Mask mean value:  tensor(0.5766, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2275  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.004e-12\n",
      "‖w_svm‖₂       : 0.05388792373852619\n",
      "‖alpha‖₁       : 0.7293452670544438\n",
      "scores min/max : -0.24157294018226147 2.0403693253239443\n",
      "Mask mean value:  tensor(0.7386, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3021  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 9.682e-04\n",
      "‖w_svm‖₂       : 2.2288728403121399e-07\n",
      "‖alpha‖₁       : 0.25999999999999984\n",
      "scores min/max : 1.2984229084661144e-09 1.4020008688084864e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.466e-17\n",
      "‖w_svm‖₂       : 0.04901382846431524\n",
      "‖alpha‖₁       : 0.9056476623054206\n",
      "scores min/max : -0.7851683897766861 1.8692987303291018\n",
      "Mask mean value:  tensor(0.3252, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6015  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.042e-03\n",
      "‖w_svm‖₂       : 7.72798189652531e-08\n",
      "‖alpha‖₁       : 0.6599999999999968\n",
      "scores min/max : 8.630226602408242e-08 2.3130397046731005e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.548e-09\n",
      "‖w_svm‖₂       : 0.0052661007020623\n",
      "‖alpha‖₁       : 0.700561250869597\n",
      "scores min/max : -2.004593886612462 0.018784271629783165\n",
      "Mask mean value:  tensor(0.4519, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2536  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.833e-05\n",
      "‖w_svm‖₂       : 0.019736630018808408\n",
      "‖alpha‖₁       : 0.78\n",
      "scores min/max : -0.009355480586031358 0.06381433133137054\n",
      "Mask mean value:  tensor(0.4844, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.1108  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.977e-15\n",
      "‖w_svm‖₂       : 1.0659931559928038e-07\n",
      "‖alpha‖₁       : 0.23999999999997168\n",
      "scores min/max : 1.7088354276478927e-08 2.8112883398180733e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.286e-21\n",
      "‖w_svm‖₂       : 1.4336079497995173e-05\n",
      "‖alpha‖₁       : 0.3599999999975135\n",
      "scores min/max : 1.0809704866744e-05 1.1665747953648155e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.877e-17\n",
      "‖w_svm‖₂       : 1.9042999105429625e-07\n",
      "‖alpha‖₁       : 0.379999999999999\n",
      "scores min/max : -4.348761601900475e-08 -7.789674931336615e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.511e-19\n",
      "‖w_svm‖₂       : 0.0001890621758258699\n",
      "‖alpha‖₁       : 0.8199999999999996\n",
      "scores min/max : -3.7723596657948015e-05 -2.2838215517409868e-05\n",
      "Mask mean value:  tensor(0.4999, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6974  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.183e-17\n",
      "‖w_svm‖₂       : 7.14985965233245e-08\n",
      "‖alpha‖₁       : 0.13999999999999643\n",
      "scores min/max : 7.480799837980334e-10 6.527606299186477e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.668e-08\n",
      "‖w_svm‖₂       : 4.7386257498931514e-08\n",
      "‖alpha‖₁       : 0.17999999999999738\n",
      "scores min/max : 4.1944231812325996e-08 5.6635488868446986e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 4.671e-08\n",
      "‖w_svm‖₂       : 0.04253792584038848\n",
      "‖alpha‖₁       : 0.9402520630290369\n",
      "scores min/max : -1.8267202316089044 0.3089658965165535\n",
      "Mask mean value:  tensor(0.4341, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3421  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.951e-10\n",
      "‖w_svm‖₂       : 0.02708585429096791\n",
      "‖alpha‖₁       : 0.6599999999999897\n",
      "scores min/max : -0.061169107863782296 0.07333504162082555\n",
      "Mask mean value:  tensor(0.4260, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.4090  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.328e-15\n",
      "‖w_svm‖₂       : 0.025823978614241794\n",
      "‖alpha‖₁       : 0.8522312505166029\n",
      "scores min/max : -2.945822744757287 1.534857971731094\n",
      "Mask mean value:  tensor(0.1430, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9328  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.086e-03\n",
      "‖w_svm‖₂       : 0.023297914280334505\n",
      "‖alpha‖₁       : 0.8233535573245035\n",
      "scores min/max : -1.882759232016141 1.5181764429692137\n",
      "Mask mean value:  tensor(0.7432, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.4949  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.460e-06\n",
      "‖w_svm‖₂       : 0.005183702049569212\n",
      "‖alpha‖₁       : 0.37999999999999995\n",
      "scores min/max : -0.0075682170888118905 0.0041059402686838276\n",
      "Mask mean value:  tensor(0.5054, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8994  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.753e-14\n",
      "‖w_svm‖₂       : 0.0031206225568195032\n",
      "‖alpha‖₁       : 0.5799999999999956\n",
      "scores min/max : 0.0009051271253236478 0.0021418423998220276\n",
      "Mask mean value:  tensor(0.5094, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3239  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.940e-13\n",
      "‖w_svm‖₂       : 0.020941897803166947\n",
      "‖alpha‖₁       : 0.38333603587293974\n",
      "scores min/max : -1.9714948592187918 0.2276441860404818\n",
      "Mask mean value:  tensor(0.6710, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1419  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.457e-14\n",
      "‖w_svm‖₂       : 0.018818676113430702\n",
      "‖alpha‖₁       : 0.5962519969815316\n",
      "scores min/max : -2.918866527946987 2.2536785153613668\n",
      "Mask mean value:  tensor(0.9059, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.4910  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.611e-03\n",
      "‖w_svm‖₂       : 0.00420642849184351\n",
      "‖alpha‖₁       : 0.45999999999999974\n",
      "scores min/max : -0.005492055556673606 -0.0036342710511199847\n",
      "Mask mean value:  tensor(0.4797, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.0849  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.878e-16\n",
      "‖w_svm‖₂       : 0.0005007602350904811\n",
      "‖alpha‖₁       : 0.43999999999999995\n",
      "scores min/max : -0.0006022633971289506 -0.0003506795238506042\n",
      "Mask mean value:  tensor(0.4972, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6460  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.031e-16\n",
      "‖w_svm‖₂       : 1.1593966356743522e-06\n",
      "‖alpha‖₁       : 0.31999999999862033\n",
      "scores min/max : -1.630421056026467e-06 -1.5142427467475105e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4898  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.647e-18\n",
      "‖w_svm‖₂       : 6.733486628515197e-07\n",
      "‖alpha‖₁       : 0.3999999999999996\n",
      "scores min/max : -1.0804216021254948e-07 1.9817920364873334e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.216e-09\n",
      "‖w_svm‖₂       : 1.667449795296218e-07\n",
      "‖alpha‖₁       : 0.23999999999999932\n",
      "scores min/max : 2.467774012041069e-07 2.619407334966434e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.844e-20\n",
      "‖w_svm‖₂       : 0.016910359521164132\n",
      "‖alpha‖₁       : 0.8599999999999983\n",
      "scores min/max : -0.04180599603479847 -0.023929455574614525\n",
      "Mask mean value:  tensor(0.3598, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0850  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.190e-14\n",
      "‖w_svm‖₂       : 0.04815564006522877\n",
      "‖alpha‖₁       : 0.9389970482630587\n",
      "scores min/max : -2.441086504778254 1.5987903702367336\n",
      "Mask mean value:  tensor(0.1555, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4135  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.050e-03\n",
      "‖w_svm‖₂       : 0.08134967043502303\n",
      "‖alpha‖₁       : 0.658238295774639\n",
      "scores min/max : -2.0144677706187872 3.897510106482848\n",
      "Mask mean value:  tensor(0.3885, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3445  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.164e-04\n",
      "‖w_svm‖₂       : 0.05888007476754396\n",
      "‖alpha‖₁       : 0.5761095014021361\n",
      "scores min/max : -1.9548881202536237 0.8678970754336678\n",
      "Mask mean value:  tensor(0.7062, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9260  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.017e-03\n",
      "‖w_svm‖₂       : 0.005569945323394482\n",
      "‖alpha‖₁       : 0.5599999999999964\n",
      "scores min/max : 0.0034701154273432174 0.005078315277104186\n",
      "Mask mean value:  tensor(0.5188, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0875  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 9.229e-05\n",
      "‖w_svm‖₂       : 0.0008840481058394513\n",
      "‖alpha‖₁       : 0.819999999999999\n",
      "scores min/max : 0.0009002949538370122 0.0026446307415926964\n",
      "Mask mean value:  tensor(0.5100, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5908  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.978e-17\n",
      "‖w_svm‖₂       : 0.006411723587224121\n",
      "‖alpha‖₁       : 0.6076644157208826\n",
      "scores min/max : -1.9835040071368448 0.3035159772788282\n",
      "Mask mean value:  tensor(0.5862, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2369  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.199e-15\n",
      "‖w_svm‖₂       : 2.3211574666158976e-07\n",
      "‖alpha‖₁       : 0.6399999999999961\n",
      "scores min/max : 2.530586621361575e-07 2.739361642631452e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.065e-20\n",
      "‖w_svm‖₂       : 1.1429226397823636e-07\n",
      "‖alpha‖₁       : 0.29999999999997146\n",
      "scores min/max : 7.961297300788868e-08 8.869817455145244e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.179e-07\n",
      "‖w_svm‖₂       : 7.601309988018695e-08\n",
      "‖alpha‖₁       : 0.5399999999999259\n",
      "scores min/max : -8.614595763150066e-08 -6.450583002247703e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.467e-20\n",
      "‖w_svm‖₂       : 1.0687161408720811e-06\n",
      "‖alpha‖₁       : 0.4999999999999923\n",
      "scores min/max : -2.662149526989539e-07 -7.315894684521317e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.755e-19\n",
      "‖w_svm‖₂       : 0.052482876226360296\n",
      "‖alpha‖₁       : 0.9199999999999351\n",
      "scores min/max : -0.40659271242749023 0.6147533567833106\n",
      "Mask mean value:  tensor(0.3761, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.8599  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 2.134e-02\n",
      "‖w_svm‖₂       : 5.62699410889806e-08\n",
      "‖alpha‖₁       : 0.23999999999997154\n",
      "scores min/max : -5.668772853296019e-08 -4.209605363024765e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.240e-20\n",
      "‖w_svm‖₂       : 0.0441503219611096\n",
      "‖alpha‖₁       : 0.8899417527945892\n",
      "scores min/max : -2.0096666529953184 0.33743823249929317\n",
      "Mask mean value:  tensor(0.2641, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0753  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.998e-08\n",
      "‖w_svm‖₂       : 1.0001010359610247e-06\n",
      "‖alpha‖₁       : 0.5999999999999948\n",
      "scores min/max : -6.496704394916764e-08 -1.4101402480119216e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.412e-19\n",
      "‖w_svm‖₂       : 0.08081987236595688\n",
      "‖alpha‖₁       : 0.4716593809502829\n",
      "scores min/max : -2.254715942849348 2.825153445957945\n",
      "Mask mean value:  tensor(0.1335, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.4345  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.284e-02\n",
      "‖w_svm‖₂       : 0.18587889402337326\n",
      "‖alpha‖₁       : 0.8825112632050072\n",
      "scores min/max : -3.5356220979177593 6.092042793776269\n",
      "Mask mean value:  tensor(0.0989, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1867  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.983e-03\n",
      "‖w_svm‖₂       : 0.060834807181637675\n",
      "‖alpha‖₁       : 0.8984623764127401\n",
      "scores min/max : -1.7314129759529533 3.6714810969666383\n",
      "Mask mean value:  tensor(0.3624, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5238  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.245e-02\n",
      "‖w_svm‖₂       : 0.05353349689046\n",
      "‖alpha‖₁       : 0.8270409810844447\n",
      "scores min/max : -1.9525644326118556 0.39927069335552595\n",
      "Mask mean value:  tensor(0.5107, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0008  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.682e-02\n",
      "‖w_svm‖₂       : 1.1023037863351636e-07\n",
      "‖alpha‖₁       : 0.5199999999999996\n",
      "scores min/max : 2.7657377989935575e-07 2.97974351327644e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.372e-19\n",
      "‖w_svm‖₂       : 2.1039601046628013e-07\n",
      "‖alpha‖₁       : 0.37999999999472955\n",
      "scores min/max : -8.332983680517907e-09 1.0620993110868117e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.901e-07\n",
      "‖w_svm‖₂       : 0.08536755842167854\n",
      "‖alpha‖₁       : 0.577793090828115\n",
      "scores min/max : -2.1048195024336627 5.8323909566616\n",
      "Mask mean value:  tensor(0.3063, dtype=torch.float64)\n",
      "max feasible return = 0.1185  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.8994701264287565e-07\n",
      "‖alpha‖₁       : 0.5800000000000001\n",
      "scores min/max : -3.588920012820788e-07 -2.6817017091825733e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0101619428757982e-07\n",
      "‖alpha‖₁       : 0.29999999999999977\n",
      "scores min/max : 3.1966782812823365e-08 3.693533227231894e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 6.97634078050195e-08\n",
      "‖alpha‖₁       : 0.5999999999999994\n",
      "scores min/max : 1.8489930907622286e-08 5.523500556890009e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.65790511350306e-08\n",
      "‖alpha‖₁       : 0.3799999999999919\n",
      "scores min/max : 1.3023771952162826e-08 2.2441139472493596e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.262532668515175e-06\n",
      "‖alpha‖₁       : 0.31999999999122086\n",
      "scores min/max : 1.849181133240541e-06 2.920342848951114e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4258  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.06335354078681667\n",
      "‖alpha‖₁       : 0.7396916142261394\n",
      "scores min/max : -3.5012465767265226 2.1174278295580486\n",
      "Mask mean value:  tensor(0.8459, dtype=torch.float64)\n",
      "max feasible return = -0.8954  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03404658946076493\n",
      "‖alpha‖₁       : 0.6150447142394474\n",
      "scores min/max : -1.8922264242321674 1.2420407883426527\n",
      "Mask mean value:  tensor(0.8326, dtype=torch.float64)\n",
      "max feasible return = 2.9308  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.5927858107740267e-07\n",
      "‖alpha‖₁       : 0.4599999999999911\n",
      "scores min/max : 1.4930722573959237e-07 7.438791082771947e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.3389184427773849e-07\n",
      "‖alpha‖₁       : 0.5199999999999902\n",
      "scores min/max : 5.224750288467959e-08 6.515139328418276e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.5263764625000315e-07\n",
      "‖alpha‖₁       : 0.5799999999999667\n",
      "scores min/max : -2.263869498134546e-07 -1.9000652598791804e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.04496501478513367\n",
      "‖alpha‖₁       : 0.6875821533469344\n",
      "scores min/max : -0.4274658303836537 1.9639132137145734\n",
      "Mask mean value:  tensor(0.4255, dtype=torch.float64)\n",
      "max feasible return = 0.0285  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03529151525688815\n",
      "‖alpha‖₁       : 0.4221245789093552\n",
      "scores min/max : -3.9290554108443785 5.1739404928908455\n",
      "Mask mean value:  tensor(0.0688, dtype=torch.float64)\n",
      "max feasible return = 0.4344  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.048236133950198865\n",
      "‖alpha‖₁       : 0.8211309384593285\n",
      "scores min/max : -5.135176233039096 3.0235839247676513\n",
      "Mask mean value:  tensor(0.9294, dtype=torch.float64)\n",
      "max feasible return = -3.4818  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.00972008633487462\n",
      "‖alpha‖₁       : 0.7999999999999996\n",
      "scores min/max : -0.050184019881335196 0.01142258220428856\n",
      "Mask mean value:  tensor(0.4795, dtype=torch.float64)\n",
      "max feasible return = -0.1744  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.033524105494382e-07\n",
      "‖alpha‖₁       : 0.6199999999999741\n",
      "scores min/max : 1.8023953790688995e-08 3.4524939242347975e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.645336577002737e-08\n",
      "‖alpha‖₁       : 0.4399999999999652\n",
      "scores min/max : 8.403279196952755e-09 1.7985827866111482e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.2648687402427057e-07\n",
      "‖alpha‖₁       : 0.27999999999997605\n",
      "scores min/max : -3.130010666194355e-07 -2.950517359447553e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  14 | train 0.005421 | val 0.006805\n",
      "-----------------------------------------Epoch:  15 ----------------------------------------\n",
      "‖w_svm‖₂       : 1.1192481356017751e-07\n",
      "‖alpha‖₁       : 0.2999999999999987\n",
      "scores min/max : 8.195326888281581e-08 9.087844627473623e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.157e-07\n",
      "‖w_svm‖₂       : 0.024616850369951954\n",
      "‖alpha‖₁       : 0.8150858919249346\n",
      "scores min/max : -11.532552412143637 2.0306787705238927\n",
      "Mask mean value:  tensor(0.7061, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.8889  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.979e-06\n",
      "‖w_svm‖₂       : 7.111542034836017e-08\n",
      "‖alpha‖₁       : 0.13999999999999732\n",
      "scores min/max : 1.0671924321525177e-09 6.836479615398629e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.632e-08\n",
      "‖w_svm‖₂       : 1.4365435309194579e-05\n",
      "‖alpha‖₁       : 0.3599999999975113\n",
      "scores min/max : 1.1151484799976983e-05 1.2012869327833745e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.903e-17\n",
      "‖w_svm‖₂       : 0.0258842172565012\n",
      "‖alpha‖₁       : 0.8522337825765725\n",
      "scores min/max : -2.9425844255715328 1.5381189087853557\n",
      "Mask mean value:  tensor(0.1443, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9350  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.127e-03\n",
      "‖w_svm‖₂       : 0.02082550530822791\n",
      "‖alpha‖₁       : 0.3833360621425077\n",
      "scores min/max : -1.971405333804313 0.22788568452448071\n",
      "Mask mean value:  tensor(0.6716, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1422  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.421e-14\n",
      "‖w_svm‖₂       : 9.985759005311823e-07\n",
      "‖alpha‖₁       : 0.5999999999999952\n",
      "scores min/max : -6.792568031834742e-08 -1.7058252031027244e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.412e-19\n",
      "‖w_svm‖₂       : 0.05354266798879005\n",
      "‖alpha‖₁       : 0.729347624615637\n",
      "scores min/max : -0.24229205396903092 2.03975574878606\n",
      "Mask mean value:  tensor(0.7366, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2938  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 9.861e-04\n",
      "‖w_svm‖₂       : 0.0031231172986358993\n",
      "‖alpha‖₁       : 0.579999999999994\n",
      "scores min/max : 0.0009633172314412612 0.0022020052628392572\n",
      "Mask mean value:  tensor(0.5097, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3241  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.634e-13\n",
      "‖w_svm‖₂       : 0.028898879413131573\n",
      "‖alpha‖₁       : 0.5510182072711648\n",
      "scores min/max : -3.5677898618956823 1.0062743758026638\n",
      "Mask mean value:  tensor(0.1096, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9290  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.934e-03\n",
      "‖w_svm‖₂       : 0.023196688755816503\n",
      "‖alpha‖₁       : 0.8233547612474239\n",
      "scores min/max : -1.8836713102322216 1.5171384815314222\n",
      "Mask mean value:  tensor(0.7410, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.4893  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.336e-06\n",
      "‖w_svm‖₂       : 5.613045230877185e-08\n",
      "‖alpha‖₁       : 0.23999999999996588\n",
      "scores min/max : -5.7382398245528135e-08 -4.2755129225072436e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.240e-20\n",
      "‖w_svm‖₂       : 0.07196705960856187\n",
      "‖alpha‖₁       : 0.5799999999999998\n",
      "scores min/max : -2.8699063823982285 1.616841160730906\n",
      "Mask mean value:  tensor(0.1508, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2890  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.178e-04\n",
      "‖w_svm‖₂       : 7.590755038990651e-08\n",
      "‖alpha‖₁       : 0.5399999999999072\n",
      "scores min/max : -9.198330851005369e-08 -7.033490099629017e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.473e-20\n",
      "‖w_svm‖₂       : 0.0610157656601872\n",
      "‖alpha‖₁       : 0.8984888390196679\n",
      "scores min/max : -1.727229406717038 3.675775083928301\n",
      "Mask mean value:  tensor(0.3700, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5300  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.284e-02\n",
      "‖w_svm‖₂       : 5.412569552787943e-08\n",
      "‖alpha‖₁       : 0.11999999999998184\n",
      "scores min/max : -7.367325013420661e-08 -4.274822232177277e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.660e-08\n",
      "‖w_svm‖₂       : 0.00016329716437430515\n",
      "‖alpha‖₁       : 0.6399999999999999\n",
      "scores min/max : -8.636870213995528e-05 -7.619408769388397e-05\n",
      "Mask mean value:  tensor(0.4996, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7638  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.170e-17\n",
      "‖w_svm‖₂       : 1.1575707401791427e-06\n",
      "‖alpha‖₁       : 0.31999999999854994\n",
      "scores min/max : -1.688642435022731e-06 -1.5721730863847132e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4898  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.653e-18\n",
      "‖w_svm‖₂       : 0.053665727022435855\n",
      "‖alpha‖₁       : 0.8270600660172214\n",
      "scores min/max : -1.9483523089017611 0.4032306991918202\n",
      "Mask mean value:  tensor(0.5258, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0022  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.597e-02\n",
      "‖w_svm‖₂       : 1.1014910836738771e-07\n",
      "‖alpha‖₁       : 0.5199999999999996\n",
      "scores min/max : 2.7283205323540456e-07 2.942323484415008e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.381e-19\n",
      "‖w_svm‖₂       : 4.1715385697468696e-07\n",
      "‖alpha‖₁       : 0.72\n",
      "scores min/max : 2.334966901064884e-07 2.907119257181409e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.783e-21\n",
      "‖w_svm‖₂       : 1.6631557342211114e-07\n",
      "‖alpha‖₁       : 0.23999999999999913\n",
      "scores min/max : 2.4731478066185316e-07 2.624927972027139e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.871e-20\n",
      "‖w_svm‖₂       : 1.0601243132125305e-07\n",
      "‖alpha‖₁       : 0.23999999999997257\n",
      "scores min/max : 2.047152641898633e-08 3.1490377668064584e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.249e-21\n",
      "‖w_svm‖₂       : 1.8926213539733935e-07\n",
      "‖alpha‖₁       : 0.37999999999999867\n",
      "scores min/max : -3.980540067055281e-08 -4.1088185144812895e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.482e-19\n",
      "‖w_svm‖₂       : 1.9825461534715339e-07\n",
      "‖alpha‖₁       : 0.4199999999999759\n",
      "scores min/max : -2.9220356381212253e-07 -2.6988291874165345e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.521e-19\n",
      "‖w_svm‖₂       : 8.114314909307189e-08\n",
      "‖alpha‖₁       : 0.3799999999999997\n",
      "scores min/max : 6.182931200871446e-09 2.4331709507034305e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.434e-22\n",
      "‖w_svm‖₂       : 0.026854827918765943\n",
      "‖alpha‖₁       : 0.6599999999999857\n",
      "scores min/max : -0.05969955084289069 0.07246462181764585\n",
      "Mask mean value:  tensor(0.4293, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.4324  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.279e-15\n",
      "‖w_svm‖₂       : 0.034227507807053976\n",
      "‖alpha‖₁       : 0.898571649753058\n",
      "scores min/max : -0.7619191207096596 1.9983758664830107\n",
      "Mask mean value:  tensor(0.8235, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4045  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.000e-15\n",
      "‖w_svm‖₂       : 0.0002153310782065549\n",
      "‖alpha‖₁       : 0.6199999999983724\n",
      "scores min/max : 4.160468287665784e-05 5.803476562003752e-05\n",
      "Mask mean value:  tensor(0.5002, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.019e-17\n",
      "‖w_svm‖₂       : 0.12608546625264935\n",
      "‖alpha‖₁       : 0.8732607486833298\n",
      "scores min/max : -12.26756807562181 2.102408209262597\n",
      "Mask mean value:  tensor(0.4242, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2639  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 8.239e-02\n",
      "‖w_svm‖₂       : 0.04823059802791786\n",
      "‖alpha‖₁       : 0.9056463239719829\n",
      "scores min/max : -0.7509446405953131 1.8812585901680752\n",
      "Mask mean value:  tensor(0.3526, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6524  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.710e-03\n",
      "‖w_svm‖₂       : 8.671798283196567e-08\n",
      "‖alpha‖₁       : 0.1799999999999963\n",
      "scores min/max : -5.767950062635737e-08 1.769006990459572e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.453e-21\n",
      "‖w_svm‖₂       : 0.0055832673802920845\n",
      "‖alpha‖₁       : 0.5599999999999996\n",
      "scores min/max : 0.003419666634208878 0.005035165569521415\n",
      "Mask mean value:  tensor(0.5186, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0870  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 9.557e-05\n",
      "‖w_svm‖₂       : 6.712818959194925e-07\n",
      "‖alpha‖₁       : 0.4\n",
      "scores min/max : -8.932193541632579e-08 2.1701225521068238e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.227e-09\n",
      "‖w_svm‖₂       : 0.0008813120417339106\n",
      "‖alpha‖₁       : 0.8199999999999993\n",
      "scores min/max : 0.0007816909349452773 0.00251655266441934\n",
      "Mask mean value:  tensor(0.5094, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5876  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.005e-16\n",
      "‖w_svm‖₂       : 2.3188037999860217e-07\n",
      "‖alpha‖₁       : 0.6399999999999988\n",
      "scores min/max : 2.706900968859218e-07 2.915333527851774e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.081e-20\n",
      "‖w_svm‖₂       : 0.051960950510365506\n",
      "‖alpha‖₁       : 0.9199999999999995\n",
      "scores min/max : -0.40046577611780454 0.6004529816476749\n",
      "Mask mean value:  tensor(0.3712, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.8357  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 2.010e-02\n",
      "‖w_svm‖₂       : 0.028276825039340148\n",
      "‖alpha‖₁       : 0.19663767279881533\n",
      "scores min/max : -2.2042426669452944 0.02138884473961416\n",
      "Mask mean value:  tensor(0.0594, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4011  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.504e-04\n",
      "‖w_svm‖₂       : 0.000500843746726284\n",
      "‖alpha‖₁       : 0.43999999999999995\n",
      "scores min/max : -0.000544176406688834 -0.00029305276350706446\n",
      "Mask mean value:  tensor(0.4975, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6470  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.067e-16\n",
      "‖w_svm‖₂       : 0.14618014385570025\n",
      "‖alpha‖₁       : 0.7593053749613319\n",
      "scores min/max : -1.7274383709059016 2.25003961797233\n",
      "Mask mean value:  tensor(0.8926, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3581  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.589e-12\n",
      "‖w_svm‖₂       : 0.016854922198104326\n",
      "‖alpha‖₁       : 0.8599999999999994\n",
      "scores min/max : -0.040150900724418896 -0.02257944204844889\n",
      "Mask mean value:  tensor(0.3664, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0874  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.100e-14\n",
      "‖w_svm‖₂       : 0.14014819936055936\n",
      "‖alpha‖₁       : 0.8799999999999999\n",
      "scores min/max : -1.3817351770607824 3.7267617269441247\n",
      "Mask mean value:  tensor(0.3427, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2648  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.376e-09\n",
      "‖w_svm‖₂       : 0.00018999781436701945\n",
      "‖alpha‖₁       : 0.8199999999999995\n",
      "scores min/max : -4.5948840313596106e-05 -3.091079523475954e-05\n",
      "Mask mean value:  tensor(0.4998, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6973  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.194e-17\n",
      "‖w_svm‖₂       : 2.2366004741071492e-08\n",
      "‖alpha‖₁       : 0.11999999999999489\n",
      "scores min/max : -1.9053791340047103e-08 -4.395984826273113e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.806e-09\n",
      "‖w_svm‖₂       : 7.679833275120261e-08\n",
      "‖alpha‖₁       : 0.6599999999999941\n",
      "scores min/max : 8.855449604388385e-08 2.3354490048140082e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.610e-09\n",
      "‖w_svm‖₂       : 0.006504451366482208\n",
      "‖alpha‖₁       : 0.6076655705604274\n",
      "scores min/max : -1.9841064335674012 0.30290288394447756\n",
      "Mask mean value:  tensor(0.5836, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2358  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.373e-15\n",
      "‖w_svm‖₂       : 2.3206770418651602e-07\n",
      "‖alpha‖₁       : 0.5799999999999997\n",
      "scores min/max : -3.6683823245184545e-07 -3.512198796609568e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.845e-19\n",
      "‖w_svm‖₂       : 0.005178072495368475\n",
      "‖alpha‖₁       : 0.37999999999999756\n",
      "scores min/max : -0.007032294927030919 0.004595558826383829\n",
      "Mask mean value:  tensor(0.5079, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9039  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.729e-14\n",
      "‖w_svm‖₂       : 0.0003122751010463201\n",
      "‖alpha‖₁       : 0.41999999999399246\n",
      "scores min/max : 0.00015995380581385098 0.0004903060414957113\n",
      "Mask mean value:  tensor(0.5023, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0001  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.769e-15\n",
      "‖w_svm‖₂       : 1.0663633184997372e-06\n",
      "‖alpha‖₁       : 0.4999999999999851\n",
      "scores min/max : -2.6382659082693504e-07 -7.059508053271906e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.737e-19\n",
      "‖w_svm‖₂       : 0.005369402342518446\n",
      "‖alpha‖₁       : 0.7005623324262771\n",
      "scores min/max : -2.0039799255588027 0.019398682475075125\n",
      "Mask mean value:  tensor(0.4549, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2552  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.147e-05\n",
      "‖w_svm‖₂       : 0.05437126947386429\n",
      "‖alpha‖₁       : 0.6599999999999997\n",
      "scores min/max : -0.37634566664426455 0.28717565123713046\n",
      "Mask mean value:  tensor(0.5181, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.828e-14\n",
      "‖w_svm‖₂       : 0.018560889668232442\n",
      "‖alpha‖₁       : 0.6815091925058793\n",
      "scores min/max : -1.9774742834661834 0.0561440780398114\n",
      "Mask mean value:  tensor(0.5875, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2351  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.646e-14\n",
      "‖w_svm‖₂       : 0.04218022062308589\n",
      "‖alpha‖₁       : 0.9402672481811722\n",
      "scores min/max : -1.8241229529958964 0.31128247355077066\n",
      "Mask mean value:  tensor(0.4439, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3400  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.999e-08\n",
      "‖w_svm‖₂       : 2.243104335903028e-07\n",
      "‖alpha‖₁       : 0.2599999999999817\n",
      "scores min/max : -3.199137498078344e-08 -1.912751357792112e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.430e-17\n",
      "‖w_svm‖₂       : 0.019692973046752844\n",
      "‖alpha‖₁       : 0.7799999999999884\n",
      "scores min/max : -0.015542046547337074 0.05650695548637208\n",
      "Mask mean value:  tensor(0.4532, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9810  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.635e-15\n",
      "‖w_svm‖₂       : 7.028272551842876e-08\n",
      "‖alpha‖₁       : 0.41999999999999993\n",
      "scores min/max : -7.26259108870818e-08 -2.5545157397768944e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.378e-08\n",
      "‖w_svm‖₂       : 2.0988722302178875e-07\n",
      "‖alpha‖₁       : 0.3799999999948992\n",
      "scores min/max : 1.53203474167653e-08 3.416005739016675e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.832e-07\n",
      "‖w_svm‖₂       : 3.5192472421157824e-06\n",
      "‖alpha‖₁       : 0.41999999999322324\n",
      "scores min/max : -1.0352519339101556e-06 5.863737950313967e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1230  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.464e-05\n",
      "‖w_svm‖₂       : 0.00013891925765199985\n",
      "‖alpha‖₁       : 0.43999999997277917\n",
      "scores min/max : -0.00019753204104833513 -5.6496523512664055e-05\n",
      "Mask mean value:  tensor(0.4995, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0246  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.598e-17\n",
      "‖w_svm‖₂       : 0.04440454654761667\n",
      "‖alpha‖₁       : 0.8899884476363575\n",
      "scores min/max : -2.006637063497091 0.3405250166389707\n",
      "Mask mean value:  tensor(0.2729, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0788  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.052e-10\n",
      "‖w_svm‖₂       : 0.0810360415566678\n",
      "‖alpha‖₁       : 0.6583219745142145\n",
      "scores min/max : -2.0473679910000495 3.8982664903924285\n",
      "Mask mean value:  tensor(0.3267, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.7490  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.989e-04\n",
      "‖w_svm‖₂       : 0.0185102332652727\n",
      "‖alpha‖₁       : 0.5962519288843412\n",
      "scores min/max : -2.9191888856981345 2.259751460321645\n",
      "Mask mean value:  tensor(0.9093, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.4966  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.686e-04\n",
      "‖w_svm‖₂       : 0.0003676686060073309\n",
      "‖alpha‖₁       : 0.7399999999996396\n",
      "scores min/max : -0.00018024953173094292 -0.00016300754541646742\n",
      "Mask mean value:  tensor(0.4992, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0625  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.087e-17\n",
      "‖w_svm‖₂       : 0.08202553074734811\n",
      "‖alpha‖₁       : 0.47186352077578053\n",
      "scores min/max : -2.334971522767701 2.7443631487333295\n",
      "Mask mean value:  tensor(0.0965, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0585  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.044e-02\n",
      "‖w_svm‖₂       : 3.775880805590485e-07\n",
      "‖alpha‖₁       : 0.2799999999999916\n",
      "scores min/max : -1.2965957586968422e-06 -1.2347711345504485e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.259e-19\n",
      "‖w_svm‖₂       : 0.13851111885806341\n",
      "‖alpha‖₁       : 0.6547056670814969\n",
      "scores min/max : -18.083057830154278 1.7457215204978058\n",
      "Mask mean value:  tensor(0.1997, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2627  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.196e-03\n",
      "‖w_svm‖₂       : 4.7091178043265045e-08\n",
      "‖alpha‖₁       : 0.17999999999999716\n",
      "scores min/max : 3.450055230671078e-08 4.919702449510512e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 4.623e-08\n",
      "‖w_svm‖₂       : 0.05892772346230844\n",
      "‖alpha‖₁       : 0.5761650327736331\n",
      "scores min/max : -1.9595941041887461 0.863997756949174\n",
      "Mask mean value:  tensor(0.6931, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.8881  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.778e-04\n",
      "‖w_svm‖₂       : 0.020700319374260763\n",
      "‖alpha‖₁       : 0.8600000000000001\n",
      "scores min/max : -0.05427157906445464 0.038528862919729054\n",
      "Mask mean value:  tensor(0.4173, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5483  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.802e-14\n",
      "‖w_svm‖₂       : 0.004200914198152301\n",
      "‖alpha‖₁       : 0.4599999999999992\n",
      "scores min/max : -0.005531973993406894 -0.003671524187993082\n",
      "Mask mean value:  tensor(0.4795, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.0841  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.000e-15\n",
      "‖w_svm‖₂       : 0.07910254386215475\n",
      "‖alpha‖₁       : 0.4190922931160652\n",
      "scores min/max : -1.8291798963398227 2.527324413920577\n",
      "Mask mean value:  tensor(0.8211, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8838  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.208e-12\n",
      "‖w_svm‖₂       : 0.04815668967527274\n",
      "‖alpha‖₁       : 0.9390098355179101\n",
      "scores min/max : -2.4362134060434384 1.6043046064769908\n",
      "Mask mean value:  tensor(0.1596, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4204  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.162e-03\n",
      "‖w_svm‖₂       : 2.754116696114051e-07\n",
      "‖alpha‖₁       : 0.6599999999999875\n",
      "scores min/max : -5.615990432054016e-07 -4.097899988470712e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.249e-18\n",
      "‖w_svm‖₂       : 0.18706109815030808\n",
      "‖alpha‖₁       : 0.8830224669954491\n",
      "scores min/max : -3.547139545021813 6.083660063043148\n",
      "Mask mean value:  tensor(0.0970, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1862  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.032e-03\n",
      "‖w_svm‖₂       : 5.4413141822946964e-08\n",
      "‖alpha‖₁       : 0.43999999999999884\n",
      "scores min/max : -1.3639266922875026e-07 -3.2265588170380896e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.191e-20\n",
      "‖w_svm‖₂       : 0.08606099182309677\n",
      "‖alpha‖₁       : 0.5779332505946897\n",
      "scores min/max : -2.1217575088275287 5.8137765968402855\n",
      "Mask mean value:  tensor(0.2733, dtype=torch.float64)\n",
      "max feasible return = 0.1118  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.909844077966733e-07\n",
      "‖alpha‖₁       : 0.5799999999999876\n",
      "scores min/max : -3.304684257419121e-07 -2.3953737437242656e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0079754369019528e-07\n",
      "‖alpha‖₁       : 0.2999999999999997\n",
      "scores min/max : 3.2790239273598734e-08 3.775871775250228e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 6.996784625856833e-08\n",
      "‖alpha‖₁       : 0.5999999999999991\n",
      "scores min/max : 1.5376732171919286e-08 5.212885267443987e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.663539418763198e-08\n",
      "‖alpha‖₁       : 0.3799999999999871\n",
      "scores min/max : 9.424244906098703e-09 1.885627227758363e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.2452413223389117e-06\n",
      "‖alpha‖₁       : 0.3199999999914297\n",
      "scores min/max : 2.1198226996657513e-06 3.181594171928662e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4258  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.06259895119758373\n",
      "‖alpha‖₁       : 0.7396961660350043\n",
      "scores min/max : -3.488244727304719 2.1319738469205824\n",
      "Mask mean value:  tensor(0.8615, dtype=torch.float64)\n",
      "max feasible return = -0.9095  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.0345096956159591\n",
      "‖alpha‖₁       : 0.6150773734264223\n",
      "scores min/max : -1.8990524261374717 1.2352184676632034\n",
      "Mask mean value:  tensor(0.8184, dtype=torch.float64)\n",
      "max feasible return = 2.8780  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.583792289914314e-07\n",
      "‖alpha‖₁       : 0.4599999999999909\n",
      "scores min/max : 2.2855709467993494e-07 8.231324406123509e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.3354606961104786e-07\n",
      "‖alpha‖₁       : 0.5199999999999905\n",
      "scores min/max : 4.466644312034475e-08 5.7570158213445165e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.5193210544953004e-07\n",
      "‖alpha‖₁       : 0.5799999999999697\n",
      "scores min/max : -2.4043649933456066e-07 -2.0406838893623147e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.04489856719410645\n",
      "‖alpha‖₁       : 0.687606638592819\n",
      "scores min/max : -0.43271097651453677 1.957729637738708\n",
      "Mask mean value:  tensor(0.4051, dtype=torch.float64)\n",
      "max feasible return = 0.0464  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03507121617696136\n",
      "‖alpha‖₁       : 0.42212796561074817\n",
      "scores min/max : -3.9235056470074516 5.182544113384702\n",
      "Mask mean value:  tensor(0.0708, dtype=torch.float64)\n",
      "max feasible return = 0.4469  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04791702786903848\n",
      "‖alpha‖₁       : 0.8211346308067029\n",
      "scores min/max : -5.12866111781174 3.0308574287446417\n",
      "Mask mean value:  tensor(0.9317, dtype=torch.float64)\n",
      "max feasible return = -3.4964  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.009571021422964829\n",
      "‖alpha‖₁       : 0.7999999999999995\n",
      "scores min/max : -0.04980055945584051 0.009970088581764517\n",
      "Mask mean value:  tensor(0.4746, dtype=torch.float64)\n",
      "max feasible return = -0.1726  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.0252596331600386e-07\n",
      "‖alpha‖₁       : 0.6199999999999721\n",
      "scores min/max : 5.0694686355222104e-08 6.720196114329059e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.6373073199502104e-08\n",
      "‖alpha‖₁       : 0.43999999999996686\n",
      "scores min/max : 8.88546188745076e-09 1.846773935735088e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.258926270312052e-07\n",
      "‖alpha‖₁       : 0.2799999999999766\n",
      "scores min/max : -3.334981148383195e-07 -3.1555276237102193e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  15 | train 0.005419 | val 0.006778\n",
      "-----------------------------------------Epoch:  16 ----------------------------------------\n",
      "‖w_svm‖₂       : 0.020562774028625268\n",
      "‖alpha‖₁       : 0.38333610658551315\n",
      "scores min/max : -1.9700068038897287 0.22965460582227315\n",
      "Mask mean value:  tensor(0.6777, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1435  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.008e-14\n",
      "‖w_svm‖₂       : 5.60249037280586e-08\n",
      "‖alpha‖₁       : 0.23999999999997113\n",
      "scores min/max : -6.542038977211611e-08 -5.08260868773033e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.256e-20\n",
      "‖w_svm‖₂       : 1.4249948464876798e-05\n",
      "‖alpha‖₁       : 0.3599999999978196\n",
      "scores min/max : 1.1274550871152507e-05 1.2129033373731621e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.870e-17\n",
      "‖w_svm‖₂       : 7.56814825035819e-08\n",
      "‖alpha‖₁       : 0.5399999999999998\n",
      "scores min/max : -8.281579692809108e-08 -6.129294404105958e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.429e-20\n",
      "‖w_svm‖₂       : 0.02652148371684833\n",
      "‖alpha‖₁       : 0.6599999999999837\n",
      "scores min/max : -0.05966928376072321 0.06901240332495029\n",
      "Mask mean value:  tensor(0.4238, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.3883  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.268e-15\n",
      "‖w_svm‖₂       : 0.03399561835578666\n",
      "‖alpha‖₁       : 0.8985719089951545\n",
      "scores min/max : -0.7623381453793772 1.997941605828598\n",
      "Mask mean value:  tensor(0.8227, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4043  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.451e-15\n",
      "‖w_svm‖₂       : 0.00652894985954568\n",
      "‖alpha‖₁       : 0.607665881618549\n",
      "scores min/max : -1.9841678637574347 0.30283941244523205\n",
      "Mask mean value:  tensor(0.5833, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2357  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.412e-15\n",
      "‖w_svm‖₂       : 0.0004990617472902601\n",
      "‖alpha‖₁       : 0.43999999999999984\n",
      "scores min/max : -0.0005404706628704247 -0.0002911356080418455\n",
      "Mask mean value:  tensor(0.4975, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6471  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.064e-16\n",
      "‖w_svm‖₂       : 3.772920686321591e-07\n",
      "‖alpha‖₁       : 0.27999999999999214\n",
      "scores min/max : -1.327010183337309e-06 -1.2651995932124374e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.230e-19\n",
      "‖w_svm‖₂       : 0.026005112916180174\n",
      "‖alpha‖₁       : 0.8522420019794528\n",
      "scores min/max : -2.9259882099883456 1.5546627118333618\n",
      "Mask mean value:  tensor(0.1510, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9454  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.310e-03\n",
      "‖w_svm‖₂       : 0.052834052564629086\n",
      "‖alpha‖₁       : 0.7293490312469382\n",
      "scores min/max : -0.24484653972232903 2.0374611690746685\n",
      "Mask mean value:  tensor(0.7296, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2637  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.042e-03\n",
      "‖w_svm‖₂       : 0.00016351577249895652\n",
      "‖alpha‖₁       : 0.6399999999999999\n",
      "scores min/max : -6.400418860054055e-05 -5.380234362185188e-05\n",
      "Mask mean value:  tensor(0.4997, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7640  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.140e-17\n",
      "‖w_svm‖₂       : 0.13753617908177174\n",
      "‖alpha‖₁       : 0.6544414340003859\n",
      "scores min/max : -18.04344464546403 1.786162057596462\n",
      "Mask mean value:  tensor(0.2286, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2784  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.663e-03\n",
      "‖w_svm‖₂       : 0.14526611581802262\n",
      "‖alpha‖₁       : 0.7590403977299436\n",
      "scores min/max : -1.731200531753949 2.2462753497482333\n",
      "Mask mean value:  tensor(0.8884, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3583  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.780e-12\n",
      "‖w_svm‖₂       : 1.0576311495381074e-07\n",
      "‖alpha‖₁       : 0.23999999999997357\n",
      "scores min/max : 7.288188332229879e-09 1.830051376933245e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.220e-21\n",
      "‖w_svm‖₂       : 2.7557347856877557e-07\n",
      "‖alpha‖₁       : 0.6599999999999792\n",
      "scores min/max : -5.632495233945604e-07 -4.113244711908333e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.249e-18\n",
      "‖w_svm‖₂       : 0.0002157394540150793\n",
      "‖alpha‖₁       : 0.6199999999984263\n",
      "scores min/max : 4.579161103390411e-05 6.228395759588002e-05\n",
      "Mask mean value:  tensor(0.5003, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.006e-17\n",
      "‖w_svm‖₂       : 0.1412796997897187\n",
      "‖alpha‖₁       : 0.8799999999999867\n",
      "scores min/max : -1.411426029607983 3.7814687213909948\n",
      "Mask mean value:  tensor(0.3271, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2642  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.446e-08\n",
      "‖w_svm‖₂       : 0.05862218371779492\n",
      "‖alpha‖₁       : 0.5761350531787469\n",
      "scores min/max : -1.9568526443878522 0.8668342492494246\n",
      "Mask mean value:  tensor(0.7021, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9137  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.015e-03\n",
      "‖w_svm‖₂       : 0.028226979234103406\n",
      "‖alpha‖₁       : 0.19663521044813412\n",
      "scores min/max : -2.199653088898955 0.025944895373136087\n",
      "Mask mean value:  tensor(0.0638, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4302  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.859e-04\n",
      "‖w_svm‖₂       : 1.099588853289246e-07\n",
      "‖alpha‖₁       : 0.5199999999999997\n",
      "scores min/max : 2.9250120223208377e-07 3.139018304293801e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.399e-19\n",
      "‖w_svm‖₂       : 0.02303424461601177\n",
      "‖alpha‖₁       : 0.8233568342413089\n",
      "scores min/max : -1.8845089539961084 1.5161279025059613\n",
      "Mask mean value:  tensor(0.7389, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.4838  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.221e-06\n",
      "‖w_svm‖₂       : 0.019701726270387642\n",
      "‖alpha‖₁       : 0.7799999999999901\n",
      "scores min/max : -0.014456713442950627 0.057486845129713915\n",
      "Mask mean value:  tensor(0.4585, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.0029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.474e-15\n",
      "‖w_svm‖₂       : 1.9772984783368451e-07\n",
      "‖alpha‖₁       : 0.4199999999999765\n",
      "scores min/max : -2.817383359434967e-07 -2.594247514981208e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.501e-19\n",
      "‖w_svm‖₂       : 1.6598791525644814e-07\n",
      "‖alpha‖₁       : 0.239999999999999\n",
      "scores min/max : 2.569885772133022e-07 2.721719341192747e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.874e-20\n",
      "‖w_svm‖₂       : 1.1167323788082387e-07\n",
      "‖alpha‖₁       : 0.29999999999999866\n",
      "scores min/max : 7.732853908759442e-08 8.625607353636262e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.149e-07\n",
      "‖w_svm‖₂       : 1.8952254930807555e-07\n",
      "‖alpha‖₁       : 0.37999999999999884\n",
      "scores min/max : -7.268417440642424e-08 -3.698534056420904e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.503e-19\n",
      "‖w_svm‖₂       : 0.00013912528701964062\n",
      "‖alpha‖₁       : 0.4399999999763937\n",
      "scores min/max : -0.00019599587020343906 -5.507707582460349e-05\n",
      "Mask mean value:  tensor(0.4995, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0246  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.605e-17\n",
      "‖w_svm‖₂       : 0.024508604621418045\n",
      "‖alpha‖₁       : 0.8150883830859933\n",
      "scores min/max : -11.529263640496392 2.0345082496618483\n",
      "Mask mean value:  tensor(0.7142, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9089  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.224e-06\n",
      "‖w_svm‖₂       : 2.3212331745750013e-07\n",
      "‖alpha‖₁       : 0.58\n",
      "scores min/max : -3.8381236099824334e-07 -3.6818536215717144e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.822e-19\n",
      "‖w_svm‖₂       : 8.120490816731515e-08\n",
      "‖alpha‖₁       : 0.3799999999999998\n",
      "scores min/max : -5.458544813404402e-09 1.2690228253521477e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.466e-22\n",
      "‖w_svm‖₂       : 0.0008791745486896648\n",
      "‖alpha‖₁       : 0.8199999999999978\n",
      "scores min/max : 0.0008376556698664172 0.002557674574081478\n",
      "Mask mean value:  tensor(0.5096, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5888  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.003e-16\n",
      "‖w_svm‖₂       : 0.018516886296301632\n",
      "‖alpha‖₁       : 0.6815091615810633\n",
      "scores min/max : -1.9778365632248815 0.055641026111113864\n",
      "Mask mean value:  tensor(0.5858, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2344  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.934e-15\n",
      "‖w_svm‖₂       : 5.436360215192392e-08\n",
      "‖alpha‖₁       : 0.4399999999999988\n",
      "scores min/max : -1.365295470077401e-07 -3.239282172505079e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.190e-20\n",
      "‖w_svm‖₂       : 7.040681713509086e-08\n",
      "‖alpha‖₁       : 0.41999999999995563\n",
      "scores min/max : -7.336831268940638e-08 -2.5849094230270915e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.338e-08\n",
      "‖w_svm‖₂       : 2.2352790009834232e-08\n",
      "‖alpha‖₁       : 0.11999999999999508\n",
      "scores min/max : -1.806838700090143e-08 -3.4192414981416378e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.795e-09\n",
      "‖w_svm‖₂       : 6.741332970297797e-07\n",
      "‖alpha‖₁       : 0.3999999999999999\n",
      "scores min/max : -7.484146078274881e-08 2.3144825175315102e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.238e-09\n",
      "‖w_svm‖₂       : 0.18605279109059863\n",
      "‖alpha‖₁       : 0.8826723734803872\n",
      "scores min/max : -3.5422839028674478 6.089778287945012\n",
      "Mask mean value:  tensor(0.0982, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1876  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.903e-03\n",
      "‖w_svm‖₂       : 4.714318915439201e-08\n",
      "‖alpha‖₁       : 0.1799999999999972\n",
      "scores min/max : 3.57483603408825e-08 5.044549601170328e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 4.605e-08\n",
      "‖w_svm‖₂       : 1.1528145405999806e-06\n",
      "‖alpha‖₁       : 0.31999999999861595\n",
      "scores min/max : -1.7214540633910229e-06 -1.6052513814192794e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4898  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.650e-18\n",
      "‖w_svm‖₂       : 9.968322299249295e-07\n",
      "‖alpha‖₁       : 0.5999999999999943\n",
      "scores min/max : -1.4512055168675314e-08 3.6357024003115473e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.398e-19\n",
      "‖w_svm‖₂       : 0.04802037266667524\n",
      "‖alpha‖₁       : 0.9389963549263343\n",
      "scores min/max : -2.444130948187682 1.5964086979913419\n",
      "Mask mean value:  tensor(0.1537, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4107  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.474e-03\n",
      "‖w_svm‖₂       : 0.04195714544337786\n",
      "‖alpha‖₁       : 0.9402563681937499\n",
      "scores min/max : -1.829115441914455 0.30623866274533545\n",
      "Mask mean value:  tensor(0.4240, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3394  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.327e-14\n",
      "‖w_svm‖₂       : 2.0945012197091684e-07\n",
      "‖alpha‖₁       : 0.37999999999491113\n",
      "scores min/max : 1.770292323999487e-08 3.653955096830593e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.333e-07\n",
      "‖w_svm‖₂       : 0.05423360632439791\n",
      "‖alpha‖₁       : 0.6599999999999997\n",
      "scores min/max : -0.37386562243394406 0.2863644998199129\n",
      "Mask mean value:  tensor(0.5207, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9103  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.553e-14\n",
      "‖w_svm‖₂       : 8.643913399876331e-08\n",
      "‖alpha‖₁       : 0.17999999999999733\n",
      "scores min/max : -6.289980197963889e-08 1.714777626471186e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.449e-21\n",
      "‖w_svm‖₂       : 0.05350351376552177\n",
      "‖alpha‖₁       : 0.8270636383383738\n",
      "scores min/max : -1.947913301453222 0.40271771165246195\n",
      "Mask mean value:  tensor(0.5273, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0036  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.592e-02\n",
      "‖w_svm‖₂       : 0.02883356279768936\n",
      "‖alpha‖₁       : 0.5510207648287601\n",
      "scores min/max : -3.5654817622155295 1.0083167243679085\n",
      "Mask mean value:  tensor(0.1103, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9344  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.002e-03\n",
      "‖w_svm‖₂       : 0.08055328302897703\n",
      "‖alpha‖₁       : 0.6582642979747295\n",
      "scores min/max : -2.0258127951216305 3.9251502258361057\n",
      "Mask mean value:  tensor(0.3663, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.1463  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.435e-04\n",
      "‖w_svm‖₂       : 0.08119247325192669\n",
      "‖alpha‖₁       : 0.4717311835793692\n",
      "scores min/max : -2.280594269128558 2.799478002566095\n",
      "Mask mean value:  tensor(0.1197, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.2967  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.218e-02\n",
      "‖w_svm‖₂       : 0.07179738423698359\n",
      "‖alpha‖₁       : 0.5799999999999998\n",
      "scores min/max : -2.853605190247107 1.6077269574933655\n",
      "Mask mean value:  tensor(0.1516, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2897  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.025e-04\n",
      "‖w_svm‖₂       : 0.12446767276121314\n",
      "‖alpha‖₁       : 0.8732643415869579\n",
      "scores min/max : -12.224339359864105 2.1431738800886277\n",
      "Mask mean value:  tensor(0.5190, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.5295  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.168e-01\n",
      "‖w_svm‖₂       : 0.003111768861135887\n",
      "‖alpha‖₁       : 0.5799999999999976\n",
      "scores min/max : 0.001288922825224548 0.002518638906526177\n",
      "Mask mean value:  tensor(0.5113, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3251  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.193e-13\n",
      "‖w_svm‖₂       : 0.020636103319689337\n",
      "‖alpha‖₁       : 0.8599999999999522\n",
      "scores min/max : -0.053839725903205585 0.0384482679130109\n",
      "Mask mean value:  tensor(0.4184, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5493  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.629e-14\n",
      "‖w_svm‖₂       : 7.706501120098669e-08\n",
      "‖alpha‖₁       : 0.6599999999999975\n",
      "scores min/max : 9.15619957885186e-08 2.365231498639384e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.602e-09\n",
      "‖w_svm‖₂       : 4.1816393363396473e-07\n",
      "‖alpha‖₁       : 0.7199999999999989\n",
      "scores min/max : 2.640115915610388e-07 3.2118689432618624e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.795e-21\n",
      "‖w_svm‖₂       : 2.3280662905400272e-07\n",
      "‖alpha‖₁       : 0.639999999999992\n",
      "scores min/max : 2.79284864521598e-07 3.002740648197616e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.169e-20\n",
      "‖w_svm‖₂       : 0.00031079803361334023\n",
      "‖alpha‖₁       : 0.41999999999877896\n",
      "scores min/max : 0.00014309412828393862 0.0004703813262655009\n",
      "Mask mean value:  tensor(0.5022, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9999  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.741e-15\n",
      "‖w_svm‖₂       : 0.018405429929341667\n",
      "‖alpha‖₁       : 0.5962524448889399\n",
      "scores min/max : -2.9256165963892995 2.255389537944541\n",
      "Mask mean value:  tensor(0.9069, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.4930  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.305e-03\n",
      "‖w_svm‖₂       : 0.00018906326419503133\n",
      "‖alpha‖₁       : 0.8199999999999996\n",
      "scores min/max : -4.414321494731143e-05 -2.925802283814012e-05\n",
      "Mask mean value:  tensor(0.4998, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6973  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.189e-17\n",
      "‖w_svm‖₂       : 0.0053624501299592\n",
      "‖alpha‖₁       : 0.7005621426904503\n",
      "scores min/max : -2.004223814552757 0.019156937166880276\n",
      "Mask mean value:  tensor(0.4537, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2546  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.436e-05\n",
      "‖w_svm‖₂       : 0.005226772151270003\n",
      "‖alpha‖₁       : 0.37999999999996464\n",
      "scores min/max : -0.0073220511260320975 0.004624360697832293\n",
      "Mask mean value:  tensor(0.5076, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9032  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.734e-14\n",
      "‖w_svm‖₂       : 0.04839789437277924\n",
      "‖alpha‖₁       : 0.9056470286373379\n",
      "scores min/max : -0.7611968798658384 1.8760470588274716\n",
      "Mask mean value:  tensor(0.3381, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6253  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.703e-03\n",
      "‖w_svm‖₂       : 2.2316110752415483e-07\n",
      "‖alpha‖₁       : 0.25999999999999984\n",
      "scores min/max : 2.3798737825044196e-08 3.652097616876027e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.487e-17\n",
      "‖w_svm‖₂       : 0.044025882012068605\n",
      "‖alpha‖₁       : 0.8899573215769314\n",
      "scores min/max : -2.009462870821499 0.33774795725467904\n",
      "Mask mean value:  tensor(0.2646, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0746  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.026e-11\n",
      "‖w_svm‖₂       : 0.005597907091796708\n",
      "‖alpha‖₁       : 0.5599999999999822\n",
      "scores min/max : 0.0036785303074406833 0.005302701235588701\n",
      "Mask mean value:  tensor(0.5199, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0897  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 8.066e-05\n",
      "‖w_svm‖₂       : 0.016936458705353544\n",
      "‖alpha‖₁       : 0.8599999999999244\n",
      "scores min/max : -0.04460074693822648 -0.027167699561115248\n",
      "Mask mean value:  tensor(0.3457, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0816  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.097e-14\n",
      "‖w_svm‖₂       : 0.05105882226071187\n",
      "‖alpha‖₁       : 0.9199999999999903\n",
      "scores min/max : -0.3932463779276754 0.5725688609687353\n",
      "Mask mean value:  tensor(0.3527, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7410  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.140e-02\n",
      "‖w_svm‖₂       : 0.061103833213994235\n",
      "‖alpha‖₁       : 0.8985238923237856\n",
      "scores min/max : -1.7276993809477257 3.6763302205116286\n",
      "Mask mean value:  tensor(0.3698, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5302  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.296e-02\n",
      "‖w_svm‖₂       : 3.54703813782278e-06\n",
      "‖alpha‖₁       : 0.41999999999309906\n",
      "scores min/max : -6.872816851594748e-07 9.396208477170172e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1230  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.482e-05\n",
      "‖w_svm‖₂       : 0.07845941293345349\n",
      "‖alpha‖₁       : 0.41902496728143757\n",
      "scores min/max : -1.8402669939997647 2.508219314618197\n",
      "Mask mean value:  tensor(0.8140, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8859  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.183e-11\n",
      "‖w_svm‖₂       : 1.0730452886696836e-06\n",
      "‖alpha‖₁       : 0.49999999999998224\n",
      "scores min/max : -2.8779679069865906e-07 -9.453583817366202e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.791e-19\n",
      "‖w_svm‖₂       : 7.136762718451957e-08\n",
      "‖alpha‖₁       : 0.13999999999999738\n",
      "scores min/max : 1.6161069331314046e-09 7.367722063426235e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.676e-08\n",
      "‖w_svm‖₂       : 0.004256137680370056\n",
      "‖alpha‖₁       : 0.45999999999999264\n",
      "scores min/max : -0.006063495942959378 -0.004125140096809712\n",
      "Mask mean value:  tensor(0.4771, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.0737  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.514e-16\n",
      "‖w_svm‖₂       : 0.0003648792437378043\n",
      "‖alpha‖₁       : 0.7399999999999997\n",
      "scores min/max : -0.00018207431838509832 -0.00016509284117143693\n",
      "Mask mean value:  tensor(0.4992, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0625  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.072e-17\n",
      "‖w_svm‖₂       : 5.4492589309683254e-08\n",
      "‖alpha‖₁       : 0.11999999999998084\n",
      "scores min/max : -7.800719591274974e-08 -4.706993312751788e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.999e-08\n",
      "‖w_svm‖₂       : 0.08601050290151335\n",
      "‖alpha‖₁       : 0.5779233351810974\n",
      "scores min/max : -2.1164060081723317 5.8190919932820755\n",
      "Mask mean value:  tensor(0.2833, dtype=torch.float64)\n",
      "max feasible return = 0.1136  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.9521540982669715e-07\n",
      "‖alpha‖₁       : 0.5799999999999774\n",
      "scores min/max : -3.7296052945539904e-07 -2.8191700997339504e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0687659700335378e-07\n",
      "‖alpha‖₁       : 0.2999999999999659\n",
      "scores min/max : 3.1521330070902766e-08 3.669092393117985e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 7.019471365756656e-08\n",
      "‖alpha‖₁       : 0.599999999999999\n",
      "scores min/max : 1.6142977863300728e-08 5.2897050878031756e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.691816368878656e-08\n",
      "‖alpha‖₁       : 0.3799999999999871\n",
      "scores min/max : 9.346103121627909e-09 1.8776661212311744e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.381131717628302e-06\n",
      "‖alpha‖₁       : 0.3199999999897928\n",
      "scores min/max : 1.934392840558166e-06 3.0719742061738965e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4258  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.06192377341304115\n",
      "‖alpha‖₁       : 0.7397004901391558\n",
      "scores min/max : -3.4926544916421767 2.1287617941715125\n",
      "Mask mean value:  tensor(0.8580, dtype=torch.float64)\n",
      "max feasible return = -0.9068  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.034942392332091045\n",
      "‖alpha‖₁       : 0.6151095819037625\n",
      "scores min/max : -1.8946693234812166 1.2396097259764487\n",
      "Mask mean value:  tensor(0.8276, dtype=torch.float64)\n",
      "max feasible return = 2.9122  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.6220799627832755e-07\n",
      "‖alpha‖₁       : 0.4599999999999922\n",
      "scores min/max : 1.2474616340120746e-07 7.192798714686586e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.3442380266557065e-07\n",
      "‖alpha‖₁       : 0.5199999999999914\n",
      "scores min/max : 6.303135784365096e-08 7.593615092499843e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.536157745072403e-07\n",
      "‖alpha‖₁       : 0.5799999999999744\n",
      "scores min/max : -2.280236880874222e-07 -1.9166827601347123e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.04461318136780416\n",
      "‖alpha‖₁       : 0.6876029772831042\n",
      "scores min/max : -0.4283431555339685 1.9615422096652457\n",
      "Mask mean value:  tensor(0.4204, dtype=torch.float64)\n",
      "max feasible return = 0.0306  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.034829312106711086\n",
      "‖alpha‖₁       : 0.42213068232796014\n",
      "scores min/max : -3.9267198666427383 5.182544350279043\n",
      "Mask mean value:  tensor(0.0703, dtype=torch.float64)\n",
      "max feasible return = 0.4436  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.047599753934036745\n",
      "‖alpha‖₁       : 0.8211370595377148\n",
      "scores min/max : -5.130916384331812 3.029329387666165\n",
      "Mask mean value:  tensor(0.9311, dtype=torch.float64)\n",
      "max feasible return = -3.4933  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.009436563952819993\n",
      "‖alpha‖₁       : 0.7999999999999996\n",
      "scores min/max : -0.04785567583862351 0.010451575056345438\n",
      "Mask mean value:  tensor(0.4788, dtype=torch.float64)\n",
      "max feasible return = -0.1741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.0566833589913103e-07\n",
      "‖alpha‖₁       : 0.6199999999999701\n",
      "scores min/max : 1.137048095943906e-08 2.7883119296602106e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.656620373844921e-08\n",
      "‖alpha‖₁       : 0.43999999999996786\n",
      "scores min/max : 8.671558860346583e-09 1.8253614383978228e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.2764075215230396e-07\n",
      "‖alpha‖₁       : 0.2799999999999768\n",
      "scores min/max : -3.065112273722477e-07 -2.885634111715658e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  16 | train 0.005417 | val 0.006786\n",
      "-----------------------------------------Epoch:  17 ----------------------------------------\n",
      "‖w_svm‖₂       : 0.00018888625894393453\n",
      "‖alpha‖₁       : 0.8199999999999996\n",
      "scores min/max : -4.252017637845814e-05 -2.7660570823361875e-05\n",
      "Mask mean value:  tensor(0.4998, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6973  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.187e-17\n",
      "‖w_svm‖₂       : 0.005388546815352056\n",
      "‖alpha‖₁       : 0.700562362387247\n",
      "scores min/max : -2.00464716237355 0.018735694168449637\n",
      "Mask mean value:  tensor(0.4516, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2534  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.968e-05\n",
      "‖w_svm‖₂       : 0.022869005635645794\n",
      "‖alpha‖₁       : 0.8233573267280111\n",
      "scores min/max : -1.8857396912759419 1.5145873334168447\n",
      "Mask mean value:  tensor(0.7358, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.4754  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.017e-06\n",
      "‖w_svm‖₂       : 3.5439209352676877e-06\n",
      "‖alpha‖₁       : 0.4199999999931513\n",
      "scores min/max : -6.565674213750366e-07 9.638034329591862e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1230  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.481e-05\n",
      "‖w_svm‖₂       : 7.148063573444795e-08\n",
      "‖alpha‖₁       : 0.1399999999999969\n",
      "scores min/max : 1.5155767385734578e-09 7.282410003822185e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.682e-08\n",
      "‖w_svm‖₂       : 0.08006253259221591\n",
      "‖alpha‖₁       : 0.6582972900348731\n",
      "scores min/max : -2.028262927769984 3.950056187768003\n",
      "Mask mean value:  tensor(0.3619, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.1085  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.477e-04\n",
      "‖w_svm‖₂       : 0.053579342071225076\n",
      "‖alpha‖₁       : 0.6599999999999998\n",
      "scores min/max : -0.3711729489598835 0.27378561037154\n",
      "Mask mean value:  tensor(0.4936, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8601  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.197e-13\n",
      "‖w_svm‖₂       : 0.016998140781939775\n",
      "‖alpha‖₁       : 0.8599999999999848\n",
      "scores min/max : -0.04731547928725307 -0.02993389847613953\n",
      "Mask mean value:  tensor(0.3334, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0782  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.207e-14\n",
      "‖w_svm‖₂       : 6.669979000536756e-07\n",
      "‖alpha‖₁       : 0.39999999999999963\n",
      "scores min/max : -1.0883249655231107e-07 1.9745327341417584e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.196e-09\n",
      "‖w_svm‖₂       : 0.006617282180787218\n",
      "‖alpha‖₁       : 0.607666932177548\n",
      "scores min/max : -1.984014493307678 0.3029686646471128\n",
      "Mask mean value:  tensor(0.5839, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2360  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.458e-15\n",
      "‖w_svm‖₂       : 2.2138204525965368e-08\n",
      "‖alpha‖₁       : 0.11999999999999439\n",
      "scores min/max : -2.045582240616577e-08 -5.774369479131636e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.798e-09\n",
      "‖w_svm‖₂       : 0.0005052639117642747\n",
      "‖alpha‖₁       : 0.43999999999999984\n",
      "scores min/max : -0.0006769831534127451 -0.00042144268342039987\n",
      "Mask mean value:  tensor(0.4968, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6448  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.062e-16\n",
      "‖w_svm‖₂       : 1.8966713518972187e-07\n",
      "‖alpha‖₁       : 0.379999999999999\n",
      "scores min/max : -2.7043661484819274e-08 8.65070349984171e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.485e-19\n",
      "‖w_svm‖₂       : 0.05192916819212084\n",
      "‖alpha‖₁       : 0.7293510418608649\n",
      "scores min/max : -0.24378388439308946 2.038823683297738\n",
      "Mask mean value:  tensor(0.7321, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2781  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.026e-03\n",
      "‖w_svm‖₂       : 0.14579972267966673\n",
      "‖alpha‖₁       : 0.7591940521332383\n",
      "scores min/max : -1.7356249617942943 2.2419274324187457\n",
      "Mask mean value:  tensor(0.8833, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3576  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.139e-11\n",
      "‖w_svm‖₂       : 5.471366039354558e-08\n",
      "‖alpha‖₁       : 0.43999999999999817\n",
      "scores min/max : -1.4435132528507612e-07 -4.0253405165138294e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.270e-20\n",
      "‖w_svm‖₂       : 0.0052732242286973215\n",
      "‖alpha‖₁       : 0.37999999999998657\n",
      "scores min/max : -0.007020416405046123 0.005168897815472362\n",
      "Mask mean value:  tensor(0.5100, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9073  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.743e-14\n",
      "‖w_svm‖₂       : 0.048214263940196414\n",
      "‖alpha‖₁       : 0.9056468342793162\n",
      "scores min/max : -0.7476758790554688 1.8818027546649532\n",
      "Mask mean value:  tensor(0.3529, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6529  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.608e-03\n",
      "‖w_svm‖₂       : 0.04817925562477278\n",
      "‖alpha‖₁       : 0.9390126674030304\n",
      "scores min/max : -2.4358719946066056 1.6049944210421776\n",
      "Mask mean value:  tensor(0.1601, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4215  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.137e-03\n",
      "‖w_svm‖₂       : 0.02021165482565836\n",
      "‖alpha‖₁       : 0.383336119956877\n",
      "scores min/max : -1.9732424412602287 0.22688165610301378\n",
      "Mask mean value:  tensor(0.6655, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1425  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.472e-14\n",
      "‖w_svm‖₂       : 0.02598042866649379\n",
      "‖alpha‖₁       : 0.6599999999999759\n",
      "scores min/max : -0.056372498602951376 0.0668442501674146\n",
      "Mask mean value:  tensor(0.4308, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.4379  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.117e-15\n",
      "‖w_svm‖₂       : 0.14037727289222388\n",
      "‖alpha‖₁       : 0.88\n",
      "scores min/max : -1.3962114483613712 3.7258478135475865\n",
      "Mask mean value:  tensor(0.3186, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2613  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.901e-05\n",
      "‖w_svm‖₂       : 0.028431637046023656\n",
      "‖alpha‖₁       : 0.19664874741415297\n",
      "scores min/max : -2.2010553448315013 0.024366663393528505\n",
      "Mask mean value:  tensor(0.0624, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4208  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.076e-04\n",
      "‖w_svm‖₂       : 0.13824520864898454\n",
      "‖alpha‖₁       : 0.6546757721017871\n",
      "scores min/max : -18.025654191318687 1.8105355392390414\n",
      "Mask mean value:  tensor(0.2491, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2851  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.033e-03\n",
      "‖w_svm‖₂       : 2.332827088631029e-07\n",
      "‖alpha‖₁       : 0.5800000000000001\n",
      "scores min/max : -3.2015667150811937e-07 -3.0453227543218044e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.821e-19\n",
      "‖w_svm‖₂       : 0.03375751295254579\n",
      "‖alpha‖₁       : 0.8985759688082378\n",
      "scores min/max : -0.7593434932099583 2.0009575638050903\n",
      "Mask mean value:  tensor(0.8276, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3959  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.516e-15\n",
      "‖w_svm‖₂       : 2.3349021092860116e-07\n",
      "‖alpha‖₁       : 0.6399999999999952\n",
      "scores min/max : 2.7496260136450716e-07 2.9589543856391077e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.177e-20\n",
      "‖w_svm‖₂       : 0.02031527140025346\n",
      "‖alpha‖₁       : 0.8599999999999995\n",
      "scores min/max : -0.05256391710476539 0.03719607991627999\n",
      "Mask mean value:  tensor(0.4197, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5495  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.016e-14\n",
      "‖w_svm‖₂       : 0.053667219041083555\n",
      "‖alpha‖₁       : 0.8271078482697194\n",
      "scores min/max : -1.9480356130341585 0.4009995915617724\n",
      "Mask mean value:  tensor(0.5267, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0052  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.606e-02\n",
      "‖w_svm‖₂       : 0.00021411725870556397\n",
      "‖alpha‖₁       : 0.6199999999984742\n",
      "scores min/max : 4.426288647914648e-05 6.0508599369450024e-05\n",
      "Mask mean value:  tensor(0.5002, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.000e-17\n",
      "‖w_svm‖₂       : 0.18754366765131744\n",
      "‖alpha‖₁       : 0.8833385749564261\n",
      "scores min/max : -3.553728487899269 6.083478512406711\n",
      "Mask mean value:  tensor(0.0966, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1879  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.367e-03\n",
      "‖w_svm‖₂       : 1.007159185966577e-06\n",
      "‖alpha‖₁       : 0.5999999999999799\n",
      "scores min/max : -1.0236086089253854e-07 -5.1414978642211474e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.455e-19\n",
      "‖w_svm‖₂       : 7.64277817043726e-08\n",
      "‖alpha‖₁       : 0.6599999999999994\n",
      "scores min/max : 8.950820076592348e-08 2.3468577428753903e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.096e-08\n",
      "‖w_svm‖₂       : 1.123435660367767e-07\n",
      "‖alpha‖₁       : 0.299999999999999\n",
      "scores min/max : 9.845947874974749e-08 1.0737988860560226e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.177e-07\n",
      "‖w_svm‖₂       : 2.232647887215522e-07\n",
      "‖alpha‖₁       : 0.2599999999999999\n",
      "scores min/max : 4.871777023230576e-08 6.144054686456569e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.549e-17\n",
      "‖w_svm‖₂       : 1.993603981325025e-07\n",
      "‖alpha‖₁       : 0.4199999999999768\n",
      "scores min/max : -3.351777223059248e-07 -3.1286473842328184e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.562e-19\n",
      "‖w_svm‖₂       : 5.5922048453711304e-08\n",
      "‖alpha‖₁       : 0.2399999999999763\n",
      "scores min/max : -5.994727827750655e-08 -4.5384436034932484e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.250e-20\n",
      "‖w_svm‖₂       : 0.00036445190110240917\n",
      "‖alpha‖₁       : 0.7399999999999995\n",
      "scores min/max : -0.00018743891945822893 -0.00017049718378267878\n",
      "Mask mean value:  tensor(0.4991, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0624  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.072e-17\n",
      "‖w_svm‖₂       : 0.00031099451848949324\n",
      "‖alpha‖₁       : 0.41999999997071197\n",
      "scores min/max : 0.00012419878674048461 0.0004515571115704574\n",
      "Mask mean value:  tensor(0.5021, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9997  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.752e-15\n",
      "‖w_svm‖₂       : 1.0643209446227803e-07\n",
      "‖alpha‖₁       : 0.23999999999997684\n",
      "scores min/max : 3.173480573884878e-08 4.2728695448980663e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.299e-21\n",
      "‖w_svm‖₂       : 0.026052343370077054\n",
      "‖alpha‖₁       : 0.8522421292574187\n",
      "scores min/max : -2.943733247680755 1.5371219055356924\n",
      "Mask mean value:  tensor(0.1439, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9345  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.129e-03\n",
      "‖w_svm‖₂       : 0.0008770946736308818\n",
      "‖alpha‖₁       : 0.8199999999999991\n",
      "scores min/max : 0.0008519590906596545 0.002579633267715574\n",
      "Mask mean value:  tensor(0.5097, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5893  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.977e-17\n",
      "‖w_svm‖₂       : 0.058616571849823836\n",
      "‖alpha‖₁       : 0.5761920645980054\n",
      "scores min/max : -1.960919033746432 0.8637015198609376\n",
      "Mask mean value:  tensor(0.6912, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.8821  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.602e-04\n",
      "‖w_svm‖₂       : 0.08211768516852319\n",
      "‖alpha‖₁       : 0.471892531355083\n",
      "scores min/max : -2.3127649333843965 2.7684476419306785\n",
      "Mask mean value:  tensor(0.1055, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1520  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.253e-02\n",
      "‖w_svm‖₂       : 1.1085902255562361e-07\n",
      "‖alpha‖₁       : 0.5199999999999996\n",
      "scores min/max : 2.7941073356105366e-07 3.0080989153349204e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.429e-19\n",
      "‖w_svm‖₂       : 7.645259550325184e-08\n",
      "‖alpha‖₁       : 0.5399999999999998\n",
      "scores min/max : -1.0786233908924003e-07 -8.633888171700813e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.547e-20\n",
      "‖w_svm‖₂       : 0.004267545824242243\n",
      "‖alpha‖₁       : 0.45999999999999874\n",
      "scores min/max : -0.006199029277731858 -0.004251622863952649\n",
      "Mask mean value:  tensor(0.4765, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.0709  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.006e-15\n",
      "‖w_svm‖₂       : 0.018137475881849795\n",
      "‖alpha‖₁       : 0.6815093830007446\n",
      "scores min/max : -1.97990269144253 0.0524273539724288\n",
      "Mask mean value:  tensor(0.5762, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2310  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.202e-12\n",
      "‖w_svm‖₂       : 0.12323882025279079\n",
      "‖alpha‖₁       : 0.8732642938921302\n",
      "scores min/max : -12.28040294316091 2.0841275513808304\n",
      "Mask mean value:  tensor(0.3779, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1394  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 2.288e-01\n",
      "‖w_svm‖₂       : 0.04153867677494233\n",
      "‖alpha‖₁       : 0.9402691783902248\n",
      "scores min/max : -1.8215768194925026 0.3134982670315129\n",
      "Mask mean value:  tensor(0.4534, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3373  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.885e-15\n",
      "‖w_svm‖₂       : 0.050499956613577326\n",
      "‖alpha‖₁       : 0.9199999999999013\n",
      "scores min/max : -0.38631248292297826 0.558453480313075\n",
      "Mask mean value:  tensor(0.3499, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7283  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.077e-02\n",
      "‖w_svm‖₂       : 0.019897833878279095\n",
      "‖alpha‖₁       : 0.7799999999999928\n",
      "scores min/max : -0.014181833953400327 0.057842169647624564\n",
      "Mask mean value:  tensor(0.4599, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.0088  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.490e-15\n",
      "‖w_svm‖₂       : 1.405637456073797e-05\n",
      "‖alpha‖₁       : 0.3599999999981749\n",
      "scores min/max : 1.1626008836335608e-05 1.245833609661103e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.838e-17\n",
      "‖w_svm‖₂       : 0.00013771663220248808\n",
      "‖alpha‖₁       : 0.4399999999716843\n",
      "scores min/max : -0.00019298866365882088 -5.388357160207677e-05\n",
      "Mask mean value:  tensor(0.4995, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0247  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.589e-17\n",
      "‖w_svm‖₂       : 0.07822207064547614\n",
      "‖alpha‖₁       : 0.4190734674805411\n",
      "scores min/max : -1.8310336086476624 2.501089090395104\n",
      "Mask mean value:  tensor(0.8204, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8848  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.296e-12\n",
      "‖w_svm‖₂       : 1.6990672717509407e-07\n",
      "‖alpha‖₁       : 0.2399999999999827\n",
      "scores min/max : 2.684725633046086e-07 2.840132632847535e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.027e-20\n",
      "‖w_svm‖₂       : 0.04401920449683688\n",
      "‖alpha‖₁       : 0.8899832879722037\n",
      "scores min/max : -2.011303973383214 0.33589978816610133\n",
      "Mask mean value:  tensor(0.2590, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0706  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.233e-05\n",
      "‖w_svm‖₂       : 5.404524531205811e-08\n",
      "‖alpha‖₁       : 0.11999999999999861\n",
      "scores min/max : -7.727260478204096e-08 -4.661161595220082e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.512e-08\n",
      "‖w_svm‖₂       : 7.156453790898922e-08\n",
      "‖alpha‖₁       : 0.42\n",
      "scores min/max : -7.512063428144078e-08 -2.8018567352297452e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.499e-08\n",
      "‖w_svm‖₂       : 0.06138984832049578\n",
      "‖alpha‖₁       : 0.8985808022154882\n",
      "scores min/max : -1.7310202776379509 3.6733770951566154\n",
      "Mask mean value:  tensor(0.3641, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5257  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.274e-02\n",
      "‖w_svm‖₂       : 0.018078041419196323\n",
      "‖alpha‖₁       : 0.5962526873313199\n",
      "scores min/max : -2.9282133091275737 2.260016624547184\n",
      "Mask mean value:  tensor(0.9094, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.4971  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.700e-04\n",
      "‖w_svm‖₂       : 4.2372572948650016e-07\n",
      "‖alpha‖₁       : 0.7199999999999986\n",
      "scores min/max : 3.1607699829484245e-07 3.732817118120228e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.880e-21\n",
      "‖w_svm‖₂       : 0.024092969954737642\n",
      "‖alpha‖₁       : 0.8150890478813522\n",
      "scores min/max : -11.526392340191775 2.0352084854851373\n",
      "Mask mean value:  tensor(0.7158, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9151  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.449e-06\n",
      "‖w_svm‖₂       : 0.07133074540523386\n",
      "‖alpha‖₁       : 0.58\n",
      "scores min/max : -2.8144899165406607 1.5834440459334997\n",
      "Mask mean value:  tensor(0.1510, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2877  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.874e-04\n",
      "‖w_svm‖₂       : 0.0031118895828265835\n",
      "‖alpha‖₁       : 0.5799999999999813\n",
      "scores min/max : 0.0016489370431626142 0.0028776749782323738\n",
      "Mask mean value:  tensor(0.5131, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3263  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.190e-13\n",
      "‖w_svm‖₂       : 8.616533444414494e-08\n",
      "‖alpha‖₁       : 0.17999999999999827\n",
      "scores min/max : -6.727285212560351e-08 1.6680241703055535e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.482e-21\n",
      "‖w_svm‖₂       : 1.0785229399305631e-06\n",
      "‖alpha‖₁       : 0.49999999999998057\n",
      "scores min/max : -2.6424705364416835e-07 -7.096515355815126e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.851e-19\n",
      "‖w_svm‖₂       : 0.005537511185982093\n",
      "‖alpha‖₁       : 0.5599999999999998\n",
      "scores min/max : 0.0030633935196832576 0.004653552871733219\n",
      "Mask mean value:  tensor(0.5167, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0832  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.178e-04\n",
      "‖w_svm‖₂       : 0.028598707590342467\n",
      "‖alpha‖₁       : 0.55102396993526\n",
      "scores min/max : -3.5591081959778657 1.0140020588323932\n",
      "Mask mean value:  tensor(0.1124, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9494  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.231e-03\n",
      "‖w_svm‖₂       : 2.0842593779228516e-07\n",
      "‖alpha‖₁       : 0.37999999999534284\n",
      "scores min/max : 1.8478858166492112e-08 3.686781312597015e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.082e-07\n",
      "‖w_svm‖₂       : 4.813112238270384e-08\n",
      "‖alpha‖₁       : 0.1799999999999974\n",
      "scores min/max : 3.494836545729771e-08 4.9659235352991836e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 4.728e-08\n",
      "‖w_svm‖₂       : 8.272077330005089e-08\n",
      "‖alpha‖₁       : 0.3799999999999999\n",
      "scores min/max : -1.4592886505304116e-08 3.5542533679683155e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.594e-22\n",
      "‖w_svm‖₂       : 0.00016091743597354977\n",
      "‖alpha‖₁       : 0.6399999999999999\n",
      "scores min/max : -6.042461869046636e-05 -5.054412708487913e-05\n",
      "Mask mean value:  tensor(0.4997, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7640  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.116e-17\n",
      "‖w_svm‖₂       : 3.8190075305871376e-07\n",
      "‖alpha‖₁       : 0.27999999999999614\n",
      "scores min/max : -1.3952772285421843e-06 -1.333549892240253e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.319e-19\n",
      "‖w_svm‖₂       : 2.7961705790172367e-07\n",
      "‖alpha‖₁       : 0.6599999999999869\n",
      "scores min/max : -6.04479160394039e-07 -4.5267739125725064e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.290e-18\n",
      "‖w_svm‖₂       : 1.1662049880679636e-06\n",
      "‖alpha‖₁       : 0.3199999999986802\n",
      "scores min/max : -1.8511529953560876e-06 -1.7355122010135528e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4898  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.704e-18\n",
      "‖w_svm‖₂       : 0.08625812480810593\n",
      "‖alpha‖₁       : 0.5779974037790725\n",
      "scores min/max : -2.125393180015026 5.807850936173502\n",
      "Mask mean value:  tensor(0.2665, dtype=torch.float64)\n",
      "max feasible return = 0.1103  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.951236209246841e-07\n",
      "‖alpha‖₁       : 0.5800000000000001\n",
      "scores min/max : -3.4241667069202084e-07 -2.5170661147291386e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0756630526091166e-07\n",
      "‖alpha‖₁       : 0.29999999999996696\n",
      "scores min/max : 3.8052048154701433e-08 4.321180160386572e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 7.019655643532514e-08\n",
      "‖alpha‖₁       : 0.5999999999999981\n",
      "scores min/max : 1.5184430987413342e-08 5.196482991822717e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.69372296342285e-08\n",
      "‖alpha‖₁       : 0.3799999999999839\n",
      "scores min/max : 9.080213130414861e-09 1.8513424628291167e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.5220192858930172e-06\n",
      "‖alpha‖₁       : 0.31999999998802947\n",
      "scores min/max : 2.6772052818208774e-06 3.901494385389795e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4258  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.06072898005887352\n",
      "‖alpha‖₁       : 0.7396997669294196\n",
      "scores min/max : -3.4811694400132356 2.1429650976324934\n",
      "Mask mean value:  tensor(0.8716, dtype=torch.float64)\n",
      "max feasible return = -0.9183  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.035630619560857155\n",
      "‖alpha‖₁       : 0.6151568895115663\n",
      "scores min/max : -1.9045827327374765 1.2296885221651146\n",
      "Mask mean value:  tensor(0.8063, dtype=torch.float64)\n",
      "max feasible return = 2.8335  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.657448766978459e-07\n",
      "‖alpha‖₁       : 0.45999999999999963\n",
      "scores min/max : 2.8479235116310287e-07 8.788987825192008e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.3643137322327488e-07\n",
      "‖alpha‖₁       : 0.5199999999999929\n",
      "scores min/max : 4.692964207378894e-08 5.983541711720817e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.5629345947257273e-07\n",
      "‖alpha‖₁       : 0.5799999999999754\n",
      "scores min/max : -2.6648614698954904e-07 -2.3015793471712646e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.045059361840173874\n",
      "‖alpha‖₁       : 0.6876453112371063\n",
      "scores min/max : -0.4350739084493858 1.9545650236775536\n",
      "Mask mean value:  tensor(0.3956, dtype=torch.float64)\n",
      "max feasible return = 0.0541  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.034401637461114666\n",
      "‖alpha‖₁       : 0.42213106270661277\n",
      "scores min/max : -3.9215061931442876 5.192440343304141\n",
      "Mask mean value:  tensor(0.0725, dtype=torch.float64)\n",
      "max feasible return = 0.4574  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04738855969192753\n",
      "‖alpha‖₁       : 0.8211409198385674\n",
      "scores min/max : -5.11314637794718 3.047682331572428\n",
      "Mask mean value:  tensor(0.9365, dtype=torch.float64)\n",
      "max feasible return = -3.5272  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.009229334951907324\n",
      "‖alpha‖₁       : 0.7999999999999995\n",
      "scores min/max : -0.047222437923698755 0.008425028864372842\n",
      "Mask mean value:  tensor(0.4720, dtype=torch.float64)\n",
      "max feasible return = -0.1717  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.091079292597149e-07\n",
      "‖alpha‖₁       : 0.6199999999999651\n",
      "scores min/max : 6.775498329825246e-08 8.427926849632624e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.727356440657399e-08\n",
      "‖alpha‖₁       : 0.43999999999997064\n",
      "scores min/max : 1.0269111552276893e-08 1.9850526726479305e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.2996610071993494e-07\n",
      "‖alpha‖₁       : 0.2799999999999785\n",
      "scores min/max : -3.7121827052494786e-07 -3.5328262415628757e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  17 | train 0.005432 | val 0.006769\n",
      "-----------------------------------------Epoch:  18 ----------------------------------------\n",
      "‖w_svm‖₂       : 0.033560235032777386\n",
      "‖alpha‖₁       : 0.8985760249995066\n",
      "scores min/max : -0.7613755364562894 1.99886555832271\n",
      "Mask mean value:  tensor(0.8241, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3993  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.983e-15\n",
      "‖w_svm‖₂       : 7.654896509923802e-08\n",
      "‖alpha‖₁       : 0.5399999999999998\n",
      "scores min/max : -8.589080012061863e-08 -6.436846037240165e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.470e-20\n",
      "‖w_svm‖₂       : 2.796674066389793e-07\n",
      "‖alpha‖₁       : 0.6599999999999868\n",
      "scores min/max : -6.057524104687689e-07 -4.539454768650565e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.291e-18\n",
      "‖w_svm‖₂       : 0.04780321145166768\n",
      "‖alpha‖₁       : 0.9390075148914716\n",
      "scores min/max : -2.4397801689542287 1.6023350237176666\n",
      "Mask mean value:  tensor(0.1579, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4184  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.634e-03\n",
      "‖w_svm‖₂       : 0.005185378897590304\n",
      "‖alpha‖₁       : 0.3799999999999983\n",
      "scores min/max : -0.0069876352864556845 0.00417622939523752\n",
      "Mask mean value:  tensor(0.5065, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9017  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.752e-14\n",
      "‖w_svm‖₂       : 0.0582695073960497\n",
      "‖alpha‖₁       : 0.5761782281919876\n",
      "scores min/max : -1.9601451970807438 0.8648390493457013\n",
      "Mask mean value:  tensor(0.6948, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.8918  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.830e-04\n",
      "‖w_svm‖₂       : 3.8206603655265633e-07\n",
      "‖alpha‖₁       : 0.27999999999999625\n",
      "scores min/max : -1.3872913800645891e-06 -1.3255617237488057e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.324e-19\n",
      "‖w_svm‖₂       : 0.019967511800101104\n",
      "‖alpha‖₁       : 0.859999999999999\n",
      "scores min/max : -0.051700728657832026 0.034677777705449275\n",
      "Mask mean value:  tensor(0.4170, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5441  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.163e-14\n",
      "‖w_svm‖₂       : 1.9998556535029642e-07\n",
      "‖alpha‖₁       : 0.4199999999999788\n",
      "scores min/max : -2.9198788949551785e-07 -2.696917834106394e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.535e-19\n",
      "‖w_svm‖₂       : 4.8173356411309245e-08\n",
      "‖alpha‖₁       : 0.1799999999999974\n",
      "scores min/max : 3.295097223146612e-08 4.7640538559122294e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 4.731e-08\n",
      "‖w_svm‖₂       : 1.1371977785813059e-07\n",
      "‖alpha‖₁       : 0.2999999999999987\n",
      "scores min/max : 8.23791626379062e-08 9.130683692472583e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.186e-07\n",
      "‖w_svm‖₂       : 0.02019815019270905\n",
      "‖alpha‖₁       : 0.38334172425422997\n",
      "scores min/max : -1.9690600612471454 0.23132103070767931\n",
      "Mask mean value:  tensor(0.6824, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1451  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.505e-14\n",
      "‖w_svm‖₂       : 0.0004977613832480133\n",
      "‖alpha‖₁       : 0.43999999999999984\n",
      "scores min/max : -0.0004897954697876567 -0.00023926252202389185\n",
      "Mask mean value:  tensor(0.4978, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6479  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.074e-16\n",
      "‖w_svm‖₂       : 5.4114359779210234e-08\n",
      "‖alpha‖₁       : 0.11999999999999693\n",
      "scores min/max : -7.799350937035193e-08 -4.729370558377192e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.170e-07\n",
      "‖w_svm‖₂       : 0.026080955026058023\n",
      "‖alpha‖₁       : 0.8522456117970556\n",
      "scores min/max : -2.9144817183684717 1.5661939417467654\n",
      "Mask mean value:  tensor(0.1561, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9518  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.354e-03\n",
      "‖w_svm‖₂       : 0.04768095689386401\n",
      "‖alpha‖₁       : 0.9056464181837558\n",
      "scores min/max : -0.739187593719267 1.8817496980363195\n",
      "Mask mean value:  tensor(0.3486, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6460  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.592e-03\n",
      "‖w_svm‖₂       : 1.1660599969387356e-06\n",
      "‖alpha‖₁       : 0.31999999999869294\n",
      "scores min/max : -1.8453730895312085e-06 -1.7298431345070222e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4898  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.706e-18\n",
      "‖w_svm‖₂       : 0.052470078926307354\n",
      "‖alpha‖₁       : 0.6599999999999997\n",
      "scores min/max : -0.3478103017735531 0.2688688724988084\n",
      "Mask mean value:  tensor(0.5252, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9286  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.082e-14\n",
      "‖w_svm‖₂       : 0.004179565650858699\n",
      "‖alpha‖₁       : 0.459999999999981\n",
      "scores min/max : -0.005539159946286214 -0.0036955086143254795\n",
      "Mask mean value:  tensor(0.4794, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.0837  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.898e-16\n",
      "‖w_svm‖₂       : 0.061652716905825786\n",
      "‖alpha‖₁       : 0.8986092049853173\n",
      "scores min/max : -1.7251633078646584 3.679353573245833\n",
      "Mask mean value:  tensor(0.3748, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5344  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.307e-02\n",
      "‖w_svm‖₂       : 0.08213449112621626\n",
      "‖alpha‖₁       : 0.47189512285712654\n",
      "scores min/max : -2.3215811207773056 2.7595273336227195\n",
      "Mask mean value:  tensor(0.1019, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1149  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.177e-02\n",
      "‖w_svm‖₂       : 1.0706010122030118e-07\n",
      "‖alpha‖₁       : 0.23999999999998012\n",
      "scores min/max : 1.2874103188109808e-09 1.2257065146904596e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.314e-21\n",
      "‖w_svm‖₂       : 0.0008806085960184872\n",
      "‖alpha‖₁       : 0.8199999999999992\n",
      "scores min/max : 0.0007613008999284906 0.0024914801889857692\n",
      "Mask mean value:  tensor(0.5093, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5870  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.013e-16\n",
      "‖w_svm‖₂       : 2.2246580349491974e-08\n",
      "‖alpha‖₁       : 0.11999999999999461\n",
      "scores min/max : -1.9736392067206186e-08 -5.0659190135110866e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.787e-09\n",
      "‖w_svm‖₂       : 0.07102323897001146\n",
      "‖alpha‖₁       : 0.5799999999999997\n",
      "scores min/max : -2.795092434038224 1.5666309244589425\n",
      "Mask mean value:  tensor(0.1461, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2797  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.220e-04\n",
      "‖w_svm‖₂       : 3.562043694252959e-06\n",
      "‖alpha‖₁       : 0.4199999999931048\n",
      "scores min/max : -1.2173744891275632e-06 3.991679470498837e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1230  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.499e-05\n",
      "‖w_svm‖₂       : 0.18760526291520407\n",
      "‖alpha‖₁       : 0.8834664828718268\n",
      "scores min/max : -3.5574478233012927 6.085596416310094\n",
      "Mask mean value:  tensor(0.0967, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1896  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.808e-03\n",
      "‖w_svm‖₂       : 0.00021209174650538477\n",
      "‖alpha‖₁       : 0.6199999999989183\n",
      "scores min/max : 4.6588328665294994e-05 6.252757505558698e-05\n",
      "Mask mean value:  tensor(0.5003, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.022e-17\n",
      "‖w_svm‖₂       : 0.003119504661700893\n",
      "‖alpha‖₁       : 0.5799999999999998\n",
      "scores min/max : 0.0017798056785341926 0.003013406784304104\n",
      "Mask mean value:  tensor(0.5138, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3267  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.931e-13\n",
      "‖w_svm‖₂       : 7.219894801332051e-08\n",
      "‖alpha‖₁       : 0.4199999999999298\n",
      "scores min/max : -7.523429703059416e-08 -2.753943651681841e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.604e-08\n",
      "‖w_svm‖₂       : 0.05115286828878537\n",
      "‖alpha‖₁       : 0.7293518937061569\n",
      "scores min/max : -0.24782043821384228 2.0351696916647715\n",
      "Mask mean value:  tensor(0.7212, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2303  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.116e-03\n",
      "‖w_svm‖₂       : 2.3750710606257223e-07\n",
      "‖alpha‖₁       : 0.5799999999999964\n",
      "scores min/max : -4.418242656840925e-07 -4.2618289771765697e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.916e-19\n",
      "‖w_svm‖₂       : 0.07793374416125813\n",
      "‖alpha‖₁       : 0.41909579718586804\n",
      "scores min/max : -1.828020145789881 2.4907240322316437\n",
      "Mask mean value:  tensor(0.8227, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8846  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.001e-12\n",
      "‖w_svm‖₂       : 0.017985133224748706\n",
      "‖alpha‖₁       : 0.6815093232707258\n",
      "scores min/max : -1.9752506771047214 0.05635800011378978\n",
      "Mask mean value:  tensor(0.5979, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2437  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.125e-12\n",
      "‖w_svm‖₂       : 0.00030578007656641414\n",
      "‖alpha‖₁       : 0.41999999999998694\n",
      "scores min/max : 0.00023401956718029532 0.0005508338033809131\n",
      "Mask mean value:  tensor(0.5026, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0007  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.738e-15\n",
      "‖w_svm‖₂       : 0.028588861529819105\n",
      "‖alpha‖₁       : 0.5510249125059009\n",
      "scores min/max : -3.5556684180888745 1.0173867508238224\n",
      "Mask mean value:  tensor(0.1137, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9588  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.377e-03\n",
      "‖w_svm‖₂       : 0.13852884882429653\n",
      "‖alpha‖₁       : 0.6547823177766\n",
      "scores min/max : -18.06684884245196 1.774111683435204\n",
      "Mask mean value:  tensor(0.2193, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2732  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.894e-03\n",
      "‖w_svm‖₂       : 0.13971851827220413\n",
      "‖alpha‖₁       : 0.8799999999999797\n",
      "scores min/max : -1.3831253322320096 3.699402655327565\n",
      "Mask mean value:  tensor(0.3242, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2623  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.984e-09\n",
      "‖w_svm‖₂       : 0.00013759626439214384\n",
      "‖alpha‖₁       : 0.43999999997325256\n",
      "scores min/max : -0.00020843206831401822 -6.983650808755433e-05\n",
      "Mask mean value:  tensor(0.4994, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0241  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.607e-17\n",
      "‖w_svm‖₂       : 7.204697617553028e-08\n",
      "‖alpha‖₁       : 0.13999999999999832\n",
      "scores min/max : 6.022271231454509e-09 1.1750783902586461e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.844e-08\n",
      "‖w_svm‖₂       : 6.808308076620506e-07\n",
      "‖alpha‖₁       : 0.3999999999999996\n",
      "scores min/max : -4.6707003573877534e-08 2.593735312618554e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.285e-09\n",
      "‖w_svm‖₂       : 0.028808230607057117\n",
      "‖alpha‖₁       : 0.1966684396116945\n",
      "scores min/max : -2.2046763233488713 0.020897898707784537\n",
      "Mask mean value:  tensor(0.0589, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3983  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.787e-04\n",
      "‖w_svm‖₂       : 2.0727720321107606e-07\n",
      "‖alpha‖₁       : 0.37999999999530876\n",
      "scores min/max : 2.8025556876769514e-08 4.6345820134657135e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.642e-07\n",
      "‖w_svm‖₂       : 0.0165160841818779\n",
      "‖alpha‖₁       : 0.8599999999999912\n",
      "scores min/max : -0.035956509293413555 -0.019723330736571273\n",
      "Mask mean value:  tensor(0.3817, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.209e-15\n",
      "‖w_svm‖₂       : 0.00036273174057387933\n",
      "‖alpha‖₁       : 0.7399999999999999\n",
      "scores min/max : -0.00018161728274205678 -0.00016483503549309612\n",
      "Mask mean value:  tensor(0.4992, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0625  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.089e-17\n",
      "‖w_svm‖₂       : 2.2795408634516026e-07\n",
      "‖alpha‖₁       : 0.2599999999999824\n",
      "scores min/max : -5.314284647595259e-08 -4.0282746591106226e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.548e-17\n",
      "‖w_svm‖₂       : 7.673355542753028e-08\n",
      "‖alpha‖₁       : 0.6599999999999993\n",
      "scores min/max : 1.0011857251679941e-07 2.452006491754606e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.648e-09\n",
      "‖w_svm‖₂       : 0.023968708960306194\n",
      "‖alpha‖₁       : 0.8150894760106595\n",
      "scores min/max : -11.522708986345318 2.0384702386427285\n",
      "Mask mean value:  tensor(0.7227, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.411e-06\n",
      "‖w_svm‖₂       : 0.04965340365667573\n",
      "‖alpha‖₁       : 0.9199999999999995\n",
      "scores min/max : -0.3697250335686725 0.5457565888134936\n",
      "Mask mean value:  tensor(0.3657, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.8145  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.697e-02\n",
      "‖w_svm‖₂       : 0.006752696697309722\n",
      "‖alpha‖₁       : 0.6076687904926343\n",
      "scores min/max : -1.9847357233604235 0.3022706361708319\n",
      "Mask mean value:  tensor(0.5809, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2347  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.691e-15\n",
      "‖w_svm‖₂       : 4.2553881115871334e-07\n",
      "‖alpha‖₁       : 0.7200000000000001\n",
      "scores min/max : 3.5467125716556635e-07 4.118733904302707e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.889e-21\n",
      "‖w_svm‖₂       : 0.0001868417412383503\n",
      "‖alpha‖₁       : 0.8199999999999991\n",
      "scores min/max : -5.538331497770437e-05 -4.089677138791319e-05\n",
      "Mask mean value:  tensor(0.4998, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6971  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.198e-17\n",
      "‖w_svm‖₂       : 0.14614791750945613\n",
      "‖alpha‖₁       : 0.759304524405409\n",
      "scores min/max : -1.7346777212885465 2.2428455128530445\n",
      "Mask mean value:  tensor(0.8843, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3594  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.106e-11\n",
      "‖w_svm‖₂       : 0.02565193661661942\n",
      "‖alpha‖₁       : 0.6599999999999651\n",
      "scores min/max : -0.05586914542672372 0.063100088760698\n",
      "Mask mean value:  tensor(0.4271, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.4057  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.100e-15\n",
      "‖w_svm‖₂       : 0.04093413414685931\n",
      "‖alpha‖₁       : 0.9402659506406539\n",
      "scores min/max : -1.8317993511153734 0.3029099433078434\n",
      "Mask mean value:  tensor(0.4122, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3341  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.680e-12\n",
      "‖w_svm‖₂       : 2.343527241042345e-07\n",
      "‖alpha‖₁       : 0.6399999999999963\n",
      "scores min/max : 3.2291864316860115e-07 3.4381216764127984e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.369e-20\n",
      "‖w_svm‖₂       : 0.04383904088597111\n",
      "‖alpha‖₁       : 0.889983389548072\n",
      "scores min/max : -2.0105929270009066 0.33665607507059936\n",
      "Mask mean value:  tensor(0.2609, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0709  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.361e-07\n",
      "‖w_svm‖₂       : 8.574946425038034e-08\n",
      "‖alpha‖₁       : 0.1799999999999979\n",
      "scores min/max : -6.990561592899106e-08 1.6424396268256622e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.453e-21\n",
      "‖w_svm‖₂       : 0.11994269983827341\n",
      "‖alpha‖₁       : 0.873270006680737\n",
      "scores min/max : -12.18869315490777 2.171758398605178\n",
      "Mask mean value:  tensor(0.5795, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7084  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.984e-01\n",
      "‖w_svm‖₂       : 8.292281559719626e-08\n",
      "‖alpha‖₁       : 0.3799999999999998\n",
      "scores min/max : -1.222372328586632e-08 5.923585386932403e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.612e-22\n",
      "‖w_svm‖₂       : 0.0001605830243204013\n",
      "‖alpha‖₁       : 0.6399999999999999\n",
      "scores min/max : -6.812609789338045e-05 -5.828661585407748e-05\n",
      "Mask mean value:  tensor(0.4997, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7639  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.124e-17\n",
      "‖w_svm‖₂       : 1.9404493095617304e-07\n",
      "‖alpha‖₁       : 0.3799999999999995\n",
      "scores min/max : -8.138167616888526e-08 -4.5703147685526e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.610e-19\n",
      "‖w_svm‖₂       : 0.017833980698858002\n",
      "‖alpha‖₁       : 0.5962535208764854\n",
      "scores min/max : -2.932440511758399 2.2610118950019817\n",
      "Mask mean value:  tensor(0.9100, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.4981  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.911e-04\n",
      "‖w_svm‖₂       : 0.019832589928822707\n",
      "‖alpha‖₁       : 0.7799999999999996\n",
      "scores min/max : -0.015120404499133247 0.05379849756136046\n",
      "Mask mean value:  tensor(0.4538, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9808  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.655e-15\n",
      "‖w_svm‖₂       : 1.018022616577477e-06\n",
      "‖alpha‖₁       : 0.5999999999999941\n",
      "scores min/max : -4.8404304260284204e-08 2.4692824130453706e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.458e-19\n",
      "‖w_svm‖₂       : 0.005513403719102495\n",
      "‖alpha‖₁       : 0.7005634081971898\n",
      "scores min/max : -2.0045165091617214 0.018867799382431695\n",
      "Mask mean value:  tensor(0.4523, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2538  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.863e-05\n",
      "‖w_svm‖₂       : 5.475304534684224e-08\n",
      "‖alpha‖₁       : 0.4399999999999965\n",
      "scores min/max : -1.462120122710773e-07 -4.2149885598586196e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.252e-20\n",
      "‖w_svm‖₂       : 0.022506674501905997\n",
      "‖alpha‖₁       : 0.8233572154492095\n",
      "scores min/max : -1.8888375968667814 1.511050237730717\n",
      "Mask mean value:  tensor(0.7280, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.4560  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.486e-06\n",
      "‖w_svm‖₂       : 5.726800768986079e-08\n",
      "‖alpha‖₁       : 0.2399999999999918\n",
      "scores min/max : -7.151231765853382e-08 -5.706348294502243e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.306e-20\n",
      "‖w_svm‖₂       : 0.078357316145856\n",
      "‖alpha‖₁       : 0.6583008496331988\n",
      "scores min/max : -2.0474599458264775 4.002336255520658\n",
      "Mask mean value:  tensor(0.3284, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.7820  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.216e-04\n",
      "‖w_svm‖₂       : 0.005581720295556376\n",
      "‖alpha‖₁       : 0.5599999999999938\n",
      "scores min/max : 0.003828486782834194 0.00544557947528238\n",
      "Mask mean value:  tensor(0.5206, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0912  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 7.207e-05\n",
      "‖w_svm‖₂       : 1.088659680206467e-06\n",
      "‖alpha‖₁       : 0.4999999999999908\n",
      "scores min/max : -2.768865661453149e-07 -8.38504614844865e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.953e-19\n",
      "‖w_svm‖₂       : 0.053534904680279036\n",
      "‖alpha‖₁       : 0.8271152575708338\n",
      "scores min/max : -1.9526685869633567 0.39602524851761023\n",
      "Mask mean value:  tensor(0.5103, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0032  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.717e-02\n",
      "‖w_svm‖₂       : 1.709925355140594e-07\n",
      "‖alpha‖₁       : 0.23999999999999017\n",
      "scores min/max : 2.890490654849522e-07 3.044207857614573e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.096e-20\n",
      "‖w_svm‖₂       : 1.129109346028429e-07\n",
      "‖alpha‖₁       : 0.5199999999999996\n",
      "scores min/max : 3.2306244235143916e-07 3.444638930538346e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.517e-19\n",
      "‖w_svm‖₂       : 1.4718417491477792e-05\n",
      "‖alpha‖₁       : 0.35999999999399745\n",
      "scores min/max : 1.2722119948650565e-05 1.3588782324539452e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.176e-17\n",
      "‖w_svm‖₂       : 0.08611587723495473\n",
      "‖alpha‖₁       : 0.5779715712789165\n",
      "scores min/max : -2.1075452499274774 5.825452597350815\n",
      "Mask mean value:  tensor(0.3002, dtype=torch.float64)\n",
      "max feasible return = 0.1165  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.005503301632963e-07\n",
      "‖alpha‖₁       : 0.5800000000000001\n",
      "scores min/max : -3.993144416034735e-07 -3.0859851135251637e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.039654618583622e-07\n",
      "‖alpha‖₁       : 0.29999999999999977\n",
      "scores min/max : 3.9493315502342e-08 4.4463129235855627e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 7.046733732451754e-08\n",
      "‖alpha‖₁       : 0.5999999999999976\n",
      "scores min/max : 2.0661292597170142e-08 5.744103517837986e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.7332399120156773e-08\n",
      "‖alpha‖₁       : 0.37999999999998174\n",
      "scores min/max : 1.1509472177012064e-08 2.0945899686218275e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.000602253418312e-06\n",
      "‖alpha‖₁       : 0.3199999999768388\n",
      "scores min/max : 2.7539679260206288e-06 4.4857196351558645e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4258  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.059660288810005145\n",
      "‖alpha‖₁       : 0.7397024654162838\n",
      "scores min/max : -3.4839521702327465 2.1420880663285278\n",
      "Mask mean value:  tensor(0.8705, dtype=torch.float64)\n",
      "max feasible return = -0.9178  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.035103456736267975\n",
      "‖alpha‖₁       : 0.6151657650022611\n",
      "scores min/max : -1.900907791112462 1.2329276414910024\n",
      "Mask mean value:  tensor(0.8154, dtype=torch.float64)\n",
      "max feasible return = 2.8659  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.730237911926023e-07\n",
      "‖alpha‖₁       : 0.4599999999999808\n",
      "scores min/max : 1.901747069392869e-07 7.85024446449664e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.3827139814087947e-07\n",
      "‖alpha‖₁       : 0.5199999999999942\n",
      "scores min/max : 6.556617700109445e-08 7.847243315364046e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.596314412329138e-07\n",
      "‖alpha‖₁       : 0.5799999999999783\n",
      "scores min/max : -2.640913211054068e-07 -2.277710477697558e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.04526207002475723\n",
      "‖alpha‖₁       : 0.6876715379154502\n",
      "scores min/max : -0.4254995472250874 1.964140753481998\n",
      "Mask mean value:  tensor(0.4306, dtype=torch.float64)\n",
      "max feasible return = 0.0202  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.0340074209478151\n",
      "‖alpha‖₁       : 0.4221328501672772\n",
      "scores min/max : -3.922604314463212 5.195963533991208\n",
      "Mask mean value:  tensor(0.0728, dtype=torch.float64)\n",
      "max feasible return = 0.4592  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04691759272364848\n",
      "‖alpha‖₁       : 0.8211470698337133\n",
      "scores min/max : -5.119905803176116 3.042026729734913\n",
      "Mask mean value:  tensor(0.9349, dtype=torch.float64)\n",
      "max feasible return = -3.5173  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.009022393763229318\n",
      "‖alpha‖₁       : 0.7999999999999996\n",
      "scores min/max : -0.04458417292036009 0.008849388291163173\n",
      "Mask mean value:  tensor(0.4769, dtype=torch.float64)\n",
      "max feasible return = -0.1734  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.145563027741285e-07\n",
      "‖alpha‖₁       : 0.6199999999999608\n",
      "scores min/max : 3.085149016336258e-08 4.7384908027152496e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.775660311162736e-08\n",
      "‖alpha‖₁       : 0.43999999999997397\n",
      "scores min/max : 1.0113567850927656e-08 1.969401947265715e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.338438653835811e-07\n",
      "‖alpha‖₁       : 0.27999999999997227\n",
      "scores min/max : -3.5710275200065255e-07 -3.3911557981016355e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  18 | train 0.005451 | val 0.006767\n",
      "-----------------------------------------Epoch:  19 ----------------------------------------\n",
      "‖w_svm‖₂       : 0.05155681997719378\n",
      "‖alpha‖₁       : 0.6599999999999998\n",
      "scores min/max : -0.34204687172225606 0.2540079191016936\n",
      "Mask mean value:  tensor(0.4978, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8787  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.117e-13\n",
      "‖w_svm‖₂       : 4.3090646275691125e-07\n",
      "‖alpha‖₁       : 0.7199999999999985\n",
      "scores min/max : 2.883462508109915e-07 3.4559684100015104e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.936e-21\n",
      "‖w_svm‖₂       : 1.4698718457640143e-05\n",
      "‖alpha‖₁       : 0.3599999999940649\n",
      "scores min/max : 1.2712314314402714e-05 1.3576827699816228e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.172e-17\n",
      "‖w_svm‖₂       : 0.04373566208649289\n",
      "‖alpha‖₁       : 0.8899813924326162\n",
      "scores min/max : -2.0064224208256443 0.34095197849056036\n",
      "Mask mean value:  tensor(0.2733, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0770  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.834e-11\n",
      "‖w_svm‖₂       : 2.3832059758885532e-07\n",
      "‖alpha‖₁       : 0.6399999999999975\n",
      "scores min/max : 3.111937588478566e-07 3.320812256932458e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.555e-20\n",
      "‖w_svm‖₂       : 1.1315077280210837e-07\n",
      "‖alpha‖₁       : 0.5199999999999997\n",
      "scores min/max : 3.1954170014957235e-07 3.4094282111574266e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.527e-19\n",
      "‖w_svm‖₂       : 2.3996873121292714e-07\n",
      "‖alpha‖₁       : 0.58\n",
      "scores min/max : -3.8248122338457473e-07 -3.668606652263895e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.914e-19\n",
      "‖w_svm‖₂       : 0.005590713600637212\n",
      "‖alpha‖₁       : 0.5599999999999862\n",
      "scores min/max : 0.0038693595517767833 0.005491376953912725\n",
      "Mask mean value:  tensor(0.5208, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0917  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.979e-05\n",
      "‖w_svm‖₂       : 0.0055287940822955324\n",
      "‖alpha‖₁       : 0.7005635127320391\n",
      "scores min/max : -2.0047644691698467 0.018621531516785406\n",
      "Mask mean value:  tensor(0.4511, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2531  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.179e-05\n",
      "‖w_svm‖₂       : 0.14634907802983657\n",
      "‖alpha‖₁       : 0.7593607879470365\n",
      "scores min/max : -1.7371986291459296 2.2404157524251938\n",
      "Mask mean value:  tensor(0.8814, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3586  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.233e-12\n",
      "‖w_svm‖₂       : 0.0002093632384453267\n",
      "‖alpha‖₁       : 0.6199999999673141\n",
      "scores min/max : 4.666784600867394e-05 6.221152513602294e-05\n",
      "Mask mean value:  tensor(0.5003, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.994e-17\n",
      "‖w_svm‖₂       : 0.0001583982249654665\n",
      "‖alpha‖₁       : 0.639999999999904\n",
      "scores min/max : -0.00010360512392827828 -9.403122448824029e-05\n",
      "Mask mean value:  tensor(0.4995, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7637  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.144e-17\n",
      "‖w_svm‖₂       : 0.005286424406827169\n",
      "‖alpha‖₁       : 0.3799999999999723\n",
      "scores min/max : -0.006707120004519215 0.00515852290721721\n",
      "Mask mean value:  tensor(0.5105, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9083  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.731e-14\n",
      "‖w_svm‖₂       : 0.020000606559180845\n",
      "‖alpha‖₁       : 0.7799999999999937\n",
      "scores min/max : -0.013771142823589539 0.05612417049939383\n",
      "Mask mean value:  tensor(0.4608, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.0111  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.538e-15\n",
      "‖w_svm‖₂       : 0.025025520351603284\n",
      "‖alpha‖₁       : 0.6599999999999331\n",
      "scores min/max : -0.051907878036251465 0.061177568466441234\n",
      "Mask mean value:  tensor(0.4365, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.4731  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.960e-15\n",
      "‖w_svm‖₂       : 1.0285145998337056e-06\n",
      "‖alpha‖₁       : 0.5999999999999941\n",
      "scores min/max : -9.921086430921845e-08 -4.8339881377477136e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.496e-19\n",
      "‖w_svm‖₂       : 0.0005027897331644214\n",
      "‖alpha‖₁       : 0.43999999999999995\n",
      "scores min/max : -0.0006451111049136468 -0.0003892318786951844\n",
      "Mask mean value:  tensor(0.4970, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6453  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.048e-16\n",
      "‖w_svm‖₂       : 1.7185344848887254e-07\n",
      "‖alpha‖₁       : 0.2399999999999894\n",
      "scores min/max : 2.8415402946673354e-07 2.9953502929549037e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.138e-20\n",
      "‖w_svm‖₂       : 2.0276829563421448e-07\n",
      "‖alpha‖₁       : 0.41999999999997945\n",
      "scores min/max : -3.7040561443171323e-07 -3.4811449998056115e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.610e-19\n",
      "‖w_svm‖₂       : 6.765441017854626e-07\n",
      "‖alpha‖₁       : 0.4\n",
      "scores min/max : -8.279392327501673e-08 2.2316377881832143e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.252e-09\n",
      "‖w_svm‖₂       : 5.4826362000808864e-08\n",
      "‖alpha‖₁       : 0.11999999999999858\n",
      "scores min/max : -8.278584320237147e-08 -5.212415418004437e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.001e-08\n",
      "‖w_svm‖₂       : 0.057733475529244925\n",
      "‖alpha‖₁       : 0.5761854986207761\n",
      "scores min/max : -1.9593299226702956 0.8667291252555162\n",
      "Mask mean value:  tensor(0.7000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9059  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.935e-04\n",
      "‖w_svm‖₂       : 0.003142202415947427\n",
      "‖alpha‖₁       : 0.5799999999999996\n",
      "scores min/max : 0.0018033187368396218 0.0030561568277010648\n",
      "Mask mean value:  tensor(0.5140, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3268  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.937e-13\n",
      "‖w_svm‖₂       : 0.07710366948422516\n",
      "‖alpha‖₁       : 0.4190620728871336\n",
      "scores min/max : -1.8576396171908325 2.4395794294557245\n",
      "Mask mean value:  tensor(0.8029, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8900  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.310e-04\n",
      "‖w_svm‖₂       : 8.708897458120009e-08\n",
      "‖alpha‖₁       : 0.17999999999999708\n",
      "scores min/max : -8.181061275456123e-08 1.5272527108546596e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.630e-21\n",
      "‖w_svm‖₂       : 0.053682291862339734\n",
      "‖alpha‖₁       : 0.8271414996250002\n",
      "scores min/max : -1.94536993042947 0.4034056671553379\n",
      "Mask mean value:  tensor(0.5367, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0084  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.550e-02\n",
      "‖w_svm‖₂       : 1.197387014132555e-06\n",
      "‖alpha‖₁       : 0.3199999999984954\n",
      "scores min/max : -2.0399227864951695e-06 -1.923300624349028e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4898  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.781e-18\n",
      "‖w_svm‖₂       : 0.02605739724391037\n",
      "‖alpha‖₁       : 0.852240349495507\n",
      "scores min/max : -2.9376369900976593 1.543289076392145\n",
      "Mask mean value:  tensor(0.1463, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9387  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.188e-03\n",
      "‖w_svm‖₂       : 2.0885021185221482e-07\n",
      "‖alpha‖₁       : 0.379999999995118\n",
      "scores min/max : -3.9921806229396907e-08 -2.1685405687155093e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.921e-07\n",
      "‖w_svm‖₂       : 0.004243859546658844\n",
      "‖alpha‖₁       : 0.4600000000000004\n",
      "scores min/max : -0.006275175338186478 -0.004370921602518313\n",
      "Mask mean value:  tensor(0.4759, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.0686  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.865e-16\n",
      "‖w_svm‖₂       : 2.3171518996203096e-07\n",
      "‖alpha‖₁       : 0.2599999999999806\n",
      "scores min/max : 4.421033044147037e-08 5.707794737264834e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.809e-17\n",
      "‖w_svm‖₂       : 0.020646316256534392\n",
      "‖alpha‖₁       : 0.3833608318668861\n",
      "scores min/max : -1.9730234972367353 0.2273390561661613\n",
      "Mask mean value:  tensor(0.6668, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1429  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.467e-14\n",
      "‖w_svm‖₂       : 0.0822194936664039\n",
      "‖alpha‖₁       : 0.47191465360805773\n",
      "scores min/max : -2.325649615020951 2.757576821714831\n",
      "Mask mean value:  tensor(0.1006, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1015  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.146e-02\n",
      "‖w_svm‖₂       : 0.18744907777801317\n",
      "‖alpha‖₁       : 0.8835747007467527\n",
      "scores min/max : -3.568256648696217 6.0829552877233475\n",
      "Mask mean value:  tensor(0.0958, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1911  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.452e-03\n",
      "‖w_svm‖₂       : 0.01745993907571342\n",
      "‖alpha‖₁       : 0.6815093913424481\n",
      "scores min/max : -1.9788299972593064 0.05128654998768195\n",
      "Mask mean value:  tensor(0.5814, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2372  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.912e-13\n",
      "‖w_svm‖₂       : 1.1826529484667847e-07\n",
      "‖alpha‖₁       : 0.29999999999996657\n",
      "scores min/max : 1.112173151183947e-07 1.2035810085153801e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.261e-07\n",
      "‖w_svm‖₂       : 5.733215329933031e-08\n",
      "‖alpha‖₁       : 0.23999999999999205\n",
      "scores min/max : -7.132750932907869e-08 -5.688168872101939e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.311e-20\n",
      "‖w_svm‖₂       : 1.9528821363106575e-07\n",
      "‖alpha‖₁       : 0.3799999999999996\n",
      "scores min/max : -4.975329390699276e-08 -1.4076455854757156e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.624e-19\n",
      "‖w_svm‖₂       : 0.07108016878539355\n",
      "‖alpha‖₁       : 0.5799999999999998\n",
      "scores min/max : -2.7921433393559485 1.5744872370250338\n",
      "Mask mean value:  tensor(0.1557, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2937  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.309e-04\n",
      "‖w_svm‖₂       : 0.13971320955980202\n",
      "‖alpha‖₁       : 0.8799999999999999\n",
      "scores min/max : -1.3782001673086508 3.702233537423689\n",
      "Mask mean value:  tensor(0.3337, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2632  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.278e-10\n",
      "‖w_svm‖₂       : 0.016685118052854806\n",
      "‖alpha‖₁       : 0.8599999999999883\n",
      "scores min/max : -0.04604882427079497 -0.02990487500080912\n",
      "Mask mean value:  tensor(0.3352, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0808  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.174e-14\n",
      "‖w_svm‖₂       : 7.793275818106372e-08\n",
      "‖alpha‖₁       : 0.6599999999999988\n",
      "scores min/max : 9.819813243467973e-08 2.4327891962382e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.716e-09\n",
      "‖w_svm‖₂       : 0.0001837600104653439\n",
      "‖alpha‖₁       : 0.8199999999999997\n",
      "scores min/max : -4.925549936426324e-05 -3.5240599599734414e-05\n",
      "Mask mean value:  tensor(0.4998, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.183e-17\n",
      "‖w_svm‖₂       : 0.07827544223324039\n",
      "‖alpha‖₁       : 0.6583384453038853\n",
      "scores min/max : -2.04082354272953 4.019522046916575\n",
      "Mask mean value:  tensor(0.3401, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.9038  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.634e-04\n",
      "‖w_svm‖₂       : 0.04035105056024508\n",
      "‖alpha‖₁       : 0.9402719136961153\n",
      "scores min/max : -1.8209575981453083 0.31343722204868557\n",
      "Mask mean value:  tensor(0.4546, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3322  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.405e-15\n",
      "‖w_svm‖₂       : 3.666774126424902e-06\n",
      "‖alpha‖₁       : 0.41999999999279175\n",
      "scores min/max : -9.243476063068486e-07 7.211709326269829e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1230  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.560e-05\n",
      "‖w_svm‖₂       : 0.00013534654155712532\n",
      "‖alpha‖₁       : 0.4399999999710668\n",
      "scores min/max : -0.000194188541516102 -5.983127359786506e-05\n",
      "Mask mean value:  tensor(0.4995, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0245  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.570e-17\n",
      "‖w_svm‖₂       : 0.05106830514197897\n",
      "‖alpha‖₁       : 0.7294201476326805\n",
      "scores min/max : -0.24609702236314474 2.0368046068840515\n",
      "Mask mean value:  tensor(0.7254, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2517  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.110e-03\n",
      "‖w_svm‖₂       : 0.028393569103338942\n",
      "‖alpha‖₁       : 0.5510270052861328\n",
      "scores min/max : -3.5515924386625897 1.0209079065073376\n",
      "Mask mean value:  tensor(0.1149, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9679  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.569e-03\n",
      "‖w_svm‖₂       : 7.266304112898632e-08\n",
      "‖alpha‖₁       : 0.4199999999999997\n",
      "scores min/max : -7.958156331375619e-08 -3.247827989060201e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.696e-08\n",
      "‖w_svm‖₂       : 0.1382231034685271\n",
      "‖alpha‖₁       : 0.6547662402819255\n",
      "scores min/max : -18.06732277538226 1.7853060226394963\n",
      "Mask mean value:  tensor(0.2276, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2765  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.563e-03\n",
      "‖w_svm‖₂       : 0.0008736820818958591\n",
      "‖alpha‖₁       : 0.8199999999999992\n",
      "scores min/max : 0.0007838897181575051 0.002504259797882294\n",
      "Mask mean value:  tensor(0.5093, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5874  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.976e-17\n",
      "‖w_svm‖₂       : 0.11856432597931639\n",
      "‖alpha‖₁       : 0.8732603231137666\n",
      "scores min/max : -12.251873044515042 2.105526872873272\n",
      "Mask mean value:  tensor(0.4254, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2694  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 9.443e-02\n",
      "‖w_svm‖₂       : 0.02240547537068034\n",
      "‖alpha‖₁       : 0.823361342611957\n",
      "scores min/max : -1.8869942594881333 1.5127547819950506\n",
      "Mask mean value:  tensor(0.7326, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.4654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.751e-06\n",
      "‖w_svm‖₂       : 0.04777844797227223\n",
      "‖alpha‖₁       : 0.9056469358242062\n",
      "scores min/max : -0.7380950075223083 1.8834895535365899\n",
      "Mask mean value:  tensor(0.3546, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6566  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.229e-03\n",
      "‖w_svm‖₂       : 7.792869653426297e-08\n",
      "‖alpha‖₁       : 0.5399999999999997\n",
      "scores min/max : -1.1434149517954933e-07 -9.281933635979485e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.606e-20\n",
      "‖w_svm‖₂       : 1.0959923664886565e-06\n",
      "‖alpha‖₁       : 0.499999999999992\n",
      "scores min/max : -2.9248201661804303e-07 -9.950607731615806e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.013e-19\n",
      "‖w_svm‖₂       : 0.01940358078042548\n",
      "‖alpha‖₁       : 0.86\n",
      "scores min/max : -0.049448769347623524 0.03256657231580863\n",
      "Mask mean value:  tensor(0.4196, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.873e-14\n",
      "‖w_svm‖₂       : 3.880993433424537e-07\n",
      "‖alpha‖₁       : 0.2799999999999975\n",
      "scores min/max : -1.5242913827077494e-06 -1.4625839060842432e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1957  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.452e-19\n",
      "‖w_svm‖₂       : 2.212103176328336e-08\n",
      "‖alpha‖₁       : 0.11999999999999468\n",
      "scores min/max : -2.271557357287966e-08 -8.046713728729917e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.792e-09\n",
      "‖w_svm‖₂       : 1.0866115804392408e-07\n",
      "‖alpha‖₁       : 0.2399999999999834\n",
      "scores min/max : 2.520267344110442e-08 3.6151293872381984e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.447e-21\n",
      "‖w_svm‖₂       : 0.00030147833356218367\n",
      "‖alpha‖₁       : 0.4199999999986139\n",
      "scores min/max : 0.00019706233870338507 0.0005050112575171759\n",
      "Mask mean value:  tensor(0.5024, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0003  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.725e-15\n",
      "‖w_svm‖₂       : 4.876720855058251e-08\n",
      "‖alpha‖₁       : 0.17999999999999772\n",
      "scores min/max : 4.8225117478337476e-08 6.290212208973743e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 4.909e-08\n",
      "‖w_svm‖₂       : 0.03308758627174943\n",
      "‖alpha‖₁       : 0.8985724274271603\n",
      "scores min/max : -0.7594399035913089 2.000802823338068\n",
      "Mask mean value:  tensor(0.8272, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3928  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.559e-15\n",
      "‖w_svm‖₂       : 8.386030644966561e-08\n",
      "‖alpha‖₁       : 0.3799999999999998\n",
      "scores min/max : -2.3654068572323873e-09 1.5781901815616773e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.661e-22\n",
      "‖w_svm‖₂       : 7.342346438138811e-08\n",
      "‖alpha‖₁       : 0.13999999999999596\n",
      "scores min/max : 4.6494500679827935e-09 1.0401368855033928e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.994e-08\n",
      "‖w_svm‖₂       : 0.028631785465619477\n",
      "‖alpha‖₁       : 0.19667015200519966\n",
      "scores min/max : -2.203425662115563 0.021790569962783952\n",
      "Mask mean value:  tensor(0.0599, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4049  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.458e-04\n",
      "‖w_svm‖₂       : 0.0069114597497988385\n",
      "‖alpha‖₁       : 0.6076708247348346\n",
      "scores min/max : -1.9846536084644 0.3023274320686555\n",
      "Mask mean value:  tensor(0.5812, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2348  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.813e-15\n",
      "‖w_svm‖₂       : 0.0003561922024896935\n",
      "‖alpha‖₁       : 0.7399999999999999\n",
      "scores min/max : -0.00019974036359238464 -0.00018355762313288486\n",
      "Mask mean value:  tensor(0.4991, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0623  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.067e-17\n",
      "‖w_svm‖₂       : 5.532929227440558e-08\n",
      "‖alpha‖₁       : 0.43999999999999534\n",
      "scores min/max : -1.487388649870612e-07 -4.4659424671839315e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.290e-20\n",
      "‖w_svm‖₂       : 0.04765948995960767\n",
      "‖alpha‖₁       : 0.9390040675061474\n",
      "scores min/max : -2.4393119550717195 1.6036365392668308\n",
      "Mask mean value:  tensor(0.1588, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4202  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.398e-03\n",
      "‖w_svm‖₂       : 0.023784869976784903\n",
      "‖alpha‖₁       : 0.8150905806273313\n",
      "scores min/max : -11.5261522227444 2.033265851342109\n",
      "Mask mean value:  tensor(0.7118, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9077  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.086e-06\n",
      "‖w_svm‖₂       : 0.06131717301527911\n",
      "‖alpha‖₁       : 0.8986032754159745\n",
      "scores min/max : -1.7212169267109536 3.684855454628059\n",
      "Mask mean value:  tensor(0.3832, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5416  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.323e-02\n",
      "‖w_svm‖₂       : 0.0483373989734708\n",
      "‖alpha‖₁       : 0.9199999999999339\n",
      "scores min/max : -0.3566957599365647 0.5106868210853202\n",
      "Mask mean value:  tensor(0.3510, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7416  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.381e-02\n",
      "‖w_svm‖₂       : 0.017558714556493885\n",
      "‖alpha‖₁       : 0.5962530546066171\n",
      "scores min/max : -2.9423062396505504 2.256166812715582\n",
      "Mask mean value:  tensor(0.9074, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.806e-03\n",
      "‖w_svm‖₂       : 2.8469447881111667e-07\n",
      "‖alpha‖₁       : 0.659999999999972\n",
      "scores min/max : -6.07538124293551e-07 -4.555586689995742e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.325e-18\n",
      "‖w_svm‖₂       : 0.08640701796080191\n",
      "‖alpha‖₁       : 0.5780358832990634\n",
      "scores min/max : -2.1240285847823293 5.808154315965267\n",
      "Mask mean value:  tensor(0.2688, dtype=torch.float64)\n",
      "max feasible return = 0.1106  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.0325979527582365e-07\n",
      "‖alpha‖₁       : 0.5799999999999977\n",
      "scores min/max : -3.7726214012738503e-07 -2.8638154214865225e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0963952026429843e-07\n",
      "‖alpha‖₁       : 0.299999999999966\n",
      "scores min/max : 3.7504775378140206e-08 4.26737478734878e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 7.083090108733706e-08\n",
      "‖alpha‖₁       : 0.5999999999999956\n",
      "scores min/max : 1.37972420505211e-08 5.058130203705211e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.740622018752192e-08\n",
      "‖alpha‖₁       : 0.37999999999999995\n",
      "scores min/max : 7.0634582430836945e-09 1.6438316288055642e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.761665944231531e-06\n",
      "‖alpha‖₁       : 0.31999999996320727\n",
      "scores min/max : 3.3444059347391966e-06 5.602118580559488e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4258  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.05918646071636133\n",
      "‖alpha‖₁       : 0.7397077754546684\n",
      "scores min/max : -3.477402367855878 2.149618408643297\n",
      "Mask mean value:  tensor(0.8772, dtype=torch.float64)\n",
      "max feasible return = -0.9230  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03479266876369237\n",
      "‖alpha‖₁       : 0.6151665730083002\n",
      "scores min/max : -1.9052358060720171 1.2283723536041684\n",
      "Mask mean value:  tensor(0.8063, dtype=torch.float64)\n",
      "max feasible return = 2.8321  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.7447001098368437e-07\n",
      "‖alpha‖₁       : 0.4599999999999882\n",
      "scores min/max : 2.076331170110802e-07 8.022595074928302e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.387449634265901e-07\n",
      "‖alpha‖₁       : 0.5199999999999944\n",
      "scores min/max : 6.361815318455006e-08 7.65244264164971e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.604952162994352e-07\n",
      "‖alpha‖₁       : 0.5799999999999774\n",
      "scores min/max : -2.629115651641949e-07 -2.265786502082605e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.04556938479521902\n",
      "‖alpha‖₁       : 0.6876958915027379\n",
      "scores min/max : -0.432491291856911 1.9571435840815352\n",
      "Mask mean value:  tensor(0.4049, dtype=torch.float64)\n",
      "max feasible return = 0.0450  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03387987353292673\n",
      "‖alpha‖₁       : 0.4221366620996627\n",
      "scores min/max : -3.921453488560576 5.1991509809759835\n",
      "Mask mean value:  tensor(0.0735, dtype=torch.float64)\n",
      "max feasible return = 0.4633  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.046772667328519084\n",
      "‖alpha‖₁       : 0.8211436258252605\n",
      "scores min/max : -5.110164011860164 3.052023393052292\n",
      "Mask mean value:  tensor(0.9376, dtype=torch.float64)\n",
      "max feasible return = -3.5345  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.008931257754105306\n",
      "‖alpha‖₁       : 0.7999999999999996\n",
      "scores min/max : -0.04426638469260883 0.008092337680696992\n",
      "Mask mean value:  tensor(0.4745, dtype=torch.float64)\n",
      "max feasible return = -0.1725  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.159125295164128e-07\n",
      "‖alpha‖₁       : 0.6199999999999587\n",
      "scores min/max : 3.9915793841797584e-08 5.6455181134186436e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.7912139284944686e-08\n",
      "‖alpha‖₁       : 0.4399999999999744\n",
      "scores min/max : 1.0507652211867648e-08 2.0088320011411976e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.3469084433619639e-07\n",
      "‖alpha‖₁       : 0.2799999999999713\n",
      "scores min/max : -3.558405843486652e-07 -3.378472016084675e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  19 | train 0.005402 | val 0.006754\n",
      "-----------------------------------------Epoch:  20 ----------------------------------------\n",
      "‖w_svm‖₂       : 2.2880618804211687e-07\n",
      "‖alpha‖₁       : 0.2599999999999999\n",
      "scores min/max : 1.8297485154681114e-08 3.1019343384068477e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.651e-17\n",
      "‖w_svm‖₂       : 0.04758705090147434\n",
      "‖alpha‖₁       : 0.9056467912660913\n",
      "scores min/max : -0.7323571598624756 1.8848137321097465\n",
      "Mask mean value:  tensor(0.3568, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6610  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.921e-03\n",
      "‖w_svm‖₂       : 0.04768810741143947\n",
      "‖alpha‖₁       : 0.939009941629179\n",
      "scores min/max : -2.4393010162404822 1.6037726964356251\n",
      "Mask mean value:  tensor(0.1589, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4204  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.389e-03\n",
      "‖w_svm‖₂       : 4.3226881917590883e-07\n",
      "‖alpha‖₁       : 0.7199999999999985\n",
      "scores min/max : 3.0077458130245906e-07 3.580795272410974e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.978e-21\n",
      "‖w_svm‖₂       : 0.00020931579233625085\n",
      "‖alpha‖₁       : 0.6199999999742565\n",
      "scores min/max : 4.435050217824038e-05 5.9889548413954584e-05\n",
      "Mask mean value:  tensor(0.5002, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.013e-17\n",
      "‖w_svm‖₂       : 0.0031311109283320843\n",
      "‖alpha‖₁       : 0.5799999999999861\n",
      "scores min/max : 0.0020293903092410553 0.0032727978871540386\n",
      "Mask mean value:  tensor(0.5151, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3275  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.532e-13\n",
      "‖w_svm‖₂       : 1.0271456712485828e-06\n",
      "‖alpha‖₁       : 0.5999999999999945\n",
      "scores min/max : -6.01495353437118e-08 -9.27738831922978e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.479e-19\n",
      "‖w_svm‖₂       : 5.513947750304839e-08\n",
      "‖alpha‖₁       : 0.4399999999999945\n",
      "scores min/max : -1.4870318928644868e-07 -4.466349332730754e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.281e-20\n",
      "‖w_svm‖₂       : 0.053822299890474254\n",
      "‖alpha‖₁       : 0.8271730088424261\n",
      "scores min/max : -1.9402757992809707 0.40863065178840147\n",
      "Mask mean value:  tensor(0.5550, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0125  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.299e-11\n",
      "‖w_svm‖₂       : 0.005568834896430538\n",
      "‖alpha‖₁       : 0.5599999999999904\n",
      "scores min/max : 0.0035748158711025966 0.005184354829562232\n",
      "Mask mean value:  tensor(0.5193, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0886  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 8.662e-05\n",
      "‖w_svm‖₂       : 0.01655370693272837\n",
      "‖alpha‖₁       : 0.8599999999999768\n",
      "scores min/max : -0.04200249505540916 -0.026231311346113284\n",
      "Mask mean value:  tensor(0.3522, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0864  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.034e-14\n",
      "‖w_svm‖₂       : 0.050889307780461794\n",
      "‖alpha‖₁       : 0.6599999999999998\n",
      "scores min/max : -0.3317672920607192 0.248803031268365\n",
      "Mask mean value:  tensor(0.5040, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8943  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.026e-13\n",
      "‖w_svm‖₂       : 2.1333551896162366e-07\n",
      "‖alpha‖₁       : 0.37999999999451517\n",
      "scores min/max : -2.4565210421308056e-08 -6.114710716489744e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.688e-07\n",
      "‖w_svm‖₂       : 0.0770804537511847\n",
      "‖alpha‖₁       : 0.41912078060625607\n",
      "scores min/max : -1.8502383668048914 2.435273175581548\n",
      "Mask mean value:  tensor(0.8084, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.078e-04\n",
      "‖w_svm‖₂       : 7.833451013060716e-08\n",
      "‖alpha‖₁       : 0.659999999999983\n",
      "scores min/max : 1.0049666973959372e-07 2.453670480986823e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.766e-09\n",
      "‖w_svm‖₂       : 0.04802663991339455\n",
      "‖alpha‖₁       : 0.9199999999999591\n",
      "scores min/max : -0.3541746692813121 0.5016083924024406\n",
      "Mask mean value:  tensor(0.3452, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7121  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.051e-02\n",
      "‖w_svm‖₂       : 0.13923115832605157\n",
      "‖alpha‖₁       : 0.6550558238140358\n",
      "scores min/max : -18.085405267937837 1.7688894667571333\n",
      "Mask mean value:  tensor(0.2153, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2704  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.693e-03\n",
      "‖w_svm‖₂       : 0.057773789270283454\n",
      "‖alpha‖₁       : 0.5762224351205587\n",
      "scores min/max : -1.9615060446413408 0.8650790054646725\n",
      "Mask mean value:  tensor(0.6943, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.8891  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.749e-04\n",
      "‖w_svm‖₂       : 0.0008769711479363556\n",
      "‖alpha‖₁       : 0.819999999999999\n",
      "scores min/max : 0.000800270991239943 0.002525547934615755\n",
      "Mask mean value:  tensor(0.5094, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5879  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.003e-16\n",
      "‖w_svm‖₂       : 3.606342859282294e-06\n",
      "‖alpha‖₁       : 0.4199999999930725\n",
      "scores min/max : -1.0323350077391937e-06 5.87494710404123e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1230  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.531e-05\n",
      "‖w_svm‖₂       : 1.574464529037717e-05\n",
      "‖alpha‖₁       : 0.35999999998859156\n",
      "scores min/max : 1.362190202829241e-05 1.4568843910737763e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.636e-17\n",
      "‖w_svm‖₂       : 0.024671984249525568\n",
      "‖alpha‖₁       : 0.6599999999999012\n",
      "scores min/max : -0.050851530127362175 0.05866116448597627\n",
      "Mask mean value:  tensor(0.4359, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.4661  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.932e-15\n",
      "‖w_svm‖₂       : 0.017254125033765198\n",
      "‖alpha‖₁       : 0.6815099612417133\n",
      "scores min/max : -1.9772068258972553 0.05222482988386845\n",
      "Mask mean value:  tensor(0.5890, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2423  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.600e-14\n",
      "‖w_svm‖₂       : 7.74546863673339e-08\n",
      "‖alpha‖₁       : 0.5399999999999232\n",
      "scores min/max : -1.1012137851018706e-07 -8.84789075081066e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.570e-20\n",
      "‖w_svm‖₂       : 2.3962234252511733e-07\n",
      "‖alpha‖₁       : 0.5799999999999998\n",
      "scores min/max : -4.024996553005006e-07 -3.86881233299158e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.933e-19\n",
      "‖w_svm‖₂       : 2.83511753055412e-07\n",
      "‖alpha‖₁       : 0.65999999999993\n",
      "scores min/max : -6.118815444136457e-07 -4.5977599810610326e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.320e-18\n",
      "‖w_svm‖₂       : 1.1280710581790527e-07\n",
      "‖alpha‖₁       : 0.5199999999999997\n",
      "scores min/max : 3.2069255704475543e-07 3.420936862788287e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.548e-19\n",
      "‖w_svm‖₂       : 3.8572667841624604e-07\n",
      "‖alpha‖₁       : 0.27999999999999736\n",
      "scores min/max : -1.5202697412790109e-06 -1.4585582073839048e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1957  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.441e-19\n",
      "‖w_svm‖₂       : 0.00527571218944502\n",
      "‖alpha‖₁       : 0.38\n",
      "scores min/max : -0.006663338070602348 0.00501348108743193\n",
      "Mask mean value:  tensor(0.5100, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9076  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.740e-14\n",
      "‖w_svm‖₂       : 0.0003584919127584421\n",
      "‖alpha‖₁       : 0.7400000000000001\n",
      "scores min/max : -0.00019998267877036614 -0.0001835903826973497\n",
      "Mask mean value:  tensor(0.4991, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0623  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.085e-17\n",
      "‖w_svm‖₂       : 0.00018468690439684047\n",
      "‖alpha‖₁       : 0.8199999999999993\n",
      "scores min/max : -5.2892323130345826e-05 -3.8749295582041514e-05\n",
      "Mask mean value:  tensor(0.4998, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6971  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.194e-17\n",
      "‖w_svm‖₂       : 0.03296937424762833\n",
      "‖alpha‖₁       : 0.898582818570255\n",
      "scores min/max : -0.7585535671632342 2.0017044914958424\n",
      "Mask mean value:  tensor(0.8286, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3896  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.453e-15\n",
      "‖w_svm‖₂       : 0.019933065740190346\n",
      "‖alpha‖₁       : 0.78\n",
      "scores min/max : -0.016859161776676394 0.05164599649943202\n",
      "Mask mean value:  tensor(0.4450, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9441  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.844e-15\n",
      "‖w_svm‖₂       : 0.028330501210100396\n",
      "‖alpha‖₁       : 0.5510297933166638\n",
      "scores min/max : -3.5484453844238315 1.0238010026168323\n",
      "Mask mean value:  tensor(0.1160, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9755  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.688e-03\n",
      "‖w_svm‖₂       : 0.14727739162083026\n",
      "‖alpha‖₁       : 0.7596381623114425\n",
      "scores min/max : -1.7390871530436054 2.2384898964798787\n",
      "Mask mean value:  tensor(0.8790, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3592  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.560e-11\n",
      "‖w_svm‖₂       : 0.1390274161482747\n",
      "‖alpha‖₁       : 0.8800000000000001\n",
      "scores min/max : -1.3699134494747738 3.6610413239281545\n",
      "Mask mean value:  tensor(0.3226, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2609  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.468e-11\n",
      "‖w_svm‖₂       : 1.1907861330181802e-06\n",
      "‖alpha‖₁       : 0.31999999999850165\n",
      "scores min/max : -2.0268627410709147e-06 -1.910266225802316e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4898  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.778e-18\n",
      "‖w_svm‖₂       : 1.7095704713294256e-07\n",
      "‖alpha‖₁       : 0.23999999999998844\n",
      "scores min/max : 2.832444577137646e-07 2.986381904399948e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.145e-20\n",
      "‖w_svm‖₂       : 2.3752438161446246e-07\n",
      "‖alpha‖₁       : 0.6399999999999968\n",
      "scores min/max : 3.2086667388770396e-07 3.4175411752486154e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.579e-20\n",
      "‖w_svm‖₂       : 6.79571224025971e-07\n",
      "‖alpha‖₁       : 0.3999999999999995\n",
      "scores min/max : -6.383996389897676e-08 2.4212483236032533e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.278e-09\n",
      "‖w_svm‖₂       : 5.4483226492351365e-08\n",
      "‖alpha‖₁       : 0.11999999999999852\n",
      "scores min/max : -8.270782826644717e-08 -5.204570434977254e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.760e-08\n",
      "‖w_svm‖₂       : 0.0042292153599696024\n",
      "‖alpha‖₁       : 0.45999999999999025\n",
      "scores min/max : -0.006244967046123696 -0.004348518800386704\n",
      "Mask mean value:  tensor(0.4761, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.0691  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.979e-16\n",
      "‖w_svm‖₂       : 0.007000571011543402\n",
      "‖alpha‖₁       : 0.6076720134074436\n",
      "scores min/max : -1.9849833391262477 0.30198305353319377\n",
      "Mask mean value:  tensor(0.5797, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.444e-15\n",
      "‖w_svm‖₂       : 0.08242016615626722\n",
      "‖alpha‖₁       : 0.47195250729867566\n",
      "scores min/max : -2.3213799361875056 2.7617622933522172\n",
      "Mask mean value:  tensor(0.1023, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1190  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.198e-02\n",
      "‖w_svm‖₂       : 1.17249923842493e-07\n",
      "‖alpha‖₁       : 0.2999999999999761\n",
      "scores min/max : 1.03931907644443e-07 1.1303781208385088e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.248e-07\n",
      "‖w_svm‖₂       : 0.18744868816677188\n",
      "‖alpha‖₁       : 0.8836869779012743\n",
      "scores min/max : -3.5735949290701594 6.083241299055606\n",
      "Mask mean value:  tensor(0.0956, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1925  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.688e-03\n",
      "‖w_svm‖₂       : 1.0791867497961924e-07\n",
      "‖alpha‖₁       : 0.23999999999998428\n",
      "scores min/max : 2.1292256915152533e-08 3.223741163364695e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.400e-21\n",
      "‖w_svm‖₂       : 5.740351349153185e-08\n",
      "‖alpha‖₁       : 0.23999999999999141\n",
      "scores min/max : -7.258022598842186e-08 -5.812491330487551e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.325e-20\n",
      "‖w_svm‖₂       : 1.0884847940776216e-06\n",
      "‖alpha‖₁       : 0.49999999999999173\n",
      "scores min/max : -2.832074402947979e-07 -9.020030844385033e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.957e-19\n",
      "‖w_svm‖₂       : 0.0221851962757643\n",
      "‖alpha‖₁       : 0.8233610085187897\n",
      "scores min/max : -1.8892017800539287 1.510314189255444\n",
      "Mask mean value:  tensor(0.7270, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.4520  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.414e-06\n",
      "‖w_svm‖₂       : 0.03983727984401945\n",
      "‖alpha‖₁       : 0.9402725840256203\n",
      "scores min/max : -1.8273337635494586 0.306780923700009\n",
      "Mask mean value:  tensor(0.4285, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3296  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.160e-10\n",
      "‖w_svm‖₂       : 0.0003027978383793982\n",
      "‖alpha‖₁       : 0.41999999999638404\n",
      "scores min/max : 0.0002027679481753603 0.0005133930043401032\n",
      "Mask mean value:  tensor(0.5024, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0003  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.737e-15\n",
      "‖w_svm‖₂       : 0.019074120815911925\n",
      "‖alpha‖₁       : 0.8600000000000001\n",
      "scores min/max : -0.049216104042666686 0.03013010520735796\n",
      "Mask mean value:  tensor(0.4155, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5382  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.375e-14\n",
      "‖w_svm‖₂       : 0.043593113090302504\n",
      "‖alpha‖₁       : 0.889989602968018\n",
      "scores min/max : -2.0089827347174087 0.33841154504727855\n",
      "Mask mean value:  tensor(0.2655, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0722  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.761e-11\n",
      "‖w_svm‖₂       : 8.365145023503026e-08\n",
      "‖alpha‖₁       : 0.3799999999999999\n",
      "scores min/max : -1.3723108364307485e-09 1.6773948885008437e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.655e-22\n",
      "‖w_svm‖₂       : 0.000500060931039764\n",
      "‖alpha‖₁       : 0.4399999999999999\n",
      "scores min/max : -0.0006158741180931358 -0.0003622197765304224\n",
      "Mask mean value:  tensor(0.4971, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.059e-16\n",
      "‖w_svm‖₂       : 0.06117588761298723\n",
      "‖alpha‖₁       : 0.8985919691772181\n",
      "scores min/max : -1.7153671478191832 3.6914147384680156\n",
      "Mask mean value:  tensor(0.3948, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5510  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.316e-02\n",
      "‖w_svm‖₂       : 8.685780923930707e-08\n",
      "‖alpha‖₁       : 0.1799999999999964\n",
      "scores min/max : -8.368284200795277e-08 1.51019505482913e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.630e-21\n",
      "‖w_svm‖₂       : 2.014750554885751e-07\n",
      "‖alpha‖₁       : 0.41999999999998006\n",
      "scores min/max : -3.582124096982006e-07 -3.359312044776323e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.590e-19\n",
      "‖w_svm‖₂       : 0.020720259567390754\n",
      "‖alpha‖₁       : 0.3833708601823017\n",
      "scores min/max : -1.9698130954313422 0.23070342263167212\n",
      "Mask mean value:  tensor(0.6794, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1448  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.945e-14\n",
      "‖w_svm‖₂       : 0.00013636340965569974\n",
      "‖alpha‖₁       : 0.43999999997896955\n",
      "scores min/max : -0.00020151667578678244 -6.598372795641068e-05\n",
      "Mask mean value:  tensor(0.4994, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0243  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.596e-17\n",
      "‖w_svm‖₂       : 1.952809547298741e-07\n",
      "‖alpha‖₁       : 0.3799999999999997\n",
      "scores min/max : -6.139315778412585e-08 -2.5726685552283003e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.626e-19\n",
      "‖w_svm‖₂       : 0.026159684709872066\n",
      "‖alpha‖₁       : 0.8522452517573054\n",
      "scores min/max : -2.9291508615419346 1.551767538009758\n",
      "Mask mean value:  tensor(0.1498, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9439  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.276e-03\n",
      "‖w_svm‖₂       : 7.288363980689124e-08\n",
      "‖alpha‖₁       : 0.42000000000000004\n",
      "scores min/max : -7.836073786279568e-08 -3.1206416202585646e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.724e-08\n",
      "‖w_svm‖₂       : 7.296526233079204e-08\n",
      "‖alpha‖₁       : 0.13999999999999696\n",
      "scores min/max : 4.844443176419459e-09 1.0588315814784087e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.957e-08\n",
      "‖w_svm‖₂       : 0.07111529338329792\n",
      "‖alpha‖₁       : 0.5799999999999998\n",
      "scores min/max : -2.791339279173347 1.579621511867224\n",
      "Mask mean value:  tensor(0.1617, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3021  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.610e-04\n",
      "‖w_svm‖₂       : 0.023704166077731556\n",
      "‖alpha‖₁       : 0.8150919462789525\n",
      "scores min/max : -11.524612062520514 2.0339195730061683\n",
      "Mask mean value:  tensor(0.7133, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9119  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.646e-06\n",
      "‖w_svm‖₂       : 2.199601319466242e-08\n",
      "‖alpha‖₁       : 0.11999999999999442\n",
      "scores min/max : -2.2960652680286707e-08 -8.279500328262515e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.786e-09\n",
      "‖w_svm‖₂       : 0.05012168985677416\n",
      "‖alpha‖₁       : 0.7294182604322854\n",
      "scores min/max : -0.247611971256553 2.035088526231284\n",
      "Mask mean value:  tensor(0.7208, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2337  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.152e-03\n",
      "‖w_svm‖₂       : 4.864146081042254e-08\n",
      "‖alpha‖₁       : 0.17999999999999797\n",
      "scores min/max : 4.790168591938573e-08 6.257167865124973e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 4.918e-08\n",
      "‖w_svm‖₂       : 0.005601573301574578\n",
      "‖alpha‖₁       : 0.7005642354672678\n",
      "scores min/max : -2.004485488497421 0.01890211897619064\n",
      "Mask mean value:  tensor(0.4524, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2538  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.866e-05\n",
      "‖w_svm‖₂       : 0.00015906705643048186\n",
      "‖alpha‖₁       : 0.6399999999999999\n",
      "scores min/max : -9.395401736922234e-05 -8.429921170672851e-05\n",
      "Mask mean value:  tensor(0.4996, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7637  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.134e-17\n",
      "‖w_svm‖₂       : 0.11652402692630948\n",
      "‖alpha‖₁       : 0.873261934656365\n",
      "scores min/max : -12.21919821518675 2.1349871346174316\n",
      "Mask mean value:  tensor(0.4954, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.4653  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 7.939e-03\n",
      "‖w_svm‖₂       : 0.017357641491986604\n",
      "‖alpha‖₁       : 0.596254288542795\n",
      "scores min/max : -2.943745000992882 2.25901434814539\n",
      "Mask mean value:  tensor(0.9090, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.4969  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.566e-03\n",
      "‖w_svm‖₂       : 0.07739967157761489\n",
      "‖alpha‖₁       : 0.6583407330737063\n",
      "scores min/max : -2.0602544406783365 4.037051167772356\n",
      "Mask mean value:  tensor(0.3081, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.5633  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.864e-04\n",
      "‖w_svm‖₂       : 0.028487605980987932\n",
      "‖alpha‖₁       : 0.19667133324028124\n",
      "scores min/max : -2.2030062102404653 0.021873051997564556\n",
      "Mask mean value:  tensor(0.0602, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4066  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.421e-04\n",
      "‖w_svm‖₂       : 0.0863447960825177\n",
      "‖alpha‖₁       : 0.5780260302603413\n",
      "scores min/max : -2.109421982840405 5.822435962007473\n",
      "Mask mean value:  tensor(0.2962, dtype=torch.float64)\n",
      "max feasible return = 0.1156  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.996370727357801e-07\n",
      "‖alpha‖₁       : 0.5800000000000001\n",
      "scores min/max : -3.8804975692366433e-07 -2.9732682990302205e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.038103718454927e-07\n",
      "‖alpha‖₁       : 0.2999999999999997\n",
      "scores min/max : 3.940464716787745e-08 4.4375730452167555e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 7.029807107049066e-08\n",
      "‖alpha‖₁       : 0.5999999999999939\n",
      "scores min/max : 1.7964705329764807e-08 5.4751498715792834e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.7151682982533095e-08\n",
      "‖alpha‖₁       : 0.3799999999999818\n",
      "scores min/max : 8.882988363304287e-09 1.8320248158250075e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.9659861462127374e-06\n",
      "‖alpha‖₁       : 0.3199999999761444\n",
      "scores min/max : 2.7942219477458544e-06 4.540878433464559e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4258  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.05846333662540821\n",
      "‖alpha‖₁       : 0.7397071270335649\n",
      "scores min/max : -3.476909772424941 2.1513631573646013\n",
      "Mask mean value:  tensor(0.8785, dtype=torch.float64)\n",
      "max feasible return = -0.9243  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.034277872776370066\n",
      "‖alpha‖₁       : 0.6151660675344552\n",
      "scores min/max : -1.906659576364298 1.2266059590647889\n",
      "Mask mean value:  tensor(0.8038, dtype=torch.float64)\n",
      "max feasible return = 2.8222  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.7215679882136615e-07\n",
      "‖alpha‖₁       : 0.4599999999999792\n",
      "scores min/max : 2.1145475999694225e-07 8.063002112394145e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.3819304874187538e-07\n",
      "‖alpha‖₁       : 0.5199999999999945\n",
      "scores min/max : 6.267452791967267e-08 7.558053688293219e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.5928244344519887e-07\n",
      "‖alpha‖₁       : 0.5799999999999783\n",
      "scores min/max : -2.674225374048448e-07 -2.310910338203647e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.04536254908933256\n",
      "‖alpha‖₁       : 0.6877002045953997\n",
      "scores min/max : -0.4265470324895574 1.9629046896334883\n",
      "Mask mean value:  tensor(0.4263, dtype=torch.float64)\n",
      "max feasible return = 0.0234  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.0336622324758706\n",
      "‖alpha‖₁       : 0.42213857229299323\n",
      "scores min/max : -3.9183731512398507 5.204847653206439\n",
      "Mask mean value:  tensor(0.0749, dtype=torch.float64)\n",
      "max feasible return = 0.4720  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04649105890230385\n",
      "‖alpha‖₁       : 0.8211480739814524\n",
      "scores min/max : -5.1093481301182875 3.053506013853326\n",
      "Mask mean value:  tensor(0.9380, dtype=torch.float64)\n",
      "max feasible return = -3.5369  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.0088002030000704\n",
      "‖alpha‖₁       : 0.7999999999999996\n",
      "scores min/max : -0.04310780033006384 0.007825034005300293\n",
      "Mask mean value:  tensor(0.4749, dtype=torch.float64)\n",
      "max feasible return = -0.1727  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.137984268058869e-07\n",
      "‖alpha‖₁       : 0.6199999999999578\n",
      "scores min/max : 3.937479064961479e-08 5.5916196647527895e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.777508849081987e-08\n",
      "‖alpha‖₁       : 0.43999999999997497\n",
      "scores min/max : 1.0178919848310907e-08 1.9759256920332205e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.2855766608231385e-07\n",
      "‖alpha‖₁       : 0.27999999999999925\n",
      "scores min/max : -3.5392725376396107e-07 -3.363220871684945e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  20 | train 0.005390 | val 0.006744\n",
      "-----------------------------------------Epoch:  21 ----------------------------------------\n",
      "‖w_svm‖₂       : 7.729027625723726e-08\n",
      "‖alpha‖₁       : 0.539999999999935\n",
      "scores min/max : -1.1176919322499401e-07 -9.012923998241737e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.568e-20\n",
      "‖w_svm‖₂       : 3.8542300562933935e-07\n",
      "‖alpha‖₁       : 0.27999999999999703\n",
      "scores min/max : -1.5392447425202624e-06 -1.4775278190486308e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1957  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.439e-19\n",
      "‖w_svm‖₂       : 8.361098735744723e-08\n",
      "‖alpha‖₁       : 0.37999999999999984\n",
      "scores min/max : 4.749808020754952e-10 1.8621790377834596e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.652e-22\n",
      "‖w_svm‖₂       : 0.024479138921398653\n",
      "‖alpha‖₁       : 0.6599999999999999\n",
      "scores min/max : -0.05058801455545453 0.05693464844750751\n",
      "Mask mean value:  tensor(0.4341, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.4504  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.911e-15\n",
      "‖w_svm‖₂       : 0.11623786068344623\n",
      "‖alpha‖₁       : 0.8732618353461731\n",
      "scores min/max : -12.219657950327033 2.1340547012071784\n",
      "Mask mean value:  tensor(0.4930, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.4586  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.818e-03\n",
      "‖w_svm‖₂       : 0.047432969086769874\n",
      "‖alpha‖₁       : 0.9199999999999979\n",
      "scores min/max : -0.34826023646085225 0.48591271062331576\n",
      "Mask mean value:  tensor(0.3388, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.6806  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.122e-02\n",
      "‖w_svm‖₂       : 2.9414193586289426e-07\n",
      "‖alpha‖₁       : 0.5799999999934606\n",
      "scores min/max : -4.913933218822783e-07 -4.724831930953538e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.367e-19\n",
      "‖w_svm‖₂       : 0.028438457343298737\n",
      "‖alpha‖₁       : 0.1966713417527976\n",
      "scores min/max : -2.202926969802431 0.021867198006167744\n",
      "Mask mean value:  tensor(0.0602, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4068  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.412e-04\n",
      "‖w_svm‖₂       : 0.04782065428474411\n",
      "‖alpha‖₁       : 0.9056471801206063\n",
      "scores min/max : -0.7355509465738722 1.8845512639814448\n",
      "Mask mean value:  tensor(0.3574, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6622  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.009e-03\n",
      "‖w_svm‖₂       : 0.017255020288656164\n",
      "‖alpha‖₁       : 0.5962545188245214\n",
      "scores min/max : -2.9455276480233996 2.2593330428913663\n",
      "Mask mean value:  tensor(0.9092, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.4973  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.336e-03\n",
      "‖w_svm‖₂       : 0.0001848430211675213\n",
      "‖alpha‖₁       : 0.819999999999999\n",
      "scores min/max : -5.307239229691976e-05 -3.8909446180342296e-05\n",
      "Mask mean value:  tensor(0.4998, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6971  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.195e-17\n",
      "‖w_svm‖₂       : 0.00559490176617288\n",
      "‖alpha‖₁       : 0.559999999999995\n",
      "scores min/max : 0.003927104962918327 0.005552094325524517\n",
      "Mask mean value:  tensor(0.5211, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0923  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.691e-05\n",
      "‖w_svm‖₂       : 4.8617952316923065e-08\n",
      "‖alpha‖₁       : 0.17999999999999786\n",
      "scores min/max : 4.8811845824375604e-08 6.349769325670848e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 4.928e-08\n",
      "‖w_svm‖₂       : 0.0070795109915098\n",
      "‖alpha‖₁       : 0.607673077029204\n",
      "scores min/max : -1.9850502733741884 0.3019052402525473\n",
      "Mask mean value:  tensor(0.5794, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2341  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.023e-15\n",
      "‖w_svm‖₂       : 7.363526990894953e-08\n",
      "‖alpha‖₁       : 0.13999999999998375\n",
      "scores min/max : 4.757334004818666e-09 1.0597799373932272e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.011e-08\n",
      "‖w_svm‖₂       : 0.0005029821613344173\n",
      "‖alpha‖₁       : 0.43999999999999995\n",
      "scores min/max : -0.0006528347839845992 -0.0003960681130953606\n",
      "Mask mean value:  tensor(0.4969, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6452  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.071e-16\n",
      "‖w_svm‖₂       : 1.0786969832652162e-07\n",
      "‖alpha‖₁       : 0.23999999999998406\n",
      "scores min/max : 2.158968799785558e-08 3.254024454247965e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.401e-21\n",
      "‖w_svm‖₂       : 2.2765472610290126e-07\n",
      "‖alpha‖₁       : 0.3799999999931215\n",
      "scores min/max : -2.847958239971901e-08 -9.389702730354779e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.003e-07\n",
      "‖w_svm‖₂       : 1.1250915761236768e-07\n",
      "‖alpha‖₁       : 0.5199999999999997\n",
      "scores min/max : 3.245819476796676e-07 3.4598484224256617e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.540e-19\n",
      "‖w_svm‖₂       : 0.04355064311425849\n",
      "‖alpha‖₁       : 0.8899968853369089\n",
      "scores min/max : -2.006414499455663 0.3410597384380418\n",
      "Mask mean value:  tensor(0.2731, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0757  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.221e-11\n",
      "‖w_svm‖₂       : 0.06133657520571367\n",
      "‖alpha‖₁       : 0.8986144021547552\n",
      "scores min/max : -1.7072779534517324 3.7000825367004366\n",
      "Mask mean value:  tensor(0.4109, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5637  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.268e-02\n",
      "‖w_svm‖₂       : 0.005321662498272885\n",
      "‖alpha‖₁       : 0.38\n",
      "scores min/max : -0.006735784665551511 0.005017422532938208\n",
      "Mask mean value:  tensor(0.5100, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9074  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.765e-14\n",
      "‖w_svm‖₂       : 5.493649242098747e-08\n",
      "‖alpha‖₁       : 0.43999999999998807\n",
      "scores min/max : -1.5085469787089307e-07 -4.6874215787019564e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.284e-20\n",
      "‖w_svm‖₂       : 0.1873188823356763\n",
      "‖alpha‖₁       : 0.8837311127883667\n",
      "scores min/max : -3.582116893812755 6.079931860413027\n",
      "Mask mean value:  tensor(0.0947, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1926  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.473e-03\n",
      "‖w_svm‖₂       : 0.023597838489084377\n",
      "‖alpha‖₁       : 0.8150928122474432\n",
      "scores min/max : -11.524203106255786 2.033396314214195\n",
      "Mask mean value:  tensor(0.7123, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9102  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.789e-06\n",
      "‖w_svm‖₂       : 0.003153354510140739\n",
      "‖alpha‖₁       : 0.5799999999999712\n",
      "scores min/max : 0.0020532667271981526 0.0033138760794430427\n",
      "Mask mean value:  tensor(0.5152, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3276  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.550e-13\n",
      "‖w_svm‖₂       : 0.005659997411642097\n",
      "‖alpha‖₁       : 0.7005647967789638\n",
      "scores min/max : -2.004739502407394 0.018650707369415327\n",
      "Mask mean value:  tensor(0.4512, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2532  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.283e-05\n",
      "‖w_svm‖₂       : 0.04970815459527375\n",
      "‖alpha‖₁       : 0.6599999999999997\n",
      "scores min/max : -0.3175643964478078 0.23720724097841034\n",
      "Mask mean value:  tensor(0.5011, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.064e-13\n",
      "‖w_svm‖₂       : 3.672596737846151e-06\n",
      "‖alpha‖₁       : 0.4199999999926667\n",
      "scores min/max : -1.0399031342463059e-06 6.076700085565379e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1230  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.559e-05\n",
      "‖w_svm‖₂       : 0.032664279843338744\n",
      "‖alpha‖₁       : 0.8985823837781265\n",
      "scores min/max : -0.7602154648526162 2.0000482528264616\n",
      "Mask mean value:  tensor(0.8258, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3922  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.772e-15\n",
      "‖w_svm‖₂       : 0.00021038527936016166\n",
      "‖alpha‖₁       : 0.619999999998812\n",
      "scores min/max : 4.848756263902703e-05 6.417207208860739e-05\n",
      "Mask mean value:  tensor(0.5003, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.074e-17\n",
      "‖w_svm‖₂       : 1.1429418510309012e-07\n",
      "‖alpha‖₁       : 0.2999999999999997\n",
      "scores min/max : 1.0183313955012726e-07 1.107333170308122e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.217e-07\n",
      "‖w_svm‖₂       : 7.24962288752344e-08\n",
      "‖alpha‖₁       : 0.42000000000000004\n",
      "scores min/max : -7.977733316456695e-08 -3.269270289948306e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.722e-08\n",
      "‖w_svm‖₂       : 0.017594385246597576\n",
      "‖alpha‖₁       : 0.681522439078859\n",
      "scores min/max : -1.975880132449808 0.053553790057615715\n",
      "Mask mean value:  tensor(0.5951, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2457  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.058e-13\n",
      "‖w_svm‖₂       : 1.0209624342286785e-06\n",
      "‖alpha‖₁       : 0.5999999999999927\n",
      "scores min/max : -7.418918523876147e-08 -2.3306821322479972e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.491e-19\n",
      "‖w_svm‖₂       : 0.00015941307649810805\n",
      "‖alpha‖₁       : 0.63999999999998\n",
      "scores min/max : -9.634469922607733e-05 -8.664788560746635e-05\n",
      "Mask mean value:  tensor(0.4995, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7637  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.147e-17\n",
      "‖w_svm‖₂       : 2.8275298049273264e-07\n",
      "‖alpha‖₁       : 0.659999999999909\n",
      "scores min/max : -6.193171101977311e-07 -4.672130183513076e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.314e-18\n",
      "‖w_svm‖₂       : 1.1901419599755925e-06\n",
      "‖alpha‖₁       : 0.31999999999846196\n",
      "scores min/max : -2.0629766543083994e-06 -1.946223401105353e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4898  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.784e-18\n",
      "‖w_svm‖₂       : 2.0094477256323532e-07\n",
      "‖alpha‖₁       : 0.4199999999999795\n",
      "scores min/max : -3.6144433458118465e-07 -3.3916799250820336e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.602e-19\n",
      "‖w_svm‖₂       : 0.07072191317392779\n",
      "‖alpha‖₁       : 0.5799999999999995\n",
      "scores min/max : -2.7612243515729413 1.560377285371283\n",
      "Mask mean value:  tensor(0.1607, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2996  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.584e-04\n",
      "‖w_svm‖₂       : 5.7178671760998444e-08\n",
      "‖alpha‖₁       : 0.23999999999999194\n",
      "scores min/max : -7.199100998716791e-08 -5.7543887296381794e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.323e-20\n",
      "‖w_svm‖₂       : 0.02628028780229785\n",
      "‖alpha‖₁       : 0.8522503038629672\n",
      "scores min/max : -2.9293766946482123 1.5515740269226845\n",
      "Mask mean value:  tensor(0.1497, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9438  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.284e-03\n",
      "‖w_svm‖₂       : 6.758139579384329e-07\n",
      "‖alpha‖₁       : 0.39999999999999963\n",
      "scores min/max : -6.741170792063989e-08 2.390697637282235e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.254e-09\n",
      "‖w_svm‖₂       : 0.02013205452705617\n",
      "‖alpha‖₁       : 0.779999999999998\n",
      "scores min/max : -0.01772344073424224 0.051207527802712816\n",
      "Mask mean value:  tensor(0.4409, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9280  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.843e-15\n",
      "‖w_svm‖₂       : 0.020445590104074617\n",
      "‖alpha‖₁       : 0.38337248960204684\n",
      "scores min/max : -1.9693606223173779 0.23145963446471826\n",
      "Mask mean value:  tensor(0.6807, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1455  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.905e-14\n",
      "‖w_svm‖₂       : 7.775525163768e-08\n",
      "‖alpha‖₁       : 0.6599999999999879\n",
      "scores min/max : 1.0349133209054316e-07 2.48371001585402e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.736e-09\n",
      "‖w_svm‖₂       : 0.018719896891266373\n",
      "‖alpha‖₁       : 0.8599999999999829\n",
      "scores min/max : -0.048138417693234375 0.028415287822031947\n",
      "Mask mean value:  tensor(0.4153, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5362  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.484e-14\n",
      "‖w_svm‖₂       : 0.02827838206966638\n",
      "‖alpha‖₁       : 0.551034795098692\n",
      "scores min/max : -3.5431465390912615 1.0287737352593576\n",
      "Mask mean value:  tensor(0.1179, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9889  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.933e-03\n",
      "‖w_svm‖₂       : 2.369869129776287e-07\n",
      "‖alpha‖₁       : 0.6399999999999973\n",
      "scores min/max : 3.252201854327838e-07 3.461056931442614e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.558e-20\n",
      "‖w_svm‖₂       : 1.7042211387490793e-07\n",
      "‖alpha‖₁       : 0.2399999999999895\n",
      "scores min/max : 2.872608303072587e-07 3.0264070240797736e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.149e-20\n",
      "‖w_svm‖₂       : 0.022056829070829016\n",
      "‖alpha‖₁       : 0.823362160275774\n",
      "scores min/max : -1.8923017637787054 1.5069330540807475\n",
      "Mask mean value:  tensor(0.7191, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.4331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.021e-06\n",
      "‖w_svm‖₂       : 0.00035939607725395475\n",
      "‖alpha‖₁       : 0.7399999999999995\n",
      "scores min/max : -0.00020570208821920098 -0.00018922705078161365\n",
      "Mask mean value:  tensor(0.4990, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0622  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.111e-17\n",
      "‖w_svm‖₂       : 1.0860181773212252e-06\n",
      "‖alpha‖₁       : 0.49999999999999145\n",
      "scores min/max : -2.8635489704127533e-07 -9.333533762352638e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.961e-19\n",
      "‖w_svm‖₂       : 0.0535703005036218\n",
      "‖alpha‖₁       : 0.8271783779280727\n",
      "scores min/max : -1.9491884660675176 0.40000426081431995\n",
      "Mask mean value:  tensor(0.5239, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0074  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.663e-02\n",
      "‖w_svm‖₂       : 0.07623486458018973\n",
      "‖alpha‖₁       : 0.41912035911998224\n",
      "scores min/max : -1.8696157630665649 2.3886389494506752\n",
      "Mask mean value:  tensor(0.7947, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8924  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.171e-04\n",
      "‖w_svm‖₂       : 0.1476502065240822\n",
      "‖alpha‖₁       : 0.7597488605309632\n",
      "scores min/max : -1.7404441513166464 2.2371692629932145\n",
      "Mask mean value:  tensor(0.8774, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3591  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.258e-12\n",
      "‖w_svm‖₂       : 0.0008808494369850991\n",
      "‖alpha‖₁       : 0.8199999999999986\n",
      "scores min/max : 0.0009228639953121707 0.0026641430322573365\n",
      "Mask mean value:  tensor(0.5101, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5913  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.006e-16\n",
      "‖w_svm‖₂       : 0.08276009882960776\n",
      "‖alpha‖₁       : 0.47201026830086223\n",
      "scores min/max : -2.3387807423845945 2.7443489124861866\n",
      "Mask mean value:  tensor(0.0956, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0493  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.047e-02\n",
      "‖w_svm‖₂       : 0.13931854444599312\n",
      "‖alpha‖₁       : 0.6551281900643254\n",
      "scores min/max : -18.068700517694506 1.793722399551872\n",
      "Mask mean value:  tensor(0.2342, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2786  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.744e-03\n",
      "‖w_svm‖₂       : 0.13861635238190806\n",
      "‖alpha‖₁       : 0.8799999999999825\n",
      "scores min/max : -1.374831743343735 3.6258520236225675\n",
      "Mask mean value:  tensor(0.2939, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2548  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.518e-04\n",
      "‖w_svm‖₂       : 0.00014131790208306145\n",
      "‖alpha‖₁       : 0.4399999998057432\n",
      "scores min/max : -0.00021361478266087946 -7.113505702490677e-05\n",
      "Mask mean value:  tensor(0.4994, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.690e-17\n",
      "‖w_svm‖₂       : 0.004274115555606423\n",
      "‖alpha‖₁       : 0.45999999999999974\n",
      "scores min/max : -0.006680241476201169 -0.004726979782587945\n",
      "Mask mean value:  tensor(0.4741, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.0606  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.023e-15\n",
      "‖w_svm‖₂       : 2.1858955965657724e-08\n",
      "‖alpha‖₁       : 0.11999999999999435\n",
      "scores min/max : -2.5044733709735638e-08 -1.0360504934806164e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.786e-09\n",
      "‖w_svm‖₂       : 8.683829315586516e-08\n",
      "‖alpha‖₁       : 0.1799999999999957\n",
      "scores min/max : -8.374454844166961e-08 1.5116012555804756e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.664e-21\n",
      "‖w_svm‖₂       : 1.7360899503579026e-05\n",
      "‖alpha‖₁       : 0.359999999982577\n",
      "scores min/max : 1.5012727439629968e-05 1.608538204007907e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3080  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.311e-17\n",
      "‖w_svm‖₂       : 0.07690214773489307\n",
      "‖alpha‖₁       : 0.6583758988269188\n",
      "scores min/max : -2.0707071606878134 4.054479112818109\n",
      "Mask mean value:  tensor(0.2925, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.3832  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.248e-03\n",
      "‖w_svm‖₂       : 0.04759761697668189\n",
      "‖alpha‖₁       : 0.9390152574745596\n",
      "scores min/max : -2.449066401235469 1.5948609922402879\n",
      "Mask mean value:  tensor(0.1523, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4097  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.797e-03\n",
      "‖w_svm‖₂       : 4.3576680126078104e-07\n",
      "‖alpha‖₁       : 0.7199999999996289\n",
      "scores min/max : 3.038387773323474e-07 3.6137336394066046e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.014e-21\n",
      "‖w_svm‖₂       : 0.00030333207959488967\n",
      "‖alpha‖₁       : 0.4199999999997962\n",
      "scores min/max : 0.00020002232216113181 0.0005117832232114626\n",
      "Mask mean value:  tensor(0.5024, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0003  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.752e-15\n",
      "‖w_svm‖₂       : 0.016616002092970854\n",
      "‖alpha‖₁       : 0.8599999999999928\n",
      "scores min/max : -0.04531866183667008 -0.02991290646437103\n",
      "Mask mean value:  tensor(0.3361, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0823  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.170e-14\n",
      "‖w_svm‖₂       : 5.44116025131203e-08\n",
      "‖alpha‖₁       : 0.11999999999999851\n",
      "scores min/max : -8.395347427382738e-08 -5.3290846815553624e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.135e-08\n",
      "‖w_svm‖₂       : 0.05720608973679175\n",
      "‖alpha‖₁       : 0.5762220820228137\n",
      "scores min/max : -1.9581891234752513 0.8694135949218761\n",
      "Mask mean value:  tensor(0.7073, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9255  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.019e-03\n",
      "‖w_svm‖₂       : 0.04923714775939961\n",
      "‖alpha‖₁       : 0.7294213827829603\n",
      "scores min/max : -0.24801753457696799 2.034489809196509\n",
      "Mask mean value:  tensor(0.7193, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2292  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.170e-03\n",
      "‖w_svm‖₂       : 1.959823720029146e-07\n",
      "‖alpha‖₁       : 0.3799999999999554\n",
      "scores min/max : -5.647445946290188e-08 -2.057635568640141e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.634e-19\n",
      "‖w_svm‖₂       : 2.2747354660983975e-07\n",
      "‖alpha‖₁       : 0.2599999999999999\n",
      "scores min/max : 2.4038073316380087e-08 3.676035878129952e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.641e-17\n",
      "‖w_svm‖₂       : 0.03936019894005857\n",
      "‖alpha‖₁       : 0.9402783336171814\n",
      "scores min/max : -1.8282122397399174 0.30562419286719034\n",
      "Mask mean value:  tensor(0.4245, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3269  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.211e-10\n",
      "‖w_svm‖₂       : 0.08668927274959316\n",
      "‖alpha‖₁       : 0.5780954498354223\n",
      "scores min/max : -2.1162745036018618 5.814845362045757\n",
      "Mask mean value:  tensor(0.2829, dtype=torch.float64)\n",
      "max feasible return = 0.1129  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.0135195851176787e-07\n",
      "‖alpha‖₁       : 0.5799999999999932\n",
      "scores min/max : -3.7912593985405693e-07 -2.8818512053570596e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0381227228875235e-07\n",
      "‖alpha‖₁       : 0.2999999999999996\n",
      "scores min/max : 3.7210121684102126e-08 4.218340262219027e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 7.034510752999972e-08\n",
      "‖alpha‖₁       : 0.5999999999999895\n",
      "scores min/max : 1.5542850962533732e-08 5.23268917206939e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.7149633797290945e-08\n",
      "‖alpha‖₁       : 0.37999999999998224\n",
      "scores min/max : 6.237851929948971e-09 1.5675020280157027e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.017408611535856e-06\n",
      "‖alpha‖₁       : 0.3199999999759239\n",
      "scores min/max : 2.8030237263062583e-06 4.5905122664624475e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4258  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.05773102483310366\n",
      "‖alpha‖₁       : 0.7397131583956438\n",
      "scores min/max : -3.472743177830829 2.156844479833988\n",
      "Mask mean value:  tensor(0.8830, dtype=torch.float64)\n",
      "max feasible return = -0.9276  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03374014568897392\n",
      "‖alpha‖₁       : 0.6151666121168415\n",
      "scores min/max : -1.9085528343693068 1.2243514362907477\n",
      "Mask mean value:  tensor(0.8003, dtype=torch.float64)\n",
      "max feasible return = 2.8085  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.7213967395831706e-07\n",
      "‖alpha‖₁       : 0.4599999999999751\n",
      "scores min/max : 2.077412946541786e-07 8.027344863492621e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.3802620339819243e-07\n",
      "‖alpha‖₁       : 0.5199999999999939\n",
      "scores min/max : 6.348231950856465e-08 7.638979644177856e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.5901547164357016e-07\n",
      "‖alpha‖₁       : 0.5799999999999788\n",
      "scores min/max : -2.630513845323493e-07 -2.2671751949565793e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.04516423837865027\n",
      "‖alpha‖₁       : 0.6877098148220252\n",
      "scores min/max : -0.4273863912391218 1.9617885199046958\n",
      "Mask mean value:  tensor(0.4224, dtype=torch.float64)\n",
      "max feasible return = 0.0256  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03347940762856468\n",
      "‖alpha‖₁       : 0.4221441928520271\n",
      "scores min/max : -3.9173008859925447 5.208837680999274\n",
      "Mask mean value:  tensor(0.0757, dtype=torch.float64)\n",
      "max feasible return = 0.4771  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04620043511520728\n",
      "‖alpha‖₁       : 0.8211484803140545\n",
      "scores min/max : -5.106644055773991 3.0568292728797415\n",
      "Mask mean value:  tensor(0.9388, dtype=torch.float64)\n",
      "max feasible return = -3.5423  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.008654318365148721\n",
      "‖alpha‖₁       : 0.7999999999999998\n",
      "scores min/max : -0.04187460853992013 0.0074844991763031495\n",
      "Mask mean value:  tensor(0.4752, dtype=torch.float64)\n",
      "max feasible return = -0.1728  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.1356309041215283e-07\n",
      "‖alpha‖₁       : 0.6199999999999561\n",
      "scores min/max : 3.987757277596857e-08 5.6423305793925937e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.76944040741752e-08\n",
      "‖alpha‖₁       : 0.43999999999996775\n",
      "scores min/max : 1.0062695522448182e-08 1.964482155356127e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.2845950898563389e-07\n",
      "‖alpha‖₁       : 0.27999999999999914\n",
      "scores min/max : -3.464855170839272e-07 -3.2887242008536265e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  21 | train 0.005390 | val 0.006728\n",
      "-----------------------------------------Epoch:  22 ----------------------------------------\n",
      "‖w_svm‖₂       : 2.2748658595593934e-07\n",
      "‖alpha‖₁       : 0.25999999999999984\n",
      "scores min/max : 2.4115880170212347e-08 3.6837635546061906e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.641e-17\n",
      "‖w_svm‖₂       : 0.14728555358072337\n",
      "‖alpha‖₁       : 0.7596407651281607\n",
      "scores min/max : -1.7407304414599587 2.2367932143504574\n",
      "Mask mean value:  tensor(0.8770, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3590  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.769e-11\n",
      "‖w_svm‖₂       : 0.003153961797888915\n",
      "‖alpha‖₁       : 0.5799999999999921\n",
      "scores min/max : 0.0022813763578213497 0.0035428754546425606\n",
      "Mask mean value:  tensor(0.5164, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3284  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.227e-13\n",
      "‖w_svm‖₂       : 0.04923279146784755\n",
      "‖alpha‖₁       : 0.7294213717569911\n",
      "scores min/max : -0.2478480940028475 2.034658419714672\n",
      "Mask mean value:  tensor(0.7198, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2313  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.166e-03\n",
      "‖w_svm‖₂       : 0.13899413595461693\n",
      "‖alpha‖₁       : 0.8800000000000001\n",
      "scores min/max : -1.3784290638223111 3.650527475141158\n",
      "Mask mean value:  tensor(0.3026, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2571  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.128e-04\n",
      "‖w_svm‖₂       : 0.02831415312986709\n",
      "‖alpha‖₁       : 0.5510374351495544\n",
      "scores min/max : -3.5416657463644095 1.0302246677386404\n",
      "Mask mean value:  tensor(0.1185, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9928  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.992e-03\n",
      "‖w_svm‖₂       : 0.017582615930261582\n",
      "‖alpha‖₁       : 0.6815230582155974\n",
      "scores min/max : -1.976116252197728 0.053307523334134625\n",
      "Mask mean value:  tensor(0.5941, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2452  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.599e-14\n",
      "‖w_svm‖₂       : 0.02203062512754332\n",
      "‖alpha‖₁       : 0.823363145367517\n",
      "scores min/max : -1.8914851737715748 1.507739075211713\n",
      "Mask mean value:  tensor(0.7212, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.4377  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.127e-06\n",
      "‖w_svm‖₂       : 0.047530106180845175\n",
      "‖alpha‖₁       : 0.9056468068594199\n",
      "scores min/max : -0.724105686380413 1.8884975317036838\n",
      "Mask mean value:  tensor(0.3668, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6798  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.929e-03\n",
      "‖w_svm‖₂       : 0.0571991934474751\n",
      "‖alpha‖₁       : 0.5762224489405173\n",
      "scores min/max : -1.9585158470027386 0.8691053792041858\n",
      "Mask mean value:  tensor(0.7063, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9226  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.030e-03\n",
      "‖w_svm‖₂       : 2.3754312061580074e-07\n",
      "‖alpha‖₁       : 0.6399999999999977\n",
      "scores min/max : 3.238616407498414e-07 3.44746567695462e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.593e-20\n",
      "‖w_svm‖₂       : 0.0707564942939541\n",
      "‖alpha‖₁       : 0.5799999999999996\n",
      "scores min/max : -2.760719291866387 1.562468015296798\n",
      "Mask mean value:  tensor(0.1633, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3033  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.292e-04\n",
      "‖w_svm‖₂       : 1.0881913770311782e-06\n",
      "‖alpha‖₁       : 0.4999999999999917\n",
      "scores min/max : -2.8925792310052187e-07 -9.623733352883825e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.967e-19\n",
      "‖w_svm‖₂       : 0.02359807749440274\n",
      "‖alpha‖₁       : 0.8150931128298815\n",
      "scores min/max : -11.523803195369123 2.033566073279821\n",
      "Mask mean value:  tensor(0.7126, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9113  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.648e-06\n",
      "‖w_svm‖₂       : 0.13902657470511826\n",
      "‖alpha‖₁       : 0.6550555629044059\n",
      "scores min/max : -18.05995205740727 1.80394909568558\n",
      "Mask mean value:  tensor(0.2428, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2813  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.028e-03\n",
      "‖w_svm‖₂       : 0.005610430768041559\n",
      "‖alpha‖₁       : 0.5599999999999998\n",
      "scores min/max : 0.004011861872943907 0.005645724293050945\n",
      "Mask mean value:  tensor(0.5215, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0932  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.249e-05\n",
      "‖w_svm‖₂       : 0.0761798244050633\n",
      "‖alpha‖₁       : 0.41910686848501055\n",
      "scores min/max : -1.8660558889796053 2.393455962687797\n",
      "Mask mean value:  tensor(0.7974, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8921  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.912e-04\n",
      "‖w_svm‖₂       : 0.007124693601553776\n",
      "‖alpha‖₁       : 0.6076736826921593\n",
      "scores min/max : -1.9850786871513635 0.30186878097107706\n",
      "Mask mean value:  tensor(0.5792, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2340  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.038e-15\n",
      "‖w_svm‖₂       : 0.028413906808051934\n",
      "‖alpha‖₁       : 0.19667622509338217\n",
      "scores min/max : -2.2053461763658384 0.01930069913488852\n",
      "Mask mean value:  tensor(0.0579, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3915  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.770e-04\n",
      "‖w_svm‖₂       : 2.3934729309345427e-07\n",
      "‖alpha‖₁       : 0.5800000000000001\n",
      "scores min/max : -3.9361449852883597e-07 -3.7799190792869385e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.934e-19\n",
      "‖w_svm‖₂       : 1.1668850002326726e-07\n",
      "‖alpha‖₁       : 0.29999999999997984\n",
      "scores min/max : 1.0397843176899285e-07 1.1304715252589471e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.246e-07\n",
      "‖w_svm‖₂       : 0.018701597182531\n",
      "‖alpha‖₁       : 0.8599999999999828\n",
      "scores min/max : -0.04795336599401402 0.028486660326566708\n",
      "Mask mean value:  tensor(0.4160, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5370  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.366e-14\n",
      "‖w_svm‖₂       : 0.0435882378177625\n",
      "‖alpha‖₁       : 0.8900026879769565\n",
      "scores min/max : -2.0064059607816844 0.3410537828156841\n",
      "Mask mean value:  tensor(0.2731, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0755  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.493e-11\n",
      "‖w_svm‖₂       : 0.020440190720452847\n",
      "‖alpha‖₁       : 0.3833729511970853\n",
      "scores min/max : -1.969495723429536 0.2313364453115781\n",
      "Mask mean value:  tensor(0.6802, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1455  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.932e-14\n",
      "‖w_svm‖₂       : 6.755226746579948e-07\n",
      "‖alpha‖₁       : 0.39999999999999947\n",
      "scores min/max : -6.407244436936517e-08 2.4194927613670073e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.258e-09\n",
      "‖w_svm‖₂       : 0.11487714080276902\n",
      "‖alpha‖₁       : 0.8732646122166239\n",
      "scores min/max : -12.219953379529397 2.1313856455628852\n",
      "Mask mean value:  tensor(0.4860, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.4392  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.653e-02\n",
      "‖w_svm‖₂       : 1.1921362116708843e-06\n",
      "‖alpha‖₁       : 0.3199999999984676\n",
      "scores min/max : -2.0327212757031613e-06 -1.915993394157613e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4898  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.790e-18\n",
      "‖w_svm‖₂       : 0.0768058686044058\n",
      "‖alpha‖₁       : 0.6583712226184928\n",
      "scores min/max : -2.0634162697789065 4.064323781986117\n",
      "Mask mean value:  tensor(0.3036, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.5147  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.087e-03\n",
      "‖w_svm‖₂       : 0.0056679425687545405\n",
      "‖alpha‖₁       : 0.7005648847540653\n",
      "scores min/max : -2.004604013533907 0.018785695977817533\n",
      "Mask mean value:  tensor(0.4519, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2535  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.057e-05\n",
      "‖w_svm‖₂       : 0.0003107216393172209\n",
      "‖alpha‖₁       : 0.4199999993256246\n",
      "scores min/max : 0.00020357560881492675 0.000523084053535608\n",
      "Mask mean value:  tensor(0.5025, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0004  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.790e-15\n",
      "‖w_svm‖₂       : 2.013205969295983e-07\n",
      "‖alpha‖₁       : 0.41999999999997933\n",
      "scores min/max : -3.534575618450405e-07 -3.311825190088126e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.607e-19\n",
      "‖w_svm‖₂       : 0.18729867574058545\n",
      "‖alpha‖₁       : 0.8837808117653616\n",
      "scores min/max : -3.5823989967412913 6.081761730270019\n",
      "Mask mean value:  tensor(0.0950, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1939  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.746e-03\n",
      "‖w_svm‖₂       : 0.000504176988012741\n",
      "‖alpha‖₁       : 0.43999999999999995\n",
      "scores min/max : -0.0006526938322271442 -0.00039477815991888795\n",
      "Mask mean value:  tensor(0.4969, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6452  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.078e-16\n",
      "‖w_svm‖₂       : 1.126702213164972e-07\n",
      "‖alpha‖₁       : 0.5199999999999997\n",
      "scores min/max : 3.196869714018011e-07 3.410890292558334e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.553e-19\n",
      "‖w_svm‖₂       : 7.801911277590009e-08\n",
      "‖alpha‖₁       : 0.6599999999999746\n",
      "scores min/max : 1.0198082541976297e-07 2.468435713962727e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.755e-09\n",
      "‖w_svm‖₂       : 0.02402649025418419\n",
      "‖alpha‖₁       : 0.6599999999999999\n",
      "scores min/max : -0.04875035158737103 0.054307970554466053\n",
      "Mask mean value:  tensor(0.4359, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.4605  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.829e-15\n",
      "‖w_svm‖₂       : 7.729527315428598e-08\n",
      "‖alpha‖₁       : 0.5399999999999575\n",
      "scores min/max : -1.113142679099899e-07 -8.967444927274824e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.584e-20\n",
      "‖w_svm‖₂       : 0.03255956036690714\n",
      "‖alpha‖₁       : 0.8985779044224023\n",
      "scores min/max : -0.7591152169880113 2.0011321978097723\n",
      "Mask mean value:  tensor(0.8276, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3893  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.532e-15\n",
      "‖w_svm‖₂       : 4.299154412813368e-07\n",
      "‖alpha‖₁       : 0.7199999999999985\n",
      "scores min/max : 3.0622978108577443e-07 3.634383334804409e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.968e-21\n",
      "‖w_svm‖₂       : 0.004269546157505904\n",
      "‖alpha‖₁       : 0.4599999999999985\n",
      "scores min/max : -0.006652111702962049 -0.004708838288192817\n",
      "Mask mean value:  tensor(0.4742, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.0610  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.020e-15\n",
      "‖w_svm‖₂       : 3.695497878253025e-06\n",
      "‖alpha‖₁       : 0.4199999999926032\n",
      "scores min/max : -1.0811214285895035e-06 5.646747862553928e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1230  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.573e-05\n",
      "‖w_svm‖₂       : 2.826604584541829e-07\n",
      "‖alpha‖₁       : 0.6599999999999999\n",
      "scores min/max : -6.143208609023957e-07 -4.6269152044988575e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.317e-18\n",
      "‖w_svm‖₂       : 7.285813103781483e-08\n",
      "‖alpha‖₁       : 0.41999999999999965\n",
      "scores min/max : -7.950816326710236e-08 -3.2368036226595846e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.759e-08\n",
      "‖w_svm‖₂       : 0.02636289580570277\n",
      "‖alpha‖₁       : 0.8522555179363639\n",
      "scores min/max : -2.926336937704745 1.5546303275998945\n",
      "Mask mean value:  tensor(0.1510, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9457  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.315e-03\n",
      "‖w_svm‖₂       : 0.04654880195451584\n",
      "‖alpha‖₁       : 0.9199999999998688\n",
      "scores min/max : -0.3379417556672544 0.46513812409939237\n",
      "Mask mean value:  tensor(0.3353, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.6657  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.149e-02\n",
      "‖w_svm‖₂       : 0.017072968274160518\n",
      "‖alpha‖₁       : 0.5962555459525352\n",
      "scores min/max : -2.948590256566459 2.2603150822451483\n",
      "Mask mean value:  tensor(0.9097, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.4982  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.053e-04\n",
      "‖w_svm‖₂       : 5.723379300533471e-08\n",
      "‖alpha‖₁       : 0.23999999999999194\n",
      "scores min/max : -7.224523688230205e-08 -5.77983801828161e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.327e-20\n",
      "‖w_svm‖₂       : 5.510846548603114e-08\n",
      "‖alpha‖₁       : 0.43999999999998524\n",
      "scores min/max : -1.5017774460980616e-07 -4.6194428743905674e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.301e-20\n",
      "‖w_svm‖₂       : 0.01658035238458907\n",
      "‖alpha‖₁       : 0.8599999999999957\n",
      "scores min/max : -0.04471394273711283 -0.029458397832396996\n",
      "Mask mean value:  tensor(0.3383, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0832  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.148e-14\n",
      "‖w_svm‖₂       : 0.061298120825095746\n",
      "‖alpha‖₁       : 0.8986307833290683\n",
      "scores min/max : -1.7071079250775554 3.700774819826493\n",
      "Mask mean value:  tensor(0.4117, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5645  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.275e-02\n",
      "‖w_svm‖₂       : 0.0001372057974423941\n",
      "‖alpha‖₁       : 0.43999999995014916\n",
      "scores min/max : -0.00020581605353065847 -6.877968671588965e-05\n",
      "Mask mean value:  tensor(0.4994, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0241  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.618e-17\n",
      "‖w_svm‖₂       : 0.00021027368380535525\n",
      "‖alpha‖₁       : 0.6199999999986353\n",
      "scores min/max : 4.58143846767495e-05 6.148231183087094e-05\n",
      "Mask mean value:  tensor(0.5003, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.077e-17\n",
      "‖w_svm‖₂       : 0.005340837857930167\n",
      "‖alpha‖₁       : 0.3799999999999863\n",
      "scores min/max : -0.006366787173162108 0.005419277951601187\n",
      "Mask mean value:  tensor(0.5119, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9109  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.741e-14\n",
      "‖w_svm‖₂       : 1.7050816199648787e-07\n",
      "‖alpha‖₁       : 0.23999999999998953\n",
      "scores min/max : 2.8344204513688043e-07 2.9882379945165124e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.161e-20\n",
      "‖w_svm‖₂       : 0.08245349127076293\n",
      "‖alpha‖₁       : 0.4719654540242821\n",
      "scores min/max : -2.309628680873766 2.7734137628235715\n",
      "Mask mean value:  tensor(0.1071, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1688  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.326e-02\n",
      "‖w_svm‖₂       : 1.0213258763505445e-06\n",
      "‖alpha‖₁       : 0.5999999999999921\n",
      "scores min/max : -6.327145362283451e-08 -1.2385126349387216e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.492e-19\n",
      "‖w_svm‖₂       : 0.00018511008942249542\n",
      "‖alpha‖₁       : 0.8199999999999992\n",
      "scores min/max : -5.385710295415891e-05 -3.965405753864575e-05\n",
      "Mask mean value:  tensor(0.4998, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6971  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.200e-17\n",
      "‖w_svm‖₂       : 0.04912564825675255\n",
      "‖alpha‖₁       : 0.659999999999997\n",
      "scores min/max : -0.31012628054924357 0.23196916485819236\n",
      "Mask mean value:  tensor(0.5017, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8986  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.053e-13\n",
      "‖w_svm‖₂       : 2.5154685178409423e-07\n",
      "‖alpha‖₁       : 0.37999999999117556\n",
      "scores min/max : -2.965609916573995e-08 -9.587741050461332e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.882e-07\n",
      "‖w_svm‖₂       : 4.8460368316376557e-08\n",
      "‖alpha‖₁       : 0.17999999999999775\n",
      "scores min/max : 4.9243467005903515e-08 6.391000575006706e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 4.942e-08\n",
      "‖w_svm‖₂       : 2.180078517601342e-08\n",
      "‖alpha‖₁       : 0.1199999999999944\n",
      "scores min/max : -2.473372841669095e-08 -1.0051270781699485e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.781e-09\n",
      "‖w_svm‖₂       : 0.00015948000301996209\n",
      "‖alpha‖₁       : 0.6399999999999846\n",
      "scores min/max : -9.481803182489321e-05 -8.511308042329286e-05\n",
      "Mask mean value:  tensor(0.4996, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7637  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.149e-17\n",
      "‖w_svm‖₂       : 1.7378932316545802e-05\n",
      "‖alpha‖₁       : 0.3599999999825441\n",
      "scores min/max : 1.4929410710685126e-05 1.6004751392452576e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.320e-17\n",
      "‖w_svm‖₂       : 3.8429767870206036e-07\n",
      "‖alpha‖₁       : 0.27999999999999603\n",
      "scores min/max : -1.5328704011546617e-06 -1.4711360655550523e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1957  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.455e-19\n",
      "‖w_svm‖₂       : 0.04744014900439038\n",
      "‖alpha‖₁       : 0.9390069526928728\n",
      "scores min/max : -2.4577070533890177 1.5865814065615829\n",
      "Mask mean value:  tensor(0.1466, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3994  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.264e-03\n",
      "‖w_svm‖₂       : 8.333980953662893e-08\n",
      "‖alpha‖₁       : 0.38\n",
      "scores min/max : 2.7488715320340523e-09 2.0895571514057833e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.642e-22\n",
      "‖w_svm‖₂       : 0.02018158368460154\n",
      "‖alpha‖₁       : 0.7799999999999989\n",
      "scores min/max : -0.01822656930781812 0.051032528388876175\n",
      "Mask mean value:  tensor(0.4386, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9185  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.898e-15\n",
      "‖w_svm‖₂       : 1.0790006996744538e-07\n",
      "‖alpha‖₁       : 0.2399999999999839\n",
      "scores min/max : 2.137343337968061e-08 3.2330424864032596e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.409e-21\n",
      "‖w_svm‖₂       : 7.350868827827242e-08\n",
      "‖alpha‖₁       : 0.139999999999979\n",
      "scores min/max : 4.854812383617575e-09 1.0702327343102197e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.012e-08\n",
      "‖w_svm‖₂       : 1.9586723592994677e-07\n",
      "‖alpha‖₁       : 0.3799999999999586\n",
      "scores min/max : -5.358926002205504e-08 -1.770092626738517e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.631e-19\n",
      "‖w_svm‖₂       : 0.0003595492013107711\n",
      "‖alpha‖₁       : 0.7399999999999994\n",
      "scores min/max : -0.00020414069698277413 -0.0001876516254945306\n",
      "Mask mean value:  tensor(0.4990, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0623  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.114e-17\n",
      "‖w_svm‖₂       : 0.0008814638852996927\n",
      "‖alpha‖₁       : 0.8199999999999991\n",
      "scores min/max : 0.0010104936750789956 0.0027521261357049617\n",
      "Mask mean value:  tensor(0.5105, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5935  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.005e-16\n",
      "‖w_svm‖₂       : 8.674539036731586e-08\n",
      "‖alpha‖₁       : 0.1799999999999951\n",
      "scores min/max : -8.885213114117493e-08 1.462059749979586e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.663e-21\n",
      "‖w_svm‖₂       : 0.03924031927645032\n",
      "‖alpha‖₁       : 0.9402857072883182\n",
      "scores min/max : -1.8324389999311101 0.30129309601038473\n",
      "Mask mean value:  tensor(0.4076, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3250  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.544e-12\n",
      "‖w_svm‖₂       : 0.053166591491912996\n",
      "‖alpha‖₁       : 0.8271559885298586\n",
      "scores min/max : -1.954223160698867 0.39514548880263844\n",
      "Mask mean value:  tensor(0.5062, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0048  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.764e-02\n",
      "‖w_svm‖₂       : 5.4268949450446134e-08\n",
      "‖alpha‖₁       : 0.11999999999999855\n",
      "scores min/max : -8.447141330678666e-08 -5.3812005259131146e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.146e-08\n",
      "‖w_svm‖₂       : 0.0863934322611305\n",
      "‖alpha‖₁       : 0.578046183810398\n",
      "scores min/max : -2.103915083107769 5.826971146071084\n",
      "Mask mean value:  tensor(0.3072, dtype=torch.float64)\n",
      "max feasible return = 0.1177  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.0096154869312e-07\n",
      "‖alpha‖₁       : 0.5799999999999863\n",
      "scores min/max : -3.89461193477087e-07 -2.9846989992020437e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0371586070441811e-07\n",
      "‖alpha‖₁       : 0.2999999999999995\n",
      "scores min/max : 4.034253548113344e-08 4.53173815056954e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 7.015617805501445e-08\n",
      "‖alpha‖₁       : 0.5999999999999831\n",
      "scores min/max : 1.8717443685007284e-08 5.550017419246223e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.7008195637614056e-08\n",
      "‖alpha‖₁       : 0.3799999999999818\n",
      "scores min/max : 8.043951524609906e-09 1.7480871397155066e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.0087044216844453e-06\n",
      "‖alpha‖₁       : 0.3199999999764484\n",
      "scores min/max : 2.9001548083846946e-06 4.668883827297452e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4258  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.057102196587903546\n",
      "‖alpha‖₁       : 0.7397109724616029\n",
      "scores min/max : -3.47596409868158 2.154695391671722\n",
      "Mask mean value:  tensor(0.8811, dtype=torch.float64)\n",
      "max feasible return = -0.9265  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.0333232897113915\n",
      "‖alpha‖₁       : 0.6151663774895927\n",
      "scores min/max : -1.9090619275359617 1.223568303527598\n",
      "Mask mean value:  tensor(0.7997, dtype=torch.float64)\n",
      "max feasible return = 2.8058  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.7173229907592124e-07\n",
      "‖alpha‖₁       : 0.4599999999999682\n",
      "scores min/max : 2.1464730462385358e-07 8.09914845357727e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.3795082706205456e-07\n",
      "‖alpha‖₁       : 0.5199999999999931\n",
      "scores min/max : 6.197378366388079e-08 7.488228013870624e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.587599944310542e-07\n",
      "‖alpha‖₁       : 0.5799999999999779\n",
      "scores min/max : -2.6848338847043907e-07 -2.3214108634707925e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.04483986373441525\n",
      "‖alpha‖₁       : 0.6877005971987501\n",
      "scores min/max : -0.42338953585066197 1.9656009069360079\n",
      "Mask mean value:  tensor(0.4368, dtype=torch.float64)\n",
      "max feasible return = 0.0107  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.033287319590990065\n",
      "‖alpha‖₁       : 0.42214541739138534\n",
      "scores min/max : -3.9161075000949057 5.2122483461500595\n",
      "Mask mean value:  tensor(0.0764, dtype=torch.float64)\n",
      "max feasible return = 0.4818  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04598300977180799\n",
      "‖alpha‖₁       : 0.8211485777765466\n",
      "scores min/max : -5.10118180688567 3.062741360819217\n",
      "Mask mean value:  tensor(0.9403, dtype=torch.float64)\n",
      "max feasible return = -3.5516  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.00854472481629684\n",
      "‖alpha‖₁       : 0.7999999999999998\n",
      "scores min/max : -0.04088477345521506 0.0073008873646427455\n",
      "Mask mean value:  tensor(0.4758, dtype=torch.float64)\n",
      "max feasible return = -0.1730  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.129916374723056e-07\n",
      "‖alpha‖₁       : 0.6199999999999555\n",
      "scores min/max : 4.084819112731191e-08 5.7396227179561346e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.766272683061491e-08\n",
      "‖alpha‖₁       : 0.439999999999961\n",
      "scores min/max : 9.963378605959525e-09 1.9547076625670165e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.2828021273260727e-07\n",
      "‖alpha‖₁       : 0.27999999999999914\n",
      "scores min/max : -3.5402033173883595e-07 -3.3640207228428656e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  22 | train 0.005385 | val 0.006725\n",
      "-----------------------------------------Epoch:  23 ----------------------------------------\n",
      "‖w_svm‖₂       : 0.04771921612270105\n",
      "‖alpha‖₁       : 0.9056472668588501\n",
      "scores min/max : -0.7308156038487528 1.8859822020331816\n",
      "Mask mean value:  tensor(0.3605, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6678  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.663e-03\n",
      "‖w_svm‖₂       : 0.00013838449632796706\n",
      "‖alpha‖₁       : 0.4399999999254004\n",
      "scores min/max : -0.00020828673079146848 -7.060996687197014e-05\n",
      "Mask mean value:  tensor(0.4994, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.641e-17\n",
      "‖w_svm‖₂       : 0.005358189189425342\n",
      "‖alpha‖₁       : 0.37999999999999995\n",
      "scores min/max : -0.006516646406082988 0.005345413753931216\n",
      "Mask mean value:  tensor(0.5115, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9100  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.759e-14\n",
      "‖w_svm‖₂       : 0.07565210897817877\n",
      "‖alpha‖₁       : 0.4190810878473\n",
      "scores min/max : -1.8675664920954997 2.380594006839873\n",
      "Mask mean value:  tensor(0.7965, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8926  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.959e-04\n",
      "‖w_svm‖₂       : 0.04750762578486561\n",
      "‖alpha‖₁       : 0.9390128509970853\n",
      "scores min/max : -2.451395653799861 1.5929258002753217\n",
      "Mask mean value:  tensor(0.1509, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4075  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.924e-03\n",
      "‖w_svm‖₂       : 2.4872056692251963e-07\n",
      "‖alpha‖₁       : 0.379999999991397\n",
      "scores min/max : -3.122642886889844e-08 -1.1279368148436187e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.681e-07\n",
      "‖w_svm‖₂       : 1.7057730314051916e-07\n",
      "‖alpha‖₁       : 0.23999999999998944\n",
      "scores min/max : 2.8217030835747956e-07 2.97550806550608e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.167e-20\n",
      "‖w_svm‖₂       : 0.04352814901156582\n",
      "‖alpha‖₁       : 0.890006882475104\n",
      "scores min/max : -2.0061041293295037 0.3413933789091064\n",
      "Mask mean value:  tensor(0.2739, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0756  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.187e-14\n",
      "‖w_svm‖₂       : 1.1255905072255775e-07\n",
      "‖alpha‖₁       : 0.5199999999999996\n",
      "scores min/max : 3.172365380276039e-07 3.386393205312206e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.560e-19\n",
      "‖w_svm‖₂       : 0.05346942321560318\n",
      "‖alpha‖₁       : 0.8271933086205919\n",
      "scores min/max : -1.9459270574779661 0.4034751883690428\n",
      "Mask mean value:  tensor(0.5361, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0104  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.585e-02\n",
      "‖w_svm‖₂       : 0.03251574616483057\n",
      "‖alpha‖₁       : 0.8985851880077063\n",
      "scores min/max : -0.757863430056587 2.0023872766943542\n",
      "Mask mean value:  tensor(0.8296, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3856  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.347e-15\n",
      "‖w_svm‖₂       : 6.745215340860305e-07\n",
      "‖alpha‖₁       : 0.39999999999999963\n",
      "scores min/max : -5.781931811559341e-08 2.483480107851624e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.260e-09\n",
      "‖w_svm‖₂       : 1.1653532777175472e-07\n",
      "‖alpha‖₁       : 0.2999999999999795\n",
      "scores min/max : 1.0366527392867993e-07 1.1272434918020763e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.246e-07\n",
      "‖w_svm‖₂       : 0.0008771851372247825\n",
      "‖alpha‖₁       : 0.8199999999999982\n",
      "scores min/max : 0.0008771745560898232 0.0026053209316681527\n",
      "Mask mean value:  tensor(0.5098, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5899  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.010e-16\n",
      "‖w_svm‖₂       : 0.0003582560400795006\n",
      "‖alpha‖₁       : 0.7399999999994422\n",
      "scores min/max : -0.00022407410121472654 -0.00020770344489069639\n",
      "Mask mean value:  tensor(0.4989, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0620  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.110e-17\n",
      "‖w_svm‖₂       : 0.04901046731196133\n",
      "‖alpha‖₁       : 0.6599999999999902\n",
      "scores min/max : -0.3094288156412961 0.23021665883367715\n",
      "Mask mean value:  tensor(0.4985, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8927  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.103e-13\n",
      "‖w_svm‖₂       : 0.007208496410152018\n",
      "‖alpha‖₁       : 0.6076748269182612\n",
      "scores min/max : -1.985291405695269 0.3016429481778827\n",
      "Mask mean value:  tensor(0.5783, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2337  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.183e-15\n",
      "‖w_svm‖₂       : 1.0891418079461852e-06\n",
      "‖alpha‖₁       : 0.49999999999999145\n",
      "scores min/max : -2.9254297586330063e-07 -9.952501647614037e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.980e-19\n",
      "‖w_svm‖₂       : 0.1402476745913035\n",
      "‖alpha‖₁       : 0.6553980659804287\n",
      "scores min/max : -18.101494168715895 1.7627437641259216\n",
      "Mask mean value:  tensor(0.2108, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2673  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.503e-03\n",
      "‖w_svm‖₂       : 5.716090223172629e-08\n",
      "‖alpha‖₁       : 0.2399999999999911\n",
      "scores min/max : -7.115594829356884e-08 -5.6697398373992955e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.334e-20\n",
      "‖w_svm‖₂       : 0.02843361388936325\n",
      "‖alpha‖₁       : 0.19668451036270818\n",
      "scores min/max : -2.2115635907851194 0.012927713842060762\n",
      "Mask mean value:  tensor(0.0524, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3551  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.516e-04\n",
      "‖w_svm‖₂       : 0.01843778598069627\n",
      "‖alpha‖₁       : 0.86\n",
      "scores min/max : -0.04730935331270018 0.027140671659248976\n",
      "Mask mean value:  tensor(0.4153, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5350  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.553e-14\n",
      "‖w_svm‖₂       : 7.353596455644473e-08\n",
      "‖alpha‖₁       : 0.4199999999999147\n",
      "scores min/max : -7.995243014358693e-08 -3.235398188791875e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.849e-08\n",
      "‖w_svm‖₂       : 1.024939776024859e-06\n",
      "‖alpha‖₁       : 0.5999999999999918\n",
      "scores min/max : -3.7750398664601484e-08 1.3135250659684809e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.498e-19\n",
      "‖w_svm‖₂       : 0.13760626270505885\n",
      "‖alpha‖₁       : 0.8800000000000001\n",
      "scores min/max : -1.3367819537270638 3.597409316832166\n",
      "Mask mean value:  tensor(0.3386, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2612  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.069e-10\n",
      "‖w_svm‖₂       : 0.01693300365245682\n",
      "‖alpha‖₁       : 0.5962552460596187\n",
      "scores min/max : -2.953221284771508 2.2580270658698534\n",
      "Mask mean value:  tensor(0.9085, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.4965  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.467e-03\n",
      "‖w_svm‖₂       : 0.1885411765599763\n",
      "‖alpha‖₁       : 0.8843232580091471\n",
      "scores min/max : -3.5851235274738706 6.081523321621158\n",
      "Mask mean value:  tensor(0.0949, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1948  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.902e-03\n",
      "‖w_svm‖₂       : 1.1852346736936806e-06\n",
      "‖alpha‖₁       : 0.3199999999986521\n",
      "scores min/max : -1.9855758431215194e-06 -1.869603766638374e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4898  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.792e-18\n",
      "‖w_svm‖₂       : 5.4539538263513805e-08\n",
      "‖alpha‖₁       : 0.11999999999999861\n",
      "scores min/max : -8.323962313604112e-08 -5.258142534573597e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.007e-08\n",
      "‖w_svm‖₂       : 0.0831917184641149\n",
      "‖alpha‖₁       : 0.47210096572934057\n",
      "scores min/max : -2.3526403146542205 2.73025880129132\n",
      "Mask mean value:  tensor(0.0907, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9974  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.197e-03\n",
      "‖w_svm‖₂       : 0.0031513966712909186\n",
      "‖alpha‖₁       : 0.5799999999999993\n",
      "scores min/max : 0.00268296741281929 0.003943089009975344\n",
      "Mask mean value:  tensor(0.5184, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3296  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.791e-13\n",
      "‖w_svm‖₂       : 3.8571141978539996e-07\n",
      "‖alpha‖₁       : 0.2799999999999959\n",
      "scores min/max : -1.4301316669067695e-06 -1.3683982158363145e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.496e-19\n",
      "‖w_svm‖₂       : 4.294189317620168e-07\n",
      "‖alpha‖₁       : 0.7199999999999985\n",
      "scores min/max : 2.8404488138670645e-07 3.412244265124615e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.990e-21\n",
      "‖w_svm‖₂       : 0.07655859600992543\n",
      "‖alpha‖₁       : 0.6584225616615111\n",
      "scores min/max : -2.0639082107508204 4.084641646571832\n",
      "Mask mean value:  tensor(0.3031, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.5096  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.111e-03\n",
      "‖w_svm‖₂       : 7.743730315284333e-08\n",
      "‖alpha‖₁       : 0.5399999999999624\n",
      "scores min/max : -1.1124751668030151e-07 -8.960805999300086e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.596e-20\n",
      "‖w_svm‖₂       : 0.061764714550432594\n",
      "‖alpha‖₁       : 0.8987064864990048\n",
      "scores min/max : -1.7074838312205798 3.7007062324958158\n",
      "Mask mean value:  tensor(0.4112, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.290e-02\n",
      "‖w_svm‖₂       : 1.0832142484562447e-07\n",
      "‖alpha‖₁       : 0.23999999999998337\n",
      "scores min/max : 2.3986190246613737e-08 3.494712683371323e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.432e-21\n",
      "‖w_svm‖₂       : 7.280193735369167e-08\n",
      "‖alpha‖₁       : 0.13999999999999885\n",
      "scores min/max : 5.342609883566466e-09 1.106646309226828e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.961e-08\n",
      "‖w_svm‖₂       : 0.00030316062612022204\n",
      "‖alpha‖₁       : 0.4199999999990694\n",
      "scores min/max : 0.00018928582065748774 0.0005006868174671369\n",
      "Mask mean value:  tensor(0.5024, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0002  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.766e-15\n",
      "‖w_svm‖₂       : 0.005622973823302194\n",
      "‖alpha‖₁       : 0.5599999999999999\n",
      "scores min/max : 0.004034690286976419 0.005675576835734684\n",
      "Mask mean value:  tensor(0.5216, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0935  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.155e-05\n",
      "‖w_svm‖₂       : 4.849107715744423e-08\n",
      "‖alpha‖₁       : 0.1799999999999977\n",
      "scores min/max : 4.8401362726479725e-08 6.307123474464088e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 4.958e-08\n",
      "‖w_svm‖₂       : 0.000209913311079027\n",
      "‖alpha‖₁       : 0.6199999999988666\n",
      "scores min/max : 3.94348591453294e-05 5.5049287015459706e-05\n",
      "Mask mean value:  tensor(0.5002, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.103e-17\n",
      "‖w_svm‖₂       : 1.7281568464131717e-05\n",
      "‖alpha‖₁       : 0.35999999998282217\n",
      "scores min/max : 1.4700019771000194e-05 1.5765391337641558e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.300e-17\n",
      "‖w_svm‖₂       : 2.8283082538194264e-07\n",
      "‖alpha‖₁       : 0.6599999999999997\n",
      "scores min/max : -6.065300879884328e-07 -4.548942204894966e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.323e-18\n",
      "‖w_svm‖₂       : 8.722777352589061e-08\n",
      "‖alpha‖₁       : 0.17999999999999483\n",
      "scores min/max : -8.498949558024227e-08 1.50118231461583e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.728e-21\n",
      "‖w_svm‖₂       : 0.023723459193475342\n",
      "‖alpha‖₁       : 0.6599999999999999\n",
      "scores min/max : -0.046255379839799826 0.05401499264488341\n",
      "Mask mean value:  tensor(0.4435, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.5166  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.731e-15\n",
      "‖w_svm‖₂       : 2.1797776753500412e-08\n",
      "‖alpha‖₁       : 0.11999999999999432\n",
      "scores min/max : -2.717512814647879e-08 -1.2490917519496787e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.790e-09\n",
      "‖w_svm‖₂       : 2.378168453817452e-07\n",
      "‖alpha‖₁       : 0.6399999999999968\n",
      "scores min/max : 3.283478603738872e-07 3.4923372538416483e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.639e-20\n",
      "‖w_svm‖₂       : 0.0235189853553482\n",
      "‖alpha‖₁       : 0.8150933779463525\n",
      "scores min/max : -11.522846072174591 2.0335937720846813\n",
      "Mask mean value:  tensor(0.7128, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9123  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.521e-06\n",
      "‖w_svm‖₂       : 0.026487992235910748\n",
      "‖alpha‖₁       : 0.8522625004560679\n",
      "scores min/max : -2.920846641330048 1.5601649940100937\n",
      "Mask mean value:  tensor(0.1535, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9489  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.363e-03\n",
      "‖w_svm‖₂       : 0.039411674976750494\n",
      "‖alpha‖₁       : 0.9403024831303647\n",
      "scores min/max : -1.8260285691824132 0.3077093419107209\n",
      "Mask mean value:  tensor(0.4330, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3261  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.343e-10\n",
      "‖w_svm‖₂       : 0.01664582551438183\n",
      "‖alpha‖₁       : 0.8599999999999952\n",
      "scores min/max : -0.04691270376155308 -0.031687359264796454\n",
      "Mask mean value:  tensor(0.3285, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0803  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.244e-14\n",
      "‖w_svm‖₂       : 0.020238744470379832\n",
      "‖alpha‖₁       : 0.38337513027068615\n",
      "scores min/max : -1.9698574973943392 0.23120974820876294\n",
      "Mask mean value:  tensor(0.6784, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1456  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.066e-14\n",
      "‖w_svm‖₂       : 0.11364395523278725\n",
      "‖alpha‖₁       : 0.873267616194327\n",
      "scores min/max : -12.212918834584551 2.136279074205428\n",
      "Mask mean value:  tensor(0.4972, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.4714  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 9.704e-03\n",
      "‖w_svm‖₂       : 0.01736557660098797\n",
      "‖alpha‖₁       : 0.6815242766611945\n",
      "scores min/max : -1.9753170711140462 0.05395628480490197\n",
      "Mask mean value:  tensor(0.5980, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2480  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.557e-14\n",
      "‖w_svm‖₂       : 0.04850206313522122\n",
      "‖alpha‖₁       : 0.729426742746921\n",
      "scores min/max : -0.24928036063410078 2.03306444566501\n",
      "Mask mean value:  tensor(0.7154, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2138  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.210e-03\n",
      "‖w_svm‖₂       : 0.028327858660590385\n",
      "‖alpha‖₁       : 0.5510445702567804\n",
      "scores min/max : -3.536264760638492 1.0353535000472431\n",
      "Mask mean value:  tensor(0.1206, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0067  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.224e-03\n",
      "‖w_svm‖₂       : 2.0142358255481263e-07\n",
      "‖alpha‖₁       : 0.41999999999997895\n",
      "scores min/max : -3.431488129928401e-07 -3.2087780457461225e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.616e-19\n",
      "‖w_svm‖₂       : 8.33709084042961e-08\n",
      "‖alpha‖₁       : 0.37999999999999984\n",
      "scores min/max : 2.047490536817092e-09 2.0195461560910255e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.643e-22\n",
      "‖w_svm‖₂       : 0.0001590154587288991\n",
      "‖alpha‖₁       : 0.6399999999999871\n",
      "scores min/max : -9.27249112579759e-05 -8.307631779693787e-05\n",
      "Mask mean value:  tensor(0.4996, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7637  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.145e-17\n",
      "‖w_svm‖₂       : 0.0219125575691159\n",
      "‖alpha‖₁       : 0.82336750665542\n",
      "scores min/max : -1.892712073496786 1.5062750419216242\n",
      "Mask mean value:  tensor(0.7180, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.4296  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.009e-06\n",
      "‖w_svm‖₂       : 5.516821021885002e-08\n",
      "‖alpha‖₁       : 0.4399999999999853\n",
      "scores min/max : -1.5042800473816048e-07 -4.645507019718269e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.310e-20\n",
      "‖w_svm‖₂       : 0.02021835322050508\n",
      "‖alpha‖₁       : 0.78\n",
      "scores min/max : -0.021092743782154314 0.0484212432081817\n",
      "Mask mean value:  tensor(0.4247, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.8609  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.296e-15\n",
      "‖w_svm‖₂       : 0.14806889211970667\n",
      "‖alpha‖₁       : 0.7598728886513613\n",
      "scores min/max : -1.7408592638591687 2.2364744196964774\n",
      "Mask mean value:  tensor(0.8768, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3592  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.459e-11\n",
      "‖w_svm‖₂       : 2.3936362663590217e-07\n",
      "‖alpha‖₁       : 0.5799999999999998\n",
      "scores min/max : -3.8942948762941727e-07 -3.738116173155646e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.953e-19\n",
      "‖w_svm‖₂       : 7.996632674542685e-08\n",
      "‖alpha‖₁       : 0.659999999999986\n",
      "scores min/max : 1.0169983551205917e-07 2.460732817098544e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.881e-09\n",
      "‖w_svm‖₂       : 0.00018461752788940567\n",
      "‖alpha‖₁       : 0.8199999999999994\n",
      "scores min/max : -5.4254992238311235e-05 -4.012669821445227e-05\n",
      "Mask mean value:  tensor(0.4998, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6971  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.199e-17\n",
      "‖w_svm‖₂       : 0.0005065263690677311\n",
      "‖alpha‖₁       : 0.43999999999999995\n",
      "scores min/max : -0.0006468490841464009 -0.0003867065376658837\n",
      "Mask mean value:  tensor(0.4970, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6453  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.107e-16\n",
      "‖w_svm‖₂       : 0.045890445159068205\n",
      "‖alpha‖₁       : 0.9199999999999982\n",
      "scores min/max : -0.330318198812718 0.4498087713988247\n",
      "Mask mean value:  tensor(0.3334, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.6581  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.166e-02\n",
      "‖w_svm‖₂       : 0.07050224129285465\n",
      "‖alpha‖₁       : 0.5799999999999994\n",
      "scores min/max : -2.737875967016656 1.5484953634988305\n",
      "Mask mean value:  tensor(0.1632, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3026  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.319e-04\n",
      "‖w_svm‖₂       : 0.057032094147234355\n",
      "‖alpha‖₁       : 0.5762576938882136\n",
      "scores min/max : -1.961592997565295 0.866899528858244\n",
      "Mask mean value:  tensor(0.6986, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.8999  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.992e-04\n",
      "‖w_svm‖₂       : 2.2760427290152296e-07\n",
      "‖alpha‖₁       : 0.25999999999999984\n",
      "scores min/max : 2.188586092218278e-08 3.4606441290622626e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.643e-17\n",
      "‖w_svm‖₂       : 0.00573817035266633\n",
      "‖alpha‖₁       : 0.7005656186924952\n",
      "scores min/max : -2.004342746056721 0.01904898474863327\n",
      "Mask mean value:  tensor(0.4531, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2542  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.748e-05\n",
      "‖w_svm‖₂       : 0.004300261099969051\n",
      "‖alpha‖₁       : 0.4599999999999984\n",
      "scores min/max : -0.006983541734449336 -0.004987176081248844\n",
      "Mask mean value:  tensor(0.4727, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.0546  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.029e-15\n",
      "‖w_svm‖₂       : 1.9567290924293e-07\n",
      "‖alpha‖₁       : 0.3799999999999592\n",
      "scores min/max : -5.4824745643312263e-08 -1.8940566518031584e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.630e-19\n",
      "‖w_svm‖₂       : 3.603383300308913e-06\n",
      "‖alpha‖₁       : 0.41999999999307136\n",
      "scores min/max : -1.218225557823222e-06 4.065300973486242e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1230  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.537e-05\n",
      "‖w_svm‖₂       : 0.08709349143410262\n",
      "‖alpha‖₁       : 0.578176902724088\n",
      "scores min/max : -2.129022028068351 5.801297535316778\n",
      "Mask mean value:  tensor(0.2598, dtype=torch.float64)\n",
      "max feasible return = 0.1088  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.0014776348919163e-07\n",
      "‖alpha‖₁       : 0.5799999999999998\n",
      "scores min/max : -3.5572779162398e-07 -2.6499475587040236e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0387303007775235e-07\n",
      "‖alpha‖₁       : 0.2999999999999994\n",
      "scores min/max : 3.4192911124620194e-08 3.9168492309315675e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 7.042500309943646e-08\n",
      "‖alpha‖₁       : 0.5999999999999756\n",
      "scores min/max : 1.1641610990677116e-08 4.842025080630113e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.7138728457009925e-08\n",
      "‖alpha‖₁       : 0.37999999999998196\n",
      "scores min/max : 2.0773279625922967e-09 1.1512269742791726e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.0155860396972016e-06\n",
      "‖alpha‖₁       : 0.31999999997555817\n",
      "scores min/max : 2.7583029253646475e-06 4.554686181356972e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4258  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.05670182875752525\n",
      "‖alpha‖₁       : 0.7397230170087803\n",
      "scores min/max : -3.463819561928668 2.167642614955134\n",
      "Mask mean value:  tensor(0.8911, dtype=torch.float64)\n",
      "max feasible return = -0.9335  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.032989510105829276\n",
      "‖alpha‖₁       : 0.6151675705812207\n",
      "scores min/max : -1.9131903402223827 1.2192145825720258\n",
      "Mask mean value:  tensor(0.7905, dtype=torch.float64)\n",
      "max feasible return = 2.7718  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.7242204146450703e-07\n",
      "‖alpha‖₁       : 0.45999999999997127\n",
      "scores min/max : 2.1149478495969679e-07 8.066590929196753e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.379790007543773e-07\n",
      "‖alpha‖₁       : 0.5199999999999922\n",
      "scores min/max : 6.200206749190632e-08 7.491059546624975e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.5895647216007535e-07\n",
      "‖alpha‖₁       : 0.579999999999977\n",
      "scores min/max : -2.5607627450666873e-07 -2.197311069688112e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.044924938378571984\n",
      "‖alpha‖₁       : 0.68772162176482\n",
      "scores min/max : -0.430803202790515 1.9580349685068608\n",
      "Mask mean value:  tensor(0.4090, dtype=torch.float64)\n",
      "max feasible return = 0.0366  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03324633185567524\n",
      "‖alpha‖₁       : 0.4221537645521528\n",
      "scores min/max : -3.913808236435291 5.216404141641764\n",
      "Mask mean value:  tensor(0.0776, dtype=torch.float64)\n",
      "max feasible return = 0.4888  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04579526003014139\n",
      "‖alpha‖₁       : 0.8211459953903368\n",
      "scores min/max : -5.098797648344195 3.0654687365591977\n",
      "Mask mean value:  tensor(0.9410, dtype=torch.float64)\n",
      "max feasible return = -3.5558  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.008450004072827897\n",
      "‖alpha‖₁       : 0.7999999999999998\n",
      "scores min/max : -0.04044574714826504 0.006740855190820565\n",
      "Mask mean value:  tensor(0.4743, dtype=torch.float64)\n",
      "max feasible return = -0.1724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.135208169323433e-07\n",
      "‖alpha‖₁       : 0.6199999999999539\n",
      "scores min/max : 4.48069094170021e-08 6.135784432290831e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.762977381986038e-08\n",
      "‖alpha‖₁       : 0.4399999999999551\n",
      "scores min/max : 9.600874591489646e-09 1.9185610639622073e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.2860217206377757e-07\n",
      "‖alpha‖₁       : 0.2799999999999989\n",
      "scores min/max : -3.346213722382435e-07 -3.1699328816186217e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  23 | train 0.005382 | val 0.006741\n",
      "-----------------------------------------Epoch:  24 ----------------------------------------\n",
      "‖w_svm‖₂       : 0.028346115397485515\n",
      "‖alpha‖₁       : 0.5510483197163749\n",
      "scores min/max : -3.5338945785974927 1.0376135915063196\n",
      "Mask mean value:  tensor(0.1215, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0128  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.327e-03\n",
      "‖w_svm‖₂       : 3.6027234958767346e-06\n",
      "‖alpha‖₁       : 0.4199999999930702\n",
      "scores min/max : -1.2225246992824047e-06 4.0250567583204544e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1230  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.537e-05\n",
      "‖w_svm‖₂       : 0.016821842981878735\n",
      "‖alpha‖₁       : 0.5962563323198026\n",
      "scores min/max : -2.9536700981227546 2.260300003328579\n",
      "Mask mean value:  tensor(0.9097, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.4984  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.867e-04\n",
      "‖w_svm‖₂       : 0.05350975422232426\n",
      "‖alpha‖₁       : 0.8272151727803883\n",
      "scores min/max : -1.9407285152336717 0.40881853271990815\n",
      "Mask mean value:  tensor(0.5548, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0147  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.607e-11\n",
      "‖w_svm‖₂       : 0.13805371623321133\n",
      "‖alpha‖₁       : 0.879999999999983\n",
      "scores min/max : -1.3531694020286644 3.6110788986676945\n",
      "Mask mean value:  tensor(0.3197, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2590  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.155e-10\n",
      "‖w_svm‖₂       : 0.047504433024095225\n",
      "‖alpha‖₁       : 0.939017178283486\n",
      "scores min/max : -2.4442677598076816 1.6004300505066016\n",
      "Mask mean value:  tensor(0.1562, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4168  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.112e-03\n",
      "‖w_svm‖₂       : 0.007286964465603066\n",
      "‖alpha‖₁       : 0.607675921408918\n",
      "scores min/max : -1.985456002117309 0.30146857575986047\n",
      "Mask mean value:  tensor(0.5775, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2334  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.251e-15\n",
      "‖w_svm‖₂       : 1.1819870122139893e-06\n",
      "‖alpha‖₁       : 0.3199999999986624\n",
      "scores min/max : -1.988512256755878e-06 -1.8726191720352029e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4898  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.793e-18\n",
      "‖w_svm‖₂       : 0.05695927552693005\n",
      "‖alpha‖₁       : 0.5762598834606341\n",
      "scores min/max : -1.961725028750303 0.8669287131289591\n",
      "Mask mean value:  tensor(0.6986, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.8997  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.000e-03\n",
      "‖w_svm‖₂       : 0.020053035851951454\n",
      "‖alpha‖₁       : 0.38337572544996323\n",
      "scores min/max : -1.9693674337112224 0.2318898112538882\n",
      "Mask mean value:  tensor(0.6801, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1462  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.999e-14\n",
      "‖w_svm‖₂       : 2.2729870858183697e-07\n",
      "‖alpha‖₁       : 0.2599999999999999\n",
      "scores min/max : 2.2013176387189087e-08 3.473333391828114e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.633e-17\n",
      "‖w_svm‖₂       : 1.0810203149472054e-07\n",
      "‖alpha‖₁       : 0.23999999999998423\n",
      "scores min/max : 2.40055771653419e-08 3.496294841115378e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.423e-21\n",
      "‖w_svm‖₂       : 5.5021472909519135e-08\n",
      "‖alpha‖₁       : 0.11999999999997898\n",
      "scores min/max : -8.512720475454035e-08 -5.4166644460011553e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.223e-07\n",
      "‖w_svm‖₂       : 0.04357850153084594\n",
      "‖alpha‖₁       : 0.8900210820313671\n",
      "scores min/max : -2.004836182372329 0.3426920895963151\n",
      "Mask mean value:  tensor(0.2777, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0771  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.298e-13\n",
      "‖w_svm‖₂       : 0.045542653818242816\n",
      "‖alpha‖₁       : 0.9199999999993218\n",
      "scores min/max : -0.32765784628585637 0.44019759128138053\n",
      "Mask mean value:  tensor(0.3273, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.6277  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.387e-02\n",
      "‖w_svm‖₂       : 0.005379117183572337\n",
      "‖alpha‖₁       : 0.3799999999999766\n",
      "scores min/max : -0.006177739016803352 0.005750419261957872\n",
      "Mask mean value:  tensor(0.5134, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9135  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.765e-14\n",
      "‖w_svm‖₂       : 1.0215974190294231e-06\n",
      "‖alpha‖₁       : 0.5999999999999919\n",
      "scores min/max : -3.552550468972729e-08 1.5360323116502467e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.502e-19\n",
      "‖w_svm‖₂       : 7.284696032005749e-08\n",
      "‖alpha‖₁       : 0.4199999999999653\n",
      "scores min/max : -7.856747195113171e-08 -3.116553921807719e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.838e-08\n",
      "‖w_svm‖₂       : 0.018141343921343367\n",
      "‖alpha‖₁       : 0.8599999999999753\n",
      "scores min/max : -0.04689124023226161 0.02530396741209215\n",
      "Mask mean value:  tensor(0.4129, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5308  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.118e-14\n",
      "‖w_svm‖₂       : 2.387931160830694e-07\n",
      "‖alpha‖₁       : 0.3799999999921297\n",
      "scores min/max : -3.112624210772313e-08 -1.156847097326362e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.583e-07\n",
      "‖w_svm‖₂       : 0.0001852826846528307\n",
      "‖alpha‖₁       : 0.8199999999999997\n",
      "scores min/max : -5.502171045882506e-05 -4.079205064889599e-05\n",
      "Mask mean value:  tensor(0.4998, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6971  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.204e-17\n",
      "‖w_svm‖₂       : 0.023311056099244623\n",
      "‖alpha‖₁       : 0.6599999999999998\n",
      "scores min/max : -0.045854795792186695 0.050615106940039095\n",
      "Mask mean value:  tensor(0.4393, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.4810  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.687e-15\n",
      "‖w_svm‖₂       : 4.836718066968499e-08\n",
      "‖alpha‖₁       : 0.17999999999999733\n",
      "scores min/max : 4.633489533211732e-08 6.100282200696941e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 4.984e-08\n",
      "‖w_svm‖₂       : 8.317499305325406e-08\n",
      "‖alpha‖₁       : 0.3799999999999998\n",
      "scores min/max : 2.158142793534046e-09 2.0307474048925467e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.633e-22\n",
      "‖w_svm‖₂       : 2.0057096260103708e-07\n",
      "‖alpha‖₁       : 0.4199999999999785\n",
      "scores min/max : -3.4351628939048916e-07 -3.2124839952809265e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.621e-19\n",
      "‖w_svm‖₂       : 0.0762696886364089\n",
      "‖alpha‖₁       : 0.6584560756227468\n",
      "scores min/max : -2.0752654951051315 4.092870339140447\n",
      "Mask mean value:  tensor(0.2863, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.3103  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.328e-03\n",
      "‖w_svm‖₂       : 0.020271503663945985\n",
      "‖alpha‖₁       : 0.7799999999999998\n",
      "scores min/max : -0.019950463677755288 0.04992682913909424\n",
      "Mask mean value:  tensor(0.4304, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.8851  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.206e-15\n",
      "‖w_svm‖₂       : 7.318251179347345e-08\n",
      "‖alpha‖₁       : 0.139999999999978\n",
      "scores min/max : 5.490055429542616e-09 1.1333022519666808e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.997e-08\n",
      "‖w_svm‖₂       : 0.028236578842346954\n",
      "‖alpha‖₁       : 0.1966828060779628\n",
      "scores min/max : -2.212641018359557 0.011536636311878477\n",
      "Mask mean value:  tensor(0.0514, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3484  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.901e-04\n",
      "‖w_svm‖₂       : 0.18803754846959125\n",
      "‖alpha‖₁       : 0.8842342998587585\n",
      "scores min/max : -3.593434267466377 6.079088346388315\n",
      "Mask mean value:  tensor(0.0941, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1950  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.400e-03\n",
      "‖w_svm‖₂       : 0.08312458229164349\n",
      "‖alpha‖₁       : 0.4720835579322731\n",
      "scores min/max : -2.350497277348615 2.7324934180491858\n",
      "Mask mean value:  tensor(0.0914, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0052  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.474e-03\n",
      "‖w_svm‖₂       : 0.0057760233493118745\n",
      "‖alpha‖₁       : 0.7005660121535914\n",
      "scores min/max : -2.004198990865332 0.019195593407240624\n",
      "Mask mean value:  tensor(0.4538, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2545  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.571e-05\n",
      "‖w_svm‖₂       : 0.14812745420856607\n",
      "‖alpha‖₁       : 0.7598921344806271\n",
      "scores min/max : -1.7414202552070357 2.2360589230053596\n",
      "Mask mean value:  tensor(0.8761, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3594  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.156e-11\n",
      "‖w_svm‖₂       : 0.04760985824777045\n",
      "‖alpha‖₁       : 0.7294254655074818\n",
      "scores min/max : -0.24932856635545309 2.0328414706212636\n",
      "Mask mean value:  tensor(0.7148, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2137  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.218e-03\n",
      "‖w_svm‖₂       : 2.3683119538686465e-07\n",
      "‖alpha‖₁       : 0.6399999999999968\n",
      "scores min/max : 3.319206397548539e-07 3.528156568657538e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.632e-20\n",
      "‖w_svm‖₂       : 0.003156460937226432\n",
      "‖alpha‖₁       : 0.5799999999999924\n",
      "scores min/max : 0.0024116952383996734 0.003675255546766107\n",
      "Mask mean value:  tensor(0.5171, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3288  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.145e-13\n",
      "‖w_svm‖₂       : 0.005625382538288598\n",
      "‖alpha‖₁       : 0.5599999999999856\n",
      "scores min/max : 0.004283591761692402 0.005925985034999657\n",
      "Mask mean value:  tensor(0.5229, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0961  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 4.867e-05\n",
      "‖w_svm‖₂       : 0.0005046696130568272\n",
      "‖alpha‖₁       : 0.4399999999999999\n",
      "scores min/max : -0.0006969507803649283 -0.0004388619125029849\n",
      "Mask mean value:  tensor(0.4967, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6445  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.118e-16\n",
      "‖w_svm‖₂       : 4.284229273078577e-07\n",
      "‖alpha‖₁       : 0.7199999999999984\n",
      "scores min/max : 2.803045429402596e-07 3.375722511772847e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.993e-21\n",
      "‖w_svm‖₂       : 0.00015977565667666502\n",
      "‖alpha‖₁       : 0.6399999999999868\n",
      "scores min/max : -9.442482388785222e-05 -8.468387979984371e-05\n",
      "Mask mean value:  tensor(0.4996, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7637  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.156e-17\n",
      "‖w_svm‖₂       : 7.981845107467648e-08\n",
      "‖alpha‖₁       : 0.6599999999999893\n",
      "scores min/max : 1.0360824602152981e-07 2.479155156100935e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.891e-09\n",
      "‖w_svm‖₂       : 0.0008757297912412299\n",
      "‖alpha‖₁       : 0.8199999999999992\n",
      "scores min/max : 0.000966524050615673 0.0026834855822010506\n",
      "Mask mean value:  tensor(0.5102, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5920  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.013e-16\n",
      "‖w_svm‖₂       : 0.004300983747828141\n",
      "‖alpha‖₁       : 0.4599999999999949\n",
      "scores min/max : -0.007056438721324842 -0.005059418315098922\n",
      "Mask mean value:  tensor(0.4724, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.0531  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.027e-15\n",
      "‖w_svm‖₂       : 1.1213958871531376e-07\n",
      "‖alpha‖₁       : 0.5199999999999996\n",
      "scores min/max : 3.0897635134634394e-07 3.3037999947069284e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.575e-19\n",
      "‖w_svm‖₂       : 7.696761633458176e-08\n",
      "‖alpha‖₁       : 0.5399999999999585\n",
      "scores min/max : -1.1443929549723367e-07 -9.278765837883644e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.589e-20\n",
      "‖w_svm‖₂       : 3.836676620531821e-07\n",
      "‖alpha‖₁       : 0.2799999999999958\n",
      "scores min/max : -1.4493359811724e-06 -1.3875912194248547e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1957  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.497e-19\n",
      "‖w_svm‖₂       : 0.1394662794239742\n",
      "‖alpha‖₁       : 0.6551794481479684\n",
      "scores min/max : -18.069709506127534 1.7945344668553784\n",
      "Mask mean value:  tensor(0.2349, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2785  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.806e-03\n",
      "‖w_svm‖₂       : 1.9527071062909892e-07\n",
      "‖alpha‖₁       : 0.37999999999995826\n",
      "scores min/max : -4.881095415521497e-08 -1.2920875038712751e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.622e-19\n",
      "‖w_svm‖₂       : 0.00021090903476017278\n",
      "‖alpha‖₁       : 0.619999999999218\n",
      "scores min/max : 4.0660988875496415e-05 5.6423661680703215e-05\n",
      "Mask mean value:  tensor(0.5002, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.138e-17\n",
      "‖w_svm‖₂       : 1.1576432346793037e-07\n",
      "‖alpha‖₁       : 0.2999999999999838\n",
      "scores min/max : 1.0280351654116463e-07 1.1184053293071009e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.243e-07\n",
      "‖w_svm‖₂       : 5.491352774384983e-08\n",
      "‖alpha‖₁       : 0.4399999999999871\n",
      "scores min/max : -1.529029506520941e-07 -4.8941616955853605e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.311e-20\n",
      "‖w_svm‖₂       : 0.07094989501826549\n",
      "‖alpha‖₁       : 0.5799999999999996\n",
      "scores min/max : -2.7689962565407322 1.572360621323801\n",
      "Mask mean value:  tensor(0.1687, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3113  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.355e-04\n",
      "‖w_svm‖₂       : 0.00013830156143395574\n",
      "‖alpha‖₁       : 0.43999999991529376\n",
      "scores min/max : -0.000214906176646491 -7.452490087159842e-05\n",
      "Mask mean value:  tensor(0.4994, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0239  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.647e-17\n",
      "‖w_svm‖₂       : 0.03207612150394342\n",
      "‖alpha‖₁       : 0.8985775321892748\n",
      "scores min/max : -0.7565019007231368 2.0037540656981503\n",
      "Mask mean value:  tensor(0.8317, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3810  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.060e-15\n",
      "‖w_svm‖₂       : 2.3880247697727064e-07\n",
      "‖alpha‖₁       : 0.579999999999991\n",
      "scores min/max : -3.8042691709548466e-07 -3.6477779114298254e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.943e-19\n",
      "‖w_svm‖₂       : 0.026605212466488865\n",
      "‖alpha‖₁       : 0.8522664033012297\n",
      "scores min/max : -2.921474987074064 1.5595974386928422\n",
      "Mask mean value:  tensor(0.1532, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9486  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.366e-03\n",
      "‖w_svm‖₂       : 1.0829036110776087e-06\n",
      "‖alpha‖₁       : 0.4999999999999927\n",
      "scores min/max : -2.8456960069439755e-07 -9.155466421417972e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.931e-19\n",
      "‖w_svm‖₂       : 0.11189457230557456\n",
      "‖alpha‖₁       : 0.8732633723108101\n",
      "scores min/max : -12.20868538933033 2.1374583363934514\n",
      "Mask mean value:  tensor(0.4994, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.4784  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.395e-02\n",
      "‖w_svm‖₂       : 2.8136263184476993e-07\n",
      "‖alpha‖₁       : 0.6599999999999998\n",
      "scores min/max : -6.115172776632616e-07 -4.5988536784113003e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.320e-18\n",
      "‖w_svm‖₂       : 0.06115313495181451\n",
      "‖alpha‖₁       : 0.8986427825733188\n",
      "scores min/max : -1.7053340414752283 3.703893015759435\n",
      "Mask mean value:  tensor(0.4163, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5686  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.234e-02\n",
      "‖w_svm‖₂       : 0.021545576016941365\n",
      "‖alpha‖₁       : 0.8233647467117072\n",
      "scores min/max : -1.8931348449466423 1.505563928447031\n",
      "Mask mean value:  tensor(0.7168, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.4258  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.884e-06\n",
      "‖w_svm‖₂       : 0.023329423596962218\n",
      "‖alpha‖₁       : 0.8150936207810946\n",
      "scores min/max : -11.52174513635632 2.033476894865313\n",
      "Mask mean value:  tensor(0.7126, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9131  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.377e-06\n",
      "‖w_svm‖₂       : 8.814524031654148e-08\n",
      "‖alpha‖₁       : 0.17999999999998878\n",
      "scores min/max : -9.563377119140548e-08 1.4047161677998936e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.846e-21\n",
      "‖w_svm‖₂       : 1.7345231908807032e-05\n",
      "‖alpha‖₁       : 0.35999999998272403\n",
      "scores min/max : 1.4612003489836158e-05 1.5686748756814666e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.341e-17\n",
      "‖w_svm‖₂       : 0.03991265468614268\n",
      "‖alpha‖₁       : 0.9403462378016354\n",
      "scores min/max : -1.8300919535080036 0.30365917454703206\n",
      "Mask mean value:  tensor(0.4169, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3254  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.672e-14\n",
      "‖w_svm‖₂       : 0.017763574131882225\n",
      "‖alpha‖₁       : 0.681539186933606\n",
      "scores min/max : -1.9748588416382187 0.05444944390230764\n",
      "Mask mean value:  tensor(0.6002, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2491  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.949e-12\n",
      "‖w_svm‖₂       : 6.732352776981511e-07\n",
      "‖alpha‖₁       : 0.4\n",
      "scores min/max : -5.911018162535974e-08 2.4711855139470697e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.242e-09\n",
      "‖w_svm‖₂       : 0.0749587189508772\n",
      "‖alpha‖₁       : 0.41908714903897015\n",
      "scores min/max : -1.874623147244609 2.3508603346553745\n",
      "Mask mean value:  tensor(0.7915, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8939  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.303e-04\n",
      "‖w_svm‖₂       : 0.04752731220341504\n",
      "‖alpha‖₁       : 0.9056469266019452\n",
      "scores min/max : -0.722769872216869 1.8891375577570062\n",
      "Mask mean value:  tensor(0.3686, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6829  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.728e-03\n",
      "‖w_svm‖₂       : 0.047838415362312675\n",
      "‖alpha‖₁       : 0.6599999999999719\n",
      "scores min/max : -0.2952419696812233 0.21967127805487885\n",
      "Mask mean value:  tensor(0.4983, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8980  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.104e-13\n",
      "‖w_svm‖₂       : 0.0003042338374691082\n",
      "‖alpha‖₁       : 0.41999999999999704\n",
      "scores min/max : 0.00018999446608573953 0.0005036140229184791\n",
      "Mask mean value:  tensor(0.5024, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0002  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.781e-15\n",
      "‖w_svm‖₂       : 1.6992772316777032e-07\n",
      "‖alpha‖₁       : 0.2399999999999901\n",
      "scores min/max : 2.7898056025409717e-07 2.943527233866946e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.190e-20\n",
      "‖w_svm‖₂       : 2.1669582831123515e-08\n",
      "‖alpha‖₁       : 0.11999999999999447\n",
      "scores min/max : -2.668842609307397e-08 -1.2011002250819738e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.788e-09\n",
      "‖w_svm‖₂       : 0.0003600430751307546\n",
      "‖alpha‖₁       : 0.7400000000000001\n",
      "scores min/max : -0.00022715801849954685 -0.0002106236830162462\n",
      "Mask mean value:  tensor(0.4989, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0620  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.138e-17\n",
      "‖w_svm‖₂       : 0.016635810063265994\n",
      "‖alpha‖₁       : 0.86\n",
      "scores min/max : -0.04677676655896105 -0.031876903433401105\n",
      "Mask mean value:  tensor(0.3281, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0807  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.260e-14\n",
      "‖w_svm‖₂       : 5.705151981162475e-08\n",
      "‖alpha‖₁       : 0.2399999999999893\n",
      "scores min/max : -6.77671117425557e-08 -5.3290498734780914e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.336e-20\n",
      "‖w_svm‖₂       : 0.08666873142897896\n",
      "‖alpha‖₁       : 0.5781056932971939\n",
      "scores min/max : -2.114829909226744 5.81515828047427\n",
      "Mask mean value:  tensor(0.2855, dtype=torch.float64)\n",
      "max feasible return = 0.1133  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.9842414387219685e-07\n",
      "‖alpha‖₁       : 0.58\n",
      "scores min/max : -3.6922161703858284e-07 -2.78495813229312e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0354852795185343e-07\n",
      "‖alpha‖₁       : 0.2999999999999993\n",
      "scores min/max : 3.752576034818816e-08 4.2503084032091346e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 7.017192892174365e-08\n",
      "‖alpha‖₁       : 0.5999999999999557\n",
      "scores min/max : 1.6242156418248224e-08 5.3013388093024524e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.688557291662463e-08\n",
      "‖alpha‖₁       : 0.37999999999998313\n",
      "scores min/max : 4.405870134470317e-09 1.3839948354914845e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.9685421377367513e-06\n",
      "‖alpha‖₁       : 0.3199999999777409\n",
      "scores min/max : 2.8671762490415376e-06 4.598417636355834e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4258  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.05589951482395132\n",
      "‖alpha‖₁       : 0.7397195777861932\n",
      "scores min/max : -3.4692211346982686 2.1635538411128206\n",
      "Mask mean value:  tensor(0.8879, dtype=torch.float64)\n",
      "max feasible return = -0.9316  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.032447742445842316\n",
      "‖alpha‖₁       : 0.6151671181041777\n",
      "scores min/max : -1.913055794117981 1.219004669237773\n",
      "Mask mean value:  tensor(0.7916, dtype=torch.float64)\n",
      "max feasible return = 2.7750  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.7082943840349083e-07\n",
      "‖alpha‖₁       : 0.4599999999999672\n",
      "scores min/max : 2.1765113737044535e-07 8.129549541643306e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.376051124251531e-07\n",
      "‖alpha‖₁       : 0.5199999999999906\n",
      "scores min/max : 6.08994353740798e-08 7.38078782637582e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.581462997003972e-07\n",
      "‖alpha‖₁       : 0.5799999999999765\n",
      "scores min/max : -2.6119910722085386e-07 -2.2484901508316813e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.044995472187192756\n",
      "‖alpha‖₁       : 0.687731973251619\n",
      "scores min/max : -0.4270357320336489 1.9618046856255174\n",
      "Mask mean value:  tensor(0.4229, dtype=torch.float64)\n",
      "max feasible return = 0.0232  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03301848798686467\n",
      "‖alpha‖₁       : 0.42215573840388404\n",
      "scores min/max : -3.91272792156706 5.220176587575777\n",
      "Mask mean value:  tensor(0.0784, dtype=torch.float64)\n",
      "max feasible return = 0.4940  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04547867171199909\n",
      "‖alpha‖₁       : 0.8211439411370598\n",
      "scores min/max : -5.094276581320046 3.070583644428534\n",
      "Mask mean value:  tensor(0.9422, dtype=torch.float64)\n",
      "max feasible return = -3.5634  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.008309366561213593\n",
      "‖alpha‖₁       : 0.7999999999999998\n",
      "scores min/max : -0.039181820915027525 0.0065415586627328975\n",
      "Mask mean value:  tensor(0.4751, dtype=torch.float64)\n",
      "max feasible return = -0.1727  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.1202411581853066e-07\n",
      "‖alpha‖₁       : 0.6199999999999534\n",
      "scores min/max : 4.576654362177335e-08 6.231907647826024e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.7512135503928275e-08\n",
      "‖alpha‖₁       : 0.43999999999994766\n",
      "scores min/max : 9.386435070010018e-09 1.897284713238603e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.281154976782901e-07\n",
      "‖alpha‖₁       : 0.27999999999999836\n",
      "scores min/max : -3.413786654693905e-07 -3.2373010040305056e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  24 | train 0.005375 | val 0.006708\n",
      "-----------------------------------------Epoch:  25 ----------------------------------------\n",
      "‖w_svm‖₂       : 7.282449666347116e-08\n",
      "‖alpha‖₁       : 0.13999999999999266\n",
      "scores min/max : 5.514784681295404e-09 1.1291884477356424e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.977e-08\n",
      "‖w_svm‖₂       : 8.82910214293057e-08\n",
      "‖alpha‖₁       : 0.1799999999999869\n",
      "scores min/max : -9.517133270674969e-08 1.412096006516937e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.858e-21\n",
      "‖w_svm‖₂       : 0.0008797854690939218\n",
      "‖alpha‖₁       : 0.8199999999999437\n",
      "scores min/max : 0.0009979007673658388 0.0027325129111939934\n",
      "Mask mean value:  tensor(0.5105, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5931  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.014e-16\n",
      "‖w_svm‖₂       : 0.020321687339825548\n",
      "‖alpha‖₁       : 0.78\n",
      "scores min/max : -0.020777767119226506 0.04943143602061578\n",
      "Mask mean value:  tensor(0.4265, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.8691  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.281e-15\n",
      "‖w_svm‖₂       : 0.05313927497666717\n",
      "‖alpha‖₁       : 0.8271941729142611\n",
      "scores min/max : -1.948212515167553 0.4014977012015089\n",
      "Mask mean value:  tensor(0.5286, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0101  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.640e-02\n",
      "‖w_svm‖₂       : 0.02159834650971219\n",
      "‖alpha‖₁       : 0.8233657159560296\n",
      "scores min/max : -1.894566740481126 1.5040921625307235\n",
      "Mask mean value:  tensor(0.7131, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.4174  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.726e-06\n",
      "‖w_svm‖₂       : 0.016633232080259668\n",
      "‖alpha‖₁       : 0.8599999999999743\n",
      "scores min/max : -0.04688818403976644 -0.031992938694073145\n",
      "Mask mean value:  tensor(0.3276, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0806  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.265e-14\n",
      "‖w_svm‖₂       : 6.716580474505742e-07\n",
      "‖alpha‖₁       : 0.4\n",
      "scores min/max : -5.387125591825406e-08 2.521579412855489e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.214e-09\n",
      "‖w_svm‖₂       : 2.2668124414996504e-07\n",
      "‖alpha‖₁       : 0.2599999999999999\n",
      "scores min/max : 2.437310219962436e-08 3.709267749299446e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.623e-17\n",
      "‖w_svm‖₂       : 0.1112606620808892\n",
      "‖alpha‖₁       : 0.8732683921291335\n",
      "scores min/max : -12.2028145450262 2.1424618169267244\n",
      "Mask mean value:  tensor(0.5110, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.5118  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.804e-03\n",
      "‖w_svm‖₂       : 0.14796913222292124\n",
      "‖alpha‖₁       : 0.7598449679278421\n",
      "scores min/max : -1.743615929841797 2.233782303897871\n",
      "Mask mean value:  tensor(0.8734, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3590  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.452e-11\n",
      "‖w_svm‖₂       : 1.723459607826212e-05\n",
      "‖alpha‖₁       : 0.3599999999831053\n",
      "scores min/max : 1.453705906431215e-05 1.5602417262652128e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.301e-17\n",
      "‖w_svm‖₂       : 1.0202012035531074e-06\n",
      "‖alpha‖₁       : 0.5999999999999946\n",
      "scores min/max : -3.8165064414346977e-08 1.271260624875067e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.505e-19\n",
      "‖w_svm‖₂       : 0.004318526980554449\n",
      "‖alpha‖₁       : 0.45999999999998153\n",
      "scores min/max : -0.007158462120641205 -0.0051440811514393975\n",
      "Mask mean value:  tensor(0.4719, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.0511  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.031e-15\n",
      "‖w_svm‖₂       : 0.000359330422496663\n",
      "‖alpha‖₁       : 0.7399999999999999\n",
      "scores min/max : -0.0002394654968504524 -0.00022299653880969642\n",
      "Mask mean value:  tensor(0.4989, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0619  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.140e-17\n",
      "‖w_svm‖₂       : 0.026741872900859148\n",
      "‖alpha‖₁       : 0.8522746611738814\n",
      "scores min/max : -2.9154731114496837 1.5655640202273582\n",
      "Mask mean value:  tensor(0.1559, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9518  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.393e-03\n",
      "‖w_svm‖₂       : 0.0001595495812574523\n",
      "‖alpha‖₁       : 0.6399999999999898\n",
      "scores min/max : -9.243129748173236e-05 -8.271785424514516e-05\n",
      "Mask mean value:  tensor(0.4996, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7637  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.157e-17\n",
      "‖w_svm‖₂       : 0.023021979448287957\n",
      "‖alpha‖₁       : 0.6599999999999998\n",
      "scores min/max : -0.044449179048076264 0.04911084220418735\n",
      "Mask mean value:  tensor(0.4416, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.4963  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.624e-15\n",
      "‖w_svm‖₂       : 1.1783191821623503e-06\n",
      "‖alpha‖₁       : 0.3199999999986874\n",
      "scores min/max : -2.0048126022699253e-06 -1.8890998531021761e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4898  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.798e-18\n",
      "‖w_svm‖₂       : 0.00014578627393857244\n",
      "‖alpha‖₁       : 0.4399999996845426\n",
      "scores min/max : -0.00023164773432177328 -8.299511797675395e-05\n",
      "Mask mean value:  tensor(0.4993, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0234  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.795e-17\n",
      "‖w_svm‖₂       : 0.07526709906258268\n",
      "‖alpha‖₁       : 0.41916973527593515\n",
      "scores min/max : -1.8768423452444973 2.341972021390429\n",
      "Mask mean value:  tensor(0.7899, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8943  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.431e-04\n",
      "‖w_svm‖₂       : 5.699177984057782e-08\n",
      "‖alpha‖₁       : 0.23999999999999086\n",
      "scores min/max : -7.00499920966672e-08 -5.55887611128653e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.342e-20\n",
      "‖w_svm‖₂       : 0.00030403278828715347\n",
      "‖alpha‖₁       : 0.4199999999921044\n",
      "scores min/max : 0.0001877147189971 0.0005008283256282453\n",
      "Mask mean value:  tensor(0.5024, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0002  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.795e-15\n",
      "‖w_svm‖₂       : 0.005652630997280104\n",
      "‖alpha‖₁       : 0.5599999999999873\n",
      "scores min/max : 0.004176961618835917 0.00583540419297688\n",
      "Mask mean value:  tensor(0.5224, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0950  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.415e-05\n",
      "‖w_svm‖₂       : 0.028339082495148615\n",
      "‖alpha‖₁       : 0.5510558873809839\n",
      "scores min/max : -3.5294057002489945 1.041769780386054\n",
      "Mask mean value:  tensor(0.1232, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0239  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.498e-03\n",
      "‖w_svm‖₂       : 1.7035026063090413e-07\n",
      "‖alpha‖₁       : 0.2399999999999901\n",
      "scores min/max : 2.6935160780531606e-07 2.847213225885353e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.237e-20\n",
      "‖w_svm‖₂       : 0.007427582939068765\n",
      "‖alpha‖₁       : 0.6076779176012764\n",
      "scores min/max : -1.9857656012172975 0.3011431462217451\n",
      "Mask mean value:  tensor(0.5761, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2328  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.444e-15\n",
      "‖w_svm‖₂       : 0.0703635963457398\n",
      "‖alpha‖₁       : 0.5799999999999994\n",
      "scores min/max : -2.725243595630863 1.5418339355124462\n",
      "Mask mean value:  tensor(0.1643, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3038  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.368e-04\n",
      "‖w_svm‖₂       : 1.085400904000651e-06\n",
      "‖alpha‖₁       : 0.49999999999999284\n",
      "scores min/max : -2.9505897517641037e-07 -1.0206866935113317e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.973e-19\n",
      "‖w_svm‖₂       : 0.017809691823421618\n",
      "‖alpha‖₁       : 0.8599999999999771\n",
      "scores min/max : -0.04599679359539859 0.023659781583131017\n",
      "Mask mean value:  tensor(0.4123, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5285  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.399e-14\n",
      "‖w_svm‖₂       : 0.04748416880270379\n",
      "‖alpha‖₁       : 0.6599999999999767\n",
      "scores min/max : -0.29087726800418195 0.216165757750842\n",
      "Mask mean value:  tensor(0.4975, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8982  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.116e-13\n",
      "‖w_svm‖₂       : 2.8223617768151294e-07\n",
      "‖alpha‖₁       : 0.6599999999999976\n",
      "scores min/max : -6.087798679767971e-07 -4.5712184054472663e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.333e-18\n",
      "‖w_svm‖₂       : 0.016560629000335187\n",
      "‖alpha‖₁       : 0.5962577091676753\n",
      "scores min/max : -2.957809597404194 2.2618636080065193\n",
      "Mask mean value:  tensor(0.9105, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.4997  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.185e-04\n",
      "‖w_svm‖₂       : 2.57947331117879e-07\n",
      "‖alpha‖₁       : 0.37999999999081013\n",
      "scores min/max : -3.4417690318670925e-08 -1.4121531650857647e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.546e-06\n",
      "‖w_svm‖₂       : 2.3757116015763717e-07\n",
      "‖alpha‖₁       : 0.6399999999999956\n",
      "scores min/max : 3.372065115634151e-07 3.5811741775530127e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.707e-20\n",
      "‖w_svm‖₂       : 0.028113711794931308\n",
      "‖alpha‖₁       : 0.1966835669155827\n",
      "scores min/max : -2.21349763679034 0.010532877723811881\n",
      "Mask mean value:  tensor(0.0506, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3433  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.217e-04\n",
      "‖w_svm‖₂       : 0.005399074592888669\n",
      "‖alpha‖₁       : 0.37999999999997125\n",
      "scores min/max : -0.005544001020041989 0.006289956888431224\n",
      "Mask mean value:  tensor(0.5163, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9186  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.725e-14\n",
      "‖w_svm‖₂       : 2.1663029181284e-08\n",
      "‖alpha‖₁       : 0.11999999999999464\n",
      "scores min/max : -2.8346490998039743e-08 -1.3674929304509869e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.796e-09\n",
      "‖w_svm‖₂       : 0.005829873136027585\n",
      "‖alpha‖₁       : 0.7005665636926061\n",
      "scores min/max : -2.0040776330483276 0.019316981093959504\n",
      "Mask mean value:  tensor(0.4544, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2548  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.426e-05\n",
      "‖w_svm‖₂       : 4.8385029318621474e-08\n",
      "‖alpha‖₁       : 0.17999999999999708\n",
      "scores min/max : 4.5520857359548954e-08 6.01905896543986e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.007e-08\n",
      "‖w_svm‖₂       : 0.04344641037621028\n",
      "‖alpha‖₁       : 0.8900268759115778\n",
      "scores min/max : -2.0047987624401595 0.34276321099927887\n",
      "Mask mean value:  tensor(0.2777, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0764  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.984e-11\n",
      "‖w_svm‖₂       : 0.04702178645531974\n",
      "‖alpha‖₁       : 0.7294289181563656\n",
      "scores min/max : -0.25038872271841744 2.031648514200003\n",
      "Mask mean value:  tensor(0.7116, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2009  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.250e-03\n",
      "‖w_svm‖₂       : 2.0082508559946374e-07\n",
      "‖alpha‖₁       : 0.41999999999997767\n",
      "scores min/max : -3.3375544843784604e-07 -3.1149506069706526e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.631e-19\n",
      "‖w_svm‖₂       : 0.05737904064952491\n",
      "‖alpha‖₁       : 0.576317386968604\n",
      "scores min/max : -1.9621716741724662 0.8666523477324914\n",
      "Mask mean value:  tensor(0.6975, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.8965  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.004e-03\n",
      "‖w_svm‖₂       : 3.8546734909197496e-07\n",
      "‖alpha‖₁       : 0.27999999999998043\n",
      "scores min/max : -1.378687393193812e-06 -1.3166879527401257e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.557e-19\n",
      "‖w_svm‖₂       : 0.047007505428224954\n",
      "‖alpha‖₁       : 0.9056459277866606\n",
      "scores min/max : -0.702880317706222 1.8959744261525033\n",
      "Mask mean value:  tensor(0.3854, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7150  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.795e-05\n",
      "‖w_svm‖₂       : 0.04735609324133191\n",
      "‖alpha‖₁       : 0.939015591959639\n",
      "scores min/max : -2.4445962186086887 1.6008098391697434\n",
      "Mask mean value:  tensor(0.1564, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4175  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.104e-03\n",
      "‖w_svm‖₂       : 0.00021033458927768091\n",
      "‖alpha‖₁       : 0.6199999999990196\n",
      "scores min/max : 3.31932847978871e-05 4.88713282251841e-05\n",
      "Mask mean value:  tensor(0.5002, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.205e-17\n",
      "‖w_svm‖₂       : 1.1602614891933269e-07\n",
      "‖alpha‖₁       : 0.2999999999999839\n",
      "scores min/max : 1.0386615203076088e-07 1.1290077530904159e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.248e-07\n",
      "‖w_svm‖₂       : 0.04458557653600747\n",
      "‖alpha‖₁       : 0.9199999999999806\n",
      "scores min/max : -0.31734751693414187 0.4184274022725982\n",
      "Mask mean value:  tensor(0.3216, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.6015  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.692e-02\n",
      "‖w_svm‖₂       : 7.71232601864669e-08\n",
      "‖alpha‖₁       : 0.5399999999999328\n",
      "scores min/max : -1.1277647841153674e-07 -9.11019568333772e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.604e-20\n",
      "‖w_svm‖₂       : 0.14057992170931705\n",
      "‖alpha‖₁       : 0.6555150094815319\n",
      "scores min/max : -18.10516064222837 1.7627057235951322\n",
      "Mask mean value:  tensor(0.2108, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2671  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.569e-03\n",
      "‖w_svm‖₂       : 0.0031685505973392722\n",
      "‖alpha‖₁       : 0.5799999999999997\n",
      "scores min/max : 0.002800109235614713 0.004073646199185217\n",
      "Mask mean value:  tensor(0.5190, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3300  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.761e-13\n",
      "‖w_svm‖₂       : 0.04031154508454022\n",
      "‖alpha‖₁       : 0.9403801542308332\n",
      "scores min/max : -1.8267857691983354 0.3069633724398947\n",
      "Mask mean value:  tensor(0.4300, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3259  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.161e-10\n",
      "‖w_svm‖₂       : 3.5945931377444275e-06\n",
      "‖alpha‖₁       : 0.4199999999930801\n",
      "scores min/max : -1.3688363261698193e-06 2.5565502422478757e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1230  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.537e-05\n",
      "‖w_svm‖₂       : 1.9534865881035384e-07\n",
      "‖alpha‖₁       : 0.3799999999999496\n",
      "scores min/max : -5.409246534137703e-08 -1.8173488906311936e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.628e-19\n",
      "‖w_svm‖₂       : 2.4117396835192803e-07\n",
      "‖alpha‖₁       : 0.579999999999846\n",
      "scores min/max : -3.783719821158362e-07 -3.625902738187882e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.989e-19\n",
      "‖w_svm‖₂       : 5.502069929386951e-08\n",
      "‖alpha‖₁       : 0.43999999999998557\n",
      "scores min/max : -1.5332948056472973e-07 -4.935284792107954e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.330e-20\n",
      "‖w_svm‖₂       : 4.2801111923950446e-07\n",
      "‖alpha‖₁       : 0.7199999999999986\n",
      "scores min/max : 2.7429935806726565e-07 3.315218817341895e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.016e-21\n",
      "‖w_svm‖₂       : 0.0618361263732812\n",
      "‖alpha‖₁       : 0.8987323393486105\n",
      "scores min/max : -1.696956992930061 3.712652505211234\n",
      "Mask mean value:  tensor(0.4335, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5819  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.073e-02\n",
      "‖w_svm‖₂       : 0.023217805416925967\n",
      "‖alpha‖₁       : 0.8150951915140642\n",
      "scores min/max : -11.520115375173052 2.034224917746398\n",
      "Mask mean value:  tensor(0.7143, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9178  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.850e-06\n",
      "‖w_svm‖₂       : 7.23439735397044e-08\n",
      "‖alpha‖₁       : 0.4199999999999987\n",
      "scores min/max : -7.814411832691276e-08 -3.0965296368633385e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.825e-08\n",
      "‖w_svm‖₂       : 0.00018562276422435075\n",
      "‖alpha‖₁       : 0.8199999999999995\n",
      "scores min/max : -5.733557801202062e-05 -4.305646559129829e-05\n",
      "Mask mean value:  tensor(0.4998, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6971  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.212e-17\n",
      "‖w_svm‖₂       : 7.835997517856848e-08\n",
      "‖alpha‖₁       : 0.6599999999999931\n",
      "scores min/max : 1.0005152834750048e-07 2.4469827613715905e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.856e-09\n",
      "‖w_svm‖₂       : 1.1484535480696609e-07\n",
      "‖alpha‖₁       : 0.5199999999999586\n",
      "scores min/max : 3.13798530234956e-07 3.352316369201417e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.666e-19\n",
      "‖w_svm‖₂       : 0.13761281389449834\n",
      "‖alpha‖₁       : 0.8799999999999857\n",
      "scores min/max : -1.3463794548578327 3.584691352275748\n",
      "Mask mean value:  tensor(0.3149, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2576  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.467e-05\n",
      "‖w_svm‖₂       : 0.07545538074542128\n",
      "‖alpha‖₁       : 0.6584682380274675\n",
      "scores min/max : -2.082252560797858 4.114995851976724\n",
      "Mask mean value:  tensor(0.2752, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.1718  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.490e-03\n",
      "‖w_svm‖₂       : 0.0175056201655118\n",
      "‖alpha‖₁       : 0.6815391859018215\n",
      "scores min/max : -1.9740375479412635 0.055791443472267686\n",
      "Mask mean value:  tensor(0.6041, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2513  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.907e-14\n",
      "‖w_svm‖₂       : 0.1880545589917789\n",
      "‖alpha‖₁       : 0.8843604679098983\n",
      "scores min/max : -3.6029709260938185 6.0757661787282915\n",
      "Mask mean value:  tensor(0.0932, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1951  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.917e-03\n",
      "‖w_svm‖₂       : 0.032071494715847505\n",
      "‖alpha‖₁       : 0.8985959790533313\n",
      "scores min/max : -0.754425142140065 2.005824735054328\n",
      "Mask mean value:  tensor(0.8349, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3748  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.874e-15\n",
      "‖w_svm‖₂       : 5.473264205728347e-08\n",
      "‖alpha‖₁       : 0.11999999999998044\n",
      "scores min/max : -8.607051084443796e-08 -5.512839931681275e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.251e-08\n",
      "‖w_svm‖₂       : 1.0693138398665573e-07\n",
      "‖alpha‖₁       : 0.23999999999998947\n",
      "scores min/max : 2.5039563321660518e-08 3.594958681539236e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.378e-21\n",
      "‖w_svm‖₂       : 0.019609771584712797\n",
      "‖alpha‖₁       : 0.38337799912365267\n",
      "scores min/max : -1.9682205581626517 0.23349525973797308\n",
      "Mask mean value:  tensor(0.6840, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1475  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.842e-14\n",
      "‖w_svm‖₂       : 0.08330145325159315\n",
      "‖alpha‖₁       : 0.47211758965487255\n",
      "scores min/max : -2.3588239641195727 2.7241151870047173\n",
      "Mask mean value:  tensor(0.0886, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9755  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.707e-03\n",
      "‖w_svm‖₂       : 0.0005076630167243982\n",
      "‖alpha‖₁       : 0.4399999999999999\n",
      "scores min/max : -0.0006998131054335758 -0.00043854804488824045\n",
      "Mask mean value:  tensor(0.4967, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6445  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.136e-16\n",
      "‖w_svm‖₂       : 8.297063546132799e-08\n",
      "‖alpha‖₁       : 0.3799999999999999\n",
      "scores min/max : 3.118579435137545e-09 2.126905772735963e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.631e-22\n",
      "‖w_svm‖₂       : 0.08713437778660602\n",
      "‖alpha‖₁       : 0.5781957499591797\n",
      "scores min/max : -2.1295804742122617 5.799673848873018\n",
      "Mask mean value:  tensor(0.2588, dtype=torch.float64)\n",
      "max feasible return = 0.1085  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.98301419376073e-07\n",
      "‖alpha‖₁       : 0.58\n",
      "scores min/max : -3.5375773250859727e-07 -2.6304138464671606e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0348133091082975e-07\n",
      "‖alpha‖₁       : 0.29999999999999927\n",
      "scores min/max : 3.4405274792504365e-08 3.938365205663036e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 6.965052516432965e-08\n",
      "‖alpha‖₁       : 0.5999999999999999\n",
      "scores min/max : 1.2658095500028887e-08 4.936863261783527e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.687724612785725e-08\n",
      "‖alpha‖₁       : 0.3799999999999856\n",
      "scores min/max : 1.0606626807689859e-09 1.0492032155588235e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.9609281473411586e-06\n",
      "‖alpha‖₁       : 0.3199999999769429\n",
      "scores min/max : 2.791988375313871e-06 4.540719110656003e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4258  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.05531751613199725\n",
      "‖alpha‖₁       : 0.7397268452929199\n",
      "scores min/max : -3.4625976237923286 2.171227081840168\n",
      "Mask mean value:  tensor(0.8935, dtype=torch.float64)\n",
      "max feasible return = -0.9354  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03199545666839821\n",
      "‖alpha‖₁       : 0.6151675762350648\n",
      "scores min/max : -1.915508611426672 1.2162629523604302\n",
      "Mask mean value:  tensor(0.7864, dtype=torch.float64)\n",
      "max feasible return = 2.7553  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.7047997637010483e-07\n",
      "‖alpha‖₁       : 0.4599999999999677\n",
      "scores min/max : 2.1347306452173954e-07 8.087658555251322e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.373688091613728e-07\n",
      "‖alpha‖₁       : 0.5199999999999892\n",
      "scores min/max : 6.17817298084346e-08 7.468853255850929e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.57855803284092e-07\n",
      "‖alpha‖₁       : 0.5799999999999756\n",
      "scores min/max : -2.5395800072509697e-07 -2.1760379239146933e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.0453967080824173\n",
      "‖alpha‖₁       : 0.6877703793296563\n",
      "scores min/max : -0.43144626869489583 1.9573951427019933\n",
      "Mask mean value:  tensor(0.4068, dtype=torch.float64)\n",
      "max feasible return = 0.0386  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.032931512845220785\n",
      "‖alpha‖₁       : 0.42216355993879645\n",
      "scores min/max : -3.910632390031452 5.2244837167280265\n",
      "Mask mean value:  tensor(0.0796, dtype=torch.float64)\n",
      "max feasible return = 0.5013  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04522198386770805\n",
      "‖alpha‖₁       : 0.8211438549129051\n",
      "scores min/max : -5.093775799029683 3.0716035942179927\n",
      "Mask mean value:  tensor(0.9425, dtype=torch.float64)\n",
      "max feasible return = -3.5650  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.008185219878808056\n",
      "‖alpha‖₁       : 0.7999999999999998\n",
      "scores min/max : -0.03837152696167354 0.006078285678390815\n",
      "Mask mean value:  tensor(0.4745, dtype=torch.float64)\n",
      "max feasible return = -0.1725  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.116144571780222e-07\n",
      "‖alpha‖₁       : 0.6199999999999528\n",
      "scores min/max : 4.80985561808253e-08 6.465289125010907e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.758870795714112e-08\n",
      "‖alpha‖₁       : 0.43999999999999995\n",
      "scores min/max : 9.1246034752414e-09 1.86946902914615e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.2808912356496411e-07\n",
      "‖alpha‖₁       : 0.27999999999999775\n",
      "scores min/max : -3.283531337202058e-07 -3.106846191575071e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  25 | train 0.005376 | val 0.006731\n",
      "-----------------------------------------Epoch:  26 ----------------------------------------\n",
      "‖w_svm‖₂       : 0.11006018459532589\n",
      "‖alpha‖₁       : 0.8732724255988744\n",
      "scores min/max : -12.201255412174024 2.141998945916059\n",
      "Mask mean value:  tensor(0.5094, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.5080  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.100e-02\n",
      "‖w_svm‖₂       : 0.020380369269154267\n",
      "‖alpha‖₁       : 0.7799999999999478\n",
      "scores min/max : -0.02279041383686311 0.04782145623450723\n",
      "Mask mean value:  tensor(0.4169, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.8294  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.605e-15\n",
      "‖w_svm‖₂       : 3.6353231790576466e-06\n",
      "‖alpha‖₁       : 0.4199999999927905\n",
      "scores min/max : -1.3889723570738877e-06 2.597209826178092e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1230  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.554e-05\n",
      "‖w_svm‖₂       : 0.017449455686112406\n",
      "‖alpha‖₁       : 0.6815392668993705\n",
      "scores min/max : -1.9741587140020962 0.05578446420073657\n",
      "Mask mean value:  tensor(0.6035, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2510  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.679e-12\n",
      "‖w_svm‖₂       : 0.007499360395549065\n",
      "‖alpha‖₁       : 0.6076789382077543\n",
      "scores min/max : -1.9859660711665361 0.3009289749995502\n",
      "Mask mean value:  tensor(0.5752, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2324  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.557e-15\n",
      "‖w_svm‖₂       : 0.047256553469977196\n",
      "‖alpha‖₁       : 0.9390091243210331\n",
      "scores min/max : -2.450406997345101 1.595195784737838\n",
      "Mask mean value:  tensor(0.1523, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4107  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.836e-03\n",
      "‖w_svm‖₂       : 1.0182300869615848e-06\n",
      "‖alpha‖₁       : 0.5999999999999939\n",
      "scores min/max : -4.372160535636759e-08 7.148430277383428e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.511e-19\n",
      "‖w_svm‖₂       : 0.00015987661049820484\n",
      "‖alpha‖₁       : 0.6399999999999995\n",
      "scores min/max : -9.470569226996505e-05 -8.495244460501899e-05\n",
      "Mask mean value:  tensor(0.4996, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7637  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.170e-17\n",
      "‖w_svm‖₂       : 2.386076943942683e-07\n",
      "‖alpha‖₁       : 0.5799999999999949\n",
      "scores min/max : -3.8204181722160486e-07 -3.664029368518987e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.963e-19\n",
      "‖w_svm‖₂       : 0.004335661813659025\n",
      "‖alpha‖₁       : 0.4599999999999902\n",
      "scores min/max : -0.007360020344688665 -0.005328126985704035\n",
      "Mask mean value:  tensor(0.4710, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.0470  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.037e-15\n",
      "‖w_svm‖₂       : 1.0695182096727568e-07\n",
      "‖alpha‖₁       : 0.23999999999998983\n",
      "scores min/max : 2.371212796396974e-08 3.461792977449367e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.384e-21\n",
      "‖w_svm‖₂       : 0.016606572608277893\n",
      "‖alpha‖₁       : 0.859999999999961\n",
      "scores min/max : -0.046968956655350676 -0.032121519839605345\n",
      "Mask mean value:  tensor(0.3271, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0805  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.279e-14\n",
      "‖w_svm‖₂       : 4.285181169609795e-07\n",
      "‖alpha‖₁       : 0.7199999999999986\n",
      "scores min/max : 2.896480719940015e-07 3.4692654099947124e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.015e-21\n",
      "‖w_svm‖₂       : 0.000507902000740694\n",
      "‖alpha‖₁       : 0.4399999999999999\n",
      "scores min/max : -0.0007160672166741909 -0.0004542862063289469\n",
      "Mask mean value:  tensor(0.4966, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6442  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.138e-16\n",
      "‖w_svm‖₂       : 0.13987888962476486\n",
      "‖alpha‖₁       : 0.6553445776731913\n",
      "scores min/max : -18.0778786198118 1.7939229152964744\n",
      "Mask mean value:  tensor(0.2343, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2778  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.817e-03\n",
      "‖w_svm‖₂       : 0.04321799228816696\n",
      "‖alpha‖₁       : 0.8900139445166577\n",
      "scores min/max : -2.0053482317108804 0.34227667436407533\n",
      "Mask mean value:  tensor(0.2760, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0754  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.752e-11\n",
      "‖w_svm‖₂       : 1.1482337993814992e-07\n",
      "‖alpha‖₁       : 0.5199999999999614\n",
      "scores min/max : 3.201901880214773e-07 3.416171066038285e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.662e-19\n",
      "‖w_svm‖₂       : 8.31049947778485e-08\n",
      "‖alpha‖₁       : 0.3799999999999999\n",
      "scores min/max : 3.080714024704981e-09 2.1231266467264043e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.647e-22\n",
      "‖w_svm‖₂       : 5.7000648383741536e-08\n",
      "‖alpha‖₁       : 0.23999999999998967\n",
      "scores min/max : -6.702158883972998e-08 -5.2548406983024476e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.349e-20\n",
      "‖w_svm‖₂       : 5.473294166095403e-08\n",
      "‖alpha‖₁       : 0.11999999999998041\n",
      "scores min/max : -8.647034960512307e-08 -5.5527062427207636e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.205e-07\n",
      "‖w_svm‖₂       : 0.0473352519639767\n",
      "‖alpha‖₁       : 0.9056466131151397\n",
      "scores min/max : -0.7134675785863124 1.8928471217116254\n",
      "Mask mean value:  tensor(0.3785, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7017  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.165e-04\n",
      "‖w_svm‖₂       : 0.0574462425200816\n",
      "‖alpha‖₁       : 0.5763251748852303\n",
      "scores min/max : -1.9576475095419865 0.8711788485305101\n",
      "Mask mean value:  tensor(0.7118, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9374  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.087e-03\n",
      "‖w_svm‖₂       : 0.07454217892273424\n",
      "‖alpha‖₁       : 0.41912409386894\n",
      "scores min/max : -1.881075196294177 2.3242709723500097\n",
      "Mask mean value:  tensor(0.7867, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8948  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.510e-04\n",
      "‖w_svm‖₂       : 0.00021092069241034617\n",
      "‖alpha‖₁       : 0.6199999999985928\n",
      "scores min/max : 4.044647773898319e-05 5.621208078110809e-05\n",
      "Mask mean value:  tensor(0.5002, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.193e-17\n",
      "‖w_svm‖₂       : 0.0008794334494156621\n",
      "‖alpha‖₁       : 0.8199999999999141\n",
      "scores min/max : 0.001012952127599838 0.0027468441590742065\n",
      "Mask mean value:  tensor(0.5105, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5935  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.020e-16\n",
      "‖w_svm‖₂       : 7.234166610133585e-08\n",
      "‖alpha‖₁       : 0.4199999999999994\n",
      "scores min/max : -7.877635592862433e-08 -3.166707561252492e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.807e-08\n",
      "‖w_svm‖₂       : 6.709370309829387e-07\n",
      "‖alpha‖₁       : 0.4\n",
      "scores min/max : -5.4810191777941426e-08 2.5140805677987535e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.251e-09\n",
      "‖w_svm‖₂       : 7.266703137152443e-08\n",
      "‖alpha‖₁       : 0.1399999999999978\n",
      "scores min/max : 6.0990354195017245e-09 1.1841179362242511e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.995e-08\n",
      "‖w_svm‖₂       : 0.02263966097134847\n",
      "‖alpha‖₁       : 0.6599999999999999\n",
      "scores min/max : -0.04436072151751842 0.04557806130874188\n",
      "Mask mean value:  tensor(0.4363, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.4522  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.592e-15\n",
      "‖w_svm‖₂       : 0.017532630243122748\n",
      "‖alpha‖₁       : 0.86\n",
      "scores min/max : -0.045926224258001136 0.021696762626626474\n",
      "Mask mean value:  tensor(0.4087, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5229  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.708e-14\n",
      "‖w_svm‖₂       : 0.005861471904928589\n",
      "‖alpha‖₁       : 0.7005668200397959\n",
      "scores min/max : -2.0044301118031185 0.01896777880048455\n",
      "Mask mean value:  tensor(0.4527, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2539  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.931e-05\n",
      "‖w_svm‖₂       : 0.1475574306818337\n",
      "‖alpha‖₁       : 0.7597241994043049\n",
      "scores min/max : -1.7482508438578137 2.2291779258579667\n",
      "Mask mean value:  tensor(0.8674, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.122e-11\n",
      "‖w_svm‖₂       : 0.005664695108814195\n",
      "‖alpha‖₁       : 0.5599999999999998\n",
      "scores min/max : 0.0045144313839345415 0.0061805493933089675\n",
      "Mask mean value:  tensor(0.5241, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0985  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 3.718e-05\n",
      "‖w_svm‖₂       : 0.02141040350465109\n",
      "‖alpha‖₁       : 0.8233661415345868\n",
      "scores min/max : -1.8955038738195569 1.502899248249308\n",
      "Mask mean value:  tensor(0.7106, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.4108  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.605e-06\n",
      "‖w_svm‖₂       : 1.0816650067810813e-06\n",
      "‖alpha‖₁       : 0.4999999999999945\n",
      "scores min/max : -2.8125338142333006e-07 -8.831409518966604e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.951e-19\n",
      "‖w_svm‖₂       : 0.019495720831815378\n",
      "‖alpha‖₁       : 0.3833784584789536\n",
      "scores min/max : -1.9673296189435923 0.2345059404509803\n",
      "Mask mean value:  tensor(0.6872, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1481  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.671e-14\n",
      "‖w_svm‖₂       : 1.941024648177785e-07\n",
      "‖alpha‖₁       : 0.3799999999999995\n",
      "scores min/max : -4.8570286051510564e-08 -1.2891251966314526e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.604e-19\n",
      "‖w_svm‖₂       : 8.755949624286263e-08\n",
      "‖alpha‖₁       : 0.17999999999999072\n",
      "scores min/max : -9.943681375730557e-08 1.364445664100842e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.866e-21\n",
      "‖w_svm‖₂       : 0.016384645217725147\n",
      "‖alpha‖₁       : 0.5962589417967608\n",
      "scores min/max : -2.9605738403852913 2.2627572797533206\n",
      "Mask mean value:  tensor(0.9110, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.5005  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.701e-05\n",
      "‖w_svm‖₂       : 2.8143998769989296e-07\n",
      "‖alpha‖₁       : 0.659999999999993\n",
      "scores min/max : -6.210809193943808e-07 -4.693811114764109e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.330e-18\n",
      "‖w_svm‖₂       : 1.7036785125179237e-05\n",
      "‖alpha‖₁       : 0.35999999998381504\n",
      "scores min/max : 1.4341935040409558e-05 1.5393355002059114e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.241e-17\n",
      "‖w_svm‖₂       : 5.488620010805655e-08\n",
      "‖alpha‖₁       : 0.43999999999998435\n",
      "scores min/max : -1.548330396530087e-07 -5.090575819179853e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.331e-20\n",
      "‖w_svm‖₂       : 0.18681991983601895\n",
      "‖alpha‖₁       : 0.8839669962188752\n",
      "scores min/max : -3.6008655032172316 6.081219631565347\n",
      "Mask mean value:  tensor(0.0941, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1977  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.634e-03\n",
      "‖w_svm‖₂       : 7.690653868105166e-08\n",
      "‖alpha‖₁       : 0.5399999999999996\n",
      "scores min/max : -1.1648271301415236e-07 -9.495757549193258e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.608e-20\n",
      "‖w_svm‖₂       : 0.05294280761980736\n",
      "‖alpha‖₁       : 0.8271988318540007\n",
      "scores min/max : -1.948155270064393 0.40177124057900965\n",
      "Mask mean value:  tensor(0.5293, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0110  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.637e-02\n",
      "‖w_svm‖₂       : 0.046291257451449935\n",
      "‖alpha‖₁       : 0.7294255575595031\n",
      "scores min/max : -0.24946437394982743 2.0324363165218116\n",
      "Mask mean value:  tensor(0.7139, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2130  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.230e-03\n",
      "‖w_svm‖₂       : 0.06141168595771731\n",
      "‖alpha‖₁       : 0.89868717455224\n",
      "scores min/max : -1.6969473611063888 3.713143647755636\n",
      "Mask mean value:  tensor(0.4339, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5824  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.049e-02\n",
      "‖w_svm‖₂       : 0.00014972052324675595\n",
      "‖alpha‖₁       : 0.4399999991596587\n",
      "scores min/max : -0.00027171389577485217 -8.78354011349726e-05\n",
      "Mask mean value:  tensor(0.4992, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0227  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.868e-17\n",
      "‖w_svm‖₂       : 2.374308743073603e-07\n",
      "‖alpha‖₁       : 0.6399999999999848\n",
      "scores min/max : 3.412593853602204e-07 3.6226827010048156e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.731e-20\n",
      "‖w_svm‖₂       : 2.3739046323126154e-07\n",
      "‖alpha‖₁       : 0.37999999999217626\n",
      "scores min/max : -3.281695146711687e-08 -1.3294279838036762e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.572e-07\n",
      "‖w_svm‖₂       : 2.002833812951523e-07\n",
      "‖alpha‖₁       : 0.41999999999997667\n",
      "scores min/max : -3.500341299174622e-07 -3.277779254320536e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.638e-19\n",
      "‖w_svm‖₂       : 0.028302838078295527\n",
      "‖alpha‖₁       : 0.5510595838687717\n",
      "scores min/max : -3.5259375748162975 1.045007438962673\n",
      "Mask mean value:  tensor(0.1246, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0326  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.601e-03\n",
      "‖w_svm‖₂       : 1.1574710624164033e-07\n",
      "‖alpha‖₁       : 0.2999999999999849\n",
      "scores min/max : 1.0404082180575575e-07 1.1306920668004768e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.252e-07\n",
      "‖w_svm‖₂       : 3.836835846678377e-07\n",
      "‖alpha‖₁       : 0.2799999999999968\n",
      "scores min/max : -1.4140966233435166e-06 -1.3523787068680453e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.551e-19\n",
      "‖w_svm‖₂       : 0.005437917348900312\n",
      "‖alpha‖₁       : 0.3799999999999967\n",
      "scores min/max : -0.005674592998417993 0.006246091849156851\n",
      "Mask mean value:  tensor(0.5160, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9180  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.750e-14\n",
      "‖w_svm‖₂       : 0.04391717538235838\n",
      "‖alpha‖₁       : 0.9199999999999667\n",
      "scores min/max : -0.3118672419232291 0.4014601931592373\n",
      "Mask mean value:  tensor(0.3122, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.5548  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 4.761e-02\n",
      "‖w_svm‖₂       : 7.81782812710206e-08\n",
      "‖alpha‖₁       : 0.6599999999999936\n",
      "scores min/max : 1.0052630629635333e-07 2.452156000584087e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.859e-09\n",
      "‖w_svm‖₂       : 0.13765050789938196\n",
      "‖alpha‖₁       : 0.879999999999985\n",
      "scores min/max : -1.3505139525473635 3.5859142374828035\n",
      "Mask mean value:  tensor(0.3089, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2566  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.763e-04\n",
      "‖w_svm‖₂       : 0.026983977395113965\n",
      "‖alpha‖₁       : 0.8522859927823239\n",
      "scores min/max : -2.915730751174709 1.5653271392487884\n",
      "Mask mean value:  tensor(0.1558, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9517  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.412e-03\n",
      "‖w_svm‖₂       : 0.07031127413629142\n",
      "‖alpha‖₁       : 0.5799999999999993\n",
      "scores min/max : -2.7175609961852634 1.5424534367845437\n",
      "Mask mean value:  tensor(0.1702, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3117  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.482e-04\n",
      "‖w_svm‖₂       : 0.03192075883511988\n",
      "‖alpha‖₁       : 0.8985957546627357\n",
      "scores min/max : -0.7527796582599431 2.007463312009903\n",
      "Mask mean value:  tensor(0.8374, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3702  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.711e-15\n",
      "‖w_svm‖₂       : 0.0001863185445247364\n",
      "‖alpha‖₁       : 0.8199999999999987\n",
      "scores min/max : -6.039722327116882e-05 -4.6010497205578875e-05\n",
      "Mask mean value:  tensor(0.4997, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6970  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.223e-17\n",
      "‖w_svm‖₂       : 2.2711640051027568e-07\n",
      "‖alpha‖₁       : 0.2599999999999918\n",
      "scores min/max : 3.9280213932815094e-08 5.2087382445278885e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.662e-17\n",
      "‖w_svm‖₂       : 0.08339269476936687\n",
      "‖alpha‖₁       : 0.47213790604321804\n",
      "scores min/max : -2.3677399124833114 2.715135730594932\n",
      "Mask mean value:  tensor(0.0857, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9452  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.863e-03\n",
      "‖w_svm‖₂       : 0.02797572036757552\n",
      "‖alpha‖₁       : 0.1966846245848282\n",
      "scores min/max : -2.219119608313048 0.004622210827569494\n",
      "Mask mean value:  tensor(0.0461, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3137  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.133e-03\n",
      "‖w_svm‖₂       : 1.6881454227381804e-07\n",
      "‖alpha‖₁       : 0.23999999999999094\n",
      "scores min/max : 2.696516594748412e-07 2.8501353400537706e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.245e-20\n",
      "‖w_svm‖₂       : 0.0031760016186416157\n",
      "‖alpha‖₁       : 0.5799999999999864\n",
      "scores min/max : 0.002822807047412468 0.004101864255063779\n",
      "Mask mean value:  tensor(0.5192, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3301  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.805e-13\n",
      "‖w_svm‖₂       : 0.0003625164429480078\n",
      "‖alpha‖₁       : 0.74\n",
      "scores min/max : -0.00026205822957775293 -0.00024529605560836534\n",
      "Mask mean value:  tensor(0.4988, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0616  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.175e-17\n",
      "‖w_svm‖₂       : 2.1548370458793703e-08\n",
      "‖alpha‖₁       : 0.11999999999999454\n",
      "scores min/max : -2.9419170065261627e-08 -1.4745305153691824e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.800e-09\n",
      "‖w_svm‖₂       : 1.1614940060521072e-06\n",
      "‖alpha‖₁       : 0.3199999999987528\n",
      "scores min/max : -2.0571501582524355e-06 -1.9419325095777125e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4898  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.806e-18\n",
      "‖w_svm‖₂       : 0.00030728704240640163\n",
      "‖alpha‖₁       : 0.41999999994591797\n",
      "scores min/max : 0.00017618459902390266 0.0004955922172915812\n",
      "Mask mean value:  tensor(0.5023, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0002  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.839e-15\n",
      "‖w_svm‖₂       : 4.801164149371012e-08\n",
      "‖alpha‖₁       : 0.1799999999999964\n",
      "scores min/max : 4.722647758990462e-08 6.190867045737281e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.024e-08\n",
      "‖w_svm‖₂       : 0.023187631464130092\n",
      "‖alpha‖₁       : 0.8150952972493373\n",
      "scores min/max : -11.520836481098833 2.0327641189764396\n",
      "Mask mean value:  tensor(0.7113, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9111  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.564e-06\n",
      "‖w_svm‖₂       : 0.040680058402037686\n",
      "‖alpha‖₁       : 0.9404146490891575\n",
      "scores min/max : -1.8293827892335006 0.30439111140242014\n",
      "Mask mean value:  tensor(0.4198, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3254  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.313e-09\n",
      "‖w_svm‖₂       : 0.04659640330661476\n",
      "‖alpha‖₁       : 0.66\n",
      "scores min/max : -0.2824753630138962 0.20728979407986975\n",
      "Mask mean value:  tensor(0.4905, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8884  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.243e-13\n",
      "‖w_svm‖₂       : 0.07483738678248696\n",
      "‖alpha‖₁       : 0.6584556088336435\n",
      "scores min/max : -2.075185711500623 4.139308818809603\n",
      "Mask mean value:  tensor(0.2843, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2892  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.380e-03\n",
      "‖w_svm‖₂       : 0.08709513810091349\n",
      "‖alpha‖₁       : 0.5781957943318847\n",
      "scores min/max : -2.134356348461376 5.7943035951795\n",
      "Mask mean value:  tensor(0.2507, dtype=torch.float64)\n",
      "max feasible return = 0.1071  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.961490280890374e-07\n",
      "‖alpha‖₁       : 0.579999999999997\n",
      "scores min/max : -3.587938265560347e-07 -2.67897799207159e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0269156638143443e-07\n",
      "‖alpha‖₁       : 0.29999999999999916\n",
      "scores min/max : 3.5303792393647964e-08 4.028422072466703e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 6.972112591629181e-08\n",
      "‖alpha‖₁       : 0.5999999999999648\n",
      "scores min/max : 1.4764106071288543e-08 5.1541769008509116e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.6541374806812904e-08\n",
      "‖alpha‖₁       : 0.37999999999996953\n",
      "scores min/max : 3.4965377582423574e-10 9.784448322680749e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.961103833451328e-06\n",
      "‖alpha‖₁       : 0.3199999999764938\n",
      "scores min/max : 2.852709857147583e-06 4.625146597526928e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4258  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.05480944895296965\n",
      "‖alpha‖₁       : 0.7397251530209079\n",
      "scores min/max : -3.467965313693834 2.166542680988853\n",
      "Mask mean value:  tensor(0.8900, dtype=torch.float64)\n",
      "max feasible return = -0.9333  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.031594375663012216\n",
      "‖alpha‖₁       : 0.6151669020282473\n",
      "scores min/max : -1.9155813246815914 1.2159550199089726\n",
      "Mask mean value:  tensor(0.7867, dtype=torch.float64)\n",
      "max feasible return = 2.7560  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.6594419893103335e-07\n",
      "‖alpha‖₁       : 0.4599999999999893\n",
      "scores min/max : 2.0729086922823168e-07 8.01911394487266e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.36295868471852e-07\n",
      "‖alpha‖₁       : 0.5199999999999894\n",
      "scores min/max : 6.383091264329032e-08 7.673708239375945e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.557684161770358e-07\n",
      "‖alpha‖₁       : 0.5799999999999789\n",
      "scores min/max : -2.5239643327855207e-07 -2.1605879590167214e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.045534458020212866\n",
      "‖alpha‖₁       : 0.6877891024890779\n",
      "scores min/max : -0.4346026811099761 1.9542525936556467\n",
      "Mask mean value:  tensor(0.3954, dtype=torch.float64)\n",
      "max feasible return = 0.0493  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03287233505065434\n",
      "‖alpha‖₁       : 0.4221682778210895\n",
      "scores min/max : -3.9088342213247924 5.227680484255694\n",
      "Mask mean value:  tensor(0.0805, dtype=torch.float64)\n",
      "max feasible return = 0.5072  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04494360847949826\n",
      "‖alpha‖₁       : 0.8211404832389597\n",
      "scores min/max : -5.091777740729228 3.0740799631275864\n",
      "Mask mean value:  tensor(0.9430, dtype=torch.float64)\n",
      "max feasible return = -3.5686  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.008086793920271113\n",
      "‖alpha‖₁       : 0.7999999999999998\n",
      "scores min/max : -0.03791040928885953 0.005570484028724509\n",
      "Mask mean value:  tensor(0.4732, dtype=torch.float64)\n",
      "max feasible return = -0.1720  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.0831476618996076e-07\n",
      "‖alpha‖₁       : 0.6199999999999535\n",
      "scores min/max : 4.963756869430087e-08 6.619112706271273e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.7097597678849175e-08\n",
      "‖alpha‖₁       : 0.4399999999999348\n",
      "scores min/max : 8.546309649143298e-09 1.813430605521253e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.2638205853297388e-07\n",
      "‖alpha‖₁       : 0.2799999999999972\n",
      "scores min/max : -3.1978915870911666e-07 -3.021100969554092e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  26 | train 0.005371 | val 0.006771\n",
      "-----------------------------------------Epoch:  27 ----------------------------------------\n",
      "‖w_svm‖₂       : 0.028455616406831065\n",
      "‖alpha‖₁       : 0.5510675661899562\n",
      "scores min/max : -3.5225418176768177 1.0484280479790895\n",
      "Mask mean value:  tensor(0.1261, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0422  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.757e-03\n",
      "‖w_svm‖₂       : 0.019381400016651035\n",
      "‖alpha‖₁       : 0.38337872153475194\n",
      "scores min/max : -1.9672275841430467 0.23469692830530986\n",
      "Mask mean value:  tensor(0.6876, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1483  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.670e-14\n",
      "‖w_svm‖₂       : 0.08313084820052756\n",
      "‖alpha‖₁       : 0.47209409570784266\n",
      "scores min/max : -2.351832969916188 2.7310372934406275\n",
      "Mask mean value:  tensor(0.0909, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0003  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.520e-03\n",
      "‖w_svm‖₂       : 3.799350896446157e-07\n",
      "‖alpha‖₁       : 0.2799999999999968\n",
      "scores min/max : -1.3957572064920239e-06 -1.3340357563206409e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.544e-19\n",
      "‖w_svm‖₂       : 4.240141114104181e-07\n",
      "‖alpha‖₁       : 0.72\n",
      "scores min/max : 2.659459749554237e-07 3.2313738388942245e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.986e-21\n",
      "‖w_svm‖₂       : 0.14795989396364295\n",
      "‖alpha‖₁       : 0.7598465685900238\n",
      "scores min/max : -1.7477643586799025 2.2297460057311986\n",
      "Mask mean value:  tensor(0.8680, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3586  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.402e-11\n",
      "‖w_svm‖₂       : 0.07470223954562683\n",
      "‖alpha‖₁       : 0.6584403182409376\n",
      "scores min/max : -2.0712447032831087 4.144359220050407\n",
      "Mask mean value:  tensor(0.2899, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.3587  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.298e-03\n",
      "‖w_svm‖₂       : 2.2591397195116794e-07\n",
      "‖alpha‖₁       : 0.259999999999993\n",
      "scores min/max : 4.9101036343223556e-08 6.190041761407064e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.623e-17\n",
      "‖w_svm‖₂       : 0.04596337059506981\n",
      "‖alpha‖₁       : 0.7294248565515277\n",
      "scores min/max : -0.2512008233557551 2.0306507221311723\n",
      "Mask mean value:  tensor(0.7088, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.1909  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.281e-03\n",
      "‖w_svm‖₂       : 2.06034726092103e-07\n",
      "‖alpha‖₁       : 0.37999999999542783\n",
      "scores min/max : -4.4210699885340646e-08 -2.594441478464474e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.826e-07\n",
      "‖w_svm‖₂       : 7.2085085702278e-08\n",
      "‖alpha‖₁       : 0.1399999999999973\n",
      "scores min/max : 6.66735401452593e-09 1.2424311696443912e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.944e-08\n",
      "‖w_svm‖₂       : 0.04656538870520205\n",
      "‖alpha‖₁       : 0.6599999999999819\n",
      "scores min/max : -0.2822772732651897 0.20698737204640083\n",
      "Mask mean value:  tensor(0.4900, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8877  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.251e-13\n",
      "‖w_svm‖₂       : 0.040650875861733576\n",
      "‖alpha‖₁       : 0.9404128117268802\n",
      "scores min/max : -1.8303629749387893 0.3034145805236731\n",
      "Mask mean value:  tensor(0.4160, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3252  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.040e-11\n",
      "‖w_svm‖₂       : 0.13935952477940777\n",
      "‖alpha‖₁       : 0.6552124631281204\n",
      "scores min/max : -18.06696460208465 1.806795141158235\n",
      "Mask mean value:  tensor(0.2452, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2811  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.107e-03\n",
      "‖w_svm‖₂       : 7.621725453302422e-08\n",
      "‖alpha‖₁       : 0.5399999999999996\n",
      "scores min/max : -1.2457699331203666e-07 -1.0305412849470089e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.605e-20\n",
      "‖w_svm‖₂       : 0.022353908628610974\n",
      "‖alpha‖₁       : 0.6599999999999997\n",
      "scores min/max : -0.04506631533508338 0.04282693079282856\n",
      "Mask mean value:  tensor(0.4293, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.3963  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.595e-15\n",
      "‖w_svm‖₂       : 6.704600850927514e-07\n",
      "‖alpha‖₁       : 0.3999999999999999\n",
      "scores min/max : -5.592678604700031e-08 2.502409023094187e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.257e-09\n",
      "‖w_svm‖₂       : 2.3680719793803415e-07\n",
      "‖alpha‖₁       : 0.5799999999999899\n",
      "scores min/max : -3.634295684238634e-07 -3.4777472286708166e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.946e-19\n",
      "‖w_svm‖₂       : 0.017407890290870157\n",
      "‖alpha‖₁       : 0.8599999999999839\n",
      "scores min/max : -0.04830947754327441 0.018567230669483986\n",
      "Mask mean value:  tensor(0.3960, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5071  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.288e-13\n",
      "‖w_svm‖₂       : 0.057402334450600466\n",
      "‖alpha‖₁       : 0.5763239079255669\n",
      "scores min/max : -1.9556279794828935 0.8733016422894554\n",
      "Mask mean value:  tensor(0.7183, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9557  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.122e-03\n",
      "‖w_svm‖₂       : 0.00021307129562950613\n",
      "‖alpha‖₁       : 0.619999999992035\n",
      "scores min/max : 3.7111958120640204e-05 5.320199318345172e-05\n",
      "Mask mean value:  tensor(0.5002, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.253e-17\n",
      "‖w_svm‖₂       : 0.047095992742305345\n",
      "‖alpha‖₁       : 0.9389910751288493\n",
      "scores min/max : -2.4573991308367966 1.5882085973436144\n",
      "Mask mean value:  tensor(0.1475, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4021  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.235e-03\n",
      "‖w_svm‖₂       : 2.158529676284048e-08\n",
      "‖alpha‖₁       : 0.11999999999999421\n",
      "scores min/max : -2.8619871166676552e-08 -1.3930468560517379e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.803e-09\n",
      "‖w_svm‖₂       : 0.027059950009237593\n",
      "‖alpha‖₁       : 0.8522883516912145\n",
      "scores min/max : -2.918429664969575 1.562650156415994\n",
      "Mask mean value:  tensor(0.1546, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9503  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.409e-03\n",
      "‖w_svm‖₂       : 0.021187386546225116\n",
      "‖alpha‖₁       : 0.8233651880830717\n",
      "scores min/max : -1.896406452369281 1.501858043222952\n",
      "Mask mean value:  tensor(0.7082, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.4048  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.457e-06\n",
      "‖w_svm‖₂       : 0.04297464421999748\n",
      "‖alpha‖₁       : 0.8899949763920671\n",
      "scores min/max : -2.00634781760781 0.3413301298800874\n",
      "Mask mean value:  tensor(0.2731, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0739  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.248e-11\n",
      "‖w_svm‖₂       : 1.7400113308757867e-05\n",
      "‖alpha‖₁       : 0.3599999999827448\n",
      "scores min/max : 1.4325894413622032e-05 1.5413633507630983e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.402e-17\n",
      "‖w_svm‖₂       : 0.016705981205408556\n",
      "‖alpha‖₁       : 0.8599999999999607\n",
      "scores min/max : -0.04833120788786175 -0.033305383186831974\n",
      "Mask mean value:  tensor(0.3217, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0786  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.344e-14\n",
      "‖w_svm‖₂       : 1.145204532743741e-07\n",
      "‖alpha‖₁       : 0.2999999999999862\n",
      "scores min/max : 1.0811403197177609e-07 1.1713533285115393e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.250e-07\n",
      "‖w_svm‖₂       : 5.457513966779463e-08\n",
      "‖alpha‖₁       : 0.43999999999998385\n",
      "scores min/max : -1.6084842116938817e-07 -5.6911130383621785e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.350e-20\n",
      "‖w_svm‖₂       : 5.694477092459818e-08\n",
      "‖alpha‖₁       : 0.23999999999996569\n",
      "scores min/max : -6.143037342009687e-08 -4.68042112334066e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.362e-20\n",
      "‖w_svm‖₂       : 0.005652967876434993\n",
      "‖alpha‖₁       : 0.559999999999987\n",
      "scores min/max : 0.0046928372850846955 0.006351577008109629\n",
      "Mask mean value:  tensor(0.5249, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1004  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 3.025e-05\n",
      "‖w_svm‖₂       : 0.0031689933423229353\n",
      "‖alpha‖₁       : 0.5799999999999561\n",
      "scores min/max : 0.0023721580570955064 0.0036450688549820587\n",
      "Mask mean value:  tensor(0.5169, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3287  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.303e-13\n",
      "‖w_svm‖₂       : 0.000878597789799355\n",
      "‖alpha‖₁       : 0.8199999999999157\n",
      "scores min/max : 0.0010447347531979713 0.0027640034811549723\n",
      "Mask mean value:  tensor(0.5106, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5941  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.021e-16\n",
      "‖w_svm‖₂       : 1.0571109678618621e-07\n",
      "‖alpha‖₁       : 0.23999999999999078\n",
      "scores min/max : 2.7848394504531865e-08 3.874308618459491e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.322e-21\n",
      "‖w_svm‖₂       : 1.157969430744846e-06\n",
      "‖alpha‖₁       : 0.3199999999987692\n",
      "scores min/max : -2.076193759454493e-06 -1.9610995357531244e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4898  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.797e-18\n",
      "‖w_svm‖₂       : 0.0003068954804883842\n",
      "‖alpha‖₁       : 0.4199999999997722\n",
      "scores min/max : 0.00016957289699358607 0.000488701311614847\n",
      "Mask mean value:  tensor(0.5023, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0001  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.831e-15\n",
      "‖w_svm‖₂       : 0.02773993306291654\n",
      "‖alpha‖₁       : 0.1966712348043684\n",
      "scores min/max : -2.211225236371515 0.0124755001082716\n",
      "Mask mean value:  tensor(0.0523, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3550  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.171e-04\n",
      "‖w_svm‖₂       : 5.360448836620093e-08\n",
      "‖alpha‖₁       : 0.11999999999999889\n",
      "scores min/max : -8.622952776286681e-08 -5.557623917131669e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.921e-08\n",
      "‖w_svm‖₂       : 2.786895562133884e-07\n",
      "‖alpha‖₁       : 0.6599999999999998\n",
      "scores min/max : -6.188639027106217e-07 -4.6722742198208737e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.321e-18\n",
      "‖w_svm‖₂       : 0.00018723667512819656\n",
      "‖alpha‖₁       : 0.819999999999999\n",
      "scores min/max : -6.184659513605562e-05 -4.731807146654023e-05\n",
      "Mask mean value:  tensor(0.4997, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6970  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.228e-17\n",
      "‖w_svm‖₂       : 1.0719010978718317e-06\n",
      "‖alpha‖₁       : 0.4999999999999958\n",
      "scores min/max : -2.7300981296489037e-07 -8.008772357405495e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.853e-19\n",
      "‖w_svm‖₂       : 0.03156431730629279\n",
      "‖alpha‖₁       : 0.8985791201884774\n",
      "scores min/max : -0.7522622538583251 2.0080059885077697\n",
      "Mask mean value:  tensor(0.8383, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3688  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.565e-15\n",
      "‖w_svm‖₂       : 1.682287950084148e-07\n",
      "‖alpha‖₁       : 0.2399999999999911\n",
      "scores min/max : 2.748371788521037e-07 2.9019641838643646e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.212e-20\n",
      "‖w_svm‖₂       : 1.9265519974883402e-07\n",
      "‖alpha‖₁       : 0.3799999999999997\n",
      "scores min/max : -3.822645930775928e-08 -2.5597540139915537e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.566e-19\n",
      "‖w_svm‖₂       : 8.958360420256667e-08\n",
      "‖alpha‖₁       : 0.17999999999997535\n",
      "scores min/max : -1.1984415426535578e-07 1.1798476318822848e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.100e-21\n",
      "‖w_svm‖₂       : 7.967943133864405e-08\n",
      "‖alpha‖₁       : 0.6599999999999648\n",
      "scores min/max : 1.0688144188623729e-07 2.51125762552643e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.939e-09\n",
      "‖w_svm‖₂       : 2.347684836332254e-07\n",
      "‖alpha‖₁       : 0.6399999999999919\n",
      "scores min/max : 3.433590145514508e-07 3.6433727011448184e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.616e-20\n",
      "‖w_svm‖₂       : 0.023168265089722546\n",
      "‖alpha‖₁       : 0.8150948324084539\n",
      "scores min/max : -11.52110569552368 2.0324628279914654\n",
      "Mask mean value:  tensor(0.7106, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9096  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.718e-06\n",
      "‖w_svm‖₂       : 0.017167262633379244\n",
      "‖alpha‖₁       : 0.6815393430554086\n",
      "scores min/max : -1.9750646701919374 0.05540608384573745\n",
      "Mask mean value:  tensor(0.5995, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2488  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.497e-14\n",
      "‖w_svm‖₂       : 0.020492896049755637\n",
      "‖alpha‖₁       : 0.7799999999999372\n",
      "scores min/max : -0.022252797316276884 0.0491440628787721\n",
      "Mask mean value:  tensor(0.4198, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.8425  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.557e-15\n",
      "‖w_svm‖₂       : 0.016317074997544985\n",
      "‖alpha‖₁       : 0.5962614325570502\n",
      "scores min/max : -2.959673221514021 2.26563056372538\n",
      "Mask mean value:  tensor(0.9125, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.5025  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.625e-10\n",
      "‖w_svm‖₂       : 0.0043471805031110086\n",
      "‖alpha‖₁       : 0.45999999999999963\n",
      "scores min/max : -0.007575669514224056 -0.005533363340786732\n",
      "Mask mean value:  tensor(0.4700, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.0425  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.043e-15\n",
      "‖w_svm‖₂       : 0.0003634935538763234\n",
      "‖alpha‖₁       : 0.7399999999999989\n",
      "scores min/max : -0.00025149041609338424 -0.00023463777784967177\n",
      "Mask mean value:  tensor(0.4988, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0618  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.171e-17\n",
      "‖w_svm‖₂       : 0.1090876596648459\n",
      "‖alpha‖₁       : 0.8732681493788123\n",
      "scores min/max : -12.203344440263624 2.137940085617057\n",
      "Mask mean value:  tensor(0.4996, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.4802  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 2.499e-02\n",
      "‖w_svm‖₂       : 1.9838169742989312e-07\n",
      "‖alpha‖₁       : 0.41999999999997584\n",
      "scores min/max : -3.7341516552919374e-07 -3.511568490234597e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.640e-19\n",
      "‖w_svm‖₂       : 0.0740418609698239\n",
      "‖alpha‖₁       : 0.4190914640681925\n",
      "scores min/max : -1.8871319976649803 2.3090069774561344\n",
      "Mask mean value:  tensor(0.7819, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8949  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.584e-04\n",
      "‖w_svm‖₂       : 0.04764089054449086\n",
      "‖alpha‖₁       : 0.9056469542849125\n",
      "scores min/max : -0.7247241680688298 1.8893961996195343\n",
      "Mask mean value:  tensor(0.3707, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6867  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.675e-03\n",
      "‖w_svm‖₂       : 0.0001612223108841713\n",
      "‖alpha‖₁       : 0.6399999999999993\n",
      "scores min/max : -0.0001020694532240421 -9.215156010993224e-05\n",
      "Mask mean value:  tensor(0.4995, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7637  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.197e-17\n",
      "‖w_svm‖₂       : 0.052627355902004236\n",
      "‖alpha‖₁       : 0.8271779283486795\n",
      "scores min/max : -1.9499164212959987 0.4001138301126786\n",
      "Mask mean value:  tensor(0.5232, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0102  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.674e-02\n",
      "‖w_svm‖₂       : 8.264605283456834e-08\n",
      "‖alpha‖₁       : 0.3799999999999998\n",
      "scores min/max : 4.234250260367145e-09 2.2384880423090654e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.628e-22\n",
      "‖w_svm‖₂       : 1.0096053506308e-06\n",
      "‖alpha‖₁       : 0.5999999999999943\n",
      "scores min/max : -6.444434048532858e-08 -1.3577961199401702e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.497e-19\n",
      "‖w_svm‖₂       : 0.043490516230004155\n",
      "‖alpha‖₁       : 0.9199999999955644\n",
      "scores min/max : -0.3155148056581348 0.38249743046549056\n",
      "Mask mean value:  tensor(0.2809, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3934  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 4.079e-02\n",
      "‖w_svm‖₂       : 7.174346133629283e-08\n",
      "‖alpha‖₁       : 0.42\n",
      "scores min/max : -7.649703404776643e-08 -2.9387235154291885e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.772e-08\n",
      "‖w_svm‖₂       : 1.1360555350000531e-07\n",
      "‖alpha‖₁       : 0.519999999999962\n",
      "scores min/max : 3.156910735965924e-07 3.3711340532929427e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.666e-19\n",
      "‖w_svm‖₂       : 0.0708463654975218\n",
      "‖alpha‖₁       : 0.5799999999999996\n",
      "scores min/max : -2.7548349431130914 1.5708331349014815\n",
      "Mask mean value:  tensor(0.1768, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3219  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.509e-04\n",
      "‖w_svm‖₂       : 0.18655031735041644\n",
      "‖alpha‖₁       : 0.8839079743326211\n",
      "scores min/max : -3.6034812607967455 6.079766163000196\n",
      "Mask mean value:  tensor(0.0939, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1978  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.031e-03\n",
      "‖w_svm‖₂       : 0.00014261769979856767\n",
      "‖alpha‖₁       : 0.43999999985959093\n",
      "scores min/max : -0.0002390012163165289 -9.507173592001992e-05\n",
      "Mask mean value:  tensor(0.4993, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0230  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.763e-17\n",
      "‖w_svm‖₂       : 0.13841113760870521\n",
      "‖alpha‖₁       : 0.879999999999977\n",
      "scores min/max : -1.3594651139389904 3.633788183194427\n",
      "Mask mean value:  tensor(0.3231, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2603  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.617e-10\n",
      "‖w_svm‖₂       : 3.6473657140242657e-06\n",
      "‖alpha‖₁       : 0.4199999999926637\n",
      "scores min/max : -1.7450192927185875e-06 -1.0136993586636064e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1229  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.557e-05\n",
      "‖w_svm‖₂       : 0.0005048913781160861\n",
      "‖alpha‖₁       : 0.43999999999994505\n",
      "scores min/max : -0.0007173663257426058 -0.00045909297000683324\n",
      "Mask mean value:  tensor(0.4966, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6442  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.161e-16\n",
      "‖w_svm‖₂       : 4.768803171040666e-08\n",
      "‖alpha‖₁       : 0.17999999999999988\n",
      "scores min/max : 4.472490513258543e-08 5.922931918080252e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 4.967e-08\n",
      "‖w_svm‖₂       : 0.0076235865699051004\n",
      "‖alpha‖₁       : 0.6076807188777057\n",
      "scores min/max : -1.9871745693330731 0.2996882867835735\n",
      "Mask mean value:  tensor(0.5699, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2303  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.951e-15\n",
      "‖w_svm‖₂       : 0.0059237047444622715\n",
      "‖alpha‖₁       : 0.700567616532108\n",
      "scores min/max : -2.0034405411626346 0.019960851032857253\n",
      "Mask mean value:  tensor(0.4576, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2564  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.513e-06\n",
      "‖w_svm‖₂       : 0.005442054090527795\n",
      "‖alpha‖₁       : 0.3799999999999485\n",
      "scores min/max : -0.006013350071417373 0.006258919755490713\n",
      "Mask mean value:  tensor(0.5155, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9169  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.786e-14\n",
      "‖w_svm‖₂       : 0.0614857695482507\n",
      "‖alpha‖₁       : 0.8987138026487524\n",
      "scores min/max : -1.6996645217829365 3.7106978364786687\n",
      "Mask mean value:  tensor(0.4286, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5785  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.129e-02\n",
      "‖w_svm‖₂       : 0.08687488015943876\n",
      "‖alpha‖₁       : 0.5781657447879179\n",
      "scores min/max : -2.1409343978112396 5.7871519468565165\n",
      "Mask mean value:  tensor(0.2401, dtype=torch.float64)\n",
      "max feasible return = 0.1053  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.9354572162131864e-07\n",
      "‖alpha‖₁       : 0.5799999999999969\n",
      "scores min/max : -3.625823248739089e-07 -2.7168495378408265e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.020328170423829e-07\n",
      "‖alpha‖₁       : 0.2999999999999991\n",
      "scores min/max : 3.67386039394717e-08 4.171970082436677e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 6.952387515376428e-08\n",
      "‖alpha‖₁       : 0.5999999999999731\n",
      "scores min/max : 1.6923728120223985e-08 5.370780789085124e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.619809002880071e-08\n",
      "‖alpha‖₁       : 0.3799999999999999\n",
      "scores min/max : 3.8190275021838327e-10 9.75361979442611e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.957813638022507e-06\n",
      "‖alpha‖₁       : 0.3199999999760172\n",
      "scores min/max : 2.975030017366878e-06 4.769621154620368e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4258  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.05437816663529737\n",
      "‖alpha‖₁       : 0.7397214824228722\n",
      "scores min/max : -3.477126991424746 2.1579304057302107\n",
      "Mask mean value:  tensor(0.8832, dtype=torch.float64)\n",
      "max feasible return = -0.9288  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.031274552438165806\n",
      "‖alpha‖₁       : 0.6151663445192841\n",
      "scores min/max : -1.9149947639292486 1.216356639257555\n",
      "Mask mean value:  tensor(0.7885, dtype=torch.float64)\n",
      "max feasible return = 2.7622  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.627712025276948e-07\n",
      "‖alpha‖₁       : 0.4599999999999899\n",
      "scores min/max : 2.1566478147966596e-07 8.102446403925267e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.3546976207267682e-07\n",
      "‖alpha‖₁       : 0.5199999999999908\n",
      "scores min/max : 6.456129970694491e-08 7.746828960543707e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.5413471286804636e-07\n",
      "‖alpha‖₁       : 0.5799999999999794\n",
      "scores min/max : -2.540251308482941e-07 -2.176931742756304e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.045274685158315106\n",
      "‖alpha‖₁       : 0.6877837994597734\n",
      "scores min/max : -0.4397032993555424 1.9492411071093507\n",
      "Mask mean value:  tensor(0.3772, dtype=torch.float64)\n",
      "max feasible return = 0.0659  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03282723830103098\n",
      "‖alpha‖₁       : 0.42217213127068465\n",
      "scores min/max : -3.9089342928014696 5.228685782128222\n",
      "Mask mean value:  tensor(0.0807, dtype=torch.float64)\n",
      "max feasible return = 0.5082  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04471486910500921\n",
      "‖alpha‖₁       : 0.8211355821060411\n",
      "scores min/max : -5.086181090169765 3.0800242646400955\n",
      "Mask mean value:  tensor(0.9444, dtype=torch.float64)\n",
      "max feasible return = -3.5769  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.008008175459605135\n",
      "‖alpha‖₁       : 0.7999999999999999\n",
      "scores min/max : -0.03761074461431883 0.005096423673834354\n",
      "Mask mean value:  tensor(0.4718, dtype=torch.float64)\n",
      "max feasible return = -0.1715  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.056763465057774e-07\n",
      "‖alpha‖₁       : 0.6199999999999539\n",
      "scores min/max : 5.6020677796494636e-08 7.257394092017558e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.707591457248409e-08\n",
      "‖alpha‖₁       : 0.43999999999999995\n",
      "scores min/max : 8.380671140368472e-09 1.7950794180235648e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.2484455226098204e-07\n",
      "‖alpha‖₁       : 0.2799999999999973\n",
      "scores min/max : -3.1660643528466195e-07 -2.9893254837862227e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  27 | train 0.005359 | val 0.006863\n",
      "-----------------------------------------Epoch:  28 ----------------------------------------\n",
      "‖w_svm‖₂       : 2.0666454549547467e-07\n",
      "‖alpha‖₁       : 0.37999999999551315\n",
      "scores min/max : -5.166313656936559e-08 -3.328464712111424e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.672e-07\n",
      "‖w_svm‖₂       : 0.020463726394991964\n",
      "‖alpha‖₁       : 0.7799999999999414\n",
      "scores min/max : -0.026141260060649693 0.045066284337103524\n",
      "Mask mean value:  tensor(0.4010, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.7638  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.177e-15\n",
      "‖w_svm‖₂       : 0.016213688917342147\n",
      "‖alpha‖₁       : 0.5962622167452456\n",
      "scores min/max : -2.96014538352624 2.2675182356029637\n",
      "Mask mean value:  tensor(0.9134, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.5038  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.247e-08\n",
      "‖w_svm‖₂       : 4.2167463328832045e-07\n",
      "‖alpha‖₁       : 0.7199999999999984\n",
      "scores min/max : 2.654745208455449e-07 3.2269224481678437e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.978e-21\n",
      "‖w_svm‖₂       : 3.773974361189682e-07\n",
      "‖alpha‖₁       : 0.2799999999999969\n",
      "scores min/max : -1.3862059538525728e-06 -1.3244766477300332e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.544e-19\n",
      "‖w_svm‖₂       : 0.04708467645674044\n",
      "‖alpha‖₁       : 0.905645961828454\n",
      "scores min/max : -0.7029668369042607 1.8972687952592455\n",
      "Mask mean value:  tensor(0.3907, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7249  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.832e-05\n",
      "‖w_svm‖₂       : 0.02856390630265241\n",
      "‖alpha‖₁       : 0.5510755062434656\n",
      "scores min/max : -3.5198193123305046 1.051074786094474\n",
      "Mask mean value:  tensor(0.1273, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0495  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.839e-03\n",
      "‖w_svm‖₂       : 2.227727137813005e-07\n",
      "‖alpha‖₁       : 0.25999999999999995\n",
      "scores min/max : 5.8164360785970474e-08 7.08801004031366e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.516e-17\n",
      "‖w_svm‖₂       : 0.017221682516092143\n",
      "‖alpha‖₁       : 0.8600000000000001\n",
      "scores min/max : -0.04999049239744796 0.015564081450495822\n",
      "Mask mean value:  tensor(0.3855, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4936  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.093e-06\n",
      "‖w_svm‖₂       : 3.575123274429805e-06\n",
      "‖alpha‖₁       : 0.41999999999289517\n",
      "scores min/max : -1.8372485851484166e-06 -1.8920758137159642e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1229  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.526e-05\n",
      "‖w_svm‖₂       : 0.07041811123238537\n",
      "‖alpha‖₁       : 0.5799999999999992\n",
      "scores min/max : -2.723108471756462 1.548365777452386\n",
      "Mask mean value:  tensor(0.1731, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3158  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.539e-04\n",
      "‖w_svm‖₂       : 7.140820449783346e-08\n",
      "‖alpha‖₁       : 0.4199999999999836\n",
      "scores min/max : -7.625404882981695e-08 -2.8970652913867827e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.770e-08\n",
      "‖w_svm‖₂       : 0.019189705815994353\n",
      "‖alpha‖₁       : 0.3833794293012246\n",
      "scores min/max : -1.9659959923152845 0.23610553086295688\n",
      "Mask mean value:  tensor(0.6921, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1491  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.230e-14\n",
      "‖w_svm‖₂       : 1.0461612511026853e-07\n",
      "‖alpha‖₁       : 0.23999999999999205\n",
      "scores min/max : 3.055913103703498e-08 4.143623721052413e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.258e-21\n",
      "‖w_svm‖₂       : 1.7469498048692947e-05\n",
      "‖alpha‖₁       : 0.35999999998265564\n",
      "scores min/max : 1.44466425414012e-05 1.5546321340807895e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.471e-17\n",
      "‖w_svm‖₂       : 0.14838240778236744\n",
      "‖alpha‖₁       : 0.759999999984058\n",
      "scores min/max : -1.7479675368109773 2.225249909961629\n",
      "Mask mean value:  tensor(0.8650, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3586  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.868e-11\n",
      "‖w_svm‖₂       : 5.377982512938356e-08\n",
      "‖alpha‖₁       : 0.11999999999998245\n",
      "scores min/max : -8.817720995909313e-08 -5.725995251340047e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.181e-08\n",
      "‖w_svm‖₂       : 0.18755257785254337\n",
      "‖alpha‖₁       : 0.8842861368264112\n",
      "scores min/max : -3.605515133435489 6.077616014674038\n",
      "Mask mean value:  tensor(0.0935, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1971  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.561e-03\n",
      "‖w_svm‖₂       : 6.704780644831237e-07\n",
      "‖alpha‖₁       : 0.39999999999999974\n",
      "scores min/max : -4.6694732806269344e-08 2.5977535186372594e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.272e-09\n",
      "‖w_svm‖₂       : 0.02207008970767443\n",
      "‖alpha‖₁       : 0.6599999999999997\n",
      "scores min/max : -0.04502929222145484 0.04041906994676406\n",
      "Mask mean value:  tensor(0.4255, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.3651  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.581e-15\n",
      "‖w_svm‖₂       : 1.9135837214453054e-07\n",
      "‖alpha‖₁       : 0.37999999999999967\n",
      "scores min/max : -4.611439123670762e-08 -1.0450120523065943e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.542e-19\n",
      "‖w_svm‖₂       : 1.064034667357124e-06\n",
      "‖alpha‖₁       : 0.4999999999999971\n",
      "scores min/max : -2.764403339062616e-07 -8.354855536076846e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.801e-19\n",
      "‖w_svm‖₂       : 0.017060416472067328\n",
      "‖alpha‖₁       : 0.6815412182350807\n",
      "scores min/max : -1.974439585801547 0.05607072114940327\n",
      "Mask mean value:  tensor(0.6025, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2507  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.096e-13\n",
      "‖w_svm‖₂       : 2.156487967410073e-08\n",
      "‖alpha‖₁       : 0.11999999999999406\n",
      "scores min/max : -3.03910693021815e-08 -1.569462233032084e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.815e-09\n",
      "‖w_svm‖₂       : 0.004357402465050379\n",
      "‖alpha‖₁       : 0.4599999999999995\n",
      "scores min/max : -0.007589014583054058 -0.0055386881297766755\n",
      "Mask mean value:  tensor(0.4699, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.0423  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.044e-15\n",
      "‖w_svm‖₂       : 0.0833131212683602\n",
      "‖alpha‖₁       : 0.47213245243251\n",
      "scores min/max : -2.365003440937951 2.7177636866092665\n",
      "Mask mean value:  tensor(0.0866, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9543  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.297e-03\n",
      "‖w_svm‖₂       : 0.00018867021157453008\n",
      "‖alpha‖₁       : 0.8199999999999992\n",
      "scores min/max : -6.840713694274863e-05 -5.364880911985868e-05\n",
      "Mask mean value:  tensor(0.4997, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6969  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.251e-17\n",
      "‖w_svm‖₂       : 7.567360351016089e-08\n",
      "‖alpha‖₁       : 0.5399999999999996\n",
      "scores min/max : -1.2977203388566564e-07 -1.0824644419169541e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.606e-20\n",
      "‖w_svm‖₂       : 0.14017561262538455\n",
      "‖alpha‖₁       : 0.6554551318021248\n",
      "scores min/max : -18.09936444715524 1.7766461759859211\n",
      "Mask mean value:  tensor(0.2208, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2717  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.019e-03\n",
      "‖w_svm‖₂       : 5.4340069947590205e-08\n",
      "‖alpha‖₁       : 0.4399999999999833\n",
      "scores min/max : -1.6495606856290167e-07 -6.102508992309253e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.371e-20\n",
      "‖w_svm‖₂       : 0.023131674176929615\n",
      "‖alpha‖₁       : 0.8150951491340658\n",
      "scores min/max : -11.520777023086811 2.032199362058993\n",
      "Mask mean value:  tensor(0.7101, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9088  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.791e-06\n",
      "‖w_svm‖₂       : 0.0003157984158933341\n",
      "‖alpha‖₁       : 0.4199999994294962\n",
      "scores min/max : 0.0001738723546625137 0.0005047368518316818\n",
      "Mask mean value:  tensor(0.5024, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0002  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.914e-15\n",
      "‖w_svm‖₂       : 0.0005060461765108329\n",
      "‖alpha‖₁       : 0.4399999999999358\n",
      "scores min/max : -0.0007286380859022707 -0.000469031916379693\n",
      "Mask mean value:  tensor(0.4966, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6440  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.172e-16\n",
      "‖w_svm‖₂       : 0.021094237226795952\n",
      "‖alpha‖₁       : 0.8233691311967379\n",
      "scores min/max : -1.8967690948634437 1.501359950261869\n",
      "Mask mean value:  tensor(0.7073, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.4021  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.441e-06\n",
      "‖w_svm‖₂       : 0.0008785884346045846\n",
      "‖alpha‖₁       : 0.8199999999999745\n",
      "scores min/max : 0.0009360333809168434 0.0026480169676196454\n",
      "Mask mean value:  tensor(0.5101, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5912  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.031e-16\n",
      "‖w_svm‖₂       : 1.136127638996199e-07\n",
      "‖alpha‖₁       : 0.29999999999998783\n",
      "scores min/max : 1.1367585043093596e-07 1.2269028714665468e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.254e-07\n",
      "‖w_svm‖₂       : 0.061408130908807715\n",
      "‖alpha‖₁       : 0.89870701741914\n",
      "scores min/max : -1.6933339335163513 3.717274043417096\n",
      "Mask mean value:  tensor(0.4418, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5886  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.327e-03\n",
      "‖w_svm‖₂       : 0.04310383590308693\n",
      "‖alpha‖₁       : 0.9199999999999716\n",
      "scores min/max : -0.31864390820906435 0.3658144686579472\n",
      "Mask mean value:  tensor(0.2546, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2584  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 3.115e-02\n",
      "‖w_svm‖₂       : 0.027363579199533434\n",
      "‖alpha‖₁       : 0.8523051369870012\n",
      "scores min/max : -2.9124031926366043 1.5686303078437465\n",
      "Mask mean value:  tensor(0.1574, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9534  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.433e-03\n",
      "‖w_svm‖₂       : 0.01673380141650771\n",
      "‖alpha‖₁       : 0.8599999999999693\n",
      "scores min/max : -0.048552117875977394 -0.033476134479220876\n",
      "Mask mean value:  tensor(0.3209, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0783  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.365e-14\n",
      "‖w_svm‖₂       : 0.0315081556654338\n",
      "‖alpha‖₁       : 0.8985898059972928\n",
      "scores min/max : -0.747957938176835 2.012300842421378\n",
      "Mask mean value:  tensor(0.8447, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3577  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.591e-15\n",
      "‖w_svm‖₂       : 0.04097524250370814\n",
      "‖alpha‖₁       : 0.9404435084471946\n",
      "scores min/max : -1.8323352560940318 0.3014662051238906\n",
      "Mask mean value:  tensor(0.4083, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3247  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.418e-14\n",
      "‖w_svm‖₂       : 9.972938992596996e-07\n",
      "‖alpha‖₁       : 0.5999999999999981\n",
      "scores min/max : -8.221019186460353e-08 -3.135381269695867e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.498e-19\n",
      "‖w_svm‖₂       : 8.941956471664674e-08\n",
      "‖alpha‖₁       : 0.17999999999997224\n",
      "scores min/max : -1.3685606473957243e-07 1.0133673214806028e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.242e-21\n",
      "‖w_svm‖₂       : 0.005660065627288117\n",
      "‖alpha‖₁       : 0.5599999999999679\n",
      "scores min/max : 0.004713037312178772 0.0063755951641822845\n",
      "Mask mean value:  tensor(0.5250, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1006  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 3.013e-05\n",
      "‖w_svm‖₂       : 2.751490120642472e-07\n",
      "‖alpha‖₁       : 0.6599999999999995\n",
      "scores min/max : -6.372069698445413e-07 -4.855586124779386e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.317e-18\n",
      "‖w_svm‖₂       : 1.6603556525165926e-07\n",
      "‖alpha‖₁       : 0.239999999999992\n",
      "scores min/max : 2.7450873495128036e-07 2.898546109952729e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.226e-20\n",
      "‖w_svm‖₂       : 0.027719083786778075\n",
      "‖alpha‖₁       : 0.19667788545679016\n",
      "scores min/max : -2.2231846685768257 0.00030707420075580527\n",
      "Mask mean value:  tensor(0.0431, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2937  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.258e-03\n",
      "‖w_svm‖₂       : 0.005941437388856144\n",
      "‖alpha‖₁       : 0.7005678087367567\n",
      "scores min/max : -2.0038769629578574 0.019527656595253316\n",
      "Mask mean value:  tensor(0.4554, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2553  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.190e-05\n",
      "‖w_svm‖₂       : 0.005487312478578918\n",
      "‖alpha‖₁       : 0.37999999999999995\n",
      "scores min/max : -0.005880379797782734 0.006568290027644745\n",
      "Mask mean value:  tensor(0.5168, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9192  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.801e-14\n",
      "‖w_svm‖₂       : 0.007667271637471093\n",
      "‖alpha‖₁       : 0.6076813361013692\n",
      "scores min/max : -1.9878642426664024 0.29897997529990294\n",
      "Mask mean value:  tensor(0.5668, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2290  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.191e-15\n",
      "‖w_svm‖₂       : 0.05271333651121121\n",
      "‖alpha‖₁       : 0.8272031680976591\n",
      "scores min/max : -1.948601539335102 0.40155836849188453\n",
      "Mask mean value:  tensor(0.5282, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0115  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.658e-02\n",
      "‖w_svm‖₂       : 0.07404982566366088\n",
      "‖alpha‖₁       : 0.6584282600611491\n",
      "scores min/max : -2.067627605566493 4.166622973161347\n",
      "Mask mean value:  tensor(0.2946, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.4158  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.269e-03\n",
      "‖w_svm‖₂       : 0.0001635754430832688\n",
      "‖alpha‖₁       : 0.6399999999999992\n",
      "scores min/max : -0.00011812745641980362 -0.00010791828798949849\n",
      "Mask mean value:  tensor(0.4994, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7636  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.257e-17\n",
      "‖w_svm‖₂       : 0.0003688778359929591\n",
      "‖alpha‖₁       : 0.7399999999999926\n",
      "scores min/max : -0.0003007623352032436 -0.000283406912269599\n",
      "Mask mean value:  tensor(0.4986, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0612  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.223e-17\n",
      "‖w_svm‖₂       : 4.739376693900001e-08\n",
      "‖alpha‖₁       : 0.17999999999999844\n",
      "scores min/max : 4.686621919215143e-08 6.14819291551321e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.027e-08\n",
      "‖w_svm‖₂       : 5.601580608965437e-08\n",
      "‖alpha‖₁       : 0.23999999999999794\n",
      "scores min/max : -6.268370825627192e-08 -4.8344404818969325e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.381e-20\n",
      "‖w_svm‖₂       : 1.958095583918278e-07\n",
      "‖alpha‖₁       : 0.4199999999999759\n",
      "scores min/max : -3.9839518924718823e-07 -3.761384718160868e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.671e-19\n",
      "‖w_svm‖₂       : 0.1379398998565632\n",
      "‖alpha‖₁       : 0.8799999999999752\n",
      "scores min/max : -1.3576213710339378 3.602278527888451\n",
      "Mask mean value:  tensor(0.3073, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2570  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.302e-04\n",
      "‖w_svm‖₂       : 0.04718615582288581\n",
      "‖alpha‖₁       : 0.9389965912452843\n",
      "scores min/max : -2.4560369630818304 1.5896900065848416\n",
      "Mask mean value:  tensor(0.1485, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4041  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.208e-03\n",
      "‖w_svm‖₂       : 0.04314823793394646\n",
      "‖alpha‖₁       : 0.8900226447118705\n",
      "scores min/max : -2.006664566436822 0.341058983710552\n",
      "Mask mean value:  tensor(0.2721, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0728  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.400e-12\n",
      "‖w_svm‖₂       : 2.31089656214564e-07\n",
      "‖alpha‖₁       : 0.6399999999999981\n",
      "scores min/max : 3.646015969897145e-07 3.855260210368859e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.546e-20\n",
      "‖w_svm‖₂       : 1.1415649773696536e-06\n",
      "‖alpha‖₁       : 0.31999999999878487\n",
      "scores min/max : -2.197436313053083e-06 -2.0823363893677e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4898  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.826e-18\n",
      "‖w_svm‖₂       : 0.07438480062750188\n",
      "‖alpha‖₁       : 0.41923758283621065\n",
      "scores min/max : -1.8996173270320877 2.2776867989304233\n",
      "Mask mean value:  tensor(0.7714, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.627e-04\n",
      "‖w_svm‖₂       : 2.3382641711179945e-07\n",
      "‖alpha‖₁       : 0.5799999999999821\n",
      "scores min/max : -3.478274225059517e-07 -3.3216496249877866e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.948e-19\n",
      "‖w_svm‖₂       : 0.10798551807190587\n",
      "‖alpha‖₁       : 0.8732919712604326\n",
      "scores min/max : -12.196811698449904 2.1421732650233545\n",
      "Mask mean value:  tensor(0.5097, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.5096  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 2.016e-02\n",
      "‖w_svm‖₂       : 1.1227500886023596e-07\n",
      "‖alpha‖₁       : 0.5199999999999554\n",
      "scores min/max : 3.1341177236304194e-07 3.348427676182741e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.693e-19\n",
      "‖w_svm‖₂       : 0.003166665012498542\n",
      "‖alpha‖₁       : 0.5799999999999457\n",
      "scores min/max : 0.002810114411731029 0.004081098791332158\n",
      "Mask mean value:  tensor(0.5191, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3301  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.927e-13\n",
      "‖w_svm‖₂       : 7.126651761935344e-08\n",
      "‖alpha‖₁       : 0.13999999999999643\n",
      "scores min/max : 8.973556938301451e-09 1.4753158978813083e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.935e-08\n",
      "‖w_svm‖₂       : 0.05761908793771452\n",
      "‖alpha‖₁       : 0.5763838931268092\n",
      "scores min/max : -1.958940439266866 0.8700475008379329\n",
      "Mask mean value:  tensor(0.7081, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.077e-03\n",
      "‖w_svm‖₂       : 8.166367477447104e-08\n",
      "‖alpha‖₁       : 0.37999999999999956\n",
      "scores min/max : 1.519981202419315e-09 1.9673397806176873e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.586e-22\n",
      "‖w_svm‖₂       : 0.00014506667690698103\n",
      "‖alpha‖₁       : 0.43999999979176896\n",
      "scores min/max : -0.00026049416593021185 -0.00011248027336123397\n",
      "Mask mean value:  tensor(0.4992, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0222  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.837e-17\n",
      "‖w_svm‖₂       : 0.000215703277132089\n",
      "‖alpha‖₁       : 0.6199999999924835\n",
      "scores min/max : 2.7065473849704374e-05 4.355561442659811e-05\n",
      "Mask mean value:  tensor(0.5002, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.395e-17\n",
      "‖w_svm‖₂       : 0.04501993787387603\n",
      "‖alpha‖₁       : 0.7294258185229299\n",
      "scores min/max : -0.25449729627969425 2.027183176741455\n",
      "Mask mean value:  tensor(0.6988, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.1492  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.385e-03\n",
      "‖w_svm‖₂       : 7.61124642611612e-08\n",
      "‖alpha‖₁       : 0.6599999999999954\n",
      "scores min/max : 1.0190554091361022e-07 2.4675981621194764e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.884e-09\n",
      "‖w_svm‖₂       : 0.04563749404837003\n",
      "‖alpha‖₁       : 0.659999999999989\n",
      "scores min/max : -0.2731414199637022 0.19815867762580286\n",
      "Mask mean value:  tensor(0.4845, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8810  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.376e-13\n",
      "‖w_svm‖₂       : 0.0872847354987092\n",
      "‖alpha‖₁       : 0.5782429669946277\n",
      "scores min/max : -2.1538260225059536 5.773728361476772\n",
      "Mask mean value:  tensor(0.2210, dtype=torch.float64)\n",
      "max feasible return = 0.1022  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.908953047278978e-07\n",
      "‖alpha‖₁       : 0.5799999999999969\n",
      "scores min/max : -3.56960838554966e-07 -2.6606325560119857e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0142074457697358e-07\n",
      "‖alpha‖₁       : 0.29999999999999905\n",
      "scores min/max : 3.630211395751789e-08 4.128481317448716e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 6.904888931057557e-08\n",
      "‖alpha‖₁       : 0.5999999999999817\n",
      "scores min/max : 1.6313389179947362e-08 5.310463472264904e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.595232394053242e-08\n",
      "‖alpha‖₁       : 0.37999999999999656\n",
      "scores min/max : -2.5187614900837383e-09 6.882152055307796e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.961021691895592e-06\n",
      "‖alpha‖₁       : 0.3199999999754018\n",
      "scores min/max : 3.0701398804833334e-06 4.893269386339535e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4258  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.05401546911812458\n",
      "‖alpha‖₁       : 0.7397294282960076\n",
      "scores min/max : -3.4727858903992104 2.1622765311575516\n",
      "Mask mean value:  tensor(0.8867, dtype=torch.float64)\n",
      "max feasible return = -0.9314  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.030916637204318014\n",
      "‖alpha‖₁       : 0.6151662351166516\n",
      "scores min/max : -1.9178883036528915 1.213250500550062\n",
      "Mask mean value:  tensor(0.7819, dtype=torch.float64)\n",
      "max feasible return = 2.7379  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.597388287756252e-07\n",
      "‖alpha‖₁       : 0.4599999999999911\n",
      "scores min/max : 2.187283760021253e-07 8.132866619518479e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.3462349403269848e-07\n",
      "‖alpha‖₁       : 0.5199999999999916\n",
      "scores min/max : 6.828669855456229e-08 8.119489814054615e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.524912910525255e-07\n",
      "‖alpha‖₁       : 0.579999999999981\n",
      "scores min/max : -2.53257516413059e-07 -2.169347766635261e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.04518638470735624\n",
      "‖alpha‖₁       : 0.6877975625189673\n",
      "scores min/max : -0.4429748733535819 1.9460692999011\n",
      "Mask mean value:  tensor(0.3657, dtype=torch.float64)\n",
      "max feasible return = 0.0760  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03286754359587209\n",
      "‖alpha‖₁       : 0.4221819963896872\n",
      "scores min/max : -3.9043134422454737 5.234497499353095\n",
      "Mask mean value:  tensor(0.0827, dtype=torch.float64)\n",
      "max feasible return = 0.5212  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.044503968086535665\n",
      "‖alpha‖₁       : 0.8211344793205432\n",
      "scores min/max : -5.085259033591194 3.0813394541885857\n",
      "Mask mean value:  tensor(0.9447, dtype=torch.float64)\n",
      "max feasible return = -3.5787  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.007915334214748906\n",
      "‖alpha‖₁       : 0.7999999999998956\n",
      "scores min/max : -0.037466873316018626 0.004330575735754049\n",
      "Mask mean value:  tensor(0.4692, dtype=torch.float64)\n",
      "max feasible return = -0.1705  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.0311325333567313e-07\n",
      "‖alpha‖₁       : 0.6199999999999548\n",
      "scores min/max : 6.292227764980692e-08 7.947431894264534e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.682426383338658e-08\n",
      "‖alpha‖₁       : 0.43999999999999995\n",
      "scores min/max : 7.941066162382045e-09 1.7510956659423543e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.272721420678363e-07\n",
      "‖alpha‖₁       : 0.27999999999997294\n",
      "scores min/max : -3.1182314937254143e-07 -2.938392611879912e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  28 | train 0.005350 | val 0.007130\n",
      "-----------------------------------------Epoch:  29 ----------------------------------------\n",
      "‖w_svm‖₂       : 2.0645424100438533e-07\n",
      "‖alpha‖₁       : 0.37999999999544143\n",
      "scores min/max : -6.497405398768248e-08 -4.647801673481917e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.523e-07\n",
      "‖w_svm‖₂       : 1.66132978636316e-07\n",
      "‖alpha‖₁       : 0.2399999999999922\n",
      "scores min/max : 2.6780466779065607e-07 2.831494318253099e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.274e-20\n",
      "‖w_svm‖₂       : 0.14818515679188868\n",
      "‖alpha‖₁       : 0.7599999999865499\n",
      "scores min/max : -1.7452336363613568 2.2178275115201407\n",
      "Mask mean value:  tensor(0.8625, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3582  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.110e-11\n",
      "‖w_svm‖₂       : 3.5165124947577154e-06\n",
      "‖alpha‖₁       : 0.4199999999931714\n",
      "scores min/max : -2.070749923938588e-06 -4.4555559864915016e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1229  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.505e-05\n",
      "‖w_svm‖₂       : 0.02057213814738193\n",
      "‖alpha‖₁       : 0.7799999999999998\n",
      "scores min/max : -0.0294029834570963 0.04256791923097886\n",
      "Mask mean value:  tensor(0.3858, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.7011  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.831e-15\n",
      "‖w_svm‖₂       : 0.017012290814528933\n",
      "‖alpha‖₁       : 0.8599999999999387\n",
      "scores min/max : -0.05142026015355684 0.012668647550817796\n",
      "Mask mean value:  tensor(0.3761, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4813  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.633e-05\n",
      "‖w_svm‖₂       : 0.00018956807750001182\n",
      "‖alpha‖₁       : 0.8199999999999996\n",
      "scores min/max : -7.495618550854097e-05 -6.005383734873052e-05\n",
      "Mask mean value:  tensor(0.4997, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6968  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.267e-17\n",
      "‖w_svm‖₂       : 0.08340817749452292\n",
      "‖alpha‖₁       : 0.4721510549269547\n",
      "scores min/max : -2.3730048506039223 2.709728186945396\n",
      "Mask mean value:  tensor(0.0841, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9281  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.539e-03\n",
      "‖w_svm‖₂       : 8.173974258974775e-08\n",
      "‖alpha‖₁       : 0.3799999999999996\n",
      "scores min/max : 9.679705062947763e-11 1.8250336682567984e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.595e-22\n",
      "‖w_svm‖₂       : 0.05298055264865635\n",
      "‖alpha‖₁       : 0.8272397127101218\n",
      "scores min/max : -1.9378492767074564 0.41237542672980526\n",
      "Mask mean value:  tensor(0.5664, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0194  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.037e-10\n",
      "‖w_svm‖₂       : 0.00598289995449253\n",
      "‖alpha‖₁       : 0.7005682966146175\n",
      "scores min/max : -2.003183020793342 0.020221646034579675\n",
      "Mask mean value:  tensor(0.4588, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2571  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.759e-06\n",
      "‖w_svm‖₂       : 1.9063634207509687e-07\n",
      "‖alpha‖₁       : 0.3799999999999996\n",
      "scores min/max : -4.973582311592961e-08 -1.406564253582489e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.532e-19\n",
      "‖w_svm‖₂       : 0.043139845341150146\n",
      "‖alpha‖₁       : 0.8900279138775568\n",
      "scores min/max : -2.007026824009567 0.34070484516667277\n",
      "Mask mean value:  tensor(0.2710, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0720  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.959e-11\n",
      "‖w_svm‖₂       : 0.04713598827846596\n",
      "‖alpha‖₁       : 0.9389985805703408\n",
      "scores min/max : -2.4507396975081406 1.5952814611743618\n",
      "Mask mean value:  tensor(0.1523, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4111  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.895e-03\n",
      "‖w_svm‖₂       : 0.004368901523962971\n",
      "‖alpha‖₁       : 0.459999999999999\n",
      "scores min/max : -0.0076795781659281335 -0.005618657915360981\n",
      "Mask mean value:  tensor(0.4695, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.0405  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.047e-15\n",
      "‖w_svm‖₂       : 7.150155988835224e-08\n",
      "‖alpha‖₁       : 0.13999999999999316\n",
      "scores min/max : 9.28711169882357e-09 1.508821555966271e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.973e-08\n",
      "‖w_svm‖₂       : 0.023077604763813882\n",
      "‖alpha‖₁       : 0.8150954289045547\n",
      "scores min/max : -11.521026433954319 2.0314270325312935\n",
      "Mask mean value:  tensor(0.7085, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9054  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.116e-06\n",
      "‖w_svm‖₂       : 7.533928614618043e-08\n",
      "‖alpha‖₁       : 0.5399999999999996\n",
      "scores min/max : -1.3693180608297628e-07 -1.1541000871169797e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.611e-20\n",
      "‖w_svm‖₂       : 0.0031662223736798026\n",
      "‖alpha‖₁       : 0.5799999999999998\n",
      "scores min/max : 0.002645880591044388 0.003916294847175192\n",
      "Mask mean value:  tensor(0.5182, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3295  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.091e-13\n",
      "‖w_svm‖₂       : 0.0008784054329492318\n",
      "‖alpha‖₁       : 0.8199999999998879\n",
      "scores min/max : 0.0009238881335773396 0.002631118609028765\n",
      "Mask mean value:  tensor(0.5100, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5908  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.037e-16\n",
      "‖w_svm‖₂       : 0.07395289985874787\n",
      "‖alpha‖₁       : 0.658458015986152\n",
      "scores min/max : -2.063786218557251 4.17959300382539\n",
      "Mask mean value:  tensor(0.2998, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.4792  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.206e-03\n",
      "‖w_svm‖₂       : 2.3463751151835644e-07\n",
      "‖alpha‖₁       : 0.5799999999999778\n",
      "scores min/max : -3.605868329478213e-07 -3.44916354239384e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.965e-19\n",
      "‖w_svm‖₂       : 0.10725639965585267\n",
      "‖alpha‖₁       : 0.8732896698138477\n",
      "scores min/max : -12.184918985788867 2.1531918491849154\n",
      "Mask mean value:  tensor(0.5356, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.5847  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.165e-02\n",
      "‖w_svm‖₂       : 0.00021534427681377393\n",
      "‖alpha‖₁       : 0.6199999999936483\n",
      "scores min/max : 3.0488443116824467e-05 4.692318683690413e-05\n",
      "Mask mean value:  tensor(0.5002, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.371e-17\n",
      "‖w_svm‖₂       : 2.1521818776951735e-08\n",
      "‖alpha‖₁       : 0.11999999999999404\n",
      "scores min/max : -3.0776260260561927e-08 -1.607868842175783e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.813e-09\n",
      "‖w_svm‖₂       : 0.04131662507739581\n",
      "‖alpha‖₁       : 0.9404738356876224\n",
      "scores min/max : -1.8288748829634152 0.30492965155098717\n",
      "Mask mean value:  tensor(0.4219, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3254  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.457e-10\n",
      "‖w_svm‖₂       : 3.760753290554843e-07\n",
      "‖alpha‖₁       : 0.27999999999999436\n",
      "scores min/max : -1.3965686386029797e-06 -1.334798143751259e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.583e-19\n",
      "‖w_svm‖₂       : 0.047080411870539976\n",
      "‖alpha‖₁       : 0.9056457879651878\n",
      "scores min/max : -0.7004163506741193 1.8986913906633731\n",
      "Mask mean value:  tensor(0.3951, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7333  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.095e-05\n",
      "‖w_svm‖₂       : 8.615790878332837e-08\n",
      "‖alpha‖₁       : 0.17999999999999225\n",
      "scores min/max : -1.3960450573148764e-07 9.599150994752962e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.048e-21\n",
      "‖w_svm‖₂       : 0.044695761766591506\n",
      "‖alpha‖₁       : 0.729424672619077\n",
      "scores min/max : -0.25393886637438357 2.0276770220102343\n",
      "Mask mean value:  tensor(0.7003, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.1567  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.370e-03\n",
      "‖w_svm‖₂       : 7.630020711907229e-08\n",
      "‖alpha‖₁       : 0.6599999999999948\n",
      "scores min/max : 1.0528712484564476e-07 2.5012389645038766e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.900e-09\n",
      "‖w_svm‖₂       : 0.020998603988537562\n",
      "‖alpha‖₁       : 0.8233701318751451\n",
      "scores min/max : -1.8969803819620397 1.5009910003784703\n",
      "Mask mean value:  tensor(0.7067, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.4002  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.418e-06\n",
      "‖w_svm‖₂       : 0.005488613353429867\n",
      "‖alpha‖₁       : 0.37999999999999995\n",
      "scores min/max : -0.005962359170259918 0.00636725302618267\n",
      "Mask mean value:  tensor(0.5160, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9177  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.826e-14\n",
      "‖w_svm‖₂       : 0.0001441152580422782\n",
      "‖alpha‖₁       : 0.4399999998642623\n",
      "scores min/max : -0.00025792716335846513 -0.00011126977541658424\n",
      "Mask mean value:  tensor(0.4992, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0223  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.826e-17\n",
      "‖w_svm‖₂       : 2.227846037484701e-07\n",
      "‖alpha‖₁       : 0.2599999999999993\n",
      "scores min/max : 7.89803495780682e-08 9.172281564575436e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.549e-17\n",
      "‖w_svm‖₂       : 0.027673595576712694\n",
      "‖alpha‖₁       : 0.19668278376581977\n",
      "scores min/max : -2.228312079435098 -0.005003675267217712\n",
      "Mask mean value:  tensor(0.0395, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2702  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.538e-03\n",
      "‖w_svm‖₂       : 6.744200473640977e-07\n",
      "‖alpha‖₁       : 0.39999999999989694\n",
      "scores min/max : -4.609303692029623e-08 2.625137640438729e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.294e-09\n",
      "‖w_svm‖₂       : 0.01699795608698455\n",
      "‖alpha‖₁       : 0.6815457919558658\n",
      "scores min/max : -1.9749729179071536 0.05494572081270423\n",
      "Mask mean value:  tensor(0.6002, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2503  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.748e-14\n",
      "‖w_svm‖₂       : 1.0398355001053566e-07\n",
      "‖alpha‖₁       : 0.23999999999999466\n",
      "scores min/max : 3.296577822221729e-08 4.38097976907931e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.244e-21\n",
      "‖w_svm‖₂       : 0.1377445694669586\n",
      "‖alpha‖₁       : 0.8799999999999777\n",
      "scores min/max : -1.3407144404526015 3.603915945568635\n",
      "Mask mean value:  tensor(0.3360, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2612  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.768e-08\n",
      "‖w_svm‖₂       : 0.00030999759941421873\n",
      "‖alpha‖₁       : 0.41999999995281323\n",
      "scores min/max : 0.00018580385599018716 0.0005109288818985325\n",
      "Mask mean value:  tensor(0.5024, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0003  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.899e-15\n",
      "‖w_svm‖₂       : 0.0704821653876764\n",
      "‖alpha‖₁       : 0.5799999999999994\n",
      "scores min/max : -2.7269894239487935 1.551585387517521\n",
      "Mask mean value:  tensor(0.1741, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3173  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.562e-04\n",
      "‖w_svm‖₂       : 0.045226449958903514\n",
      "‖alpha‖₁       : 0.6599999999999877\n",
      "scores min/max : -0.26820527193542565 0.19445936991482227\n",
      "Mask mean value:  tensor(0.4843, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8826  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.381e-13\n",
      "‖w_svm‖₂       : 4.759174149037436e-08\n",
      "‖alpha‖₁       : 0.17999999999999847\n",
      "scores min/max : 4.619182814904104e-08 6.080634696056234e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.032e-08\n",
      "‖w_svm‖₂       : 0.018890226951003997\n",
      "‖alpha‖₁       : 0.38338082107394617\n",
      "scores min/max : -1.9652900019917694 0.23710922095340486\n",
      "Mask mean value:  tensor(0.6944, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1499  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.395e-14\n",
      "‖w_svm‖₂       : 0.005662992509256312\n",
      "‖alpha‖₁       : 0.5599999999999667\n",
      "scores min/max : 0.0047348320683056975 0.0063991360502355325\n",
      "Mask mean value:  tensor(0.5252, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1008  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 2.997e-05\n",
      "‖w_svm‖₂       : 0.057314296956550354\n",
      "‖alpha‖₁       : 0.5763743087711788\n",
      "scores min/max : -1.9593838150430891 0.8694631133127981\n",
      "Mask mean value:  tensor(0.7066, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9216  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.038e-03\n",
      "‖w_svm‖₂       : 5.367506065218922e-08\n",
      "‖alpha‖₁       : 0.11999999999998381\n",
      "scores min/max : -9.051106115388196e-08 -5.960945567961782e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.193e-07\n",
      "‖w_svm‖₂       : 0.0005066951345503143\n",
      "‖alpha‖₁       : 0.43999999999999995\n",
      "scores min/max : -0.0007584748262412841 -0.0004982020513592238\n",
      "Mask mean value:  tensor(0.4964, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6435  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.193e-16\n",
      "‖w_svm‖₂       : 1.0024231978992749e-06\n",
      "‖alpha‖₁       : 0.5999999999999753\n",
      "scores min/max : -1.0382474048265179e-07 -5.279923998490836e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.507e-19\n",
      "‖w_svm‖₂       : 0.02162090090051906\n",
      "‖alpha‖₁       : 0.6599999999999996\n",
      "scores min/max : -0.044951779776601114 0.03649485341824917\n",
      "Mask mean value:  tensor(0.4195, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.3152  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.549e-15\n",
      "‖w_svm‖₂       : 0.04228221599852445\n",
      "‖alpha‖₁       : 0.9199999999999955\n",
      "scores min/max : -0.31418932427305524 0.3440081794929519\n",
      "Mask mean value:  tensor(0.2376, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1737  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 2.725e-02\n",
      "‖w_svm‖₂       : 1.7463063572351893e-05\n",
      "‖alpha‖₁       : 0.3599999999827179\n",
      "scores min/max : 1.4652946914715802e-05 1.575370996675748e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.518e-17\n",
      "‖w_svm‖₂       : 1.145263340289208e-06\n",
      "‖alpha‖₁       : 0.3199999999988163\n",
      "scores min/max : -2.275677395208273e-06 -2.16072485402177e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4898  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.844e-18\n",
      "‖w_svm‖₂       : 1.0603695917895158e-06\n",
      "‖alpha‖₁       : 0.49999999999999906\n",
      "scores min/max : -2.587916695403815e-07 -6.599497316824707e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.773e-19\n",
      "‖w_svm‖₂       : 1.124918137251124e-07\n",
      "‖alpha‖₁       : 0.519999999999954\n",
      "scores min/max : 3.291211178749976e-07 3.505473742676948e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.719e-19\n",
      "‖w_svm‖₂       : 0.00016315032626384538\n",
      "‖alpha‖₁       : 0.6399999999998788\n",
      "scores min/max : -0.0001234310481692223 -0.00011327485677976987\n",
      "Mask mean value:  tensor(0.4994, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7635  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.257e-17\n",
      "‖w_svm‖₂       : 0.01670651369749228\n",
      "‖alpha‖₁       : 0.8599999999999963\n",
      "scores min/max : -0.04834105307263338 -0.0333142614408988\n",
      "Mask mean value:  tensor(0.3217, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0786  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.364e-14\n",
      "‖w_svm‖₂       : 2.752290489908004e-07\n",
      "‖alpha‖₁       : 0.6599999999999997\n",
      "scores min/max : -6.533209136969274e-07 -5.016803979854629e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.339e-18\n",
      "‖w_svm‖₂       : 0.06163944582061685\n",
      "‖alpha‖₁       : 0.8987378185499424\n",
      "scores min/max : -1.6948280072182125 3.7162372964590586\n",
      "Mask mean value:  tensor(0.4391, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5867  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.845e-03\n",
      "‖w_svm‖₂       : 5.617441598845214e-08\n",
      "‖alpha‖₁       : 0.23999999999999533\n",
      "scores min/max : -6.262434033748954e-08 -4.8253134607155505e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.403e-20\n",
      "‖w_svm‖₂       : 2.3211110824596344e-07\n",
      "‖alpha‖₁       : 0.6399999999999278\n",
      "scores min/max : 3.7607259762415645e-07 3.9721158228826306e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.686e-20\n",
      "‖w_svm‖₂       : 0.07390097737966213\n",
      "‖alpha‖₁       : 0.4192427232727085\n",
      "scores min/max : -1.9002474138953696 2.2606968718776845\n",
      "Mask mean value:  tensor(0.7711, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8951  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.616e-04\n",
      "‖w_svm‖₂       : 0.027546287224309723\n",
      "‖alpha‖₁       : 0.852312764514979\n",
      "scores min/max : -2.9097852355817677 1.5712014902877989\n",
      "Mask mean value:  tensor(0.1585, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9546  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.438e-03\n",
      "‖w_svm‖₂       : 0.007793015371381444\n",
      "‖alpha‖₁       : 0.6076832126742271\n",
      "scores min/max : -1.9884302004771768 0.2983991286262321\n",
      "Mask mean value:  tensor(0.5643, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2280  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.433e-15\n",
      "‖w_svm‖₂       : 0.015958589132382024\n",
      "‖alpha‖₁       : 0.5962652090400553\n",
      "scores min/max : -2.9637347774864384 2.2699451660101486\n",
      "Mask mean value:  tensor(0.9146, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.5054  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.082e-12\n",
      "‖w_svm‖₂       : 4.198093245623395e-07\n",
      "‖alpha‖₁       : 0.72\n",
      "scores min/max : 2.6218276218421315e-07 3.1955033800262287e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.985e-21\n",
      "‖w_svm‖₂       : 0.18724760161221418\n",
      "‖alpha‖₁       : 0.8842640418175362\n",
      "scores min/max : -3.6126323541391603 6.075477181662899\n",
      "Mask mean value:  tensor(0.0929, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.638e-03\n",
      "‖w_svm‖₂       : 0.0003690795516206643\n",
      "‖alpha‖₁       : 0.7399999999999985\n",
      "scores min/max : -0.0003017994819110064 -0.0002844251076138865\n",
      "Mask mean value:  tensor(0.4986, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0612  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.231e-17\n",
      "‖w_svm‖₂       : 0.028669218576443986\n",
      "‖alpha‖₁       : 0.5510864600106843\n",
      "scores min/max : -3.5144566272551376 1.0562416816742974\n",
      "Mask mean value:  tensor(0.1296, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0639  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.013e-03\n",
      "‖w_svm‖₂       : 1.954935366893746e-07\n",
      "‖alpha‖₁       : 0.4199999999999757\n",
      "scores min/max : -4.3244936273780487e-07 -4.101927886937386e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.696e-19\n",
      "‖w_svm‖₂       : 0.03138020424085996\n",
      "‖alpha‖₁       : 0.8986046225556131\n",
      "scores min/max : -0.7432020338388405 2.017067011414445\n",
      "Mask mean value:  tensor(0.8517, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3453  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.248e-15\n",
      "‖w_svm‖₂       : 0.1404543427182865\n",
      "‖alpha‖₁       : 0.6555331301347914\n",
      "scores min/max : -18.104692061056568 1.7712647677711668\n",
      "Mask mean value:  tensor(0.2168, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2698  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.975e-03\n",
      "‖w_svm‖₂       : 1.1266775416474168e-07\n",
      "‖alpha‖₁       : 0.299999999999991\n",
      "scores min/max : 1.2710082662573272e-07 1.3610103886608575e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.271e-07\n",
      "‖w_svm‖₂       : 5.379940723646295e-08\n",
      "‖alpha‖₁       : 0.4399999999999842\n",
      "scores min/max : -1.7588396075976958e-07 -7.195254522967269e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.417e-20\n",
      "‖w_svm‖₂       : 7.074953847081988e-08\n",
      "‖alpha‖₁       : 0.4199999999999998\n",
      "scores min/max : -7.528925124836772e-08 -2.81412506693679e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.746e-08\n",
      "‖w_svm‖₂       : 0.08707111722011035\n",
      "‖alpha‖₁       : 0.5782081365408491\n",
      "scores min/max : -2.1480343601232765 5.7791435882624\n",
      "Mask mean value:  tensor(0.2293, dtype=torch.float64)\n",
      "max feasible return = 0.1035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.897515363070939e-07\n",
      "‖alpha‖₁       : 0.5799999999999972\n",
      "scores min/max : -3.934533102489696e-07 -3.025620546947836e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0120764183358096e-07\n",
      "‖alpha‖₁       : 0.29999999999999893\n",
      "scores min/max : 4.2529685195380563e-08 4.7513273479096044e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 6.873632132305936e-08\n",
      "‖alpha‖₁       : 0.5999999999999841\n",
      "scores min/max : 2.3978599098650837e-08 6.077127287274642e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.57499062419331e-08\n",
      "‖alpha‖₁       : 0.3799999999999976\n",
      "scores min/max : 1.6949123391751148e-09 1.108744209724837e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.9591932870559434e-06\n",
      "‖alpha‖₁       : 0.31999999997523754\n",
      "scores min/max : 3.2440803037487502e-06 5.076290790064803e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4258  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.053408290623902635\n",
      "‖alpha‖₁       : 0.7397264399387025\n",
      "scores min/max : -3.479893077526315 2.154912356054505\n",
      "Mask mean value:  tensor(0.8810, dtype=torch.float64)\n",
      "max feasible return = -0.9277  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.030493484754434478\n",
      "‖alpha‖₁       : 0.6151653948215894\n",
      "scores min/max : -1.9160534930706006 1.2148351299446942\n",
      "Mask mean value:  tensor(0.7870, dtype=torch.float64)\n",
      "max feasible return = 2.7556  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.588107935133944e-07\n",
      "‖alpha‖₁       : 0.45999999999998864\n",
      "scores min/max : 2.1189780602592082e-07 8.066118186129462e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.3443805972165643e-07\n",
      "‖alpha‖₁       : 0.5199999999999894\n",
      "scores min/max : 7.277979671065804e-08 8.569097385167705e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.5192616940187847e-07\n",
      "‖alpha‖₁       : 0.5799999999999818\n",
      "scores min/max : -2.6431524884428326e-07 -2.2799749001401498e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.04486748654578446\n",
      "‖alpha‖₁       : 0.6877932598903898\n",
      "scores min/max : -0.44147663101443313 1.947678054177505\n",
      "Mask mean value:  tensor(0.3705, dtype=torch.float64)\n",
      "max feasible return = 0.0702  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03272678310629087\n",
      "‖alpha‖₁       : 0.42218426063495185\n",
      "scores min/max : -3.903962660840335 5.23662915561056\n",
      "Mask mean value:  tensor(0.0832, dtype=torch.float64)\n",
      "max feasible return = 0.5242  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04426809429964141\n",
      "‖alpha‖₁       : 0.8211331111444947\n",
      "scores min/max : -5.086868504088364 3.0801661363572435\n",
      "Mask mean value:  tensor(0.9444, dtype=torch.float64)\n",
      "max feasible return = -3.5772  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.007803016186909205\n",
      "‖alpha‖₁       : 0.799999999999911\n",
      "scores min/max : -0.03643788636334161 0.004262121365079407\n",
      "Mask mean value:  tensor(0.4702, dtype=torch.float64)\n",
      "max feasible return = -0.1709  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.021182353779095e-07\n",
      "‖alpha‖₁       : 0.6199999999999555\n",
      "scores min/max : 5.897258075202533e-08 7.55239552004921e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.677071463574762e-08\n",
      "‖alpha‖₁       : 0.43999999999999984\n",
      "scores min/max : 7.935612115059547e-09 1.750563315513124e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.233144618792163e-07\n",
      "‖alpha‖₁       : 0.2799999999999948\n",
      "scores min/max : -3.174596603229702e-07 -2.997552070437786e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  29 | train 0.005344 | val 0.006984\n",
      "-----------------------------------------Epoch:  30 ----------------------------------------\n",
      "‖w_svm‖₂       : 0.016778277034901458\n",
      "‖alpha‖₁       : 0.8599999999999995\n",
      "scores min/max : -0.05220971548586404 0.010243989300527053\n",
      "Mask mean value:  tensor(0.3694, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4721  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.293e-05\n",
      "‖w_svm‖₂       : 0.020674833486362024\n",
      "‖alpha‖₁       : 0.7799999999999981\n",
      "scores min/max : -0.028485783747209937 0.044193465077047134\n",
      "Mask mean value:  tensor(0.3904, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.7212  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.712e-15\n",
      "‖w_svm‖₂       : 1.954315516216832e-07\n",
      "‖alpha‖₁       : 0.4199999999999756\n",
      "scores min/max : -4.2945042093008634e-07 -4.071948955607371e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.701e-19\n",
      "‖w_svm‖₂       : 1.1387819059015625e-06\n",
      "‖alpha‖₁       : 0.31999999999882417\n",
      "scores min/max : -2.3185145514231716e-06 -2.203586055284285e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4898  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.854e-18\n",
      "‖w_svm‖₂       : 0.016755236790010383\n",
      "‖alpha‖₁       : 0.8599999999999997\n",
      "scores min/max : -0.04978449264297593 -0.03472071450398331\n",
      "Mask mean value:  tensor(0.3155, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0768  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.440e-14\n",
      "‖w_svm‖₂       : 0.028742346091286407\n",
      "‖alpha‖₁       : 0.5510912107217252\n",
      "scores min/max : -3.5120246621260787 1.058649909735783\n",
      "Mask mean value:  tensor(0.1308, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0708  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.084e-03\n",
      "‖w_svm‖₂       : 1.6559968662202936e-07\n",
      "‖alpha‖₁       : 0.23999999999999244\n",
      "scores min/max : 2.7978170239546075e-07 2.9512550901586417e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.298e-20\n",
      "‖w_svm‖₂       : 0.00021611282002005405\n",
      "‖alpha‖₁       : 0.6199999999957927\n",
      "scores min/max : 3.127772825493115e-05 4.782845520139784e-05\n",
      "Mask mean value:  tensor(0.5002, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.428e-17\n",
      "‖w_svm‖₂       : 2.2146385084334e-07\n",
      "‖alpha‖₁       : 0.25999999999999945\n",
      "scores min/max : 9.529447804911382e-08 1.0803226255946957e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.570e-17\n",
      "‖w_svm‖₂       : 0.015973622775238387\n",
      "‖alpha‖₁       : 0.5962664302689105\n",
      "scores min/max : -2.9629160490608597 2.271157397743943\n",
      "Mask mean value:  tensor(0.9152, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.5061  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.025e-12\n",
      "‖w_svm‖₂       : 1.7527846555487487e-05\n",
      "‖alpha‖₁       : 0.3599999999826101\n",
      "scores min/max : 1.4711853488913918e-05 1.582301549483639e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.565e-17\n",
      "‖w_svm‖₂       : 0.00019000536444166896\n",
      "‖alpha‖₁       : 0.8199999999999996\n",
      "scores min/max : -7.794310533751797e-05 -6.297665228687833e-05\n",
      "Mask mean value:  tensor(0.4997, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6967  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.281e-17\n",
      "‖w_svm‖₂       : 0.07056994406585149\n",
      "‖alpha‖₁       : 0.5799999999999994\n",
      "scores min/max : -2.730619598486124 1.5586267628874853\n",
      "Mask mean value:  tensor(0.1799, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3251  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.607e-04\n",
      "‖w_svm‖₂       : 7.59400997428452e-08\n",
      "‖alpha‖₁       : 0.6599999999999949\n",
      "scores min/max : 1.0673720441766803e-07 2.515632194818972e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.917e-09\n",
      "‖w_svm‖₂       : 0.04500026277617328\n",
      "‖alpha‖₁       : 0.6599999999999864\n",
      "scores min/max : -0.26707786824538193 0.1916350608869748\n",
      "Mask mean value:  tensor(0.4790, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8730  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.521e-13\n",
      "‖w_svm‖₂       : 0.02148686445464821\n",
      "‖alpha‖₁       : 0.6599999999999995\n",
      "scores min/max : -0.045653235529534375 0.03464670661502048\n",
      "Mask mean value:  tensor(0.4143, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.2741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.561e-15\n",
      "‖w_svm‖₂       : 0.027790423468897343\n",
      "‖alpha‖₁       : 0.8523257938202057\n",
      "scores min/max : -2.9105916999586947 1.5704122962185836\n",
      "Mask mean value:  tensor(0.1582, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9542  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.462e-03\n",
      "‖w_svm‖₂       : 3.744687057712669e-07\n",
      "‖alpha‖₁       : 0.27999999999999414\n",
      "scores min/max : -1.4042625152428947e-06 -1.3424862647762208e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.631e-19\n",
      "‖w_svm‖₂       : 2.7445794834602323e-07\n",
      "‖alpha‖₁       : 0.6599999999999997\n",
      "scores min/max : -6.517434583105919e-07 -5.001040562140507e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.336e-18\n",
      "‖w_svm‖₂       : 0.06162493911785969\n",
      "‖alpha‖₁       : 0.898738683638474\n",
      "scores min/max : -1.6862984372553078 3.7249913019004834\n",
      "Mask mean value:  tensor(0.4571, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6004  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.268e-03\n",
      "‖w_svm‖₂       : 4.738666069593046e-08\n",
      "‖alpha‖₁       : 0.17999999999999822\n",
      "scores min/max : 4.5669121250378227e-08 6.030100118060695e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.104e-08\n",
      "‖w_svm‖₂       : 0.07342486225651912\n",
      "‖alpha‖₁       : 0.6584495990307874\n",
      "scores min/max : -2.0673348400698712 4.191156526085849\n",
      "Mask mean value:  tensor(0.2937, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.4078  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.293e-03\n",
      "‖w_svm‖₂       : 1.9046225779195424e-07\n",
      "‖alpha‖₁       : 0.37999999999999934\n",
      "scores min/max : -3.832904453312546e-08 -2.646872460775821e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.527e-19\n",
      "‖w_svm‖₂       : 7.100252052835032e-08\n",
      "‖alpha‖₁       : 0.13999999999999704\n",
      "scores min/max : 9.54553086999621e-09 1.5296623417457326e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.959e-08\n",
      "‖w_svm‖₂       : 0.023037283234796542\n",
      "‖alpha‖₁       : 0.8150968274997241\n",
      "scores min/max : -11.521123770418235 2.0306861184370124\n",
      "Mask mean value:  tensor(0.7069, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9023  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.485e-06\n",
      "‖w_svm‖₂       : 0.08316375334125273\n",
      "‖alpha‖₁       : 0.4721061538059014\n",
      "scores min/max : -2.356669286396747 2.7261251616374818\n",
      "Mask mean value:  tensor(0.0893, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9830  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.372e-03\n",
      "‖w_svm‖₂       : 2.338239144477831e-07\n",
      "‖alpha‖₁       : 0.5799999999999919\n",
      "scores min/max : -3.4294793323554266e-07 -3.2729401720999527e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.969e-19\n",
      "‖w_svm‖₂       : 0.04205492854825915\n",
      "‖alpha‖₁       : 0.9199999999999894\n",
      "scores min/max : -0.31759173796194995 0.3326626084760138\n",
      "Mask mean value:  tensor(0.2188, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0785  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.603e-02\n",
      "‖w_svm‖₂       : 0.04728962016606786\n",
      "‖alpha‖₁       : 0.9056457854381694\n",
      "scores min/max : -0.7028403467985128 1.900059500124569\n",
      "Mask mean value:  tensor(0.4020, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7462  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.224e-05\n",
      "‖w_svm‖₂       : 1.1259431640843404e-07\n",
      "‖alpha‖₁       : 0.2999999999999914\n",
      "scores min/max : 1.2783601983645297e-07 1.368342963160517e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.275e-07\n",
      "‖w_svm‖₂       : 0.007818544388369087\n",
      "‖alpha‖₁       : 0.6076835894934871\n",
      "scores min/max : -1.9886565248496137 0.29816615793341344\n",
      "Mask mean value:  tensor(0.5633, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2276  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.519e-15\n",
      "‖w_svm‖₂       : 0.04423151794297135\n",
      "‖alpha‖₁       : 0.7294230069763178\n",
      "scores min/max : -0.2546905875670079 2.0268456576412124\n",
      "Mask mean value:  tensor(0.6980, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.1472  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.397e-03\n",
      "‖w_svm‖₂       : 0.005532931601330078\n",
      "‖alpha‖₁       : 0.37999999999999967\n",
      "scores min/max : -0.006012929862470456 0.006478311051549834\n",
      "Mask mean value:  tensor(0.5164, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9182  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.879e-14\n",
      "‖w_svm‖₂       : 0.01875395122404364\n",
      "‖alpha‖₁       : 0.3833810201359343\n",
      "scores min/max : -1.9638707159511797 0.2386427427661787\n",
      "Mask mean value:  tensor(0.6996, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1508  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.121e-14\n",
      "‖w_svm‖₂       : 8.15129473120337e-08\n",
      "‖alpha‖₁       : 0.3799999999999995\n",
      "scores min/max : 4.599365058571138e-09 2.2754355848314942e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.597e-22\n",
      "‖w_svm‖₂       : 1.0528472959219096e-06\n",
      "‖alpha‖₁       : 0.49999999999999967\n",
      "scores min/max : -2.607477319022359e-07 -6.800109797491267e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.727e-19\n",
      "‖w_svm‖₂       : 0.05253753189258967\n",
      "‖alpha‖₁       : 0.8272058912771025\n",
      "scores min/max : -1.9478554806694643 0.4024913366827326\n",
      "Mask mean value:  tensor(0.5313, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0128  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.646e-02\n",
      "‖w_svm‖₂       : 7.080178110299329e-08\n",
      "‖alpha‖₁       : 0.4199999999999594\n",
      "scores min/max : -7.481684529373417e-08 -2.745837899444212e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.786e-08\n",
      "‖w_svm‖₂       : 0.000881554091265401\n",
      "‖alpha‖₁       : 0.819999999999942\n",
      "scores min/max : 0.001030889007874904 0.002745239349502247\n",
      "Mask mean value:  tensor(0.5106, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5936  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.041e-16\n",
      "‖w_svm‖₂       : 0.0031785568797697726\n",
      "‖alpha‖₁       : 0.5799999999999993\n",
      "scores min/max : 0.002427967668350017 0.0037079974888383563\n",
      "Mask mean value:  tensor(0.5172, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3289  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.467e-13\n",
      "‖w_svm‖₂       : 0.0416222633674468\n",
      "‖alpha‖₁       : 0.9405025332707112\n",
      "scores min/max : -1.831850804086809 0.3019716107255538\n",
      "Mask mean value:  tensor(0.4103, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3247  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.564e-14\n",
      "‖w_svm‖₂       : 3.5846210360942354e-06\n",
      "‖alpha‖₁       : 0.4199999999927551\n",
      "scores min/max : -2.2490913074577698e-06 -6.024421578138238e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1229  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.539e-05\n",
      "‖w_svm‖₂       : 0.10611397666367786\n",
      "‖alpha‖₁       : 0.8732933506438888\n",
      "scores min/max : -12.195994533544276 2.1399645889026404\n",
      "Mask mean value:  tensor(0.5038, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.4934  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 3.219e-02\n",
      "‖w_svm‖₂       : 2.3119834107419296e-07\n",
      "‖alpha‖₁       : 0.6399999999999039\n",
      "scores min/max : 3.824025012778624e-07 4.035752022518371e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.672e-20\n",
      "‖w_svm‖₂       : 8.511019509951436e-08\n",
      "‖alpha‖₁       : 0.1799999999999921\n",
      "scores min/max : -1.5408822986188242e-07 8.146135758469339e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.116e-21\n",
      "‖w_svm‖₂       : 2.0683393239500405e-07\n",
      "‖alpha‖₁       : 0.37999999999525624\n",
      "scores min/max : -8.377001136404065e-08 -6.507816300492381e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.869e-07\n",
      "‖w_svm‖₂       : 7.486628030320149e-08\n",
      "‖alpha‖₁       : 0.5399999999999295\n",
      "scores min/max : -1.4942043293866174e-07 -1.2775902304944796e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.649e-20\n",
      "‖w_svm‖₂       : 4.1755993240671293e-07\n",
      "‖alpha‖₁       : 0.7199999999999984\n",
      "scores min/max : 2.457116333935353e-07 3.0292181579423784e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.999e-21\n",
      "‖w_svm‖₂       : 0.021100255185385622\n",
      "‖alpha‖₁       : 0.8233764779009022\n",
      "scores min/max : -1.8989419536707526 1.4990360200008899\n",
      "Mask mean value:  tensor(0.7015, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.3886  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.271e-06\n",
      "‖w_svm‖₂       : 0.0001450553548672339\n",
      "‖alpha‖₁       : 0.43999999988049043\n",
      "scores min/max : -0.00027324169929155134 -0.00012452557003190723\n",
      "Mask mean value:  tensor(0.4991, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0217  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.871e-17\n",
      "‖w_svm‖₂       : 1.1188739556858947e-07\n",
      "‖alpha‖₁       : 0.5199999999999478\n",
      "scores min/max : 3.262421824801834e-07 3.476755191235601e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.757e-19\n",
      "‖w_svm‖₂       : 5.600058699410566e-08\n",
      "‖alpha‖₁       : 0.2399999999999955\n",
      "scores min/max : -6.239835299116703e-08 -4.802859340741937e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.426e-20\n",
      "‖w_svm‖₂       : 0.137677723626909\n",
      "‖alpha‖₁       : 0.8799999999999752\n",
      "scores min/max : -1.344626844665902 3.5958775432862944\n",
      "Mask mean value:  tensor(0.3247, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2597  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.738e-10\n",
      "‖w_svm‖₂       : 6.676193586269212e-07\n",
      "‖alpha‖₁       : 0.3999999999999999\n",
      "scores min/max : -3.7426503161083596e-08 2.688890661697971e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.281e-09\n",
      "‖w_svm‖₂       : 0.05705142059568062\n",
      "‖alpha‖₁       : 0.5763768259627209\n",
      "scores min/max : -1.9578525242298987 0.8709499592220614\n",
      "Mask mean value:  tensor(0.7114, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9350  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.099e-03\n",
      "‖w_svm‖₂       : 5.309851492143585e-08\n",
      "‖alpha‖₁       : 0.11999999999998726\n",
      "scores min/max : -9.225205328789948e-08 -6.139773617766613e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.204e-07\n",
      "‖w_svm‖₂       : 9.933921623350414e-07\n",
      "‖alpha‖₁       : 0.5999999999999821\n",
      "scores min/max : -9.428782778058299e-08 -4.326436530084664e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.514e-19\n",
      "‖w_svm‖₂       : 0.03125488314600258\n",
      "‖alpha‖₁       : 0.8986037120512246\n",
      "scores min/max : -0.7414361101572594 2.018820815935015\n",
      "Mask mean value:  tensor(0.8541, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3407  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.043e-15\n",
      "‖w_svm‖₂       : 1.0279073167697958e-07\n",
      "‖alpha‖₁       : 0.23999999999999605\n",
      "scores min/max : 3.6152968094049174e-08 4.697717681331451e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.186e-21\n",
      "‖w_svm‖₂       : 0.1867766056260157\n",
      "‖alpha‖₁       : 0.8841668609268043\n",
      "scores min/max : -3.6138661240004106 6.0776447955143\n",
      "Mask mean value:  tensor(0.0932, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1988  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.328e-03\n",
      "‖w_svm‖₂       : 0.04693964427104593\n",
      "‖alpha‖₁       : 0.9389906496902738\n",
      "scores min/max : -2.4544869842370862 1.5921228778936745\n",
      "Mask mean value:  tensor(0.1500, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4074  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.114e-03\n",
      "‖w_svm‖₂       : 0.00036983048909848207\n",
      "‖alpha‖₁       : 0.7399999999999989\n",
      "scores min/max : -0.0003315477732291767 -0.00031410265981036576\n",
      "Mask mean value:  tensor(0.4984, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0609  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.259e-17\n",
      "‖w_svm‖₂       : 0.02755208221128123\n",
      "‖alpha‖₁       : 0.19668343054400134\n",
      "scores min/max : -2.2361263313195945 -0.013014351978009944\n",
      "Mask mean value:  tensor(0.0347, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2378  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.065e-03\n",
      "‖w_svm‖₂       : 0.0056716780156003494\n",
      "‖alpha‖₁       : 0.5599999999999635\n",
      "scores min/max : 0.004735891400969704 0.006405243171179499\n",
      "Mask mean value:  tensor(0.5252, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1009  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 3.005e-05\n",
      "‖w_svm‖₂       : 0.14844560117222524\n",
      "‖alpha‖₁       : 0.7599984224160634\n",
      "scores min/max : -1.7532930853299757 2.2242975255581054\n",
      "Mask mean value:  tensor(0.8605, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3582  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.878e-11\n",
      "‖w_svm‖₂       : 0.0737864275260859\n",
      "‖alpha‖₁       : 0.4192750624608951\n",
      "scores min/max : -1.904039796396828 2.2474672467012833\n",
      "Mask mean value:  tensor(0.7679, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8949  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.592e-04\n",
      "‖w_svm‖₂       : 0.0043917313347616145\n",
      "‖alpha‖₁       : 0.4599999999999989\n",
      "scores min/max : -0.007874848817696812 -0.0057907942427326276\n",
      "Mask mean value:  tensor(0.4686, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.0366  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.054e-15\n",
      "‖w_svm‖₂       : 0.00016389083688316447\n",
      "‖alpha‖₁       : 0.6399999999999993\n",
      "scores min/max : -0.00011956074049861023 -0.0001093122153796054\n",
      "Mask mean value:  tensor(0.4994, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7635  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.279e-17\n",
      "‖w_svm‖₂       : 0.042958914956340695\n",
      "‖alpha‖₁       : 0.8900274516693868\n",
      "scores min/max : -2.0078151950292726 0.3399825666992645\n",
      "Mask mean value:  tensor(0.2686, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0703  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.722e-14\n",
      "‖w_svm‖₂       : 0.006065455374168283\n",
      "‖alpha‖₁       : 0.7005691892281234\n",
      "scores min/max : -2.002784385859505 0.02062424435322227\n",
      "Mask mean value:  tensor(0.4608, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2581  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.534e-12\n",
      "‖w_svm‖₂       : 2.1422407145303356e-08\n",
      "‖alpha‖₁       : 0.1199999999999941\n",
      "scores min/max : -3.2050122455977746e-08 -1.735552267268666e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.825e-09\n",
      "‖w_svm‖₂       : 0.0005068964212485013\n",
      "‖alpha‖₁       : 0.43999999999994077\n",
      "scores min/max : -0.0007522487385670691 -0.0004916252767378144\n",
      "Mask mean value:  tensor(0.4965, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6436  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.233e-16\n",
      "‖w_svm‖₂       : 0.00031191156413575054\n",
      "‖alpha‖₁       : 0.4199999999512011\n",
      "scores min/max : 0.00018371884897848477 0.0005128615829292851\n",
      "Mask mean value:  tensor(0.5024, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0003  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.962e-15\n",
      "‖w_svm‖₂       : 0.017284166265454502\n",
      "‖alpha‖₁       : 0.6815561124871792\n",
      "scores min/max : -1.9738856865440297 0.05609721363464305\n",
      "Mask mean value:  tensor(0.6052, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2530  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.434e-13\n",
      "‖w_svm‖₂       : 0.14063514570019287\n",
      "‖alpha‖₁       : 0.6555843453004664\n",
      "scores min/max : -18.123344754582504 1.7527517233024372\n",
      "Mask mean value:  tensor(0.2039, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2627  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.153e-03\n",
      "‖w_svm‖₂       : 5.364929442010664e-08\n",
      "‖alpha‖₁       : 0.4399999999999856\n",
      "scores min/max : -1.768623640936601e-07 -7.292133302510445e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.420e-20\n",
      "‖w_svm‖₂       : 0.08703934515753282\n",
      "‖alpha‖₁       : 0.5782081332496832\n",
      "scores min/max : -2.158492488951638 5.768374250122084\n",
      "Mask mean value:  tensor(0.2145, dtype=torch.float64)\n",
      "max feasible return = 0.1011  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.893458473001278e-07\n",
      "‖alpha‖₁       : 0.5799999999999976\n",
      "scores min/max : -3.6745418800154443e-07 -2.765985697403248e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.011462330522797e-07\n",
      "‖alpha‖₁       : 0.299999999999999\n",
      "scores min/max : 3.922710947827792e-08 4.421017636821994e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 6.87774790047275e-08\n",
      "‖alpha‖₁       : 0.5999999999999783\n",
      "scores min/max : 2.1966184411926395e-08 5.875547004539042e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.5717728173295e-08\n",
      "‖alpha‖₁       : 0.37999999999999784\n",
      "scores min/max : -1.5136800417379986e-09 7.877127887011707e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.9316731204311653e-06\n",
      "‖alpha‖₁       : 0.31999999997523776\n",
      "scores min/max : 3.293132643621644e-06 5.098255412735037e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4258  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.052928840371160614\n",
      "‖alpha‖₁       : 0.7397315857506684\n",
      "scores min/max : -3.4746077920436735 2.1599612857939277\n",
      "Mask mean value:  tensor(0.8854, dtype=torch.float64)\n",
      "max feasible return = -0.9308  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.030160707434422257\n",
      "‖alpha‖₁       : 0.615166176619264\n",
      "scores min/max : -1.9194199986900355 1.211258255112781\n",
      "Mask mean value:  tensor(0.7792, dtype=torch.float64)\n",
      "max feasible return = 2.7270  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.691337715028366e-07\n",
      "‖alpha‖₁       : 0.45999999999944263\n",
      "scores min/max : 2.438331660052915e-07 8.514700926626205e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.3454404277450425e-07\n",
      "‖alpha‖₁       : 0.5199999999999803\n",
      "scores min/max : 6.993132580574537e-08 8.285218922131425e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.518377928917652e-07\n",
      "‖alpha‖₁       : 0.5799999999999814\n",
      "scores min/max : -2.5917809373466094e-07 -2.2285734574154053e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.044668396483041044\n",
      "‖alpha‖₁       : 0.6877905829967976\n",
      "scores min/max : -0.4457590619704142 1.9434783508097526\n",
      "Mask mean value:  tensor(0.3558, dtype=torch.float64)\n",
      "max feasible return = 0.0836  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.032735760427267266\n",
      "‖alpha‖₁       : 0.422195285380487\n",
      "scores min/max : -3.9011443012169233 5.241128645204797\n",
      "Mask mean value:  tensor(0.0847, dtype=torch.float64)\n",
      "max feasible return = 0.5338  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04408053311164412\n",
      "‖alpha‖₁       : 0.821126378185222\n",
      "scores min/max : -5.077021755282512 3.0902504321876987\n",
      "Mask mean value:  tensor(0.9467, dtype=torch.float64)\n",
      "max feasible return = -3.5907  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.007708350829927979\n",
      "‖alpha‖₁       : 0.7999999999999268\n",
      "scores min/max : -0.03616332810320222 0.0035884320488221632\n",
      "Mask mean value:  tensor(0.4681, dtype=torch.float64)\n",
      "max feasible return = -0.1701  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.0188860656783536e-07\n",
      "‖alpha‖₁       : 0.6199999999999547\n",
      "scores min/max : 7.456789449057808e-08 9.111949126837947e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.677777771597685e-08\n",
      "‖alpha‖₁       : 0.4399999999999999\n",
      "scores min/max : 7.547564282920481e-09 1.711748236403409e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.2397758971379576e-07\n",
      "‖alpha‖₁       : 0.2799999999999918\n",
      "scores min/max : -3.085251261374081e-07 -2.9075340049009335e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  30 | train 0.005336 | val 0.007245\n",
      "-----------------------------------------Epoch:  31 ----------------------------------------\n",
      "‖w_svm‖₂       : 7.073971655746898e-08\n",
      "‖alpha‖₁       : 0.41999999999999993\n",
      "scores min/max : -7.265445388708184e-08 -2.5544315398990805e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.795e-08\n",
      "‖w_svm‖₂       : 1.9033937453803229e-07\n",
      "‖alpha‖₁       : 0.37999999999999956\n",
      "scores min/max : -5.209875978102192e-08 -1.642321984612195e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.540e-19\n",
      "‖w_svm‖₂       : 0.0003118361397772324\n",
      "‖alpha‖₁       : 0.4199999999514012\n",
      "scores min/max : 0.00018394350881788322 0.000512928320356526\n",
      "Mask mean value:  tensor(0.5024, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0003  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.958e-15\n",
      "‖w_svm‖₂       : 0.021121224721892977\n",
      "‖alpha‖₁       : 0.8233780229364343\n",
      "scores min/max : -1.8983912493781732 1.499589520646826\n",
      "Mask mean value:  tensor(0.7030, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.3919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.408e-06\n",
      "‖w_svm‖₂       : 1.752868382983291e-05\n",
      "‖alpha‖₁       : 0.3599999999826231\n",
      "scores min/max : 1.4644738535986928e-05 1.5756574645591158e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.607e-17\n",
      "‖w_svm‖₂       : 6.689325290691135e-07\n",
      "‖alpha‖₁       : 0.3999999999999998\n",
      "scores min/max : -3.7292814719609574e-08 2.692079401320663e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.289e-09\n",
      "‖w_svm‖₂       : 0.027504450181588588\n",
      "‖alpha‖₁       : 0.19668086424419642\n",
      "scores min/max : -2.234468659615938 -0.01134574004313145\n",
      "Mask mean value:  tensor(0.0356, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2443  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.952e-03\n",
      "‖w_svm‖₂       : 0.0001637761325672231\n",
      "‖alpha‖₁       : 0.6399999999999991\n",
      "scores min/max : -0.0001185314457737335 -0.00010829724791212262\n",
      "Mask mean value:  tensor(0.4994, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7636  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.277e-17\n",
      "‖w_svm‖₂       : 1.1373469038277554e-06\n",
      "‖alpha‖₁       : 0.319999999998847\n",
      "scores min/max : -2.3089530703409067e-06 -2.194144432075803e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4898  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.874e-18\n",
      "‖w_svm‖₂       : 0.0031720120053352935\n",
      "‖alpha‖₁       : 0.5799999999999992\n",
      "scores min/max : 0.002574042389640892 0.0038488074114169935\n",
      "Mask mean value:  tensor(0.5179, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3293  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.266e-13\n",
      "‖w_svm‖₂       : 2.305903359455955e-07\n",
      "‖alpha‖₁       : 0.6399999999999977\n",
      "scores min/max : 3.890835405108856e-07 4.099348081391411e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.720e-20\n",
      "‖w_svm‖₂       : 0.13795379725654716\n",
      "‖alpha‖₁       : 0.8799999999999999\n",
      "scores min/max : -1.349210151535901 3.613182853366359\n",
      "Mask mean value:  tensor(0.3275, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2604  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.291e-07\n",
      "‖w_svm‖₂       : 3.589868768920062e-06\n",
      "‖alpha‖₁       : 0.41999999999274895\n",
      "scores min/max : -2.434293910163882e-06 -7.869540316690733e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1229  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.551e-05\n",
      "‖w_svm‖₂       : 7.5026060317041e-08\n",
      "‖alpha‖₁       : 0.5399999999999345\n",
      "scores min/max : -1.4534716549274045e-07 -1.2369111951825939e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.643e-20\n",
      "‖w_svm‖₂       : 0.14015074018622495\n",
      "‖alpha‖₁       : 0.6554485038281885\n",
      "scores min/max : -18.099144265162693 1.7769854668139708\n",
      "Mask mean value:  tensor(0.2210, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2717  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.074e-03\n",
      "‖w_svm‖₂       : 5.6125324140506075e-08\n",
      "‖alpha‖₁       : 0.23999999999999566\n",
      "scores min/max : -6.19087756870171e-08 -4.754038542532667e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.428e-20\n",
      "‖w_svm‖₂       : 0.06144003278530979\n",
      "‖alpha‖₁       : 0.8987300797532385\n",
      "scores min/max : -1.6918353653927136 3.7198228503778203\n",
      "Mask mean value:  tensor(0.4458, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5921  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.569e-03\n",
      "‖w_svm‖₂       : 0.05249403376388676\n",
      "‖alpha‖₁       : 0.8272170122390436\n",
      "scores min/max : -1.9419109832564527 0.4085592108642666\n",
      "Mask mean value:  tensor(0.5527, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0174  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.596e-11\n",
      "‖w_svm‖₂       : 2.0712716400901713e-07\n",
      "‖alpha‖₁       : 0.3799999999952697\n",
      "scores min/max : -7.218589886449125e-08 -5.350706640461049e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.848e-07\n",
      "‖w_svm‖₂       : 0.02125830975799287\n",
      "‖alpha‖₁       : 0.6599999999999996\n",
      "scores min/max : -0.0450752329565213 0.03314670846188078\n",
      "Mask mean value:  tensor(0.4139, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.2686  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.531e-15\n",
      "‖w_svm‖₂       : 0.00036928084805241187\n",
      "‖alpha‖₁       : 0.7399999999999989\n",
      "scores min/max : -0.0003292873468672552 -0.0003118940160009128\n",
      "Mask mean value:  tensor(0.4984, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0609  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.255e-17\n",
      "‖w_svm‖₂       : 1.1259553054819099e-07\n",
      "‖alpha‖₁       : 0.2999999999999921\n",
      "scores min/max : 1.2685273160848886e-07 1.3584725755893954e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.280e-07\n",
      "‖w_svm‖₂       : 0.027866335518215674\n",
      "‖alpha‖₁       : 0.8523304438237872\n",
      "scores min/max : -2.9043336174032177 1.5766584139985003\n",
      "Mask mean value:  tensor(0.1612, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9573  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.356e-03\n",
      "‖w_svm‖₂       : 0.004399103326047235\n",
      "‖alpha‖₁       : 0.4599999999999994\n",
      "scores min/max : -0.008011922412878729 -0.00591949925097645\n",
      "Mask mean value:  tensor(0.4680, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.0338  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.059e-15\n",
      "‖w_svm‖₂       : 0.047288669292006445\n",
      "‖alpha‖₁       : 0.9056458771779763\n",
      "scores min/max : -0.7054273901492543 1.8990581545986318\n",
      "Mask mean value:  tensor(0.3993, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7410  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.224e-05\n",
      "‖w_svm‖₂       : 4.743612233590089e-08\n",
      "‖alpha‖₁       : 0.17999999999999816\n",
      "scores min/max : 4.2849042383910545e-08 5.7503456581014834e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.086e-08\n",
      "‖w_svm‖₂       : 0.04155149084610527\n",
      "‖alpha‖₁       : 0.9199999999723703\n",
      "scores min/max : -0.3128350860817053 0.32190411006969744\n",
      "Mask mean value:  tensor(0.2153, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0620  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.341e-02\n",
      "‖w_svm‖₂       : 5.314163499389566e-08\n",
      "‖alpha‖₁       : 0.11999999999998691\n",
      "scores min/max : -9.257744230178571e-08 -6.171212119257604e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.158e-08\n",
      "‖w_svm‖₂       : 0.01670201147116852\n",
      "‖alpha‖₁       : 0.8599999999999424\n",
      "scores min/max : -0.04837834843472047 -0.033539569839487235\n",
      "Mask mean value:  tensor(0.3210, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0787  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.386e-14\n",
      "‖w_svm‖₂       : 1.0283930665677562e-07\n",
      "‖alpha‖₁       : 0.2399999999999961\n",
      "scores min/max : 3.533779906266213e-08 4.6159309315772525e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.195e-21\n",
      "‖w_svm‖₂       : 3.7373921381339855e-07\n",
      "‖alpha‖₁       : 0.27999999999999403\n",
      "scores min/max : -1.3825605265993854e-06 -1.320782166482652e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.632e-19\n",
      "‖w_svm‖₂       : 0.00568508720794226\n",
      "‖alpha‖₁       : 0.5599999999999783\n",
      "scores min/max : 0.004911849402972507 0.006589525454561117\n",
      "Mask mean value:  tensor(0.5261, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1027  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 2.803e-05\n",
      "‖w_svm‖₂       : 1.9491755593182473e-07\n",
      "‖alpha‖₁       : 0.4199999999999747\n",
      "scores min/max : -4.2583560217877487e-07 -4.0358106051973883e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.716e-19\n",
      "‖w_svm‖₂       : 0.07275655021125071\n",
      "‖alpha‖₁       : 0.6584323669899934\n",
      "scores min/max : -2.0646336747813336 4.211173938109022\n",
      "Mask mean value:  tensor(0.2968, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.4471  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.260e-03\n",
      "‖w_svm‖₂       : 0.005537070820715765\n",
      "‖alpha‖₁       : 0.3799999999999997\n",
      "scores min/max : -0.005738071933275496 0.006631365066568792\n",
      "Mask mean value:  tensor(0.5173, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9200  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.851e-14\n",
      "‖w_svm‖₂       : 0.07084676245329163\n",
      "‖alpha‖₁       : 0.5799999999999996\n",
      "scores min/max : -2.7472433944818886 1.5743139739878633\n",
      "Mask mean value:  tensor(0.1867, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3347  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.628e-04\n",
      "‖w_svm‖₂       : 0.0008819329948463963\n",
      "‖alpha‖₁       : 0.8199999999999978\n",
      "scores min/max : 0.000995798505529535 0.0027111315131685167\n",
      "Mask mean value:  tensor(0.5104, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5928  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.046e-16\n",
      "‖w_svm‖₂       : 1.0529955388500685e-06\n",
      "‖alpha‖₁       : 0.49999999999999883\n",
      "scores min/max : -2.540845393895288e-07 -6.125228405175506e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.741e-19\n",
      "‖w_svm‖₂       : 0.028858395752856307\n",
      "‖alpha‖₁       : 0.5511018006842552\n",
      "scores min/max : -3.506863944412386 1.0636474337936113\n",
      "Mask mean value:  tensor(0.1331, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0851  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.188e-03\n",
      "‖w_svm‖₂       : 0.016505978929031776\n",
      "‖alpha‖₁       : 0.859999999999999\n",
      "scores min/max : -0.05299295699953385 0.007533562612273819\n",
      "Mask mean value:  tensor(0.3622, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4622  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.004e-05\n",
      "‖w_svm‖₂       : 1.6508228162055652e-07\n",
      "‖alpha‖₁       : 0.2399999999999915\n",
      "scores min/max : 2.7858168855872975e-07 2.939345531008078e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.329e-20\n",
      "‖w_svm‖₂       : 8.147471993246738e-08\n",
      "‖alpha‖₁       : 0.37999999999999956\n",
      "scores min/max : -6.100738554388132e-10 1.754621801774231e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.624e-22\n",
      "‖w_svm‖₂       : 0.0002167802076569335\n",
      "‖alpha‖₁       : 0.6199999999962977\n",
      "scores min/max : 2.6846412527916903e-05 4.349833880427938e-05\n",
      "Mask mean value:  tensor(0.5002, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.493e-17\n",
      "‖w_svm‖₂       : 0.006064818605059034\n",
      "‖alpha‖₁       : 0.7005691441513588\n",
      "scores min/max : -2.0032718825656533 0.020138380971039423\n",
      "Mask mean value:  tensor(0.4584, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2568  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.246e-06\n",
      "‖w_svm‖₂       : 0.10499074359933966\n",
      "‖alpha‖₁       : 0.8732915117134066\n",
      "scores min/max : -12.178867873738524 2.1556505260888787\n",
      "Mask mean value:  tensor(0.5409, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.6012  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.017e-02\n",
      "‖w_svm‖₂       : 0.03103701596501662\n",
      "‖alpha‖₁       : 0.8985946604916113\n",
      "scores min/max : -0.7417980813410785 2.0184618205714986\n",
      "Mask mean value:  tensor(0.8536, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3412  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.034e-15\n",
      "‖w_svm‖₂       : 0.042002048511787696\n",
      "‖alpha‖₁       : 0.9405363283825638\n",
      "scores min/max : -1.8348993887624907 0.2989247181050091\n",
      "Mask mean value:  tensor(0.3984, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3238  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.164e-14\n",
      "‖w_svm‖₂       : 0.022876293054742668\n",
      "‖alpha‖₁       : 0.8150962782320239\n",
      "scores min/max : -11.519565039359598 2.0313664479378915\n",
      "Mask mean value:  tensor(0.7085, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9067  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.977e-06\n",
      "‖w_svm‖₂       : 0.044224636088012546\n",
      "‖alpha‖₁       : 0.6599999999999948\n",
      "scores min/max : -0.25804867620531047 0.18533354451253337\n",
      "Mask mean value:  tensor(0.4799, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8785  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.495e-13\n",
      "‖w_svm‖₂       : 0.017422766510048815\n",
      "‖alpha‖₁       : 0.6815607806498549\n",
      "scores min/max : -1.9734294956178973 0.056538072734108685\n",
      "Mask mean value:  tensor(0.6073, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2542  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.072e-12\n",
      "‖w_svm‖₂       : 2.213009413862377e-07\n",
      "‖alpha‖₁       : 0.25999999999999845\n",
      "scores min/max : 9.74667755577914e-08 1.1021914979586203e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.593e-17\n",
      "‖w_svm‖₂       : 0.00014507453046679067\n",
      "‖alpha‖₁       : 0.43999999988636035\n",
      "scores min/max : -0.00027998707922989 -0.00013114117677117562\n",
      "Mask mean value:  tensor(0.4991, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0215  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.892e-17\n",
      "‖w_svm‖₂       : 0.0005086165903428278\n",
      "‖alpha‖₁       : 0.43999999999994854\n",
      "scores min/max : -0.0007948439392421241 -0.0005319856656696817\n",
      "Mask mean value:  tensor(0.4962, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6429  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.225e-16\n",
      "‖w_svm‖₂       : 0.14759376893567913\n",
      "‖alpha‖₁       : 0.7597447636808412\n",
      "scores min/max : -1.7598673929846256 2.2177273023269253\n",
      "Mask mean value:  tensor(0.8512, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3551  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.950e-12\n",
      "‖w_svm‖₂       : 0.05663167669880366\n",
      "‖alpha‖₁       : 0.576357371649748\n",
      "scores min/max : -1.9539123496314283 0.874771872969196\n",
      "Mask mean value:  tensor(0.7234, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9692  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.165e-03\n",
      "‖w_svm‖₂       : 2.3358056346962718e-07\n",
      "‖alpha‖₁       : 0.5799999999999893\n",
      "scores min/max : -3.5514503896818804e-07 -3.3949003125330164e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.981e-19\n",
      "‖w_svm‖₂       : 7.115098170713794e-08\n",
      "‖alpha‖₁       : 0.13999999999999543\n",
      "scores min/max : 1.1283433245892333e-08 1.7070369320738537e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.050e-08\n",
      "‖w_svm‖₂       : 2.1253766564631863e-08\n",
      "‖alpha‖₁       : 0.11999999999999496\n",
      "scores min/max : -3.256233036029938e-08 -1.7907099021736524e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.819e-09\n",
      "‖w_svm‖₂       : 2.740282622483937e-07\n",
      "‖alpha‖₁       : 0.6599999999999997\n",
      "scores min/max : -6.74348506929688e-07 -5.22707548525166e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.351e-18\n",
      "‖w_svm‖₂       : 0.07313213834123689\n",
      "‖alpha‖₁       : 0.41921882883843803\n",
      "scores min/max : -1.9118555016939145 2.2311650100869764\n",
      "Mask mean value:  tensor(0.7607, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8934  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.395e-04\n",
      "‖w_svm‖₂       : 5.340175468488318e-08\n",
      "‖alpha‖₁       : 0.439999999999986\n",
      "scores min/max : -1.8040666847990583e-07 -7.645733218998931e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.450e-20\n",
      "‖w_svm‖₂       : 7.544296603340373e-08\n",
      "‖alpha‖₁       : 0.6599999999999959\n",
      "scores min/max : 1.0925990225439745e-07 2.541521210150551e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.960e-09\n",
      "‖w_svm‖₂       : 0.019105338772833665\n",
      "‖alpha‖₁       : 0.38339461472311487\n",
      "scores min/max : -1.9623630943884423 0.24014704970954423\n",
      "Mask mean value:  tensor(0.7053, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1516  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.354e-14\n",
      "‖w_svm‖₂       : 9.946196413894257e-07\n",
      "‖alpha‖₁       : 0.5999999999999488\n",
      "scores min/max : -1.104152032444108e-07 -5.9336768484609733e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.538e-19\n",
      "‖w_svm‖₂       : 0.18580399800152814\n",
      "‖alpha‖₁       : 0.8838858944826413\n",
      "scores min/max : -3.620540578220216 6.075577866701966\n",
      "Mask mean value:  tensor(0.0926, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1987  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.614e-03\n",
      "‖w_svm‖₂       : 1.1188462445301283e-07\n",
      "‖alpha‖₁       : 0.5199999999999503\n",
      "scores min/max : 3.33235169300028e-07 3.5465887827609265e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.774e-19\n",
      "‖w_svm‖₂       : 0.0001901120738622677\n",
      "‖alpha‖₁       : 0.8199999999999996\n",
      "scores min/max : -8.53271393905906e-05 -7.035274105768568e-05\n",
      "Mask mean value:  tensor(0.4996, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6966  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.303e-17\n",
      "‖w_svm‖₂       : 0.04672501783700844\n",
      "‖alpha‖₁       : 0.938978776322701\n",
      "scores min/max : -2.4634360118599314 1.5836184305047225\n",
      "Mask mean value:  tensor(0.1444, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3968  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.365e-03\n",
      "‖w_svm‖₂       : 0.015922894764381017\n",
      "‖alpha‖₁       : 0.596271685870564\n",
      "scores min/max : -2.9633317295505446 2.2746525871519303\n",
      "Mask mean value:  tensor(0.9168, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.5080  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.114e-07\n",
      "‖w_svm‖₂       : 0.08279858498495105\n",
      "‖alpha‖₁       : 0.4720566710390572\n",
      "scores min/max : -2.3409984109164705 2.741044470294518\n",
      "Mask mean value:  tensor(0.0946, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0393  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.101e-02\n",
      "‖w_svm‖₂       : 0.04263955861435832\n",
      "‖alpha‖₁       : 0.8900071394217597\n",
      "scores min/max : -2.008576111939967 0.33925748536978273\n",
      "Mask mean value:  tensor(0.2663, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0689  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.588e-12\n",
      "‖w_svm‖₂       : 0.007979059394965357\n",
      "‖alpha‖₁       : 0.6076860564109009\n",
      "scores min/max : -1.989363336395484 0.29744617192322775\n",
      "Mask mean value:  tensor(0.5602, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2263  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.936e-15\n",
      "‖w_svm‖₂       : 0.020711632606469108\n",
      "‖alpha‖₁       : 0.7799999999999991\n",
      "scores min/max : -0.03052436813749369 0.042037171546006946\n",
      "Mask mean value:  tensor(0.3807, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6804  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.120e-15\n",
      "‖w_svm‖₂       : 4.2588710679977554e-07\n",
      "‖alpha‖₁       : 0.7199999999996605\n",
      "scores min/max : 2.511157328767964e-07 3.0865798223655317e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.053e-21\n",
      "‖w_svm‖₂       : 0.043280090017934356\n",
      "‖alpha‖₁       : 0.7294226937158161\n",
      "scores min/max : -0.25552914733194054 2.0258251501546107\n",
      "Mask mean value:  tensor(0.6951, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.1371  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.430e-03\n",
      "‖w_svm‖₂       : 8.390706413403183e-08\n",
      "‖alpha‖₁       : 0.17999999999999428\n",
      "scores min/max : -1.614811435520665e-07 7.345403717783567e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.110e-21\n",
      "‖w_svm‖₂       : 0.08628248972893379\n",
      "‖alpha‖₁       : 0.5780783486901632\n",
      "scores min/max : -2.1367426698115377 5.789959713299593\n",
      "Mask mean value:  tensor(0.2466, dtype=torch.float64)\n",
      "max feasible return = 0.1062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.8848457098417383e-07\n",
      "‖alpha‖₁       : 0.58\n",
      "scores min/max : -3.93016297935874e-07 -3.0228334591296725e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0121849302816243e-07\n",
      "‖alpha‖₁       : 0.29999999999999893\n",
      "scores min/max : 4.3252397496932174e-08 4.82355897177168e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 6.853730912074989e-08\n",
      "‖alpha‖₁       : 0.5999999999999761\n",
      "scores min/max : 2.768487614067809e-08 6.447083256265753e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.561092856216521e-08\n",
      "‖alpha‖₁       : 0.37999999999999784\n",
      "scores min/max : 1.2434780781313515e-09 1.0633585205773376e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.921848578049138e-06\n",
      "‖alpha‖₁       : 0.31999999997481976\n",
      "scores min/max : 3.4203381417133714e-06 5.218390311672663e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4258  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.05229290102769135\n",
      "‖alpha‖₁       : 0.7397240709916888\n",
      "scores min/max : -3.4799510557105244 2.1543802597163424\n",
      "Mask mean value:  tensor(0.8810, dtype=torch.float64)\n",
      "max feasible return = -0.9281  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.02978025585582419\n",
      "‖alpha‖₁       : 0.6151655516526453\n",
      "scores min/max : -1.9190513723770193 1.2114012704496322\n",
      "Mask mean value:  tensor(0.7807, dtype=torch.float64)\n",
      "max feasible return = 2.7316  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.588168580600451e-07\n",
      "‖alpha‖₁       : 0.459999999999991\n",
      "scores min/max : 2.3258649948141112e-07 8.271697596180323e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.348159985834396e-07\n",
      "‖alpha‖₁       : 0.5199999999999743\n",
      "scores min/max : 7.475198595872113e-08 8.767708989597843e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.5206862836384857e-07\n",
      "‖alpha‖₁       : 0.5799999999999819\n",
      "scores min/max : -2.6712895068344244e-07 -2.3080889727536645e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.04460291486952982\n",
      "‖alpha‖₁       : 0.6877856662324107\n",
      "scores min/max : -0.43898579675905547 1.950250166871324\n",
      "Mask mean value:  tensor(0.3788, dtype=torch.float64)\n",
      "max feasible return = 0.0613  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03258294602825067\n",
      "‖alpha‖₁       : 0.422196756680123\n",
      "scores min/max : -3.899093514457415 5.244957232115297\n",
      "Mask mean value:  tensor(0.0860, dtype=torch.float64)\n",
      "max feasible return = 0.5416  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04389954413416776\n",
      "‖alpha‖₁       : 0.8211220106069088\n",
      "scores min/max : -5.069600279619991 3.0979489147351127\n",
      "Mask mean value:  tensor(0.9483, dtype=torch.float64)\n",
      "max feasible return = -3.6004  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.007603977635180322\n",
      "‖alpha‖₁       : 0.7999999999999428\n",
      "scores min/max : -0.035260973183614375 0.0034736871725991663\n",
      "Mask mean value:  tensor(0.4688, dtype=torch.float64)\n",
      "max feasible return = -0.1703  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.020575984625189e-07\n",
      "‖alpha‖₁       : 0.6199999999999557\n",
      "scores min/max : 7.150473506422796e-08 8.805547310420474e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.685558182966898e-08\n",
      "‖alpha‖₁       : 0.43999999999999995\n",
      "scores min/max : 7.565831712364157e-09 1.7135616804512984e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.2580797792615874e-07\n",
      "‖alpha‖₁       : 0.27999999999997516\n",
      "scores min/max : -3.1724644721015156e-07 -2.9933621996816313e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  31 | train 0.005332 | val 0.006772\n",
      "-----------------------------------------Epoch:  32 ----------------------------------------\n",
      "‖w_svm‖₂       : 4.743957760687747e-08\n",
      "‖alpha‖₁       : 0.17999999999999813\n",
      "scores min/max : 4.492062516889916e-08 5.957060248753093e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.138e-08\n",
      "‖w_svm‖₂       : 0.028011811201091436\n",
      "‖alpha‖₁       : 0.8523365832563279\n",
      "scores min/max : -2.908535205375328 1.5724474026384099\n",
      "Mask mean value:  tensor(0.1591, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9552  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.458e-03\n",
      "‖w_svm‖₂       : 0.0005085126919451992\n",
      "‖alpha‖₁       : 0.43999999999995754\n",
      "scores min/max : -0.0008299429453349164 -0.000566984183131441\n",
      "Mask mean value:  tensor(0.4961, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6423  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.226e-16\n",
      "‖w_svm‖₂       : 0.0003113357431770019\n",
      "‖alpha‖₁       : 0.4199999999554922\n",
      "scores min/max : 0.00018816952477286359 0.000516129133303233\n",
      "Mask mean value:  tensor(0.5024, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0004  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.950e-15\n",
      "‖w_svm‖₂       : 0.015921311961413456\n",
      "‖alpha‖₁       : 0.5962720327954241\n",
      "scores min/max : -2.9633817561448743 2.2746217235303643\n",
      "Mask mean value:  tensor(0.9168, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.5080  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.909e-12\n",
      "‖w_svm‖₂       : 0.046643966846222795\n",
      "‖alpha‖₁       : 0.9389694451888456\n",
      "scores min/max : -2.463129641656117 1.5839014051349043\n",
      "Mask mean value:  tensor(0.1446, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.337e-03\n",
      "‖w_svm‖₂       : 0.047488740847875265\n",
      "‖alpha‖₁       : 0.9056462840274981\n",
      "scores min/max : -0.7139214787673022 1.8960936402638304\n",
      "Mask mean value:  tensor(0.3918, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7269  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.109e-05\n",
      "‖w_svm‖₂       : 2.126196766266934e-08\n",
      "‖alpha‖₁       : 0.11999999999999486\n",
      "scores min/max : -3.171548955963852e-08 -1.7057367955235794e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.816e-09\n",
      "‖w_svm‖₂       : 0.0410486227219175\n",
      "‖alpha‖₁       : 0.9199998973317671\n",
      "scores min/max : -0.31008180038679 0.31020110724558225\n",
      "Mask mean value:  tensor(0.2060, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0161  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 9.987e-03\n",
      "‖w_svm‖₂       : 0.042521803148884416\n",
      "‖alpha‖₁       : 0.8899971987098926\n",
      "scores min/max : -2.009453706473425 0.3383781590155828\n",
      "Mask mean value:  tensor(0.2638, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0676  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.064e-08\n",
      "‖w_svm‖₂       : 0.18514680193201646\n",
      "‖alpha‖₁       : 0.8836628872803802\n",
      "scores min/max : -3.6159730272069806 6.0807370648356835\n",
      "Mask mean value:  tensor(0.0936, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2009  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.166e-05\n",
      "‖w_svm‖₂       : 1.0352026300412705e-07\n",
      "‖alpha‖₁       : 0.23999999999999355\n",
      "scores min/max : 3.944080543012767e-08 5.029485110940618e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.243e-21\n",
      "‖w_svm‖₂       : 9.955907195842248e-07\n",
      "‖alpha‖₁       : 0.599999999999938\n",
      "scores min/max : -1.3498237968570563e-07 -8.388908115302718e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.531e-19\n",
      "‖w_svm‖₂       : 5.300977842751878e-08\n",
      "‖alpha‖₁       : 0.11999999999998909\n",
      "scores min/max : -9.488891293340951e-08 -6.406098458479803e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.163e-07\n",
      "‖w_svm‖₂       : 2.3335824438153324e-07\n",
      "‖alpha‖₁       : 0.5800000000000001\n",
      "scores min/max : -3.5116984342324844e-07 -3.3554792659590314e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.980e-19\n",
      "‖w_svm‖₂       : 0.017547964568083944\n",
      "‖alpha‖₁       : 0.6815654947321063\n",
      "scores min/max : -1.9740014065293074 0.05599630536024259\n",
      "Mask mean value:  tensor(0.6046, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2527  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.838e-12\n",
      "‖w_svm‖₂       : 0.00014501300206456384\n",
      "‖alpha‖₁       : 0.4399999998781645\n",
      "scores min/max : -0.00027960871920654565 -0.00013090752970012896\n",
      "Mask mean value:  tensor(0.4991, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0215  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.900e-17\n",
      "‖w_svm‖₂       : 2.2195231056038084e-07\n",
      "‖alpha‖₁       : 0.2599999999999961\n",
      "scores min/max : 1.1099089106586657e-07 1.2376387000432897e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.633e-17\n",
      "‖w_svm‖₂       : 1.0545695463734019e-06\n",
      "‖alpha‖₁       : 0.49999999999999795\n",
      "scores min/max : -2.483690782132186e-07 -5.550271052113768e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.750e-19\n",
      "‖w_svm‖₂       : 7.115582324974389e-08\n",
      "‖alpha‖₁       : 0.1399999999999972\n",
      "scores min/max : 1.1176331786774127e-08 1.6940767512929253e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.067e-08\n",
      "‖w_svm‖₂       : 0.0002163249458738324\n",
      "‖alpha‖₁       : 0.6199999999959389\n",
      "scores min/max : 3.0681924061380406e-05 4.726336836748606e-05\n",
      "Mask mean value:  tensor(0.5002, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.476e-17\n",
      "‖w_svm‖₂       : 1.1178953545629385e-07\n",
      "‖alpha‖₁       : 0.5199999999999529\n",
      "scores min/max : 3.398885956811036e-07 3.6130648568059225e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.779e-19\n",
      "‖w_svm‖₂       : 0.020731927132460297\n",
      "‖alpha‖₁       : 0.7799999999999998\n",
      "scores min/max : -0.028447858330831053 0.044254865753627026\n",
      "Mask mean value:  tensor(0.3906, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.7218  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.299e-15\n",
      "‖w_svm‖₂       : 0.020919507459725373\n",
      "‖alpha‖₁       : 0.6599999999999893\n",
      "scores min/max : -0.04594296536991205 0.02950080989872301\n",
      "Mask mean value:  tensor(0.4052, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.1992  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.529e-15\n",
      "‖w_svm‖₂       : 0.07274294978864385\n",
      "‖alpha‖₁       : 0.4191777456550229\n",
      "scores min/max : -1.907513628197274 2.231890223491809\n",
      "Mask mean value:  tensor(0.7649, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8947  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.474e-04\n",
      "‖w_svm‖₂       : 0.01628734977165336\n",
      "‖alpha‖₁       : 0.8599999999999981\n",
      "scores min/max : -0.05296164480572284 0.006073009194712899\n",
      "Mask mean value:  tensor(0.3595, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4581  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.263e-05\n",
      "‖w_svm‖₂       : 0.10398904152254523\n",
      "‖alpha‖₁       : 0.873290300611046\n",
      "scores min/max : -12.183717277629196 2.149317400560773\n",
      "Mask mean value:  tensor(0.5256, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.5571  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.838e-02\n",
      "‖w_svm‖₂       : 0.00019014374666365328\n",
      "‖alpha‖₁       : 0.8199999999999985\n",
      "scores min/max : -8.653573248091972e-05 -7.155954169455912e-05\n",
      "Mask mean value:  tensor(0.4996, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6966  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.306e-17\n",
      "‖w_svm‖₂       : 7.102833286612298e-08\n",
      "‖alpha‖₁       : 0.4199999999999936\n",
      "scores min/max : -7.188445847373881e-08 -2.4677525354330665e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.819e-08\n",
      "‖w_svm‖₂       : 0.05630746546538961\n",
      "‖alpha‖₁       : 0.5763407212887295\n",
      "scores min/max : -1.9543465361650725 0.874305738844688\n",
      "Mask mean value:  tensor(0.7220, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9652  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.152e-03\n",
      "‖w_svm‖₂       : 8.175146524524185e-08\n",
      "‖alpha‖₁       : 0.37999999999999956\n",
      "scores min/max : 9.825105065344364e-10 1.9140323185075857e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.659e-22\n",
      "‖w_svm‖₂       : 0.019225897446394875\n",
      "‖alpha‖₁       : 0.38339970320521255\n",
      "scores min/max : -1.9618546322431172 0.24065440407351368\n",
      "Mask mean value:  tensor(0.7072, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1518  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.094e-14\n",
      "‖w_svm‖₂       : 1.9067313277170017e-07\n",
      "‖alpha‖₁       : 0.3799999999999997\n",
      "scores min/max : -4.3257991295122885e-08 -7.59535342550342e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.558e-19\n",
      "‖w_svm‖₂       : 6.689068610950425e-07\n",
      "‖alpha‖₁       : 0.39999999999999974\n",
      "scores min/max : -5.339870366611798e-08 2.5292518667471874e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.292e-09\n",
      "‖w_svm‖₂       : 3.924128332917421e-06\n",
      "‖alpha‖₁       : 0.41999999999022236\n",
      "scores min/max : -2.73061406649466e-06 -9.656533326639983e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1229  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.706e-05\n",
      "‖w_svm‖₂       : 0.028844003690844215\n",
      "‖alpha‖₁       : 0.5511058960366317\n",
      "scores min/max : -3.5046075144893045 1.0657235438192112\n",
      "Mask mean value:  tensor(0.1341, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0909  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.190e-03\n",
      "‖w_svm‖₂       : 1.1377669534035826e-06\n",
      "‖alpha‖₁       : 0.3199999999988143\n",
      "scores min/max : -2.4661075756552148e-06 -2.351285685770044e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4897  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.909e-18\n",
      "‖w_svm‖₂       : 0.042908366964719585\n",
      "‖alpha‖₁       : 0.7294202327408883\n",
      "scores min/max : -0.2554489535918114 2.0258401998573254\n",
      "Mask mean value:  tensor(0.6952, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.1384  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.429e-03\n",
      "‖w_svm‖₂       : 0.00036963967753833707\n",
      "‖alpha‖₁       : 0.7399999999999997\n",
      "scores min/max : -0.0003266734597748384 -0.00030924635773723865\n",
      "Mask mean value:  tensor(0.4984, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0610  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.269e-17\n",
      "‖w_svm‖₂       : 2.0744961032778567e-07\n",
      "‖alpha‖₁       : 0.37999999999516876\n",
      "scores min/max : -8.330355244354322e-08 -6.452534897059888e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.136e-06\n",
      "‖w_svm‖₂       : 0.02717717765594551\n",
      "‖alpha‖₁       : 0.19667291178536125\n",
      "scores min/max : -2.230287618897462 -0.007446668160906791\n",
      "Mask mean value:  tensor(0.0381, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2608  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.676e-03\n",
      "‖w_svm‖₂       : 0.01659156125744909\n",
      "‖alpha‖₁       : 0.8599999999999809\n",
      "scores min/max : -0.046632270891591554 -0.03198939197721842\n",
      "Mask mean value:  tensor(0.3280, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0811  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.316e-14\n",
      "‖w_svm‖₂       : 0.006133905383129595\n",
      "‖alpha‖₁       : 0.7005698317973549\n",
      "scores min/max : -2.0029182756134034 0.020493970718076272\n",
      "Mask mean value:  tensor(0.4601, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2577  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.393e-12\n",
      "‖w_svm‖₂       : 0.004402758904440405\n",
      "‖alpha‖₁       : 0.45999999999999525\n",
      "scores min/max : -0.008174296409988796 -0.006074612189735144\n",
      "Mask mean value:  tensor(0.4672, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.0304  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.063e-15\n",
      "‖w_svm‖₂       : 0.022672220542228148\n",
      "‖alpha‖₁       : 0.8150966876299057\n",
      "scores min/max : -11.519018001849197 2.031179192769322\n",
      "Mask mean value:  tensor(0.7081, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9068  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.904e-06\n",
      "‖w_svm‖₂       : 0.1389718630358272\n",
      "‖alpha‖₁       : 0.8799999999999999\n",
      "scores min/max : -1.376276704967949 3.657793742012333\n",
      "Mask mean value:  tensor(0.3103, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2589  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.594e-04\n",
      "‖w_svm‖₂       : 0.07112827366999222\n",
      "‖alpha‖₁       : 0.5799999999999997\n",
      "scores min/max : -2.7689823149750197 1.5876530921249037\n",
      "Mask mean value:  tensor(0.1868, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3357  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.632e-04\n",
      "‖w_svm‖₂       : 0.0008811256213294106\n",
      "‖alpha‖₁       : 0.8199999999999806\n",
      "scores min/max : 0.0011793886961866031 0.0028901876289787022\n",
      "Mask mean value:  tensor(0.5113, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5973  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.058e-16\n",
      "‖w_svm‖₂       : 0.1469547032975465\n",
      "‖alpha‖₁       : 0.7595582173224419\n",
      "scores min/max : -1.7587828326148776 2.2188558305359916\n",
      "Mask mean value:  tensor(0.8528, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3559  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.183e-11\n",
      "‖w_svm‖₂       : 2.3161832711664117e-07\n",
      "‖alpha‖₁       : 0.6399999999999247\n",
      "scores min/max : 4.0605867307762096e-07 4.2720254290860025e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.897e-20\n",
      "‖w_svm‖₂       : 5.325751484971063e-08\n",
      "‖alpha‖₁       : 0.4399999999999875\n",
      "scores min/max : -1.8277741697487489e-07 -7.878158546802922e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.452e-20\n",
      "‖w_svm‖₂       : 7.551174711787263e-08\n",
      "‖alpha‖₁       : 0.6599999999999955\n",
      "scores min/max : 1.1456087694975506e-07 2.594218429700514e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.013e-09\n",
      "‖w_svm‖₂       : 0.005543200890833893\n",
      "‖alpha‖₁       : 0.37999999999999795\n",
      "scores min/max : -0.006797115909610213 0.005361496494720751\n",
      "Mask mean value:  tensor(0.5113, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9093  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.034e-14\n",
      "‖w_svm‖₂       : 0.08244907498903668\n",
      "‖alpha‖₁       : 0.4720014789593175\n",
      "scores min/max : -2.3190944342775444 2.76249770305644\n",
      "Mask mean value:  tensor(0.1029, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1260  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.318e-02\n",
      "‖w_svm‖₂       : 1.9504137066579481e-07\n",
      "‖alpha‖₁       : 0.41999999999997395\n",
      "scores min/max : -4.530717974178425e-07 -4.308207658954063e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.744e-19\n",
      "‖w_svm‖₂       : 7.493244643297859e-08\n",
      "‖alpha‖₁       : 0.5399999999999999\n",
      "scores min/max : -1.5520941710418857e-07 -1.3368922986348718e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.684e-20\n",
      "‖w_svm‖₂       : 0.03067934295431656\n",
      "‖alpha‖₁       : 0.8985871107706244\n",
      "scores min/max : -0.7414769603102808 2.0187806938298514\n",
      "Mask mean value:  tensor(0.8540, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3395  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.950e-15\n",
      "‖w_svm‖₂       : 1.7510732084715142e-05\n",
      "‖alpha‖₁       : 0.35999999998268084\n",
      "scores min/max : 1.4799626743731144e-05 1.590922413357951e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.671e-17\n",
      "‖w_svm‖₂       : 0.04350565674156734\n",
      "‖alpha‖₁       : 0.6599999999999485\n",
      "scores min/max : -0.24939693390221662 0.1795359651399746\n",
      "Mask mean value:  tensor(0.4814, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8850  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.456e-13\n",
      "‖w_svm‖₂       : 4.196618930217112e-07\n",
      "‖alpha‖₁       : 0.7199999999999988\n",
      "scores min/max : 2.5855415578580536e-07 3.15730091280559e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.020e-21\n",
      "‖w_svm‖₂       : 0.008063121008075715\n",
      "‖alpha‖₁       : 0.6076873892272192\n",
      "scores min/max : -1.9896333065900815 0.2971754618474485\n",
      "Mask mean value:  tensor(0.5590, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2258  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.118e-15\n",
      "‖w_svm‖₂       : 0.060726586162251454\n",
      "‖alpha‖₁       : 0.8986412958638609\n",
      "scores min/max : -1.6943023499875483 3.7179559181666604\n",
      "Mask mean value:  tensor(0.4412, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5888  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.197e-03\n",
      "‖w_svm‖₂       : 0.0031816727647955823\n",
      "‖alpha‖₁       : 0.5799999999999912\n",
      "scores min/max : 0.001834197328786642 0.003115341033167851\n",
      "Mask mean value:  tensor(0.5142, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3270  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.540e-13\n",
      "‖w_svm‖₂       : 0.04239430803577842\n",
      "‖alpha‖₁       : 0.9405710244087611\n",
      "scores min/max : -1.8367168745893787 0.2971038823805833\n",
      "Mask mean value:  tensor(0.3914, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3232  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.943e-13\n",
      "‖w_svm‖₂       : 0.00567787303082927\n",
      "‖alpha‖₁       : 0.5599999999999979\n",
      "scores min/max : 0.005296562137709952 0.006971111000610107\n",
      "Mask mean value:  tensor(0.5280, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1067  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 2.439e-05\n",
      "‖w_svm‖₂       : 0.07242234501910444\n",
      "‖alpha‖₁       : 0.6584317236601749\n",
      "scores min/max : -2.0452054465129894 4.240543589816134\n",
      "Mask mean value:  tensor(0.3270, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.7913  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.288e-04\n",
      "‖w_svm‖₂       : 8.346824828484625e-08\n",
      "‖alpha‖₁       : 0.1799999999999946\n",
      "scores min/max : -1.7128759643774468e-07 6.35707693510479e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.108e-21\n",
      "‖w_svm‖₂       : 2.744404736938791e-07\n",
      "‖alpha‖₁       : 0.6599999999999924\n",
      "scores min/max : -6.949279752426619e-07 -5.432183212021383e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.366e-18\n",
      "‖w_svm‖₂       : 0.051850081437945554\n",
      "‖alpha‖₁       : 0.8271604170746147\n",
      "scores min/max : -1.9528487993844577 0.3977294495975745\n",
      "Mask mean value:  tensor(0.5139, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0105  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.744e-02\n",
      "‖w_svm‖₂       : 0.1386544177523745\n",
      "‖alpha‖₁       : 0.655044315438789\n",
      "scores min/max : -18.04134610677281 1.8364224479500617\n",
      "Mask mean value:  tensor(0.2736, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2839  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.262e-03\n",
      "‖w_svm‖₂       : 0.00016352064077389014\n",
      "‖alpha‖₁       : 0.6399999999999273\n",
      "scores min/max : -0.00012737512538395687 -0.00011717287988343534\n",
      "Mask mean value:  tensor(0.4994, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7635  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.302e-17\n",
      "‖w_svm‖₂       : 0.020873540803496085\n",
      "‖alpha‖₁       : 0.8233719193103312\n",
      "scores min/max : -1.9011989805159464 1.4964110027018556\n",
      "Mask mean value:  tensor(0.6951, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.3735  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.005e-06\n",
      "‖w_svm‖₂       : 5.6221636367317216e-08\n",
      "‖alpha‖₁       : 0.23999999999999957\n",
      "scores min/max : -5.86742302083408e-08 -4.4392735020111035e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.454e-20\n",
      "‖w_svm‖₂       : 3.7422590642713363e-07\n",
      "‖alpha‖₁       : 0.2799999999999955\n",
      "scores min/max : -1.4426394885516421e-06 -1.380874208838678e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.692e-19\n",
      "‖w_svm‖₂       : 1.1278026830775476e-07\n",
      "‖alpha‖₁       : 0.29999999999999316\n",
      "scores min/max : 1.3468447117695726e-07 1.4367500546386162e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.305e-07\n",
      "‖w_svm‖₂       : 1.655718274449681e-07\n",
      "‖alpha‖₁       : 0.2399999999999916\n",
      "scores min/max : 2.888846521035291e-07 3.0423922456952735e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.403e-20\n",
      "‖w_svm‖₂       : 0.08593845241559456\n",
      "‖alpha‖₁       : 0.5780253526609787\n",
      "scores min/max : -2.1316430866313807 5.794474848201553\n",
      "Mask mean value:  tensor(0.2550, dtype=torch.float64)\n",
      "max feasible return = 0.1075  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.886820462828177e-07\n",
      "‖alpha‖₁       : 0.5800000000000001\n",
      "scores min/max : -3.996759636258505e-07 -3.089640646381197e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0135081510139033e-07\n",
      "‖alpha‖₁       : 0.29999999999999905\n",
      "scores min/max : 4.540200011600508e-08 5.038412992735955e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 6.847865048817994e-08\n",
      "‖alpha‖₁       : 0.599999999999964\n",
      "scores min/max : 3.2044470444799424e-08 6.882441536911948e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.5560904508474304e-08\n",
      "‖alpha‖₁       : 0.3799999999999987\n",
      "scores min/max : 3.123365931200528e-09 1.2510178832053321e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.920968282191932e-06\n",
      "‖alpha‖₁       : 0.3199999999740158\n",
      "scores min/max : 3.5678361965123816e-06 5.365046475115067e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4258  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.05175258707145187\n",
      "‖alpha‖₁       : 0.7397197705110661\n",
      "scores min/max : -3.47946927127981 2.154677259788667\n",
      "Mask mean value:  tensor(0.8815, dtype=torch.float64)\n",
      "max feasible return = -0.9286  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.029450771044718944\n",
      "‖alpha‖₁       : 0.6151649111300095\n",
      "scores min/max : -1.9206492198163727 1.2096052952332024\n",
      "Mask mean value:  tensor(0.7772, dtype=torch.float64)\n",
      "max feasible return = 2.7185  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.5948428738798807e-07\n",
      "‖alpha‖₁       : 0.4599999999999915\n",
      "scores min/max : 2.489155533698958e-07 8.434556672213512e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.3521497342110483e-07\n",
      "‖alpha‖₁       : 0.5199999999999718\n",
      "scores min/max : 7.319646459920707e-08 8.612295948243653e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.5258996019925473e-07\n",
      "‖alpha‖₁       : 0.5799999999999814\n",
      "scores min/max : -2.7130013024350577e-07 -2.3497470584920115e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.04466311176267671\n",
      "‖alpha‖₁       : 0.6877953362683114\n",
      "scores min/max : -0.44028335438112504 1.948917792602451\n",
      "Mask mean value:  tensor(0.3743, dtype=torch.float64)\n",
      "max feasible return = 0.0655  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03248628063211354\n",
      "‖alpha‖₁       : 0.42220066862611194\n",
      "scores min/max : -3.8969072375273024 5.2486885493563005\n",
      "Mask mean value:  tensor(0.0872, dtype=torch.float64)\n",
      "max feasible return = 0.5499  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04375222130481217\n",
      "‖alpha‖₁       : 0.8211205523054534\n",
      "scores min/max : -5.062359419094809 3.105460714719089\n",
      "Mask mean value:  tensor(0.9499, dtype=torch.float64)\n",
      "max feasible return = -3.6095  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.007508753299686548\n",
      "‖alpha‖₁       : 0.7999999999999549\n",
      "scores min/max : -0.03466816199945505 0.003122366167450967\n",
      "Mask mean value:  tensor(0.4682, dtype=torch.float64)\n",
      "max feasible return = -0.1701  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.0259301779882395e-07\n",
      "‖alpha‖₁       : 0.6199999999999555\n",
      "scores min/max : 7.839595668975514e-08 9.494643598101611e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.6991075852575816e-08\n",
      "‖alpha‖₁       : 0.43999999999999984\n",
      "scores min/max : 7.355733589985726e-09 1.692592226601818e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.2630489139693937e-07\n",
      "‖alpha‖₁       : 0.27999999999997666\n",
      "scores min/max : -3.2155994301514324e-07 -3.03648042635191e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  32 | train 0.005330 | val 0.006708\n",
      "-----------------------------------------Epoch:  33 ----------------------------------------\n",
      "‖w_svm‖₂       : 0.004409555295781331\n",
      "‖alpha‖₁       : 0.4599999999999511\n",
      "scores min/max : -0.008229379891572589 -0.006122341913784982\n",
      "Mask mean value:  tensor(0.4670, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.0293  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.066e-15\n",
      "‖w_svm‖₂       : 0.13890435341561844\n",
      "‖alpha‖₁       : 0.6551177356954079\n",
      "scores min/max : -18.053191760975892 1.8251452702971598\n",
      "Mask mean value:  tensor(0.2622, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2835  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.096e-03\n",
      "‖w_svm‖₂       : 1.6560190752444123e-07\n",
      "‖alpha‖₁       : 0.2399999999999914\n",
      "scores min/max : 2.876910730431057e-07 3.030455424754625e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.403e-20\n",
      "‖w_svm‖₂       : 0.042781998123260744\n",
      "‖alpha‖₁       : 0.7294194693290179\n",
      "scores min/max : -0.25602904084009537 2.0252354269848056\n",
      "Mask mean value:  tensor(0.6935, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.1311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.448e-03\n",
      "‖w_svm‖₂       : 0.005688349822634888\n",
      "‖alpha‖₁       : 0.5599999999999964\n",
      "scores min/max : 0.005205642668170252 0.0068861637793277185\n",
      "Mask mean value:  tensor(0.5275, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1058  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 2.518e-05\n",
      "‖w_svm‖₂       : 1.0269744012917046e-07\n",
      "‖alpha‖₁       : 0.2399999999999964\n",
      "scores min/max : 3.771575698601163e-08 4.850471280518564e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.221e-21\n",
      "‖w_svm‖₂       : 7.599269304794292e-08\n",
      "‖alpha‖₁       : 0.6599999999999944\n",
      "scores min/max : 1.1428472552744861e-07 2.5902066068511504e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.018e-09\n",
      "‖w_svm‖₂       : 0.043473688422174105\n",
      "‖alpha‖₁       : 0.6599999999999999\n",
      "scores min/max : -0.24925444213570014 0.17901601790404387\n",
      "Mask mean value:  tensor(0.4803, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8829  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.486e-13\n",
      "‖w_svm‖₂       : 0.016142832278654207\n",
      "‖alpha‖₁       : 0.8599999999999973\n",
      "scores min/max : -0.05234865154561606 0.0055839353683607845\n",
      "Mask mean value:  tensor(0.3602, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4583  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.209e-05\n",
      "‖w_svm‖₂       : 0.0001447675108011045\n",
      "‖alpha‖₁       : 0.43999999987028726\n",
      "scores min/max : -0.00028377286372459625 -0.0001355516674556447\n",
      "Mask mean value:  tensor(0.4991, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0213  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.907e-17\n",
      "‖w_svm‖₂       : 0.0206806349151943\n",
      "‖alpha‖₁       : 0.78\n",
      "scores min/max : -0.02999177269949599 0.04195558789193948\n",
      "Mask mean value:  tensor(0.3830, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6893  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.145e-15\n",
      "‖w_svm‖₂       : 0.0008814832551305563\n",
      "‖alpha‖₁       : 0.819999999999995\n",
      "scores min/max : 0.0011080140926854158 0.002822851647583267\n",
      "Mask mean value:  tensor(0.5109, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5956  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.049e-16\n",
      "‖w_svm‖₂       : 8.19335505048323e-08\n",
      "‖alpha‖₁       : 0.37999999999999934\n",
      "scores min/max : -8.582120958594547e-10 1.7300206402828523e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.680e-22\n",
      "‖w_svm‖₂       : 0.04242275342886855\n",
      "‖alpha‖₁       : 0.9405742942480712\n",
      "scores min/max : -1.83644431503145 0.2973778488271982\n",
      "Mask mean value:  tensor(0.3924, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3233  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.453e-10\n",
      "‖w_svm‖₂       : 0.07226742581198675\n",
      "‖alpha‖₁       : 0.4191402994220725\n",
      "scores min/max : -1.9065961202632864 2.2267177768489548\n",
      "Mask mean value:  tensor(0.7658, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8953  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.489e-04\n",
      "‖w_svm‖₂       : 0.00016341500565049999\n",
      "‖alpha‖₁       : 0.6399999999999574\n",
      "scores min/max : -0.00012730016077097228 -0.00011711103815231414\n",
      "Mask mean value:  tensor(0.4994, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7635  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.304e-17\n",
      "‖w_svm‖₂       : 0.0005084205639591646\n",
      "‖alpha‖₁       : 0.43999999999996153\n",
      "scores min/max : -0.0008401477014019989 -0.0005769720168515412\n",
      "Mask mean value:  tensor(0.4960, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6421  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.229e-16\n",
      "‖w_svm‖₂       : 0.005550316422121746\n",
      "‖alpha‖₁       : 0.37999999999999623\n",
      "scores min/max : -0.006247090419635656 0.005906752335046145\n",
      "Mask mean value:  tensor(0.5140, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9142  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.955e-14\n",
      "‖w_svm‖₂       : 7.132003004367401e-08\n",
      "‖alpha‖₁       : 0.13999999999999763\n",
      "scores min/max : 1.1795451457773718e-08 1.7555500512958648e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.112e-08\n",
      "‖w_svm‖₂       : 0.13959816398943703\n",
      "‖alpha‖₁       : 0.8799999999999675\n",
      "scores min/max : -1.3908242285634986 3.6902670359137493\n",
      "Mask mean value:  tensor(0.3062, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2590  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.190e-04\n",
      "‖w_svm‖₂       : 0.0003687382805482525\n",
      "‖alpha‖₁       : 0.7399999999999991\n",
      "scores min/max : -0.00033028619866602173 -0.00031294396958250526\n",
      "Mask mean value:  tensor(0.4984, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0609  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.271e-17\n",
      "‖w_svm‖₂       : 2.3386194193588888e-07\n",
      "‖alpha‖₁       : 0.58\n",
      "scores min/max : -3.634095909167656e-07 -3.4779514764717187e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.990e-19\n",
      "‖w_svm‖₂       : 0.08206387611583438\n",
      "‖alpha‖₁       : 0.47194071068094173\n",
      "scores min/max : -2.294264625555207 2.7871987914241685\n",
      "Mask mean value:  tensor(0.1136, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.2349  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.468e-02\n",
      "‖w_svm‖₂       : 0.1461253443066455\n",
      "‖alpha‖₁       : 0.7593154506407274\n",
      "scores min/max : -1.7606784376612976 2.216913942452244\n",
      "Mask mean value:  tensor(0.8501, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3551  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.366e-11\n",
      "‖w_svm‖₂       : 0.017698427442656813\n",
      "‖alpha‖₁       : 0.6815703687623358\n",
      "scores min/max : -1.973532504536438 0.05640645696408675\n",
      "Mask mean value:  tensor(0.6068, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2539  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.326e-13\n",
      "‖w_svm‖₂       : 0.10303654402242851\n",
      "‖alpha‖₁       : 0.8732886454539142\n",
      "scores min/max : -12.17376663243412 2.1582747050022606\n",
      "Mask mean value:  tensor(0.5464, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.6181  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.181e-02\n",
      "‖w_svm‖₂       : 0.00021571928454659246\n",
      "‖alpha‖₁       : 0.6199999999956363\n",
      "scores min/max : 3.098618019842215e-05 4.747481134575664e-05\n",
      "Mask mean value:  tensor(0.5002, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.468e-17\n",
      "‖w_svm‖₂       : 0.03050429438580474\n",
      "‖alpha‖₁       : 0.8985767810580458\n",
      "scores min/max : -0.7425513994314524 2.017695780655648\n",
      "Mask mean value:  tensor(0.8525, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3418  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.017e-15\n",
      "‖w_svm‖₂       : 0.060436748206874696\n",
      "‖alpha‖₁       : 0.8986106829149539\n",
      "scores min/max : -1.6933349386280179 3.718965563903561\n",
      "Mask mean value:  tensor(0.4433, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5904  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.796e-03\n",
      "‖w_svm‖₂       : 0.02078593035757273\n",
      "‖alpha‖₁       : 0.6599999999999994\n",
      "scores min/max : -0.04517908649895258 0.02872465848974419\n",
      "Mask mean value:  tensor(0.4068, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.2097  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.493e-15\n",
      "‖w_svm‖₂       : 0.0717981176567347\n",
      "‖alpha‖₁       : 0.579999999999948\n",
      "scores min/max : -2.813999671594103 1.6247563295593834\n",
      "Mask mean value:  tensor(0.1991, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3531  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.587e-04\n",
      "‖w_svm‖₂       : 0.016551858109659146\n",
      "‖alpha‖₁       : 0.8599999999999924\n",
      "scores min/max : -0.04785398744247263 -0.03328126252345502\n",
      "Mask mean value:  tensor(0.3225, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0796  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.380e-14\n",
      "‖w_svm‖₂       : 0.18354970138452356\n",
      "‖alpha‖₁       : 0.8831305463054129\n",
      "scores min/max : -3.624892308400545 6.075255768539613\n",
      "Mask mean value:  tensor(0.0924, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1992  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.357e-03\n",
      "‖w_svm‖₂       : 2.3211393891480058e-07\n",
      "‖alpha‖₁       : 0.6399999999999444\n",
      "scores min/max : 4.07395557726598e-07 4.285104678071763e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.898e-20\n",
      "‖w_svm‖₂       : 3.748676889813139e-07\n",
      "‖alpha‖₁       : 0.2799999999999958\n",
      "scores min/max : -1.5008218976606427e-06 -1.4390610229887396e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1957  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.687e-19\n",
      "‖w_svm‖₂       : 0.016164784312342678\n",
      "‖alpha‖₁       : 0.5962802073343689\n",
      "scores min/max : -2.9619208142798694 2.276129769269207\n",
      "Mask mean value:  tensor(0.9175, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.5087  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.372e-06\n",
      "‖w_svm‖₂       : 0.0031986896266177744\n",
      "‖alpha‖₁       : 0.5799999999999902\n",
      "scores min/max : 0.0022564580151867054 0.003551662169816769\n",
      "Mask mean value:  tensor(0.5164, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3284  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.750e-13\n",
      "‖w_svm‖₂       : 0.008113991627707698\n",
      "‖alpha‖₁       : 0.6076881838504546\n",
      "scores min/max : -1.9896063093127827 0.297198387189648\n",
      "Mask mean value:  tensor(0.5591, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2258  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.137e-15\n",
      "‖w_svm‖₂       : 0.028843761566135797\n",
      "‖alpha‖₁       : 0.5511092226512122\n",
      "scores min/max : -3.501540297776625 1.0686524962314106\n",
      "Mask mean value:  tensor(0.1355, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0994  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.139e-03\n",
      "‖w_svm‖₂       : 2.081495788830071e-07\n",
      "‖alpha‖₁       : 0.3799999999951666\n",
      "scores min/max : -9.07743453910824e-08 -7.199623724561766e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.382e-06\n",
      "‖w_svm‖₂       : 8.354212138157664e-08\n",
      "‖alpha‖₁       : 0.17999999999999394\n",
      "scores min/max : -1.7210709682947464e-07 6.281605691533728e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.121e-21\n",
      "‖w_svm‖₂       : 7.168489556708922e-08\n",
      "‖alpha‖₁       : 0.419999999999997\n",
      "scores min/max : -7.340261824942585e-08 -2.624130199051081e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.916e-08\n",
      "‖w_svm‖₂       : 2.755868484825426e-07\n",
      "‖alpha‖₁       : 0.6599999999999854\n",
      "scores min/max : -6.981234348779657e-07 -5.463682335135495e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.369e-18\n",
      "‖w_svm‖₂       : 4.3806426540075055e-06\n",
      "‖alpha‖₁       : 0.41999999998540205\n",
      "scores min/max : -3.049711275081528e-06 -1.1136345699993025e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1229  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.915e-05\n",
      "‖w_svm‖₂       : 5.3208195213274176e-08\n",
      "‖alpha‖₁       : 0.43999999999998796\n",
      "scores min/max : -1.850214665916185e-07 -8.103477948807086e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.469e-20\n",
      "‖w_svm‖₂       : 1.153892171765344e-06\n",
      "‖alpha‖₁       : 0.3199999999985725\n",
      "scores min/max : -2.535984596636859e-06 -2.4202905953979117e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4897  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.939e-18\n",
      "‖w_svm‖₂       : 5.669514136615546e-08\n",
      "‖alpha‖₁       : 0.23999999999996813\n",
      "scores min/max : -6.216678721900031e-08 -4.755482193665752e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.460e-20\n",
      "‖w_svm‖₂       : 0.019464936082311216\n",
      "‖alpha‖₁       : 0.3834085589841626\n",
      "scores min/max : -1.9606919797058662 0.24181356698965528\n",
      "Mask mean value:  tensor(0.7115, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1524  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.014e-14\n",
      "‖w_svm‖₂       : 0.051537392054740706\n",
      "‖alpha‖₁       : 0.8271379134943322\n",
      "scores min/max : -1.957775951884618 0.3928835378225591\n",
      "Mask mean value:  tensor(0.4963, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0076  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.830e-02\n",
      "‖w_svm‖₂       : 0.00018887220706780963\n",
      "‖alpha‖₁       : 0.8199999999999983\n",
      "scores min/max : -8.762353559982165e-05 -7.286414048309397e-05\n",
      "Mask mean value:  tensor(0.4996, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6966  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.303e-17\n",
      "‖w_svm‖₂       : 2.1096588124090196e-08\n",
      "‖alpha‖₁       : 0.11999999999999512\n",
      "scores min/max : -3.2572983023236074e-08 -1.7926683199116482e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.814e-09\n",
      "‖w_svm‖₂       : 2.2320563907545246e-07\n",
      "‖alpha‖₁       : 0.25999999999999673\n",
      "scores min/max : 1.1545933019200618e-07 1.2822668391279462e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.731e-17\n",
      "‖w_svm‖₂       : 1.0593604888234698e-06\n",
      "‖alpha‖₁       : 0.4999999999999999\n",
      "scores min/max : -2.529446166858106e-07 -6.022987733231954e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.814e-19\n",
      "‖w_svm‖₂       : 0.02253165984459952\n",
      "‖alpha‖₁       : 0.8150948972487598\n",
      "scores min/max : -11.518502524682534 2.031030598181523\n",
      "Mask mean value:  tensor(0.7079, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9067  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.889e-06\n",
      "‖w_svm‖₂       : 0.04019789003496759\n",
      "‖alpha‖₁       : 0.9199999999999918\n",
      "scores min/max : -0.301178278584335 0.2932599432661327\n",
      "Mask mean value:  tensor(0.2037, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0074  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 9.568e-03\n",
      "‖w_svm‖₂       : 0.0003094765062934905\n",
      "‖alpha‖₁       : 0.4199999999640025\n",
      "scores min/max : 0.00018807231607561778 0.0005121951014293849\n",
      "Mask mean value:  tensor(0.5024, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0003  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.953e-15\n",
      "‖w_svm‖₂       : 4.2120689470661194e-07\n",
      "‖alpha‖₁       : 0.7199999999999988\n",
      "scores min/max : 2.426509332517603e-07 2.9983293892381344e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.036e-21\n",
      "‖w_svm‖₂       : 0.020838899419066203\n",
      "‖alpha‖₁       : 0.8233741433909839\n",
      "scores min/max : -1.9017610092895485 1.4957201865908998\n",
      "Mask mean value:  tensor(0.6935, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.3696  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.339e-06\n",
      "‖w_svm‖₂       : 0.00617443605376869\n",
      "‖alpha‖₁       : 0.7005702227022896\n",
      "scores min/max : -2.0032521176947014 0.02016044524618059\n",
      "Mask mean value:  tensor(0.4585, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2568  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.002e-06\n",
      "‖w_svm‖₂       : 1.1309371866699946e-07\n",
      "‖alpha‖₁       : 0.29999999999999327\n",
      "scores min/max : 1.41867656791423e-07 1.508574170085982e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.321e-07\n",
      "‖w_svm‖₂       : 0.04212665787740481\n",
      "‖alpha‖₁       : 0.8899825992214067\n",
      "scores min/max : -2.009307786329072 0.3385387691988697\n",
      "Mask mean value:  tensor(0.2640, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0669  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.720e-11\n",
      "‖w_svm‖₂       : 5.319644409213498e-08\n",
      "‖alpha‖₁       : 0.11999999999998902\n",
      "scores min/max : -9.602048122608395e-08 -6.519100961126667e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.314e-08\n",
      "‖w_svm‖₂       : 6.661666004697314e-07\n",
      "‖alpha‖₁       : 0.3999999999999999\n",
      "scores min/max : -3.4983147660250794e-08 2.7105661923516357e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.276e-09\n",
      "‖w_svm‖₂       : 1.9568138313637345e-07\n",
      "‖alpha‖₁       : 0.419999999999973\n",
      "scores min/max : -4.577113109581737e-07 -4.354638269683706e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.773e-19\n",
      "‖w_svm‖₂       : 0.027016424756845774\n",
      "‖alpha‖₁       : 0.19666893959589796\n",
      "scores min/max : -2.2244672979462523 -0.0017085536465068357\n",
      "Mask mean value:  tensor(0.0419, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2860  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.362e-03\n",
      "‖w_svm‖₂       : 0.02837605139789677\n",
      "‖alpha‖₁       : 0.8523570780913089\n",
      "scores min/max : -2.9062105500225357 1.5747081660695978\n",
      "Mask mean value:  tensor(0.1602, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9563  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.443e-03\n",
      "‖w_svm‖₂       : 1.000364361747587e-06\n",
      "‖alpha‖₁       : 0.5999999999999138\n",
      "scores min/max : -1.2287745021947911e-07 -7.175422600154411e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.562e-19\n",
      "‖w_svm‖₂       : 0.047571844759400184\n",
      "‖alpha‖₁       : 0.905646233615671\n",
      "scores min/max : -0.7211705620085818 1.8930280574105187\n",
      "Mask mean value:  tensor(0.3834, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7114  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.281e-04\n",
      "‖w_svm‖₂       : 0.055777792317149195\n",
      "‖alpha‖₁       : 0.576318858973982\n",
      "scores min/max : -1.954251999561969 0.8743123776298501\n",
      "Mask mean value:  tensor(0.7223, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9655  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.141e-03\n",
      "‖w_svm‖₂       : 1.9119675622148975e-07\n",
      "‖alpha‖₁       : 0.37999999999999967\n",
      "scores min/max : -4.595729748712859e-08 -1.028435390058806e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.563e-19\n",
      "‖w_svm‖₂       : 0.04632678262822527\n",
      "‖alpha‖₁       : 0.9389495454076549\n",
      "scores min/max : -2.4649925064481573 1.582613933222681\n",
      "Mask mean value:  tensor(0.1437, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.303e-03\n",
      "‖w_svm‖₂       : 1.7456294483408772e-05\n",
      "‖alpha‖₁       : 0.3599999999828637\n",
      "scores min/max : 1.4719311935285388e-05 1.582187782872738e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.656e-17\n",
      "‖w_svm‖₂       : 0.07244827886130249\n",
      "‖alpha‖₁       : 0.658469715587188\n",
      "scores min/max : -2.0443885002901996 4.247532431101167\n",
      "Mask mean value:  tensor(0.3283, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.8042  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.101e-04\n",
      "‖w_svm‖₂       : 1.1110830548403086e-07\n",
      "‖alpha‖₁       : 0.5199999999999851\n",
      "scores min/max : 3.2739216616198694e-07 3.4876447849718695e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.792e-19\n",
      "‖w_svm‖₂       : 4.7662804628534325e-08\n",
      "‖alpha‖₁       : 0.17999999999999794\n",
      "scores min/max : 4.442601156771575e-08 5.910662592101563e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.237e-08\n",
      "‖w_svm‖₂       : 7.53435171043596e-08\n",
      "‖alpha‖₁       : 0.5399999999999395\n",
      "scores min/max : -1.6200565322735303e-07 -1.4037752540815051e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.745e-20\n",
      "‖w_svm‖₂       : 0.08581208684077166\n",
      "‖alpha‖₁       : 0.5780142151298086\n",
      "scores min/max : -2.135788568281859 5.78964927292483\n",
      "Mask mean value:  tensor(0.2480, dtype=torch.float64)\n",
      "max feasible return = 0.1063  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.920554562051298e-07\n",
      "‖alpha‖₁       : 0.5799999999999895\n",
      "scores min/max : -3.830191223203235e-07 -2.9204465754078884e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.016331496775174e-07\n",
      "‖alpha‖₁       : 0.299999999999999\n",
      "scores min/max : 4.014187250981075e-08 4.512403442748982e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 6.868225301594043e-08\n",
      "‖alpha‖₁       : 0.5999999999999649\n",
      "scores min/max : 2.6029305448428363e-08 6.280628139165043e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.5723562897308825e-08\n",
      "‖alpha‖₁       : 0.37999999999999834\n",
      "scores min/max : -2.3123917571539007e-09 7.0768428860158655e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.9536424795476767e-06\n",
      "‖alpha‖₁       : 0.3199999999722841\n",
      "scores min/max : 3.4350318439670437e-06 5.241872420581449e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4258  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.051259094883445246\n",
      "‖alpha‖₁       : 0.7397201774310675\n",
      "scores min/max : -3.4756836363450256 2.1582231940219927\n",
      "Mask mean value:  tensor(0.8846, dtype=torch.float64)\n",
      "max feasible return = -0.9308  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.029113868513895254\n",
      "‖alpha‖₁       : 0.6151654215182756\n",
      "scores min/max : -1.922573629161652 1.2074822608835405\n",
      "Mask mean value:  tensor(0.7728, dtype=torch.float64)\n",
      "max feasible return = 2.7022  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.609487458115673e-07\n",
      "‖alpha‖₁       : 0.45999999999999147\n",
      "scores min/max : 2.2580948717769965e-07 8.203708369101152e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.354623660609381e-07\n",
      "‖alpha‖₁       : 0.5199999999999724\n",
      "scores min/max : 7.855476754473632e-08 9.148110227402932e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.5315910501784016e-07\n",
      "‖alpha‖₁       : 0.5799999999999813\n",
      "scores min/max : -2.5990723789109744e-07 -2.2357947135419152e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.04440035503173249\n",
      "‖alpha‖₁       : 0.6877917221166361\n",
      "scores min/max : -0.44070219070349037 1.9483027206817318\n",
      "Mask mean value:  tensor(0.3723, dtype=torch.float64)\n",
      "max feasible return = 0.0664  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03248403092364929\n",
      "‖alpha‖₁       : 0.42220935457139386\n",
      "scores min/max : -3.8951421654821057 5.251953913253891\n",
      "Mask mean value:  tensor(0.0884, dtype=torch.float64)\n",
      "max feasible return = 0.5571  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.043590023242184466\n",
      "‖alpha‖₁       : 0.8211177619736822\n",
      "scores min/max : -5.05260303496482 3.1154921475076796\n",
      "Mask mean value:  tensor(0.9519, dtype=torch.float64)\n",
      "max feasible return = -3.6209  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.007409800434837814\n",
      "‖alpha‖₁       : 0.7999999999999443\n",
      "scores min/max : -0.03406186320324864 0.002793635092210715\n",
      "Mask mean value:  tensor(0.4677, dtype=torch.float64)\n",
      "max feasible return = -0.1700  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.036222024977713e-07\n",
      "‖alpha‖₁       : 0.6199999999999553\n",
      "scores min/max : 7.60879815483826e-08 9.263845721529071e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.705131854894142e-08\n",
      "‖alpha‖₁       : 0.43999999999999995\n",
      "scores min/max : 7.187852314391092e-09 1.6757769895114593e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.2505198504227364e-07\n",
      "‖alpha‖₁       : 0.2799999999999934\n",
      "scores min/max : -2.9607441130408297e-07 -2.783174896682414e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  33 | train 0.005330 | val 0.006751\n",
      "-----------------------------------------Epoch:  34 ----------------------------------------\n",
      "‖w_svm‖₂       : 1.157373232652922e-06\n",
      "‖alpha‖₁       : 0.31999999999847767\n",
      "scores min/max : -2.49098286540717e-06 -2.374926808511971e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4897  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.964e-18\n",
      "‖w_svm‖₂       : 0.030523414764465073\n",
      "‖alpha‖₁       : 0.8985855366145095\n",
      "scores min/max : -0.7401302471055707 2.0200998908912697\n",
      "Mask mean value:  tensor(0.8558, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3358  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.775e-15\n",
      "‖w_svm‖₂       : 3.75344750705507e-07\n",
      "‖alpha‖₁       : 0.2799999999999943\n",
      "scores min/max : -1.3781206106356364e-06 -1.3163459090252191e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.749e-19\n",
      "‖w_svm‖₂       : 5.676993531658458e-08\n",
      "‖alpha‖₁       : 0.2399999999999592\n",
      "scores min/max : -6.037091055287389e-08 -4.569861427977609e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.477e-20\n",
      "‖w_svm‖₂       : 0.0003100286413472311\n",
      "‖alpha‖₁       : 0.4199999999632717\n",
      "scores min/max : 0.00017020014869088644 0.0004954761350378775\n",
      "Mask mean value:  tensor(0.5023, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0002  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.977e-15\n",
      "‖w_svm‖₂       : 7.571431928659475e-08\n",
      "‖alpha‖₁       : 0.6599999999999957\n",
      "scores min/max : 1.0985681691609731e-07 2.5472164265299175e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.070e-09\n",
      "‖w_svm‖₂       : 1.9127222719068726e-07\n",
      "‖alpha‖₁       : 0.3799999999999996\n",
      "scores min/max : -4.307022241118383e-08 -7.396570445200105e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.569e-19\n",
      "‖w_svm‖₂       : 0.0284104043919897\n",
      "‖alpha‖₁       : 0.852358329432862\n",
      "scores min/max : -2.903840941025321 1.5770903834690766\n",
      "Mask mean value:  tensor(0.1614, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9574  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.386e-03\n",
      "‖w_svm‖₂       : 0.04005014176712661\n",
      "‖alpha‖₁       : 0.9199999999999909\n",
      "scores min/max : -0.30146463447391936 0.2881821296743974\n",
      "Mask mean value:  tensor(0.1984, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9811  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 9.919e-03\n",
      "‖w_svm‖₂       : 0.00014454916783155828\n",
      "‖alpha‖₁       : 0.43999999983105503\n",
      "scores min/max : -0.0002879919575880293 -0.0001403475832974937\n",
      "Mask mean value:  tensor(0.4990, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0211  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.918e-17\n",
      "‖w_svm‖₂       : 0.00021535527195111584\n",
      "‖alpha‖₁       : 0.6199999999949254\n",
      "scores min/max : 1.857507906480576e-05 3.500808286145482e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.541e-17\n",
      "‖w_svm‖₂       : 0.042226754969480626\n",
      "‖alpha‖₁       : 0.8899923066511262\n",
      "scores min/max : -2.009291820109874 0.3385705267366217\n",
      "Mask mean value:  tensor(0.2641, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0669  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.253e-11\n",
      "‖w_svm‖₂       : 0.0005098640442779284\n",
      "‖alpha‖₁       : 0.43999999999996486\n",
      "scores min/max : -0.0008407342282101607 -0.0005761831635365628\n",
      "Mask mean value:  tensor(0.4960, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6421  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.254e-16\n",
      "‖w_svm‖₂       : 7.125846363391457e-08\n",
      "‖alpha‖₁       : 0.13999999999999752\n",
      "scores min/max : 1.2312893126954987e-08 1.8060881945653374e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.145e-08\n",
      "‖w_svm‖₂       : 0.02708037663211863\n",
      "‖alpha‖₁       : 0.19667284755016975\n",
      "scores min/max : -2.227965662954397 -0.0052416378854985915\n",
      "Mask mean value:  tensor(0.0395, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2704  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.597e-03\n",
      "‖w_svm‖₂       : 0.1388321134709569\n",
      "‖alpha‖₁       : 0.6551299085058523\n",
      "scores min/max : -18.069124635678268 1.8135557435633642\n",
      "Mask mean value:  tensor(0.2512, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2815  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.044e-03\n",
      "‖w_svm‖₂       : 8.38024610792079e-08\n",
      "‖alpha‖₁       : 0.1799999999999941\n",
      "scores min/max : -1.768636019797491e-07 5.810458774968937e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.247e-21\n",
      "‖w_svm‖₂       : 0.04217530438431102\n",
      "‖alpha‖₁       : 0.7294201680455064\n",
      "scores min/max : -0.2570113008747846 2.0241428576300153\n",
      "Mask mean value:  tensor(0.6903, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.1187  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.485e-03\n",
      "‖w_svm‖₂       : 0.005704091297620784\n",
      "‖alpha‖₁       : 0.5599999999999927\n",
      "scores min/max : 0.005288341603499944 0.006977884309012231\n",
      "Mask mean value:  tensor(0.5279, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1067  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 2.454e-05\n",
      "‖w_svm‖₂       : 0.020736825371240244\n",
      "‖alpha‖₁       : 0.7799999999999999\n",
      "scores min/max : -0.03166388653576584 0.040664267268415155\n",
      "Mask mean value:  tensor(0.3753, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6574  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.506e-15\n",
      "‖w_svm‖₂       : 4.032219311605163e-06\n",
      "‖alpha‖₁       : 0.4199999999894633\n",
      "scores min/max : -3.0346820059351165e-06 -1.2324468368726213e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1229  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.768e-05\n",
      "‖w_svm‖₂       : 0.006191968528570593\n",
      "‖alpha‖₁       : 0.700570454297687\n",
      "scores min/max : -2.0029655276505958 0.020448628734962154\n",
      "Mask mean value:  tensor(0.4599, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2576  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.183e-10\n",
      "‖w_svm‖₂       : 1.6573969118201588e-07\n",
      "‖alpha‖₁       : 0.23999999999999136\n",
      "scores min/max : 2.8004020913758295e-07 2.9540196287925057e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.449e-20\n",
      "‖w_svm‖₂       : 1.952191771809756e-07\n",
      "‖alpha‖₁       : 0.41999999999997295\n",
      "scores min/max : -4.650084533080651e-07 -4.427601684578129e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.780e-19\n",
      "‖w_svm‖₂       : 0.02050644156612317\n",
      "‖alpha‖₁       : 0.6599999999999995\n",
      "scores min/max : -0.043703354627520785 0.027925757315415398\n",
      "Mask mean value:  tensor(0.4103, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.2344  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.432e-15\n",
      "‖w_svm‖₂       : 0.04287707729296011\n",
      "‖alpha‖₁       : 0.659999999999954\n",
      "scores min/max : -0.2444577551151813 0.17253733924757164\n",
      "Mask mean value:  tensor(0.4724, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8698  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.748e-13\n",
      "‖w_svm‖₂       : 0.00036867942952646774\n",
      "‖alpha‖₁       : 0.74\n",
      "scores min/max : -0.00035990512920319333 -0.0003425684524905951\n",
      "Mask mean value:  tensor(0.4983, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0606  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.288e-17\n",
      "‖w_svm‖₂       : 0.01956266226304588\n",
      "‖alpha‖₁       : 0.3834133424239325\n",
      "scores min/max : -1.9613300069285646 0.24117562894993066\n",
      "Mask mean value:  tensor(0.7091, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1520  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.175e-14\n",
      "‖w_svm‖₂       : 2.3365485775953576e-07\n",
      "‖alpha‖₁       : 0.5799999999999998\n",
      "scores min/max : -3.357621096880036e-07 -3.201436855393841e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.998e-19\n",
      "‖w_svm‖₂       : 0.046337036869010154\n",
      "‖alpha‖₁       : 0.9389496421246581\n",
      "scores min/max : -2.4649416240950552 1.5826483524136306\n",
      "Mask mean value:  tensor(0.1437, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.304e-03\n",
      "‖w_svm‖₂       : 0.01631516392291279\n",
      "‖alpha‖₁       : 0.5962859283543938\n",
      "scores min/max : -2.9609420182030965 2.277123949205328\n",
      "Mask mean value:  tensor(0.9179, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.5092  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.401e-09\n",
      "‖w_svm‖₂       : 0.1838956308260719\n",
      "‖alpha‖₁       : 0.8833041780680593\n",
      "scores min/max : -3.624301759992133 6.077263448206856\n",
      "Mask mean value:  tensor(0.0928, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2006  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.021e-03\n",
      "‖w_svm‖₂       : 0.00016348099017628878\n",
      "‖alpha‖₁       : 0.6399999999999842\n",
      "scores min/max : -0.00013923571620988651 -0.0001290383472543602\n",
      "Mask mean value:  tensor(0.4993, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7634  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.339e-17\n",
      "‖w_svm‖₂       : 2.1148471305256655e-08\n",
      "‖alpha‖₁       : 0.1199999999999956\n",
      "scores min/max : -3.3356915556197106e-08 -1.873150178307856e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.828e-09\n",
      "‖w_svm‖₂       : 0.01797360603550754\n",
      "‖alpha‖₁       : 0.6815808100962103\n",
      "scores min/max : -1.9737448287086719 0.05625201617474994\n",
      "Mask mean value:  tensor(0.6058, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2533  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.567e-14\n",
      "‖w_svm‖₂       : 0.08193897286484546\n",
      "‖alpha‖₁       : 0.4719317889006598\n",
      "scores min/max : -2.2922184137874226 2.7884958726546656\n",
      "Mask mean value:  tensor(0.1144, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.2430  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.467e-02\n",
      "‖w_svm‖₂       : 7.10348934964067e-08\n",
      "‖alpha‖₁       : 0.41999999999999993\n",
      "scores min/max : -7.090341823973964e-08 -2.3778398157743744e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.888e-08\n",
      "‖w_svm‖₂       : 1.1271762864953913e-07\n",
      "‖alpha‖₁       : 0.2999999999999934\n",
      "scores min/max : 1.4299299906071388e-07 1.519819993132714e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.329e-07\n",
      "‖w_svm‖₂       : 0.13998319391660816\n",
      "‖alpha‖₁       : 0.8800000000000001\n",
      "scores min/max : -1.3927657901989998 3.7198487929242265\n",
      "Mask mean value:  tensor(0.3206, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2623  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.219e-06\n",
      "‖w_svm‖₂       : 1.0336542559442425e-07\n",
      "‖alpha‖₁       : 0.2399999999999919\n",
      "scores min/max : 4.634326086825938e-08 5.7168366472133244e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.281e-21\n",
      "‖w_svm‖₂       : 0.04776486150981375\n",
      "‖alpha‖₁       : 0.9056468390396196\n",
      "scores min/max : -0.7298708891293837 1.8891141083582261\n",
      "Mask mean value:  tensor(0.3724, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.867e-03\n",
      "‖w_svm‖₂       : 5.303508908860199e-08\n",
      "‖alpha‖₁       : 0.1199999999999895\n",
      "scores min/max : -9.667062949920584e-08 -6.584582529143931e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.304e-08\n",
      "‖w_svm‖₂       : 0.10190897578519703\n",
      "‖alpha‖₁       : 0.8732929896434871\n",
      "scores min/max : -12.182794404224229 2.1474281305378122\n",
      "Mask mean value:  tensor(0.5203, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.5427  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 2.993e-02\n",
      "‖w_svm‖₂       : 0.020659471922358418\n",
      "‖alpha‖₁       : 0.8233720994828713\n",
      "scores min/max : -1.90131385660049 1.4960299764244245\n",
      "Mask mean value:  tensor(0.6946, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.3717  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.079e-06\n",
      "‖w_svm‖₂       : 0.003185358371276121\n",
      "‖alpha‖₁       : 0.5799999999999914\n",
      "scores min/max : 0.0021750940443452348 0.0034599562051334867\n",
      "Mask mean value:  tensor(0.5160, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3281  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.934e-13\n",
      "‖w_svm‖₂       : 2.7494121896612517e-07\n",
      "‖alpha‖₁       : 0.6599999999999845\n",
      "scores min/max : -6.934102273448648e-07 -5.416530085427134e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.381e-18\n",
      "‖w_svm‖₂       : 9.979414422313988e-07\n",
      "‖alpha‖₁       : 0.5999999999999112\n",
      "scores min/max : -1.476109179365218e-07 -9.648401975672783e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.547e-19\n",
      "‖w_svm‖₂       : 4.1989162091382937e-07\n",
      "‖alpha‖₁       : 0.7199999999999989\n",
      "scores min/max : 2.4279325024518957e-07 2.9997056699174677e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.021e-21\n",
      "‖w_svm‖₂       : 0.07225619438509032\n",
      "‖alpha‖₁       : 0.65845368003917\n",
      "scores min/max : -2.024534355892916 4.269832688352825\n",
      "Mask mean value:  tensor(0.3640, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.1609  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.507e-04\n",
      "‖w_svm‖₂       : 8.198138480642995e-08\n",
      "‖alpha‖₁       : 0.3799999999999995\n",
      "scores min/max : 6.986677459397849e-11 1.8227962483957956e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.711e-22\n",
      "‖w_svm‖₂       : 0.060275225592234416\n",
      "‖alpha‖₁       : 0.8986045614852027\n",
      "scores min/max : -1.698297635973069 3.714333547408566\n",
      "Mask mean value:  tensor(0.4333, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5830  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.040e-02\n",
      "‖w_svm‖₂       : 0.005564796314068343\n",
      "‖alpha‖₁       : 0.38\n",
      "scores min/max : -0.006338889581555859 0.005706730213931012\n",
      "Mask mean value:  tensor(0.5132, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9128  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.982e-14\n",
      "‖w_svm‖₂       : 0.004418508248105877\n",
      "‖alpha‖₁       : 0.4599999999999999\n",
      "scores min/max : -0.008412269541917152 -0.0062952999541620175\n",
      "Mask mean value:  tensor(0.4661, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.0255  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.071e-15\n",
      "‖w_svm‖₂       : 0.02894771286887088\n",
      "‖alpha‖₁       : 0.5511182569549691\n",
      "scores min/max : -3.4978944387323043 1.0721755622908533\n",
      "Mask mean value:  tensor(0.1373, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1097  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.090e-03\n",
      "‖w_svm‖₂       : 0.015793326649774946\n",
      "‖alpha‖₁       : 0.8599999999999965\n",
      "scores min/max : -0.05231426225608731 0.0032201566257105893\n",
      "Mask mean value:  tensor(0.3559, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4516  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.632e-05\n",
      "‖w_svm‖₂       : 0.0001893558774417467\n",
      "‖alpha‖₁       : 0.819999999999998\n",
      "scores min/max : -9.180875574265994e-05 -7.697420091338863e-05\n",
      "Mask mean value:  tensor(0.4996, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6965  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.320e-17\n",
      "‖w_svm‖₂       : 2.228635448281409e-07\n",
      "‖alpha‖₁       : 0.25999999999999496\n",
      "scores min/max : 1.1397301111548037e-07 1.26756618549328e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.695e-17\n",
      "‖w_svm‖₂       : 0.05545758844211846\n",
      "‖alpha‖₁       : 0.5762982220568955\n",
      "scores min/max : -1.9519127234800198 0.8765642659783209\n",
      "Mask mean value:  tensor(0.7292, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9854  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.180e-03\n",
      "‖w_svm‖₂       : 2.3132355336309969e-07\n",
      "‖alpha‖₁       : 0.6399999999999939\n",
      "scores min/max : 4.1716559684548175e-07 4.381242610523351e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.985e-20\n",
      "‖w_svm‖₂       : 4.7723713529007746e-08\n",
      "‖alpha‖₁       : 0.17999999999999775\n",
      "scores min/max : 4.192792914264073e-08 5.661437757632659e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.216e-08\n",
      "‖w_svm‖₂       : 0.04296158315111851\n",
      "‖alpha‖₁       : 0.9406235876064701\n",
      "scores min/max : -1.8377389735599419 0.2960874735700866\n",
      "Mask mean value:  tensor(0.3875, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3227  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.445e-14\n",
      "‖w_svm‖₂       : 1.0614322977438305e-06\n",
      "‖alpha‖₁       : 0.4999999999999656\n",
      "scores min/max : -2.444102289157017e-07 -5.019271058835966e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.825e-19\n",
      "‖w_svm‖₂       : 6.737831190950453e-07\n",
      "‖alpha‖₁       : 0.3999999999998849\n",
      "scores min/max : -4.0889765071774963e-08 2.680144466747506e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.314e-09\n",
      "‖w_svm‖₂       : 0.02240886544949421\n",
      "‖alpha‖₁       : 0.8150952908556283\n",
      "scores min/max : -11.517024623418585 2.031958856688637\n",
      "Mask mean value:  tensor(0.7099, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9121  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.280e-06\n",
      "‖w_svm‖₂       : 0.008235679421742777\n",
      "‖alpha‖₁       : 0.6076901279625666\n",
      "scores min/max : -1.9903036097600502 0.296491595740169\n",
      "Mask mean value:  tensor(0.5560, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2246  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.529e-15\n",
      "‖w_svm‖₂       : 1.1068063644899959e-07\n",
      "‖alpha‖₁       : 0.5199999999999871\n",
      "scores min/max : 3.3875466480028077e-07 3.6012799906908557e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.802e-19\n",
      "‖w_svm‖₂       : 1.7488807314104615e-05\n",
      "‖alpha‖₁       : 0.35999999998278054\n",
      "scores min/max : 1.4625633671189214e-05 1.573175125850292e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.703e-17\n",
      "‖w_svm‖₂       : 0.07148105516641164\n",
      "‖alpha‖₁       : 0.41911632087902556\n",
      "scores min/max : -1.9091333164986275 2.2061172042537613\n",
      "Mask mean value:  tensor(0.7637, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8957  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.456e-04\n",
      "‖w_svm‖₂       : 7.507832462849919e-08\n",
      "‖alpha‖₁       : 0.5399999999999451\n",
      "scores min/max : -1.6016826179314432e-07 -1.38554248694079e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.726e-20\n",
      "‖w_svm‖₂       : 5.296192274258061e-08\n",
      "‖alpha‖₁       : 0.4399999999999884\n",
      "scores min/max : -1.8674675190069494e-07 -8.278016785274883e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.476e-20\n",
      "‖w_svm‖₂       : 0.07178446543488962\n",
      "‖alpha‖₁       : 0.5799999999999482\n",
      "scores min/max : -2.8128434830442894 1.6226615818933323\n",
      "Mask mean value:  tensor(0.1973, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3508  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.610e-04\n",
      "‖w_svm‖₂       : 2.0789432971510958e-07\n",
      "‖alpha‖₁       : 0.37999999999517264\n",
      "scores min/max : -8.573357713110703e-08 -6.693192174820317e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.487e-07\n",
      "‖w_svm‖₂       : 0.01646222053712342\n",
      "‖alpha‖₁       : 0.8599999999998971\n",
      "scores min/max : -0.046558827709404726 -0.03240351121508074\n",
      "Mask mean value:  tensor(0.3269, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0816  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.338e-14\n",
      "‖w_svm‖₂       : 0.0008860244487628811\n",
      "‖alpha‖₁       : 0.819999999999962\n",
      "scores min/max : 0.0012030057767810553 0.0028888303870049743\n",
      "Mask mean value:  tensor(0.5113, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5976  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.312e-16\n",
      "‖w_svm‖₂       : 0.051549534121555364\n",
      "‖alpha‖₁       : 0.8271486224338428\n",
      "scores min/max : -1.953366954796538 0.39737486821743473\n",
      "Mask mean value:  tensor(0.5124, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0108  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.747e-02\n",
      "‖w_svm‖₂       : 0.14579428157820415\n",
      "‖alpha‖₁       : 0.7592227604583717\n",
      "scores min/max : -1.759556981094336 2.2181175318310946\n",
      "Mask mean value:  tensor(0.8516, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3564  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.692e-11\n",
      "‖w_svm‖₂       : 0.08551801543015268\n",
      "‖alpha‖₁       : 0.5779672342889357\n",
      "scores min/max : -2.124026830462371 5.80101320189394\n",
      "Mask mean value:  tensor(0.2679, dtype=torch.float64)\n",
      "max feasible return = 0.1095  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.893761769978673e-07\n",
      "‖alpha‖₁       : 0.5799999999999998\n",
      "scores min/max : -3.868298963420674e-07 -2.9610148705836227e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0151684976093222e-07\n",
      "‖alpha‖₁       : 0.29999999999999905\n",
      "scores min/max : 4.3943781382710886e-08 4.892450339077492e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 6.832926339666246e-08\n",
      "‖alpha‖₁       : 0.5999999999999432\n",
      "scores min/max : 3.190093299135445e-08 6.866524212791446e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.5498624920225814e-08\n",
      "‖alpha‖₁       : 0.37999999999999856\n",
      "scores min/max : 5.767015519177148e-10 9.964586824526756e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 6.047855360245911e-06\n",
      "‖alpha‖₁       : 0.3199999999392339\n",
      "scores min/max : 7.274788776637212e-06 1.1155124633100434e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64)\n",
      "max feasible return = 2.4260  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.05075069062584989\n",
      "‖alpha‖₁       : 0.7397156979971529\n",
      "scores min/max : -3.4757417029449673 2.1580146596993366\n",
      "Mask mean value:  tensor(0.8846, dtype=torch.float64)\n",
      "max feasible return = -0.9309  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.028817065559222626\n",
      "‖alpha‖₁       : 0.6151646574662877\n",
      "scores min/max : -1.9247780151661311 1.205099555456816\n",
      "Mask mean value:  tensor(0.7676, dtype=torch.float64)\n",
      "max feasible return = 2.6830  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.6056609381420324e-07\n",
      "‖alpha‖₁       : 0.4599999999999901\n",
      "scores min/max : 2.611786690552639e-07 8.558035024657717e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.3561570008526102e-07\n",
      "‖alpha‖₁       : 0.5199999999999728\n",
      "scores min/max : 7.377795346240736e-08 8.670360842091483e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.5328915239708074e-07\n",
      "‖alpha‖₁       : 0.5799999999999779\n",
      "scores min/max : -2.6863116080099206e-07 -2.3228387436394193e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.044122523974034815\n",
      "‖alpha‖₁       : 0.6877853944339365\n",
      "scores min/max : -0.43846568343222575 1.9503709575867332\n",
      "Mask mean value:  tensor(0.3796, dtype=torch.float64)\n",
      "max feasible return = 0.0585  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03237469602922182\n",
      "‖alpha‖₁       : 0.4222118989646336\n",
      "scores min/max : -3.891669279956424 5.256843636235871\n",
      "Mask mean value:  tensor(0.0903, dtype=torch.float64)\n",
      "max feasible return = 0.5696  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04353357671739341\n",
      "‖alpha‖₁       : 0.8211206472890712\n",
      "scores min/max : -5.044981680914315 3.1233078672152064\n",
      "Mask mean value:  tensor(0.9534, dtype=torch.float64)\n",
      "max feasible return = -3.6292  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.00732600938599483\n",
      "‖alpha‖₁       : 0.7999999999999317\n",
      "scores min/max : -0.0336705460944606 0.0023499133352294773\n",
      "Mask mean value:  tensor(0.4666, dtype=torch.float64)\n",
      "max feasible return = -0.1696  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.03225155145901e-07\n",
      "‖alpha‖₁       : 0.6199999999999561\n",
      "scores min/max : 8.844309230829279e-08 1.0499191329296386e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.713375937660001e-08\n",
      "‖alpha‖₁       : 0.43999999999999984\n",
      "scores min/max : 6.948335330530773e-09 1.651833925754695e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.2702982956335534e-07\n",
      "‖alpha‖₁       : 0.27999999999998326\n",
      "scores min/max : -3.1151094075594915e-07 -2.9360933781558994e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  34 | train 0.005327 | val 0.006645\n",
      "-----------------------------------------Epoch:  35 ----------------------------------------\n",
      "‖w_svm‖₂       : 1.9516300762063688e-07\n",
      "‖alpha‖₁       : 0.4199999999999723\n",
      "scores min/max : -4.55837840760084e-07 -4.3359066824153785e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.776e-19\n",
      "‖w_svm‖₂       : 0.04249636994107012\n",
      "‖alpha‖₁       : 0.6599999999999944\n",
      "scores min/max : -0.23861526755819706 0.1705940026298517\n",
      "Mask mean value:  tensor(0.4787, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8843  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.534e-13\n",
      "‖w_svm‖₂       : 1.0566439638924158e-06\n",
      "‖alpha‖₁       : 0.49999999999999983\n",
      "scores min/max : -2.4936471783419586e-07 -5.664427566509536e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.809e-19\n",
      "‖w_svm‖₂       : 0.13965655020573509\n",
      "‖alpha‖₁       : 0.8799999999999594\n",
      "scores min/max : -1.3867110618697156 3.7011877763477936\n",
      "Mask mean value:  tensor(0.3194, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2618  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.172e-06\n",
      "‖w_svm‖₂       : 7.531023947393981e-08\n",
      "‖alpha‖₁       : 0.6599999999999963\n",
      "scores min/max : 1.1170333777933831e-07 2.5662533165303063e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.051e-09\n",
      "‖w_svm‖₂       : 7.158276067714527e-08\n",
      "‖alpha‖₁       : 0.13999999999999588\n",
      "scores min/max : 1.3295273467413505e-08 1.90762055537579e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.231e-08\n",
      "‖w_svm‖₂       : 0.00824438671887269\n",
      "‖alpha‖₁       : 0.6076902684931853\n",
      "scores min/max : -1.9903868675934195 0.29640838945541526\n",
      "Mask mean value:  tensor(0.5556, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2244  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.562e-15\n",
      "‖w_svm‖₂       : 4.769913889321557e-08\n",
      "‖alpha‖₁       : 0.1799999999999978\n",
      "scores min/max : 3.926851845201927e-08 5.393457871389217e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.235e-08\n",
      "‖w_svm‖₂       : 0.004430536587173226\n",
      "‖alpha‖₁       : 0.45999999999999974\n",
      "scores min/max : -0.008440844066862423 -0.006310811552412893\n",
      "Mask mean value:  tensor(0.4660, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.0251  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.073e-15\n",
      "‖w_svm‖₂       : 8.410831316315719e-08\n",
      "‖alpha‖₁       : 0.17999999999999208\n",
      "scores min/max : -1.7809548397136805e-07 5.732124248166808e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.290e-21\n",
      "‖w_svm‖₂       : 0.022416357546733105\n",
      "‖alpha‖₁       : 0.8150958511180955\n",
      "scores min/max : -11.516488947853384 2.0323865912898675\n",
      "Mask mean value:  tensor(0.7108, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9143  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.003e-06\n",
      "‖w_svm‖₂       : 0.006239014342390699\n",
      "‖alpha‖₁       : 0.7005709752100111\n",
      "scores min/max : -2.0026303833944983 0.02078372046865671\n",
      "Mask mean value:  tensor(0.4615, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2584  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.825e-12\n",
      "‖w_svm‖₂       : 0.01809875691718634\n",
      "‖alpha‖₁       : 0.6815841044092011\n",
      "scores min/max : -1.9718945226676134 0.0579583530211058\n",
      "Mask mean value:  tensor(0.6143, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2581  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.005e-15\n",
      "‖w_svm‖₂       : 0.0032034538567362026\n",
      "‖alpha‖₁       : 0.5799999999999895\n",
      "scores min/max : 0.0026568895842533483 0.00395592956425956\n",
      "Mask mean value:  tensor(0.5184, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3296  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.304e-13\n",
      "‖w_svm‖₂       : 0.030441642133919707\n",
      "‖alpha‖₁       : 0.898588419815902\n",
      "scores min/max : -0.7392942566442849 2.0209261666852165\n",
      "Mask mean value:  tensor(0.8570, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3333  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.687e-15\n",
      "‖w_svm‖₂       : 0.019736109749601546\n",
      "‖alpha‖₁       : 0.38341963668991275\n",
      "scores min/max : -1.960348338662726 0.24215511851216637\n",
      "Mask mean value:  tensor(0.7127, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1526  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.128e-14\n",
      "‖w_svm‖₂       : 8.212475784987839e-08\n",
      "‖alpha‖₁       : 0.3799999999999993\n",
      "scores min/max : -4.745878046538057e-09 1.3413810955170536e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.738e-22\n",
      "‖w_svm‖₂       : 0.00018898365923669956\n",
      "‖alpha‖₁       : 0.8199999999999956\n",
      "scores min/max : -9.192364103971918e-05 -7.71480517646894e-05\n",
      "Mask mean value:  tensor(0.4996, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6965  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.324e-17\n",
      "‖w_svm‖₂       : 0.027179791075575624\n",
      "‖alpha‖₁       : 0.19667822941516822\n",
      "scores min/max : -2.228543743937489 -0.005819940310278743\n",
      "Mask mean value:  tensor(0.0391, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2679  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.656e-03\n",
      "‖w_svm‖₂       : 0.04214928441087169\n",
      "‖alpha‖₁       : 0.8899946834079333\n",
      "scores min/max : -2.009856389149883 0.338009030492241\n",
      "Mask mean value:  tensor(0.2624, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0657  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.161e-12\n",
      "‖w_svm‖₂       : 0.005713738214646321\n",
      "‖alpha‖₁       : 0.5599999999999985\n",
      "scores min/max : 0.005217037497074652 0.006913032531991187\n",
      "Mask mean value:  tensor(0.5276, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1060  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 2.515e-05\n",
      "‖w_svm‖₂       : 3.960692933436275e-06\n",
      "‖alpha‖₁       : 0.41999999999006205\n",
      "scores min/max : -3.2339973481745345e-06 -1.4619371880416888e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1229  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.751e-05\n",
      "‖w_svm‖₂       : 0.07279988749598236\n",
      "‖alpha‖₁       : 0.6585319403441278\n",
      "scores min/max : -2.0453418261918777 4.248453411887978\n",
      "Mask mean value:  tensor(0.3267, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.7862  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.589e-04\n",
      "‖w_svm‖₂       : 7.53036023345819e-08\n",
      "‖alpha‖₁       : 0.5399999999999452\n",
      "scores min/max : -1.5779135381919e-07 -1.361805719422639e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.736e-20\n",
      "‖w_svm‖₂       : 0.10092054086809978\n",
      "‖alpha‖₁       : 0.8733008040081573\n",
      "scores min/max : -12.16050876010245 2.1687691481302163\n",
      "Mask mean value:  tensor(0.5703, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.6888  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 7.160e-02\n",
      "‖w_svm‖₂       : 1.9160876168308018e-07\n",
      "‖alpha‖₁       : 0.3799999999999997\n",
      "scores min/max : -5.0415147047206836e-08 -1.4747000892701675e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.589e-19\n",
      "‖w_svm‖₂       : 1.7463179126297537e-05\n",
      "‖alpha‖₁       : 0.35999999998282417\n",
      "scores min/max : 1.4436689036294122e-05 1.5538067045916026e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.710e-17\n",
      "‖w_svm‖₂       : 0.07140158606986383\n",
      "‖alpha‖₁       : 0.58\n",
      "scores min/max : -2.782259215671333 1.6019471685826228\n",
      "Mask mean value:  tensor(0.1946, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3465  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.648e-04\n",
      "‖w_svm‖₂       : 0.015691738372714842\n",
      "‖alpha‖₁       : 0.8599999999999972\n",
      "scores min/max : -0.05158363198539342 0.003255328711867924\n",
      "Mask mean value:  tensor(0.3580, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4537  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.458e-05\n",
      "‖w_svm‖₂       : 0.14656703551318603\n",
      "‖alpha‖₁       : 0.7594499120395134\n",
      "scores min/max : -1.7592800175413943 2.2183075607841216\n",
      "Mask mean value:  tensor(0.8520, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3568  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.978e-12\n",
      "‖w_svm‖₂       : 0.00021447620783155124\n",
      "‖alpha‖₁       : 0.6199999999940622\n",
      "scores min/max : 1.2375344672728634e-05 2.8674372194645204e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.568e-17\n",
      "‖w_svm‖₂       : 0.0001426973895167551\n",
      "‖alpha‖₁       : 0.4399999999019931\n",
      "scores min/max : -0.00028762522422347693 -0.00014245494800950394\n",
      "Mask mean value:  tensor(0.4990, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0211  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.897e-17\n",
      "‖w_svm‖₂       : 0.051779229075981995\n",
      "‖alpha‖₁       : 0.8271835314628361\n",
      "scores min/max : -1.944719055502242 0.4061087045343095\n",
      "Mask mean value:  tensor(0.5436, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0169  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.547e-02\n",
      "‖w_svm‖₂       : 0.08221972503524318\n",
      "‖alpha‖₁       : 0.47198307242339266\n",
      "scores min/max : -2.3096142780217606 2.770514472440855\n",
      "Mask mean value:  tensor(0.1066, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1636  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.426e-02\n",
      "‖w_svm‖₂       : 0.06094421407552655\n",
      "‖alpha‖₁       : 0.8986877874874867\n",
      "scores min/max : -1.6877147837987894 3.7252052849978887\n",
      "Mask mean value:  tensor(0.4556, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6000  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.417e-03\n",
      "‖w_svm‖₂       : 0.01648302076543603\n",
      "‖alpha‖₁       : 0.5962919666155964\n",
      "scores min/max : -2.961038333114419 2.2740929361922007\n",
      "Mask mean value:  tensor(0.9166, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.5077  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.356e-07\n",
      "‖w_svm‖₂       : 0.0005131086821601953\n",
      "‖alpha‖₁       : 0.4399999999999705\n",
      "scores min/max : -0.0008606401235734395 -0.0005924776112885082\n",
      "Mask mean value:  tensor(0.4959, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6418  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.261e-16\n",
      "‖w_svm‖₂       : 0.028661693221829432\n",
      "‖alpha‖₁       : 0.8523721221046664\n",
      "scores min/max : -2.908695865613095 1.5722277954608985\n",
      "Mask mean value:  tensor(0.1590, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9550  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.516e-03\n",
      "‖w_svm‖₂       : 0.04628775396228749\n",
      "‖alpha‖₁       : 0.9389524126086034\n",
      "scores min/max : -2.4574095957489397 1.5906330259939887\n",
      "Mask mean value:  tensor(0.1489, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4060  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.104e-03\n",
      "‖w_svm‖₂       : 0.005621691120563\n",
      "‖alpha‖₁       : 0.3799999999999985\n",
      "scores min/max : -0.004828519275446613 0.007270009786920356\n",
      "Mask mean value:  tensor(0.5210, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9266  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.909e-14\n",
      "‖w_svm‖₂       : 2.0975754853300716e-08\n",
      "‖alpha‖₁       : 0.11999999999999497\n",
      "scores min/max : -3.6233783892442224e-08 -2.1579863801112576e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.833e-09\n",
      "‖w_svm‖₂       : 7.148216604982213e-08\n",
      "‖alpha‖₁       : 0.42000000000000004\n",
      "scores min/max : -7.105061496370844e-08 -2.394188791548216e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.034e-08\n",
      "‖w_svm‖₂       : 5.3472070956483514e-08\n",
      "‖alpha‖₁       : 0.43999999999998807\n",
      "scores min/max : -1.9221973062039026e-07 -8.82178235357898e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.579e-20\n",
      "‖w_svm‖₂       : 0.02026551743554298\n",
      "‖alpha‖₁       : 0.6599999999999996\n",
      "scores min/max : -0.040577327213599426 0.028519786980712232\n",
      "Mask mean value:  tensor(0.4219, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.3207  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.325e-15\n",
      "‖w_svm‖₂       : 2.3589673404024386e-07\n",
      "‖alpha‖₁       : 0.5800000000000001\n",
      "scores min/max : -2.9124790625902976e-07 -2.7562890454246307e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.026e-19\n",
      "‖w_svm‖₂       : 1.6879661639761287e-07\n",
      "‖alpha‖₁       : 0.23999999999997745\n",
      "scores min/max : 2.7283789875085924e-07 2.8848023896798424e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.575e-20\n",
      "‖w_svm‖₂       : 2.2498934007381905e-07\n",
      "‖alpha‖₁       : 0.2599999999999944\n",
      "scores min/max : 1.7668797306057538e-07 1.894749084290893e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.957e-17\n",
      "‖w_svm‖₂       : 1.0053012823570324e-06\n",
      "‖alpha‖₁       : 0.599999999999998\n",
      "scores min/max : -1.6726991237325527e-07 -1.1641438538570912e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.609e-19\n",
      "‖w_svm‖₂       : 0.055590003702625256\n",
      "‖alpha‖₁       : 0.5763376802934966\n",
      "scores min/max : -1.958593117973198 0.8698213428835221\n",
      "Mask mean value:  tensor(0.7087, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.061e-03\n",
      "‖w_svm‖₂       : 0.01654995546058347\n",
      "‖alpha‖₁       : 0.8599999999999295\n",
      "scores min/max : -0.053914900734017945 -0.039792648041888756\n",
      "Mask mean value:  tensor(0.2953, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0726  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.762e-14\n",
      "‖w_svm‖₂       : 0.0003644103938974568\n",
      "‖alpha‖₁       : 0.7399999999998654\n",
      "scores min/max : -0.00039070425105324185 -0.000373766663785455\n",
      "Mask mean value:  tensor(0.4981, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0603  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.295e-17\n",
      "‖w_svm‖₂       : 0.047465076205954616\n",
      "‖alpha‖₁       : 0.9056459422613269\n",
      "scores min/max : -0.7144239135658231 1.8943128535073908\n",
      "Mask mean value:  tensor(0.3850, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7145  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.224e-04\n",
      "‖w_svm‖₂       : 0.07183944432901783\n",
      "‖alpha‖₁       : 0.4192018405141765\n",
      "scores min/max : -1.9221751221764838 2.1864332201879724\n",
      "Mask mean value:  tensor(0.7511, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8918  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.152e-04\n",
      "‖w_svm‖₂       : 1.193693209823346e-06\n",
      "‖alpha‖₁       : 0.3199999999978186\n",
      "scores min/max : -2.640032414915719e-06 -2.521624386489223e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4897  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.061e-18\n",
      "‖w_svm‖₂       : 1.1208808792599061e-07\n",
      "‖alpha‖₁       : 0.5199999999999347\n",
      "scores min/max : 3.159298149140006e-07 3.373180563237642e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.843e-19\n",
      "‖w_svm‖₂       : 2.0986003442056988e-07\n",
      "‖alpha‖₁       : 0.3799999999952845\n",
      "scores min/max : -1.4382848748153744e-07 -1.2509104857512077e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.142e-07\n",
      "‖w_svm‖₂       : 0.029220444203141085\n",
      "‖alpha‖₁       : 0.5511373927879529\n",
      "scores min/max : -3.4890445748274246 1.0808728296581445\n",
      "Mask mean value:  tensor(0.1419, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1363  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.160e-03\n",
      "‖w_svm‖₂       : 0.00016167096973742926\n",
      "‖alpha‖₁       : 0.6399999999999554\n",
      "scores min/max : -0.00016151964347603173 -0.00015154655679763438\n",
      "Mask mean value:  tensor(0.4992, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7632  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.369e-17\n",
      "‖w_svm‖₂       : 0.020884774739209393\n",
      "‖alpha‖₁       : 0.779999999999995\n",
      "scores min/max : -0.03090123189607237 0.04238263229269684\n",
      "Mask mean value:  tensor(0.3792, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6747  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.421e-15\n",
      "‖w_svm‖₂       : 0.13902096110221104\n",
      "‖alpha‖₁       : 0.6552203168975885\n",
      "scores min/max : -18.085497235301634 1.8022198005825603\n",
      "Mask mean value:  tensor(0.2411, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2788  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.108e-03\n",
      "‖w_svm‖₂       : 0.041465011276210335\n",
      "‖alpha‖₁       : 0.7294223383282716\n",
      "scores min/max : -0.25823496808993374 2.0227784164654463\n",
      "Mask mean value:  tensor(0.6865, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.1034  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.531e-03\n",
      "‖w_svm‖₂       : 5.356436958340559e-08\n",
      "‖alpha‖₁       : 0.11999999999998921\n",
      "scores min/max : -9.824883456463117e-08 -6.742145784639804e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.165e-07\n",
      "‖w_svm‖₂       : 6.622712756960198e-07\n",
      "‖alpha‖₁       : 0.39999999999999986\n",
      "scores min/max : -4.691750775100949e-08 2.591905518236812e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.264e-09\n",
      "‖w_svm‖₂       : 0.0008813524840041776\n",
      "‖alpha‖₁       : 0.8199999999999974\n",
      "scores min/max : 0.0010028303525031514 0.002589516788748277\n",
      "Mask mean value:  tensor(0.5100, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5909  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.062e-16\n",
      "‖w_svm‖₂       : 0.02076659010745881\n",
      "‖alpha‖₁       : 0.8233765231135132\n",
      "scores min/max : -1.902777286855329 1.49432338031193\n",
      "Mask mean value:  tensor(0.6904, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.3618  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.036e-05\n",
      "‖w_svm‖₂       : 2.340796821233581e-07\n",
      "‖alpha‖₁       : 0.6399999999999952\n",
      "scores min/max : 4.064615089163215e-07 4.273673084221061e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.011e-19\n",
      "‖w_svm‖₂       : 0.1841579527271032\n",
      "‖alpha‖₁       : 0.8834815429061194\n",
      "scores min/max : -3.6354789733736066 6.070372609014983\n",
      "Mask mean value:  tensor(0.0915, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1983  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.627e-03\n",
      "‖w_svm‖₂       : 3.783631504008569e-07\n",
      "‖alpha‖₁       : 0.2799999999999955\n",
      "scores min/max : -1.3821057799601404e-06 -1.320351858971184e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.870e-19\n",
      "‖w_svm‖₂       : 1.1395953372665926e-07\n",
      "‖alpha‖₁       : 0.2999999999999935\n",
      "scores min/max : 1.6051847918524025e-07 1.6950712293555316e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.370e-07\n",
      "‖w_svm‖₂       : 0.0003075977045761841\n",
      "‖alpha‖₁       : 0.4199999999569511\n",
      "scores min/max : 0.0001224983591428901 0.00044262417443250834\n",
      "Mask mean value:  tensor(0.5021, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9996  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.986e-15\n",
      "‖w_svm‖₂       : 4.23988663068623e-07\n",
      "‖alpha‖₁       : 0.7199999999999992\n",
      "scores min/max : 1.4997876115696492e-07 2.0715448571798538e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.086e-21\n",
      "‖w_svm‖₂       : 2.78389476033079e-07\n",
      "‖alpha‖₁       : 0.6599999999999924\n",
      "scores min/max : -6.726695375738185e-07 -5.209649329775524e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.406e-18\n",
      "‖w_svm‖₂       : 1.0800295691915949e-07\n",
      "‖alpha‖₁       : 0.23999999999997884\n",
      "scores min/max : 6.737081072410197e-08 7.839991125690155e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.562e-21\n",
      "‖w_svm‖₂       : 0.043321502887421946\n",
      "‖alpha‖₁       : 0.9406606435607019\n",
      "scores min/max : -1.8260794154431021 0.30775087958229264\n",
      "Mask mean value:  tensor(0.4331, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3254  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.936e-10\n",
      "‖w_svm‖₂       : 0.0392314278003657\n",
      "‖alpha‖₁       : 0.9199999999999894\n",
      "scores min/max : -0.2948308936918713 0.27091839922681293\n",
      "Mask mean value:  tensor(0.1922, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9524  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 8.417e-03\n",
      "‖w_svm‖₂       : 5.66632914104541e-08\n",
      "‖alpha‖₁       : 0.23999999999997543\n",
      "scores min/max : -5.516048939208074e-08 -4.059285029516762e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.483e-20\n",
      "‖w_svm‖₂       : 0.08568081749861133\n",
      "‖alpha‖₁       : 0.577998935296784\n",
      "scores min/max : -2.1378617444226715 5.7870535666838725\n",
      "Mask mean value:  tensor(0.2446, dtype=torch.float64)\n",
      "max feasible return = 0.1057  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.945870003397447e-07\n",
      "‖alpha‖₁       : 0.58\n",
      "scores min/max : -3.843411483279565e-07 -2.936126482005596e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0263551121680935e-07\n",
      "‖alpha‖₁       : 0.29999999999999893\n",
      "scores min/max : 3.454522537943764e-08 3.9528180431248024e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 6.893628772804125e-08\n",
      "‖alpha‖₁       : 0.5999999999999833\n",
      "scores min/max : 2.2870709846135526e-08 5.965486863345569e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.607467057919744e-08\n",
      "‖alpha‖₁       : 0.3799999999999987\n",
      "scores min/max : -6.726310193946421e-09 2.658825347805583e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 5.7028384844767585e-06\n",
      "‖alpha‖₁       : 0.3199999999520458\n",
      "scores min/max : 5.828313173248142e-06 9.337007609089429e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4259  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.05043908848259964\n",
      "‖alpha‖₁       : 0.7397245182019125\n",
      "scores min/max : -3.469869316538393 2.1636356469031086\n",
      "Mask mean value:  tensor(0.8891, dtype=torch.float64)\n",
      "max feasible return = -0.9340  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.02853555259723981\n",
      "‖alpha‖₁       : 0.6151664407623816\n",
      "scores min/max : -1.9239375688530038 1.2057800575134978\n",
      "Mask mean value:  tensor(0.7701, dtype=torch.float64)\n",
      "max feasible return = 2.6918  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.6568341195431487e-07\n",
      "‖alpha‖₁       : 0.4599999999999904\n",
      "scores min/max : 1.5374776289925564e-07 7.483312138847786e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.3658513259071446e-07\n",
      "‖alpha‖₁       : 0.5199999999999785\n",
      "scores min/max : 9.164346514729576e-08 1.0456548460345422e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.55297038221212e-07\n",
      "‖alpha‖₁       : 0.5799999999999841\n",
      "scores min/max : -2.403254101803296e-07 -2.0401239474898432e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.043976481551968656\n",
      "‖alpha‖₁       : 0.6877859905121994\n",
      "scores min/max : -0.43953338173636775 1.9491733375117086\n",
      "Mask mean value:  tensor(0.3756, dtype=torch.float64)\n",
      "max feasible return = 0.0618  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.032467194853971315\n",
      "‖alpha‖₁       : 0.4222251457941759\n",
      "scores min/max : -3.8921280357270422 5.257725953902337\n",
      "Mask mean value:  tensor(0.0904, dtype=torch.float64)\n",
      "max feasible return = 0.5701  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.043354019203577385\n",
      "‖alpha‖₁       : 0.821116911466905\n",
      "scores min/max : -5.040502014571872 3.1280678051007267\n",
      "Mask mean value:  tensor(0.9542, dtype=torch.float64)\n",
      "max feasible return = -3.6341  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.007239750989763958\n",
      "‖alpha‖₁       : 0.7999999999999118\n",
      "scores min/max : -0.03268162867725205 0.0026373393280656465\n",
      "Mask mean value:  tensor(0.4689, dtype=torch.float64)\n",
      "max feasible return = -0.1704  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.072661329200508e-07\n",
      "‖alpha‖₁       : 0.619999999999955\n",
      "scores min/max : 5.788300788838911e-08 7.443352747666879e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.736394880234542e-08\n",
      "‖alpha‖₁       : 0.43999999999999984\n",
      "scores min/max : 6.29064103030412e-09 1.586033700473074e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.3049104241711684e-07\n",
      "‖alpha‖₁       : 0.2799999999999725\n",
      "scores min/max : -2.580077537451786e-07 -2.40007618277695e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  35 | train 0.005325 | val 0.006771\n",
      "-----------------------------------------Epoch:  36 ----------------------------------------\n",
      "‖w_svm‖₂       : 0.1384279067336549\n",
      "‖alpha‖₁       : 0.655076540020409\n",
      "scores min/max : -18.064492359491574 1.8260275310319252\n",
      "Mask mean value:  tensor(0.2629, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2825  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.911e-03\n",
      "‖w_svm‖₂       : 0.0302597176648592\n",
      "‖alpha‖₁       : 0.8985831273923778\n",
      "scores min/max : -0.7372963615292233 2.0229196866965746\n",
      "Mask mean value:  tensor(0.8597, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3285  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.395e-15\n",
      "‖w_svm‖₂       : 0.1460603959442482\n",
      "‖alpha‖₁       : 0.7593014031282047\n",
      "scores min/max : -1.762290457253025 2.2151483245251598\n",
      "Mask mean value:  tensor(0.8477, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3553  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.726e-11\n",
      "‖w_svm‖₂       : 1.7446677764159478e-05\n",
      "‖alpha‖₁       : 0.35999999998277055\n",
      "scores min/max : 1.4486171537527438e-05 1.558025037000172e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.733e-17\n",
      "‖w_svm‖₂       : 1.937129333363372e-07\n",
      "‖alpha‖₁       : 0.3799999999999407\n",
      "scores min/max : -1.7323678509851468e-08 1.865130712272933e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.605e-19\n",
      "‖w_svm‖₂       : 0.07260293199707882\n",
      "‖alpha‖₁       : 0.6585423252024573\n",
      "scores min/max : -2.0265413345919217 4.2742782397831425\n",
      "Mask mean value:  tensor(0.3601, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.1227  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.528e-04\n",
      "‖w_svm‖₂       : 0.00013889663519682207\n",
      "‖alpha‖₁       : 0.43999999999748657\n",
      "scores min/max : -0.0002763079971487706 -0.000135458792791219\n",
      "Mask mean value:  tensor(0.4991, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0214  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.816e-17\n",
      "‖w_svm‖₂       : 0.0461911387735224\n",
      "‖alpha‖₁       : 0.9389393562765448\n",
      "scores min/max : -2.459326861422803 1.588716561530903\n",
      "Mask mean value:  tensor(0.1476, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4037  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.191e-03\n",
      "‖w_svm‖₂       : 0.10033390893049525\n",
      "‖alpha‖₁       : 0.873297319893489\n",
      "scores min/max : -12.211116926672528 2.116844977067071\n",
      "Mask mean value:  tensor(0.4434, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3250  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 8.494e-02\n",
      "‖w_svm‖₂       : 6.692205797431881e-07\n",
      "‖alpha‖₁       : 0.3999999999998949\n",
      "scores min/max : -4.5326632327513185e-08 2.635087505763287e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.287e-09\n",
      "‖w_svm‖₂       : 8.242260267424144e-08\n",
      "‖alpha‖₁       : 0.37999999999999934\n",
      "scores min/max : 6.6544139497739845e-09 2.4812590085071562e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.730e-22\n",
      "‖w_svm‖₂       : 0.04122419785421545\n",
      "‖alpha‖₁       : 0.7294223111754616\n",
      "scores min/max : -0.2574038141348189 2.0235682634251866\n",
      "Mask mean value:  tensor(0.6888, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.1142  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.505e-03\n",
      "‖w_svm‖₂       : 0.06059159275149153\n",
      "‖alpha‖₁       : 0.8986559585544971\n",
      "scores min/max : -1.6910769650441542 3.722216687001655\n",
      "Mask mean value:  tensor(0.4489, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5951  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.836e-03\n",
      "‖w_svm‖₂       : 0.003214107577519334\n",
      "‖alpha‖₁       : 0.5799999999999916\n",
      "scores min/max : 0.00268682272266351 0.003995350609257778\n",
      "Mask mean value:  tensor(0.5186, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3298  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.312e-13\n",
      "‖w_svm‖₂       : 0.005740316374387847\n",
      "‖alpha‖₁       : 0.5599999999999992\n",
      "scores min/max : 0.0055741703009056275 0.007286291534398804\n",
      "Mask mean value:  tensor(0.5294, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1097  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 2.241e-05\n",
      "‖w_svm‖₂       : 4.803255862485527e-08\n",
      "‖alpha‖₁       : 0.17999999999999727\n",
      "scores min/max : 4.564429562176061e-08 6.033430233473883e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.379e-08\n",
      "‖w_svm‖₂       : 2.0963766686070558e-08\n",
      "‖alpha‖₁       : 0.11999999999999499\n",
      "scores min/max : -3.508867507088372e-08 -2.043346992261931e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.831e-09\n",
      "‖w_svm‖₂       : 3.783421660068238e-07\n",
      "‖alpha‖₁       : 0.2799999999999953\n",
      "scores min/max : -1.3746178188919375e-06 -1.3128609770062365e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.858e-19\n",
      "‖w_svm‖₂       : 0.02874268295784681\n",
      "‖alpha‖₁       : 0.852374418633957\n",
      "scores min/max : -2.9039019189975055 1.577071412565604\n",
      "Mask mean value:  tensor(0.1614, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9575  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.406e-03\n",
      "‖w_svm‖₂       : 0.03890242852499063\n",
      "‖alpha‖₁       : 0.919999999999989\n",
      "scores min/max : -0.2897671816839363 0.26695082591358277\n",
      "Mask mean value:  tensor(0.1955, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9701  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 9.126e-03\n",
      "‖w_svm‖₂       : 1.9698038517932918e-07\n",
      "‖alpha‖₁       : 0.4199999999999711\n",
      "scores min/max : -4.762592439196406e-07 -4.5401735940563014e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.834e-19\n",
      "‖w_svm‖₂       : 0.0003642898364820105\n",
      "‖alpha‖₁       : 0.739999999999793\n",
      "scores min/max : -0.0003774292520667708 -0.0003605028734366151\n",
      "Mask mean value:  tensor(0.4982, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0604  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.308e-17\n",
      "‖w_svm‖₂       : 1.0567337190757686e-07\n",
      "‖alpha‖₁       : 0.23999999999998856\n",
      "scores min/max : 5.0507427133890846e-08 6.139571767419087e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.433e-21\n",
      "‖w_svm‖₂       : 0.055275132146961455\n",
      "‖alpha‖₁       : 0.5763259415313525\n",
      "scores min/max : -1.9560379088671576 0.8722735763723157\n",
      "Mask mean value:  tensor(0.7165, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9485  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.105e-03\n",
      "‖w_svm‖₂       : 8.246624421101151e-08\n",
      "‖alpha‖₁       : 0.1799999999999992\n",
      "scores min/max : -1.8473649781389326e-07 4.902806513009116e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.282e-21\n",
      "‖w_svm‖₂       : 0.029231121254500555\n",
      "‖alpha‖₁       : 0.5511426808141405\n",
      "scores min/max : -3.4861440046785668 1.0836027224750011\n",
      "Mask mean value:  tensor(0.1433, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.084e-03\n",
      "‖w_svm‖₂       : 0.139522612480596\n",
      "‖alpha‖₁       : 0.8799999999999555\n",
      "scores min/max : -1.3759686591493883 3.7030360288365003\n",
      "Mask mean value:  tensor(0.3378, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2641  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.123e-08\n",
      "‖w_svm‖₂       : 0.07156394184981771\n",
      "‖alpha‖₁       : 0.5799999999999614\n",
      "scores min/max : -2.7913625643648636 1.6129368952817904\n",
      "Mask mean value:  tensor(0.2017, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3558  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.607e-04\n",
      "‖w_svm‖₂       : 0.020675207896427485\n",
      "‖alpha‖₁       : 0.8233753437271693\n",
      "scores min/max : -1.9028769010873103 1.4940906347771747\n",
      "Mask mean value:  tensor(0.6900, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.3607  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.066e-05\n",
      "‖w_svm‖₂       : 5.3196683043290184e-08\n",
      "‖alpha‖₁       : 0.4399999999999884\n",
      "scores min/max : -1.912311542750659e-07 -8.723034763714073e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.549e-20\n",
      "‖w_svm‖₂       : 0.0008838152219868452\n",
      "‖alpha‖₁       : 0.8199999999999045\n",
      "scores min/max : 0.001121384284108272 0.002707902489948815\n",
      "Mask mean value:  tensor(0.5106, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5939  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.067e-16\n",
      "‖w_svm‖₂       : 0.0044537232357884805\n",
      "‖alpha‖₁       : 0.45999999999999985\n",
      "scores min/max : -0.008660921492582473 -0.006503583851375172\n",
      "Mask mean value:  tensor(0.4650, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.0207  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.082e-15\n",
      "‖w_svm‖₂       : 0.08198579560644191\n",
      "‖alpha‖₁       : 0.47195107313953477\n",
      "scores min/max : -2.3037762446115044 2.7752109090246155\n",
      "Mask mean value:  tensor(0.1088, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1869  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.478e-02\n",
      "‖w_svm‖₂       : 4.249570652663221e-07\n",
      "‖alpha‖₁       : 0.7199999999999993\n",
      "scores min/max : 2.1476263669343097e-07 2.7199217560443074e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.138e-21\n",
      "‖w_svm‖₂       : 0.041609170323638714\n",
      "‖alpha‖₁       : 0.65999999999999\n",
      "scores min/max : -0.22977393149145964 0.1625014330865684\n",
      "Mask mean value:  tensor(0.4744, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8799  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.674e-13\n",
      "‖w_svm‖₂       : 0.1834158133363729\n",
      "‖alpha‖₁       : 0.8832808378764623\n",
      "scores min/max : -3.634185189125998 6.075852006098745\n",
      "Mask mean value:  tensor(0.0922, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2012  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.563e-03\n",
      "‖w_svm‖₂       : 0.015375692322689311\n",
      "‖alpha‖₁       : 0.859999999999997\n",
      "scores min/max : -0.05130035826042745 0.0013740289349746422\n",
      "Mask mean value:  tensor(0.3552, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4490  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.740e-05\n",
      "‖w_svm‖₂       : 5.335001720760762e-08\n",
      "‖alpha‖₁       : 0.11999999999998923\n",
      "scores min/max : -9.82699041312978e-08 -6.74423669545817e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.023e-07\n",
      "‖w_svm‖₂       : 0.047756371256605125\n",
      "‖alpha‖₁       : 0.905646780893508\n",
      "scores min/max : -0.7292038828641367 1.8888837909386944\n",
      "Mask mean value:  tensor(0.3712, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6887  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.979e-03\n",
      "‖w_svm‖₂       : 5.691347816755748e-08\n",
      "‖alpha‖₁       : 0.2399999999999825\n",
      "scores min/max : -5.760118581448147e-08 -4.30760526308566e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.524e-20\n",
      "‖w_svm‖₂       : 7.53798394078836e-08\n",
      "‖alpha‖₁       : 0.6599999999999975\n",
      "scores min/max : 1.1275236953607645e-07 2.5773076964253347e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.129e-09\n",
      "‖w_svm‖₂       : 0.04373201226974474\n",
      "‖alpha‖₁       : 0.9406950643079655\n",
      "scores min/max : -1.8359982510594115 0.2978263935444587\n",
      "Mask mean value:  tensor(0.3942, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3232  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.344e-13\n",
      "‖w_svm‖₂       : 1.0637091671366425e-06\n",
      "‖alpha‖₁       : 0.49999999999999933\n",
      "scores min/max : -2.511609058404144e-07 -5.8404662571133264e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.894e-19\n",
      "‖w_svm‖₂       : 7.227064140830927e-08\n",
      "‖alpha‖₁       : 0.13999999999999352\n",
      "scores min/max : 1.3942621321378709e-08 1.9737135878838958e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.387e-08\n",
      "‖w_svm‖₂       : 1.1390784183652984e-07\n",
      "‖alpha‖₁       : 0.2999999999999936\n",
      "scores min/max : 1.4771280062914313e-07 1.5670104869344931e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.368e-07\n",
      "‖w_svm‖₂       : 4.11982902696796e-06\n",
      "‖alpha‖₁       : 0.4199999999889191\n",
      "scores min/max : -3.426264279261785e-06 -1.6027672287776259e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1229  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.842e-05\n",
      "‖w_svm‖₂       : 1.198740961704092e-06\n",
      "‖alpha‖₁       : 0.3199999999976494\n",
      "scores min/max : -2.650561499497016e-06 -2.531588047898901e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4897  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.097e-18\n",
      "‖w_svm‖₂       : 2.331851743510295e-07\n",
      "‖alpha‖₁       : 0.6399999999999962\n",
      "scores min/max : 4.2813493434188035e-07 4.4902310150994703e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.025e-19\n",
      "‖w_svm‖₂       : 0.027142886030360854\n",
      "‖alpha‖₁       : 0.19667715713132256\n",
      "scores min/max : -2.224543867690204 -0.001900975256330345\n",
      "Mask mean value:  tensor(0.0418, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2855  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.393e-03\n",
      "‖w_svm‖₂       : 0.0005120975948453397\n",
      "‖alpha‖₁       : 0.4399999999999841\n",
      "scores min/max : -0.0008938261347337629 -0.0006251723138621223\n",
      "Mask mean value:  tensor(0.4957, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6413  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.283e-16\n",
      "‖w_svm‖₂       : 2.0850039248112995e-07\n",
      "‖alpha‖₁       : 0.37999999999538653\n",
      "scores min/max : -1.0256575847677469e-07 -8.389555892735468e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.592e-07\n",
      "‖w_svm‖₂       : 2.774332516914809e-07\n",
      "‖alpha‖₁       : 0.6599999999999819\n",
      "scores min/max : -7.045255855985651e-07 -5.527547915210366e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.424e-18\n",
      "‖w_svm‖₂       : 2.3742632993527433e-07\n",
      "‖alpha‖₁       : 0.5799999999999774\n",
      "scores min/max : -3.355764674032242e-07 -3.1991427527394454e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.075e-19\n",
      "‖w_svm‖₂       : 0.02006310110313444\n",
      "‖alpha‖₁       : 0.6599999999999998\n",
      "scores min/max : -0.04255206821213131 0.024616840899619206\n",
      "Mask mean value:  tensor(0.4096, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.2235  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.368e-15\n",
      "‖w_svm‖₂       : 0.022206814626715105\n",
      "‖alpha‖₁       : 0.8150964492115345\n",
      "scores min/max : -11.514785730914772 2.033003827004511\n",
      "Mask mean value:  tensor(0.7123, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9187  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.427e-06\n",
      "‖w_svm‖₂       : 0.0063086304559695945\n",
      "‖alpha‖₁       : 0.7005716599831457\n",
      "scores min/max : -2.00255971323893 0.0208574234910296\n",
      "Mask mean value:  tensor(0.4619, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2586  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.596e-12\n",
      "‖w_svm‖₂       : 2.2487417187981767e-07\n",
      "‖alpha‖₁       : 0.25999999999999457\n",
      "scores min/max : 1.2643662112070848e-07 1.3922262325970087e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.833e-17\n",
      "‖w_svm‖₂       : 0.00021372320433442704\n",
      "‖alpha‖₁       : 0.6199999999953243\n",
      "scores min/max : 1.4751873741544145e-05 3.093685044628889e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.608e-17\n",
      "‖w_svm‖₂       : 0.05141060290239792\n",
      "‖alpha‖₁       : 0.8271548257176835\n",
      "scores min/max : -1.9505808763500283 0.40032529871109035\n",
      "Mask mean value:  tensor(0.5228, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0132  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.693e-02\n",
      "‖w_svm‖₂       : 0.0001618466829141894\n",
      "‖alpha‖₁       : 0.6399999999998951\n",
      "scores min/max : -0.00013221040547840667 -0.0001222157649048999\n",
      "Mask mean value:  tensor(0.4994, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7634  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.332e-17\n",
      "‖w_svm‖₂       : 0.020147432223418905\n",
      "‖alpha‖₁       : 0.3834365401444142\n",
      "scores min/max : -1.9589613181923533 0.24353719659284928\n",
      "Mask mean value:  tensor(0.7178, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1532  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.232e-14\n",
      "‖w_svm‖₂       : 0.005609570053303116\n",
      "‖alpha‖₁       : 0.3800000000000111\n",
      "scores min/max : -0.005916562940642217 0.005818338399090128\n",
      "Mask mean value:  tensor(0.5143, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9148  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.068e-14\n",
      "‖w_svm‖₂       : 7.227308244454865e-08\n",
      "‖alpha‖₁       : 0.41999999999999865\n",
      "scores min/max : -6.735562635375356e-08 -2.0189463175105144e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.141e-08\n",
      "‖w_svm‖₂       : 1.1008880189900888e-07\n",
      "‖alpha‖₁       : 0.5199999999999994\n",
      "scores min/max : 3.1998964764602296e-07 3.414136448134633e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.849e-19\n",
      "‖w_svm‖₂       : 0.016391256012663474\n",
      "‖alpha‖₁       : 0.5962948139970856\n",
      "scores min/max : -2.9279414943142794 2.2810598024554998\n",
      "Mask mean value:  tensor(0.9197, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.5107  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.173e-08\n",
      "‖w_svm‖₂       : 0.01634874164208008\n",
      "‖alpha‖₁       : 0.86\n",
      "scores min/max : -0.046447657131717196 -0.03276795670649038\n",
      "Mask mean value:  tensor(0.3259, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0821  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.353e-14\n",
      "‖w_svm‖₂       : 0.02069076185976765\n",
      "‖alpha‖₁       : 0.7799999999999988\n",
      "scores min/max : -0.033937777323974905 0.037912537447548744\n",
      "Mask mean value:  tensor(0.3644, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6112  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.166e-15\n",
      "‖w_svm‖₂       : 0.0003072573390131342\n",
      "‖alpha‖₁       : 0.41999999996131143\n",
      "scores min/max : 0.00015680232867725723 0.0004762589271619262\n",
      "Mask mean value:  tensor(0.5022, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0000  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.020e-15\n",
      "‖w_svm‖₂       : 0.008412276859439705\n",
      "‖alpha‖₁       : 0.6076929895440909\n",
      "scores min/max : -1.990913685905117 0.2958693951071562\n",
      "Mask mean value:  tensor(0.5533, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2234  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.951e-15\n",
      "‖w_svm‖₂       : 7.568981463494449e-08\n",
      "‖alpha‖₁       : 0.5399999999999614\n",
      "scores min/max : -1.6655366750785547e-07 -1.4495712674987986e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.799e-20\n",
      "‖w_svm‖₂       : 0.07126870477509399\n",
      "‖alpha‖₁       : 0.41915741247047034\n",
      "scores min/max : -1.9067137422742815 2.195233018356526\n",
      "Mask mean value:  tensor(0.7663, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8970  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.562e-04\n",
      "‖w_svm‖₂       : 1.6821344527827745e-07\n",
      "‖alpha‖₁       : 0.23999999999997917\n",
      "scores min/max : 2.697669772409793e-07 2.853720601723167e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.630e-20\n",
      "‖w_svm‖₂       : 0.04185101545467874\n",
      "‖alpha‖₁       : 0.889984856639014\n",
      "scores min/max : -2.0120599253060134 0.33579921921220995\n",
      "Mask mean value:  tensor(0.2559, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0619  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.644e-05\n",
      "‖w_svm‖₂       : 1.0033641704782588e-06\n",
      "‖alpha‖₁       : 0.5999999999999979\n",
      "scores min/max : -1.0067301959491699e-07 -4.981570202913184e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.585e-19\n",
      "‖w_svm‖₂       : 0.01801184481410228\n",
      "‖alpha‖₁       : 0.6815890581445421\n",
      "scores min/max : -1.97182451333994 0.05799443272079745\n",
      "Mask mean value:  tensor(0.6143, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2585  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.565e-12\n",
      "‖w_svm‖₂       : 0.000187566593345989\n",
      "‖alpha‖₁       : 0.8199999999999987\n",
      "scores min/max : -9.326379560295174e-05 -7.872757107904766e-05\n",
      "Mask mean value:  tensor(0.4996, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6965  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.335e-17\n",
      "‖w_svm‖₂       : 0.08555741333270442\n",
      "‖alpha‖₁       : 0.5779914110167523\n",
      "scores min/max : -2.1415389674310705 5.782426093183332\n",
      "Mask mean value:  tensor(0.2386, dtype=torch.float64)\n",
      "max feasible return = 0.1047  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.9263643632058343e-07\n",
      "‖alpha‖₁       : 0.58\n",
      "scores min/max : -3.5401749269695856e-07 -2.632880618917788e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0232463899695932e-07\n",
      "‖alpha‖₁       : 0.29999999999999916\n",
      "scores min/max : 3.630605857528309e-08 4.1285347070666956e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 6.861062120921935e-08\n",
      "‖alpha‖₁       : 0.5999999999999295\n",
      "scores min/max : 2.3685256529500855e-08 6.044052108822481e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.574240243811247e-08\n",
      "‖alpha‖₁       : 0.3799999999999984\n",
      "scores min/max : -6.977973927773325e-09 2.4098698027967386e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.1998923019189456e-05\n",
      "‖alpha‖₁       : 0.31999999985942673\n",
      "scores min/max : 1.3205349027060457e-05 2.1030950854922137e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64)\n",
      "max feasible return = 2.4262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.0499352015525327\n",
      "‖alpha‖₁       : 0.739723010028019\n",
      "scores min/max : -3.4650072594026278 2.1683632919112723\n",
      "Mask mean value:  tensor(0.8928, dtype=torch.float64)\n",
      "max feasible return = -0.9364  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.028262649728169185\n",
      "‖alpha‖₁       : 0.6151664302769473\n",
      "scores min/max : -1.9299027944516598 1.1996373658345112\n",
      "Mask mean value:  tensor(0.7549, dtype=torch.float64)\n",
      "max feasible return = 2.6371  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.6452495670692603e-07\n",
      "‖alpha‖₁       : 0.45999999999999047\n",
      "scores min/max : 2.391674648248804e-07 8.33743555034454e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.3679579460194626e-07\n",
      "‖alpha‖₁       : 0.5199999999999774\n",
      "scores min/max : 7.775880414508825e-08 9.068198071754708e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.555433062547464e-07\n",
      "‖alpha‖₁       : 0.5799999999999729\n",
      "scores min/max : -2.503364619756481e-07 -2.139687200396609e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.043999877480348205\n",
      "‖alpha‖₁       : 0.6877954401146104\n",
      "scores min/max : -0.44527535148592756 1.9433434226913224\n",
      "Mask mean value:  tensor(0.3559, dtype=torch.float64)\n",
      "max feasible return = 0.0806  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03259689048585671\n",
      "‖alpha‖₁       : 0.4222382936625492\n",
      "scores min/max : -3.887595206233826 5.262890222630818\n",
      "Mask mean value:  tensor(0.0928, dtype=torch.float64)\n",
      "max feasible return = 0.5855  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.043332592528208924\n",
      "‖alpha‖₁       : 0.8211175284313595\n",
      "scores min/max : -5.025667256493968 3.1430027963064253\n",
      "Mask mean value:  tensor(0.9568, dtype=torch.float64)\n",
      "max feasible return = -3.6482  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.007159614463178432\n",
      "‖alpha‖₁       : 0.7999999999998934\n",
      "scores min/max : -0.03295968742823818 0.001503377878948546\n",
      "Mask mean value:  tensor(0.4643, dtype=torch.float64)\n",
      "max feasible return = -0.1687  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.0637256226768876e-07\n",
      "‖alpha‖₁       : 0.6199999999999561\n",
      "scores min/max : 9.024450659953276e-08 1.0679284970524046e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.749477267987594e-08\n",
      "‖alpha‖₁       : 0.4399999999999998\n",
      "scores min/max : 6.303462665344389e-09 1.5873390248669734e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.290855891279403e-07\n",
      "‖alpha‖₁       : 0.27999999999998265\n",
      "scores min/max : -2.759262396050895e-07 -2.580148421387073e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  36 | train 0.005335 | val 0.006825\n",
      "-----------------------------------------Epoch:  37 ----------------------------------------\n",
      "‖w_svm‖₂       : 0.1460546656332111\n",
      "‖alpha‖₁       : 0.7593040804580424\n",
      "scores min/max : -1.7605816986268041 2.2168623756419574\n",
      "Mask mean value:  tensor(0.8501, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3570  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.641e-11\n",
      "‖w_svm‖₂       : 0.005722393517852846\n",
      "‖alpha‖₁       : 0.5599999999999999\n",
      "scores min/max : 0.005319046780994983 0.0070213473569939655\n",
      "Mask mean value:  tensor(0.5281, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1070  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 2.437e-05\n",
      "‖w_svm‖₂       : 0.00018756737720811568\n",
      "‖alpha‖₁       : 0.8199999999999987\n",
      "scores min/max : -9.32383724212414e-05 -7.870195061916577e-05\n",
      "Mask mean value:  tensor(0.4996, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6965  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.335e-17\n",
      "‖w_svm‖₂       : 1.96391951219123e-07\n",
      "‖alpha‖₁       : 0.41999999999997106\n",
      "scores min/max : -4.522346089344413e-07 -4.299916770998314e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.826e-19\n",
      "‖w_svm‖₂       : 8.229765460069899e-08\n",
      "‖alpha‖₁       : 0.17999999999999916\n",
      "scores min/max : -1.8276309090412415e-07 5.098855094159718e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.268e-21\n",
      "‖w_svm‖₂       : 7.568229353231023e-08\n",
      "‖alpha‖₁       : 0.5399999999999613\n",
      "scores min/max : -1.6584742389529619e-07 -1.4425299612922917e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.798e-20\n",
      "‖w_svm‖₂       : 2.2485066985343167e-07\n",
      "‖alpha‖₁       : 0.2599999999999947\n",
      "scores min/max : 1.274760128331926e-07 1.4026045068465274e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.817e-17\n",
      "‖w_svm‖₂       : 3.964503696496931e-06\n",
      "‖alpha‖₁       : 0.4199999999901543\n",
      "scores min/max : -3.4005723468729678e-06 -1.6352674090314972e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1229  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.774e-05\n",
      "‖w_svm‖₂       : 0.051612430547421435\n",
      "‖alpha‖₁       : 0.8271807708351208\n",
      "scores min/max : -1.9406499445306662 0.41029029919421084\n",
      "Mask mean value:  tensor(0.5582, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0202  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.586e-10\n",
      "‖w_svm‖₂       : 0.016417336915567637\n",
      "‖alpha‖₁       : 0.5962947657961747\n",
      "scores min/max : -2.9312039945096853 2.28044657116821\n",
      "Mask mean value:  tensor(0.9194, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.5105  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.226e-08\n",
      "‖w_svm‖₂       : 0.13834192245386526\n",
      "‖alpha‖₁       : 0.6550638298156268\n",
      "scores min/max : -18.088693440032568 1.8033112534798645\n",
      "Mask mean value:  tensor(0.2420, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2785  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.107e-03\n",
      "‖w_svm‖₂       : 1.6804049266870075e-07\n",
      "‖alpha‖₁       : 0.2399999999999805\n",
      "scores min/max : 2.655007329171274e-07 2.810756230184123e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.624e-20\n",
      "‖w_svm‖₂       : 0.0005115812274493253\n",
      "‖alpha‖₁       : 0.43999999999998085\n",
      "scores min/max : -0.000819832055425555 -0.0005518451788137195\n",
      "Mask mean value:  tensor(0.4961, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6425  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.292e-16\n",
      "‖w_svm‖₂       : 1.0634348772096136e-06\n",
      "‖alpha‖₁       : 0.49999999999999944\n",
      "scores min/max : -2.6017168165646127e-07 -6.743212873782028e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.887e-19\n",
      "‖w_svm‖₂       : 0.015408718003842541\n",
      "‖alpha‖₁       : 0.8599999999999969\n",
      "scores min/max : -0.05143531881076038 0.001431792710697273\n",
      "Mask mean value:  tensor(0.3549, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4488  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.764e-05\n",
      "‖w_svm‖₂       : 0.04363092427575038\n",
      "‖alpha‖₁       : 0.9406871943892439\n",
      "scores min/max : -1.8325939017071544 0.3012366474493149\n",
      "Mask mean value:  tensor(0.4074, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3242  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.365e-13\n",
      "‖w_svm‖₂       : 7.215541698720611e-08\n",
      "‖alpha‖₁       : 0.13999999999999507\n",
      "scores min/max : 1.4004676986207542e-08 1.9787896619030307e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.375e-08\n",
      "‖w_svm‖₂       : 2.3741229444837802e-07\n",
      "‖alpha‖₁       : 0.5799999999999781\n",
      "scores min/max : -3.2162600317167744e-07 -3.0595840887857414e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.078e-19\n",
      "‖w_svm‖₂       : 0.02066394117658601\n",
      "‖alpha‖₁       : 0.8233771214872041\n",
      "scores min/max : -1.902050959949173 1.49498901437482\n",
      "Mask mean value:  tensor(0.6923, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.3660  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.256e-06\n",
      "‖w_svm‖₂       : 0.0032118474515128808\n",
      "‖alpha‖₁       : 0.5799999999999913\n",
      "scores min/max : 0.0028843505074449254 0.0041894239683158\n",
      "Mask mean value:  tensor(0.5196, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3304  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.106e-13\n",
      "‖w_svm‖₂       : 0.0003073401948730791\n",
      "‖alpha‖₁       : 0.4199999999608054\n",
      "scores min/max : 0.00015232152256452634 0.00047194558876605255\n",
      "Mask mean value:  tensor(0.5022, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9999  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.017e-15\n",
      "‖w_svm‖₂       : 0.00016188682562310702\n",
      "‖alpha‖₁       : 0.6399999999999999\n",
      "scores min/max : -0.00013029419859925971 -0.00012029456163653205\n",
      "Mask mean value:  tensor(0.4994, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7635  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.331e-17\n",
      "‖w_svm‖₂       : 0.07119798345129177\n",
      "‖alpha‖₁       : 0.4191439414487419\n",
      "scores min/max : -1.9044194772837146 2.1985943964157304\n",
      "Mask mean value:  tensor(0.7684, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8975  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.595e-04\n",
      "‖w_svm‖₂       : 0.005598235347927426\n",
      "‖alpha‖₁       : 0.37999999999998946\n",
      "scores min/max : -0.005210919346630732 0.006464874454070235\n",
      "Mask mean value:  tensor(0.5176, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9208  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.958e-14\n",
      "‖w_svm‖₂       : 0.018025967015432333\n",
      "‖alpha‖₁       : 0.6815890958698405\n",
      "scores min/max : -1.9717922339553295 0.05802750625860825\n",
      "Mask mean value:  tensor(0.6144, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2585  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.644e-14\n",
      "‖w_svm‖₂       : 0.00631746937339733\n",
      "‖alpha‖₁       : 0.7005718360875344\n",
      "scores min/max : -2.0020540870955164 0.0213609212220784\n",
      "Mask mean value:  tensor(0.4643, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2599  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.914e-13\n",
      "‖w_svm‖₂       : 0.020129586471275055\n",
      "‖alpha‖₁       : 0.6599999999999997\n",
      "scores min/max : -0.04109263097653165 0.02661351067409306\n",
      "Mask mean value:  tensor(0.4175, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.2850  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.332e-15\n",
      "‖w_svm‖₂       : 0.0044374716571683355\n",
      "‖alpha‖₁       : 0.45999999999999974\n",
      "scores min/max : -0.008540881631713107 -0.006399527912443935\n",
      "Mask mean value:  tensor(0.4655, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.0231  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.078e-15\n",
      "‖w_svm‖₂       : 7.53943189059589e-08\n",
      "‖alpha‖₁       : 0.6599999999999968\n",
      "scores min/max : 1.057643086193167e-07 2.507420771630055e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.117e-09\n",
      "‖w_svm‖₂       : 0.07195564308192007\n",
      "‖alpha‖₁       : 0.5799999999999568\n",
      "scores min/max : -2.813022486732599 1.6326540952388724\n",
      "Mask mean value:  tensor(0.2105, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3676  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.512e-04\n",
      "‖w_svm‖₂       : 0.027230836676043646\n",
      "‖alpha‖₁       : 0.19668111147087997\n",
      "scores min/max : -2.225910357842601 -0.0031792026182216465\n",
      "Mask mean value:  tensor(0.0409, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2795  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.408e-03\n",
      "‖w_svm‖₂       : 1.1385911274567787e-07\n",
      "‖alpha‖₁       : 0.2999999999999936\n",
      "scores min/max : 1.4694103666773663e-07 1.559300148787842e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.367e-07\n",
      "‖w_svm‖₂       : 2.096968045606906e-08\n",
      "‖alpha‖₁       : 0.11999999999999506\n",
      "scores min/max : -3.577607989425037e-08 -2.112518335448476e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.831e-09\n",
      "‖w_svm‖₂       : 4.810324253086693e-08\n",
      "‖alpha‖₁       : 0.1799999999999975\n",
      "scores min/max : 3.90331431961521e-08 5.372009640148782e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.337e-08\n",
      "‖w_svm‖₂       : 0.18309589070764584\n",
      "‖alpha‖₁       : 0.8831461479575221\n",
      "scores min/max : -3.6288774417388567 6.078411665395841\n",
      "Mask mean value:  tensor(0.0929, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2025  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.664e-05\n",
      "‖w_svm‖₂       : 0.028906996948906187\n",
      "‖alpha‖₁       : 0.8523856492426771\n",
      "scores min/max : -2.8932736623307784 1.5876358084031323\n",
      "Mask mean value:  tensor(0.1668, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9622  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.965e-04\n",
      "‖w_svm‖₂       : 1.7511249712086657e-05\n",
      "‖alpha‖₁       : 0.3599999999825563\n",
      "scores min/max : 1.416019617648303e-05 1.5260727223560657e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.801e-17\n",
      "‖w_svm‖₂       : 5.311290167139328e-08\n",
      "‖alpha‖₁       : 0.43999999999998857\n",
      "scores min/max : -1.8908497332540532e-07 -8.510067078397056e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.537e-20\n",
      "‖w_svm‖₂       : 6.681913911233552e-07\n",
      "‖alpha‖₁       : 0.3999999999999998\n",
      "scores min/max : -1.479777537014951e-08 2.9123964900230393e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.309e-09\n",
      "‖w_svm‖₂       : 0.04746510926117853\n",
      "‖alpha‖₁       : 0.9056466102375405\n",
      "scores min/max : -0.7252053289854049 1.8891050172982946\n",
      "Mask mean value:  tensor(0.3700, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6867  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.869e-03\n",
      "‖w_svm‖₂       : 0.00013937893290974604\n",
      "‖alpha‖₁       : 0.43999999999913564\n",
      "scores min/max : -0.0002892641311042939 -0.00014739601450479737\n",
      "Mask mean value:  tensor(0.4990, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0209  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.876e-17\n",
      "‖w_svm‖₂       : 7.207710402287011e-08\n",
      "‖alpha‖₁       : 0.42000000000000004\n",
      "scores min/max : -6.750958625276591e-08 -2.0412873636757205e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.115e-08\n",
      "‖w_svm‖₂       : 8.275000351610703e-08\n",
      "‖alpha‖₁       : 0.3799999999999994\n",
      "scores min/max : -3.977696388791465e-09 1.4180997284682932e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.815e-22\n",
      "‖w_svm‖₂       : 2.7738546469778445e-07\n",
      "‖alpha‖₁       : 0.6599999999999839\n",
      "scores min/max : -6.83070265745526e-07 -5.313062754216026e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.421e-18\n",
      "‖w_svm‖₂       : 1.0032518387765714e-06\n",
      "‖alpha‖₁       : 0.5999999999999979\n",
      "scores min/max : -9.031619601254339e-08 -3.945734757736701e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.583e-19\n",
      "‖w_svm‖₂       : 0.07277986075914936\n",
      "‖alpha‖₁       : 0.6585729142538326\n",
      "scores min/max : -2.027092258466203 4.274733497362969\n",
      "Mask mean value:  tensor(0.3590, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.1126  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.524e-04\n",
      "‖w_svm‖₂       : 0.008405896547841083\n",
      "‖alpha‖₁       : 0.6076928893048077\n",
      "scores min/max : -1.9909279889182914 0.29585667657855397\n",
      "Mask mean value:  tensor(0.5532, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2234  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.953e-15\n",
      "‖w_svm‖₂       : 0.13967025382590878\n",
      "‖alpha‖₁       : 0.8799999999999625\n",
      "scores min/max : -1.3753120827639733 3.724988986449286\n",
      "Mask mean value:  tensor(0.3522, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2663  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.764e-09\n",
      "‖w_svm‖₂       : 2.331459508772167e-07\n",
      "‖alpha‖₁       : 0.6399999999999967\n",
      "scores min/max : 4.2092617503638583e-07 4.418153398309454e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.023e-19\n",
      "‖w_svm‖₂       : 0.020103706014544254\n",
      "‖alpha‖₁       : 0.38343477083502897\n",
      "scores min/max : -1.9595759237019137 0.24292317520784168\n",
      "Mask mean value:  tensor(0.7156, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1529  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.178e-14\n",
      "‖w_svm‖₂       : 0.041815766805345135\n",
      "‖alpha‖₁       : 0.8899830818891757\n",
      "scores min/max : -2.0133691426698235 0.33445744898715085\n",
      "Mask mean value:  tensor(0.2521, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0600  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.654e-05\n",
      "‖w_svm‖₂       : 0.016331891444885904\n",
      "‖alpha‖₁       : 0.8599999999999197\n",
      "scores min/max : -0.0468712252521952 -0.033198910651267524\n",
      "Mask mean value:  tensor(0.3240, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0816  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.370e-14\n",
      "‖w_svm‖₂       : 4.244014739109378e-07\n",
      "‖alpha‖₁       : 0.7199999999999992\n",
      "scores min/max : 1.964297849492446e-07 2.5360697470407643e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.127e-21\n",
      "‖w_svm‖₂       : 0.022200070916918793\n",
      "‖alpha‖₁       : 0.815094650100783\n",
      "scores min/max : -11.515187969731395 2.03261606225551\n",
      "Mask mean value:  tensor(0.7114, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9167  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.566e-06\n",
      "‖w_svm‖₂       : 1.100693306076369e-07\n",
      "‖alpha‖₁       : 0.5199999999999994\n",
      "scores min/max : 3.0811217517808346e-07 3.29537270614911e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.840e-19\n",
      "‖w_svm‖₂       : 5.691271219756024e-08\n",
      "‖alpha‖₁       : 0.23999999999998278\n",
      "scores min/max : -5.950101743738285e-08 -4.497801506463954e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.519e-20\n",
      "‖w_svm‖₂       : 0.030121495722501408\n",
      "‖alpha‖₁       : 0.8985791670879749\n",
      "scores min/max : -0.7368936025396989 2.0232890448711705\n",
      "Mask mean value:  tensor(0.8601, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3271  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.338e-15\n",
      "‖w_svm‖₂       : 0.03880372428568515\n",
      "‖alpha‖₁       : 0.9199993142420885\n",
      "scores min/max : -0.28987574324563503 0.25983882680670956\n",
      "Mask mean value:  tensor(0.1926, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9577  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 8.475e-03\n",
      "‖w_svm‖₂       : 0.09909191801726705\n",
      "‖alpha‖₁       : 0.8733033791590858\n",
      "scores min/max : -12.156631858797855 2.1704025107563547\n",
      "Mask mean value:  tensor(0.5736, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.6991  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.932e-02\n",
      "‖w_svm‖₂       : 0.05517277957602672\n",
      "‖alpha‖₁       : 0.5763219674117468\n",
      "scores min/max : -1.9567961742384021 0.871640292943057\n",
      "Mask mean value:  tensor(0.7144, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9422  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.094e-03\n",
      "‖w_svm‖₂       : 0.020698704360753786\n",
      "‖alpha‖₁       : 0.7799999999999989\n",
      "scores min/max : -0.03539786198768032 0.036513585008471905\n",
      "Mask mean value:  tensor(0.3578, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.5832  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.449e-15\n",
      "‖w_svm‖₂       : 0.0008835734255877213\n",
      "‖alpha‖₁       : 0.8199999999998311\n",
      "scores min/max : 0.000951502677805284 0.002535107548282834\n",
      "Mask mean value:  tensor(0.5098, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5896  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.072e-16\n",
      "‖w_svm‖₂       : 0.00036526917528997876\n",
      "‖alpha‖₁       : 0.7399999999999997\n",
      "scores min/max : -0.0004028849157500983 -0.00038586740890114886\n",
      "Mask mean value:  tensor(0.4981, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0601  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.320e-17\n",
      "‖w_svm‖₂       : 0.08174049247617496\n",
      "‖alpha‖₁       : 0.47192479014214783\n",
      "scores min/max : -2.2755874898798103 2.803659116999001\n",
      "Mask mean value:  tensor(0.1221, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.3212  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.375e-02\n",
      "‖w_svm‖₂       : 5.335393534110096e-08\n",
      "‖alpha‖₁       : 0.11999999999998949\n",
      "scores min/max : -9.748342716693123e-08 -6.665997323970108e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.101e-07\n",
      "‖w_svm‖₂       : 0.060440298312552876\n",
      "‖alpha‖₁       : 0.8986452372553038\n",
      "scores min/max : -1.6938618546144513 3.719346266141565\n",
      "Mask mean value:  tensor(0.4430, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5906  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.949e-03\n",
      "‖w_svm‖₂       : 3.7732372210338497e-07\n",
      "‖alpha‖₁       : 0.27999999999999564\n",
      "scores min/max : -1.3270481150333256e-06 -1.2653024310595566e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.872e-19\n",
      "‖w_svm‖₂       : 1.0377948585630194e-07\n",
      "‖alpha‖₁       : 0.2399999999999974\n",
      "scores min/max : 6.013730574196484e-08 7.091803468658216e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.385e-21\n",
      "‖w_svm‖₂       : 2.0745643859516559e-07\n",
      "‖alpha‖₁       : 0.37999999999553763\n",
      "scores min/max : -1.3522760195219485e-07 -1.1669277761864431e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.451e-07\n",
      "‖w_svm‖₂       : 0.040683472418878916\n",
      "‖alpha‖₁       : 0.7294186312223996\n",
      "scores min/max : -0.2586055908942494 2.0222765695360634\n",
      "Mask mean value:  tensor(0.6851, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.0991  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.548e-03\n",
      "‖w_svm‖₂       : 1.2032770424282102e-06\n",
      "‖alpha‖₁       : 0.31999999999749684\n",
      "scores min/max : -2.6909611969244285e-06 -2.5715930288425684e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4897  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.116e-18\n",
      "‖w_svm‖₂       : 0.04134482463024335\n",
      "‖alpha‖₁       : 0.6599999999999899\n",
      "scores min/max : -0.23031033683765978 0.1572735674758008\n",
      "Mask mean value:  tensor(0.4593, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8507  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.452e-13\n",
      "‖w_svm‖₂       : 0.02935598541051508\n",
      "‖alpha‖₁       : 0.5511500375728589\n",
      "scores min/max : -3.4831934551955244 1.0865382024559929\n",
      "Mask mean value:  tensor(0.1450, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1540  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.045e-03\n",
      "‖w_svm‖₂       : 0.00021356367372671\n",
      "‖alpha‖₁       : 0.6199999999961581\n",
      "scores min/max : 9.572985623430754e-06 2.573422075255335e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.600e-17\n",
      "‖w_svm‖₂       : 1.942856324374766e-07\n",
      "‖alpha‖₁       : 0.37999999999994893\n",
      "scores min/max : -2.457758473803398e-08 1.134693960877582e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.629e-19\n",
      "‖w_svm‖₂       : 0.04598437112247389\n",
      "‖alpha‖₁       : 0.9389295162200666\n",
      "scores min/max : -2.468613280240693 1.57989371826012\n",
      "Mask mean value:  tensor(0.1419, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3927  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.210e-03\n",
      "‖w_svm‖₂       : 0.08508476256490626\n",
      "‖alpha‖₁       : 0.5779122872753892\n",
      "scores min/max : -2.1194396201128813 5.804282722114732\n",
      "Mask mean value:  tensor(0.2760, dtype=torch.float64)\n",
      "max feasible return = 0.1108  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.9529332624488144e-07\n",
      "‖alpha‖₁       : 0.5799999999999812\n",
      "scores min/max : -3.948677466831591e-07 -3.038441949442797e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0248384889994839e-07\n",
      "‖alpha‖₁       : 0.29999999999999916\n",
      "scores min/max : 3.795865143127758e-08 4.293942267688006e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 6.832808035212389e-08\n",
      "‖alpha‖₁       : 0.5999999999999809\n",
      "scores min/max : 2.5352995674952418e-08 6.213820252153885e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.582052682448694e-08\n",
      "‖alpha‖₁       : 0.3799999999999879\n",
      "scores min/max : -5.978214395361242e-09 3.451055043790647e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 8.634616017714372e-06\n",
      "‖alpha‖₁       : 0.31999999987351435\n",
      "scores min/max : 8.753449333501351e-06 1.5031843850560963e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64)\n",
      "max feasible return = 2.4260  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.049534484311032224\n",
      "‖alpha‖₁       : 0.7397129024593336\n",
      "scores min/max : -3.47433935146029 2.158875251730682\n",
      "Mask mean value:  tensor(0.8857, dtype=torch.float64)\n",
      "max feasible return = -0.9320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.027994784264095303\n",
      "‖alpha‖₁       : 0.6151651253128645\n",
      "scores min/max : -1.9261559929294663 1.2032549233254768\n",
      "Mask mean value:  tensor(0.7651, dtype=torch.float64)\n",
      "max feasible return = 2.6731  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.6508479457270265e-07\n",
      "‖alpha‖₁       : 0.4599999999999884\n",
      "scores min/max : 1.673614260358246e-07 7.619537907351293e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.3679760344210815e-07\n",
      "‖alpha‖₁       : 0.5199999999999838\n",
      "scores min/max : 9.152719759535084e-08 1.0444535639917116e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.554660780854861e-07\n",
      "‖alpha‖₁       : 0.5799999999999823\n",
      "scores min/max : -2.480341558381551e-07 -2.1170503485372236e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.04405454913183167\n",
      "‖alpha‖₁       : 0.6878003601684635\n",
      "scores min/max : -0.4370619489851961 1.9515682311852203\n",
      "Mask mean value:  tensor(0.3840, dtype=torch.float64)\n",
      "max feasible return = 0.0533  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03267091186398371\n",
      "‖alpha‖₁       : 0.4222416462192313\n",
      "scores min/max : -3.88712582485383 5.263193723357206\n",
      "Mask mean value:  tensor(0.0930, dtype=torch.float64)\n",
      "max feasible return = 0.5868  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.043248169775237676\n",
      "‖alpha‖₁       : 0.8211217653644316\n",
      "scores min/max : -5.024314360035241 3.1446132907393647\n",
      "Mask mean value:  tensor(0.9570, dtype=torch.float64)\n",
      "max feasible return = -3.6496  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.007088295529697444\n",
      "‖alpha‖₁       : 0.7999999999998877\n",
      "scores min/max : -0.0318500525469851 0.0020191909104146245\n",
      "Mask mean value:  tensor(0.4676, dtype=torch.float64)\n",
      "max feasible return = -0.1699  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.0673723062787075e-07\n",
      "‖alpha‖₁       : 0.6199999999999568\n",
      "scores min/max : 6.382819386556922e-08 8.037418814101507e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.7492676728635654e-08\n",
      "‖alpha‖₁       : 0.43999999999999984\n",
      "scores min/max : 6.420725231046193e-09 1.5990646175970976e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.2936671828419718e-07\n",
      "‖alpha‖₁       : 0.2799999999999808\n",
      "scores min/max : -2.639448324692589e-07 -2.4602717896422373e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  37 | train 0.005329 | val 0.006627\n",
      "-----------------------------------------Epoch:  38 ----------------------------------------\n",
      "‖w_svm‖₂       : 1.1044888163613977e-07\n",
      "‖alpha‖₁       : 0.519999999999998\n",
      "scores min/max : 3.135366084953426e-07 3.3494882306707933e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.841e-19\n",
      "‖w_svm‖₂       : 2.3356605226033673e-07\n",
      "‖alpha‖₁       : 0.6399999999999965\n",
      "scores min/max : 4.1475100504420567e-07 4.3563913343311694e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.021e-19\n",
      "‖w_svm‖₂       : 1.045749244454582e-07\n",
      "‖alpha‖₁       : 0.23999999999999339\n",
      "scores min/max : 6.573396554021911e-08 7.65553322712116e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.438e-21\n",
      "‖w_svm‖₂       : 3.794758093084109e-07\n",
      "‖alpha‖₁       : 0.2799999999999678\n",
      "scores min/max : -1.3967466281645703e-06 -1.334466894329562e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.931e-19\n",
      "‖w_svm‖₂       : 1.9659404074491572e-07\n",
      "‖alpha‖₁       : 0.41999999999996973\n",
      "scores min/max : -5.055815040975967e-07 -4.833409838763353e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.880e-19\n",
      "‖w_svm‖₂       : 0.14527954880615757\n",
      "‖alpha‖₁       : 0.7590788173848647\n",
      "scores min/max : -1.7653099871120546 2.2122565043780305\n",
      "Mask mean value:  tensor(0.8432, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3545  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.954e-11\n",
      "‖w_svm‖₂       : 0.07208673332702166\n",
      "‖alpha‖₁       : 0.579999999999936\n",
      "scores min/max : -2.820838679028477 1.6435460049178543\n",
      "Mask mean value:  tensor(0.2189, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3779  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.369e-04\n",
      "‖w_svm‖₂       : 0.016454435735543796\n",
      "‖alpha‖₁       : 0.8599999999999983\n",
      "scores min/max : -0.05395788125964922 -0.04022640414813071\n",
      "Mask mean value:  tensor(0.2940, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0728  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.809e-14\n",
      "‖w_svm‖₂       : 0.0003068945546415643\n",
      "‖alpha‖₁       : 0.4199999999592891\n",
      "scores min/max : 0.00012379372411447126 0.00044247685329436943\n",
      "Mask mean value:  tensor(0.5021, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9996  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.007e-15\n",
      "‖w_svm‖₂       : 5.313466237744865e-08\n",
      "‖alpha‖₁       : 0.4399999999999877\n",
      "scores min/max : -1.9764562920491215e-07 -9.367318878329713e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.635e-20\n",
      "‖w_svm‖₂       : 1.930179175576001e-07\n",
      "‖alpha‖₁       : 0.3799999999999998\n",
      "scores min/max : -1.6596228893154002e-08 1.9064486715257657e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.604e-19\n",
      "‖w_svm‖₂       : 0.0032388629142649722\n",
      "‖alpha‖₁       : 0.5799999999999885\n",
      "scores min/max : 0.0026072064864255486 0.00393463957643525\n",
      "Mask mean value:  tensor(0.5183, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3296  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.493e-13\n",
      "‖w_svm‖₂       : 0.046009042131612866\n",
      "‖alpha‖₁       : 0.9389325780526238\n",
      "scores min/max : -2.465807134706986 1.582732370694801\n",
      "Mask mean value:  tensor(0.1436, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3964  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.220e-03\n",
      "‖w_svm‖₂       : 6.691582877610485e-07\n",
      "‖alpha‖₁       : 0.39999999999989544\n",
      "scores min/max : -4.844343599134486e-08 2.6009360574681185e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.291e-09\n",
      "‖w_svm‖₂       : 0.00016146147703999914\n",
      "‖alpha‖₁       : 0.6399999999999002\n",
      "scores min/max : -0.00016549232021352661 -0.0001555451247936021\n",
      "Mask mean value:  tensor(0.4992, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7632  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.407e-17\n",
      "‖w_svm‖₂       : 2.077758290454486e-08\n",
      "‖alpha‖₁       : 0.11999999999999447\n",
      "scores min/max : -3.59940950079037e-08 -2.1316362306328254e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.839e-09\n",
      "‖w_svm‖₂       : 7.591994393543197e-08\n",
      "‖alpha‖₁       : 0.5399999999999359\n",
      "scores min/max : -1.892838932089624e-07 -1.6768314712121524e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.928e-20\n",
      "‖w_svm‖₂       : 0.015211323986337589\n",
      "‖alpha‖₁       : 0.8599999999999974\n",
      "scores min/max : -0.051006709798328685 0.0006906938150091253\n",
      "Mask mean value:  tensor(0.3548, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4480  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.779e-05\n",
      "‖w_svm‖₂       : 0.01986991308730298\n",
      "‖alpha‖₁       : 0.6599999999999998\n",
      "scores min/max : -0.04064648008196402 0.02477476565151625\n",
      "Mask mean value:  tensor(0.4162, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.2726  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.295e-15\n",
      "‖w_svm‖₂       : 0.016293693341698116\n",
      "‖alpha‖₁       : 0.596297337439158\n",
      "scores min/max : -2.9125821686224547 2.273504996092889\n",
      "Mask mean value:  tensor(0.9163, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.5072  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.523e-07\n",
      "‖w_svm‖₂       : 8.282330911896866e-08\n",
      "‖alpha‖₁       : 0.3799999999999994\n",
      "scores min/max : 1.0626434268265113e-08 2.878599783610689e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.782e-22\n",
      "‖w_svm‖₂       : 0.05489225597273487\n",
      "‖alpha‖₁       : 0.5763042998842435\n",
      "scores min/max : -1.954216427438722 0.8740568938110432\n",
      "Mask mean value:  tensor(0.7221, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.125e-03\n",
      "‖w_svm‖₂       : 5.690125205162409e-08\n",
      "‖alpha‖₁       : 0.23999999999998312\n",
      "scores min/max : -5.0425214828163116e-08 -3.590424444622017e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.522e-20\n",
      "‖w_svm‖₂       : 7.218810158274527e-08\n",
      "‖alpha‖₁       : 0.41999999999999893\n",
      "scores min/max : -6.842786529592146e-08 -2.1276773799985174e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.190e-08\n",
      "‖w_svm‖₂       : 0.09853820235150232\n",
      "‖alpha‖₁       : 0.8732996079124375\n",
      "scores min/max : -12.20501729469226 2.1205912450553073\n",
      "Mask mean value:  tensor(0.4520, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3498  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 7.921e-02\n",
      "‖w_svm‖₂       : 0.008490346732520528\n",
      "‖alpha‖₁       : 0.6076942160735647\n",
      "scores min/max : -1.9908943637163792 0.2958671307541746\n",
      "Mask mean value:  tensor(0.5533, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2235  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.008e-15\n",
      "‖w_svm‖₂       : 0.006355615570247552\n",
      "‖alpha‖₁       : 0.7005721103509149\n",
      "scores min/max : -2.003285918918733 0.020134889081270795\n",
      "Mask mean value:  tensor(0.4584, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2567  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.697e-06\n",
      "‖w_svm‖₂       : 0.027123453385707347\n",
      "‖alpha‖₁       : 0.19668105206409106\n",
      "scores min/max : -2.2209298197134135 0.0015279010941536136\n",
      "Mask mean value:  tensor(0.0443, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3022  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.219e-03\n",
      "‖w_svm‖₂       : 0.020362308716927867\n",
      "‖alpha‖₁       : 0.3834455111954005\n",
      "scores min/max : -1.9608155606118651 0.24168002974675779\n",
      "Mask mean value:  tensor(0.7111, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1522  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.469e-14\n",
      "‖w_svm‖₂       : 0.020579941420056008\n",
      "‖alpha‖₁       : 0.8233725850521584\n",
      "scores min/max : -1.9047076143129211 1.4920648265269958\n",
      "Mask mean value:  tensor(0.6848, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.3486  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.661e-05\n",
      "‖w_svm‖₂       : 1.0677649476164742e-06\n",
      "‖alpha‖₁       : 0.49999999999999883\n",
      "scores min/max : -2.6193807356819343e-07 -6.915503401245282e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.942e-19\n",
      "‖w_svm‖₂       : 0.04104570816084394\n",
      "‖alpha‖₁       : 0.6599999999999877\n",
      "scores min/max : -0.22615041005311098 0.1555722200082632\n",
      "Mask mean value:  tensor(0.4629, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8593  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.210e-13\n",
      "‖w_svm‖₂       : 0.06047610639193168\n",
      "‖alpha‖₁       : 0.8986300405537764\n",
      "scores min/max : -1.6900642900887988 3.723627774610545\n",
      "Mask mean value:  tensor(0.4513, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5970  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.179e-03\n",
      "‖w_svm‖₂       : 0.005760400107431199\n",
      "‖alpha‖₁       : 0.5599999999999993\n",
      "scores min/max : 0.005906499148887294 0.007632932395547741\n",
      "Mask mean value:  tensor(0.5311, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1132  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 2.044e-05\n",
      "‖w_svm‖₂       : 0.051602873178562034\n",
      "‖alpha‖₁       : 0.8271691087039691\n",
      "scores min/max : -1.9499889744575354 0.4008805632901307\n",
      "Mask mean value:  tensor(0.5249, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0135  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.683e-02\n",
      "‖w_svm‖₂       : 0.041707004782588264\n",
      "‖alpha‖₁       : 0.8899808250273299\n",
      "scores min/max : -2.0103217630469716 0.337633781738611\n",
      "Mask mean value:  tensor(0.2609, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0641  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.702e-09\n",
      "‖w_svm‖₂       : 1.6855124127026038e-07\n",
      "‖alpha‖₁       : 0.23999999999998098\n",
      "scores min/max : 2.808083116186677e-07 2.963507415258073e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.686e-20\n",
      "‖w_svm‖₂       : 4.8490428845999015e-08\n",
      "‖alpha‖₁       : 0.17999999999999694\n",
      "scores min/max : 4.269800081824143e-08 5.7397554033578754e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.495e-08\n",
      "‖w_svm‖₂       : 2.3948117907493797e-07\n",
      "‖alpha‖₁       : 0.579999999999976\n",
      "scores min/max : -3.078666001650231e-07 -2.9219692525934635e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.108e-19\n",
      "‖w_svm‖₂       : 0.07307070221969646\n",
      "‖alpha‖₁       : 0.6586456550766068\n",
      "scores min/max : -2.03580798154447 4.273362183256258\n",
      "Mask mean value:  tensor(0.3433, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.9593  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.894e-04\n",
      "‖w_svm‖₂       : 0.13938180556414753\n",
      "‖alpha‖₁       : 0.8799999999999901\n",
      "scores min/max : -1.367693664410566 3.700568060092095\n",
      "Mask mean value:  tensor(0.3502, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2655  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.400e-08\n",
      "‖w_svm‖₂       : 0.03006409247795205\n",
      "‖alpha‖₁       : 0.89858995428011\n",
      "scores min/max : -0.736652984051716 2.0235516798559825\n",
      "Mask mean value:  tensor(0.8605, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3258  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.357e-15\n",
      "‖w_svm‖₂       : 0.0820828761210294\n",
      "‖alpha‖₁       : 0.47196423351301586\n",
      "scores min/max : -2.2937561105662754 2.7853068795832607\n",
      "Mask mean value:  tensor(0.1133, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.2324  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.507e-02\n",
      "‖w_svm‖₂       : 1.0091453191208974e-06\n",
      "‖alpha‖₁       : 0.599999999999997\n",
      "scores min/max : -1.3582171932082437e-07 -8.496485168667204e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.623e-19\n",
      "‖w_svm‖₂       : 2.2640584219176024e-07\n",
      "‖alpha‖₁       : 0.2599999999999947\n",
      "scores min/max : 1.5308496214021322e-07 1.6586864810468e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.964e-17\n",
      "‖w_svm‖₂       : 0.0008923608967277614\n",
      "‖alpha‖₁       : 0.8199999999999996\n",
      "scores min/max : 0.0010794509509411193 0.0026987486405377742\n",
      "Mask mean value:  tensor(0.5105, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5934  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.076e-16\n",
      "‖w_svm‖₂       : 0.004464832767047723\n",
      "‖alpha‖₁       : 0.45999999999999935\n",
      "scores min/max : -0.008828917840656723 -0.006653018561112625\n",
      "Mask mean value:  tensor(0.4642, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.0174  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.082e-15\n",
      "‖w_svm‖₂       : 0.0709341358860462\n",
      "‖alpha‖₁       : 0.41917498311164697\n",
      "scores min/max : -1.9132485061550029 2.174657614038501\n",
      "Mask mean value:  tensor(0.7603, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8962  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.450e-04\n",
      "‖w_svm‖₂       : 0.018108483754524562\n",
      "‖alpha‖₁       : 0.6815939977988273\n",
      "scores min/max : -1.9707322009820183 0.05881154486195027\n",
      "Mask mean value:  tensor(0.6192, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2615  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.598e-13\n",
      "‖w_svm‖₂       : 2.790750319724658e-07\n",
      "‖alpha‖₁       : 0.6599999999999899\n",
      "scores min/max : -7.016299492059976e-07 -5.499037003600123e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.446e-18\n",
      "‖w_svm‖₂       : 7.539508065138845e-08\n",
      "‖alpha‖₁       : 0.6599999999999977\n",
      "scores min/max : 1.103805608199041e-07 2.5536119677283744e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.000e-08\n",
      "‖w_svm‖₂       : 0.029085089647106398\n",
      "‖alpha‖₁       : 0.8523899875672893\n",
      "scores min/max : -2.8963472905938934 1.5845618552842393\n",
      "Mask mean value:  tensor(0.1652, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9608  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.126e-03\n",
      "‖w_svm‖₂       : 0.00018603776673137276\n",
      "‖alpha‖₁       : 0.8199999999999986\n",
      "scores min/max : -9.52588580288869e-05 -8.098659656008238e-05\n",
      "Mask mean value:  tensor(0.4996, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6964  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.341e-17\n",
      "‖w_svm‖₂       : 2.0892948915879928e-07\n",
      "‖alpha‖₁       : 0.3799999999953948\n",
      "scores min/max : -1.209771096158964e-07 -1.0233026019459389e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.044e-07\n",
      "‖w_svm‖₂       : 1.1480759423534752e-07\n",
      "‖alpha‖₁       : 0.29999999999999366\n",
      "scores min/max : 1.5637557721151437e-07 1.6536449789700442e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.398e-07\n",
      "‖w_svm‖₂       : 0.04765356687806566\n",
      "‖alpha‖₁       : 0.9056469700924771\n",
      "scores min/max : -0.7293497097887508 1.886111909893069\n",
      "Mask mean value:  tensor(0.3606, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6694  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.841e-03\n",
      "‖w_svm‖₂       : 3.996211968656489e-06\n",
      "‖alpha‖₁       : 0.41999999999000065\n",
      "scores min/max : -3.502075995720135e-06 -1.7321362614194857e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1229  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.807e-05\n",
      "‖w_svm‖₂       : 0.18326240637549365\n",
      "‖alpha‖₁       : 0.8832402269678807\n",
      "scores min/max : -3.6406719722344176 6.069911071923278\n",
      "Mask mean value:  tensor(0.0912, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1988  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.574e-03\n",
      "‖w_svm‖₂       : 1.760669908208502e-05\n",
      "‖alpha‖₁       : 0.3599999999820952\n",
      "scores min/max : 1.433229434383263e-05 1.543391150525032e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.915e-17\n",
      "‖w_svm‖₂       : 0.03812824276545659\n",
      "‖alpha‖₁       : 0.9199995074359011\n",
      "scores min/max : -0.2819997316335781 0.25337388668834415\n",
      "Mask mean value:  tensor(0.1921, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9557  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 8.265e-03\n",
      "‖w_svm‖₂       : 4.2875137215828343e-07\n",
      "‖alpha‖₁       : 0.7199999999999994\n",
      "scores min/max : 1.903738039249989e-07 2.476036025356713e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.192e-21\n",
      "‖w_svm‖₂       : 0.04041917639747056\n",
      "‖alpha‖₁       : 0.7294185715522291\n",
      "scores min/max : -0.2610269739798134 2.0197944375952352\n",
      "Mask mean value:  tensor(0.6779, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.0682  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.632e-03\n",
      "‖w_svm‖₂       : 0.005640021994136584\n",
      "‖alpha‖₁       : 0.37999999999998946\n",
      "scores min/max : -0.005874860767917517 0.005944854330618969\n",
      "Mask mean value:  tensor(0.5148, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9157  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.115e-14\n",
      "‖w_svm‖₂       : 8.167470892505762e-08\n",
      "‖alpha‖₁       : 0.17999999999999894\n",
      "scores min/max : -1.9278473313214452e-07 4.102110423405851e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.287e-21\n",
      "‖w_svm‖₂       : 1.208636038663545e-06\n",
      "‖alpha‖₁       : 0.3199999999974534\n",
      "scores min/max : -2.689086250603003e-06 -2.5698201444804877e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4897  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.145e-18\n",
      "‖w_svm‖₂       : 0.0005147185163832754\n",
      "‖alpha‖₁       : 0.43999999999998224\n",
      "scores min/max : -0.0008866124123280694 -0.0006134645955634885\n",
      "Mask mean value:  tensor(0.4958, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6414  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.302e-16\n",
      "‖w_svm‖₂       : 7.291684443536562e-08\n",
      "‖alpha‖₁       : 0.1399999999999862\n",
      "scores min/max : 1.4550743204047646e-08 2.0372899550707408e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.529e-08\n",
      "‖w_svm‖₂       : 5.368598389735761e-08\n",
      "‖alpha‖₁       : 0.11999999999998026\n",
      "scores min/max : -9.902362041793353e-08 -6.808297261142465e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.507e-08\n",
      "‖w_svm‖₂       : 0.020684530961544163\n",
      "‖alpha‖₁       : 0.7799999999999998\n",
      "scores min/max : -0.03391486524583229 0.03701948264078747\n",
      "Mask mean value:  tensor(0.3641, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6092  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.339e-15\n",
      "‖w_svm‖₂       : 0.044179296670000756\n",
      "‖alpha‖₁       : 0.9407367641403808\n",
      "scores min/max : -1.8328650612352533 0.3009612662424771\n",
      "Mask mean value:  tensor(0.4064, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3241  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.318e-14\n",
      "‖w_svm‖₂       : 0.022017234364685435\n",
      "‖alpha‖₁       : 0.8150959348444617\n",
      "scores min/max : -11.51423684801798 2.032735565377588\n",
      "Mask mean value:  tensor(0.7118, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9183  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.417e-06\n",
      "‖w_svm‖₂       : 0.00036399468572782264\n",
      "‖alpha‖₁       : 0.7399999999999997\n",
      "scores min/max : -0.0003981088402344702 -0.00038120984056003377\n",
      "Mask mean value:  tensor(0.4981, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0602  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.342e-17\n",
      "‖w_svm‖₂       : 0.0001428788266911728\n",
      "‖alpha‖₁       : 0.4399999998252794\n",
      "scores min/max : -0.0003038097714595418 -0.00015505458815437672\n",
      "Mask mean value:  tensor(0.4990, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0205  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.972e-17\n",
      "‖w_svm‖₂       : 0.0002126011743608702\n",
      "‖alpha‖₁       : 0.6199999999933532\n",
      "scores min/max : 8.507983120512697e-06 2.452283449341322e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.649e-17\n",
      "‖w_svm‖₂       : 0.1379777542557621\n",
      "‖alpha‖₁       : 0.6549879790553198\n",
      "scores min/max : -18.072775542333485 1.8224952662255556\n",
      "Mask mean value:  tensor(0.2594, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2817  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.327e-03\n",
      "‖w_svm‖₂       : 0.02944308251613115\n",
      "‖alpha‖₁       : 0.5511592848422204\n",
      "scores min/max : -3.478333061956409 1.091256470629679\n",
      "Mask mean value:  tensor(0.1477, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1692  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.826e-03\n",
      "‖w_svm‖₂       : 0.08535647854006795\n",
      "‖alpha‖₁       : 0.5779724031198219\n",
      "scores min/max : -2.132115511096659 5.790666422732544\n",
      "Mask mean value:  tensor(0.2536, dtype=torch.float64)\n",
      "max feasible return = 0.1069  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.943791712444005e-07\n",
      "‖alpha‖₁       : 0.5799999999999985\n",
      "scores min/max : -3.7493924485523705e-07 -2.8415359839178174e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0281086371713968e-07\n",
      "‖alpha‖₁       : 0.2999999999999993\n",
      "scores min/max : 3.992459602957021e-08 4.4901329907407294e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 6.83031223773469e-08\n",
      "‖alpha‖₁       : 0.5999999999999185\n",
      "scores min/max : 2.6824876309253625e-08 6.357280108014758e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.5665400653289004e-08\n",
      "‖alpha‖₁       : 0.3799999999999983\n",
      "scores min/max : -5.684516728783077e-09 3.693819903762236e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 5.739953700045656e-06\n",
      "‖alpha‖₁       : 0.31999999994111883\n",
      "scores min/max : 6.624498246370778e-06 1.0339705683113299e-05\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4259  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04909337391692666\n",
      "‖alpha‖₁       : 0.7397169852224219\n",
      "scores min/max : -3.46420278438879 2.1688830270922272\n",
      "Mask mean value:  tensor(0.8935, dtype=torch.float64)\n",
      "max feasible return = -0.9370  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.027744315433844577\n",
      "‖alpha‖₁       : 0.6151654514806878\n",
      "scores min/max : -1.9320667253433643 1.1971766758066413\n",
      "Mask mean value:  tensor(0.7498, dtype=torch.float64)\n",
      "max feasible return = 2.6183  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.6701799243394073e-07\n",
      "‖alpha‖₁       : 0.45999999999999097\n",
      "scores min/max : 2.2916394255003384e-07 8.23747583447519e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.378595895064013e-07\n",
      "‖alpha‖₁       : 0.5199999999999784\n",
      "scores min/max : 8.263578845252065e-08 9.55579354708398e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.5738167808899393e-07\n",
      "‖alpha‖₁       : 0.5799999999999654\n",
      "scores min/max : -2.5688015802067443e-07 -2.2048518275331423e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.04438568558924207\n",
      "‖alpha‖₁       : 0.6878287782208269\n",
      "scores min/max : -0.4454562359714286 1.9431593794489128\n",
      "Mask mean value:  tensor(0.3552, dtype=torch.float64)\n",
      "max feasible return = 0.0812  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03286647120244768\n",
      "‖alpha‖₁       : 0.4222576338852783\n",
      "scores min/max : -3.882627300430183 5.268057463072617\n",
      "Mask mean value:  tensor(0.0954, dtype=torch.float64)\n",
      "max feasible return = 0.6025  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04324841454764801\n",
      "‖alpha‖₁       : 0.8211210967258683\n",
      "scores min/max : -5.014696957365466 3.1542780959108243\n",
      "Mask mean value:  tensor(0.9585, dtype=torch.float64)\n",
      "max feasible return = -3.6577  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.007007590491720052\n",
      "‖alpha‖₁       : 0.7999999999998695\n",
      "scores min/max : -0.03198408156877132 0.0010513259119426964\n",
      "Mask mean value:  tensor(0.4638, dtype=torch.float64)\n",
      "max feasible return = -0.1686  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.0832540731358485e-07\n",
      "‖alpha‖₁       : 0.6199999999999573\n",
      "scores min/max : 8.742240849999907e-08 1.0396768262456668e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.784500357675029e-08\n",
      "‖alpha‖₁       : 0.4399999999999998\n",
      "scores min/max : 6.227994810693274e-09 1.5797747357486386e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.3060160540349729e-07\n",
      "‖alpha‖₁       : 0.2799999999999801\n",
      "scores min/max : -2.780275880223018e-07 -2.6010973553966196e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  38 | train 0.005331 | val 0.006692\n",
      "-----------------------------------------Epoch:  39 ----------------------------------------\n",
      "‖w_svm‖₂       : 0.0008897426074231246\n",
      "‖alpha‖₁       : 0.8199999999999992\n",
      "scores min/max : 0.0010374817104876824 0.0026432564821783164\n",
      "Mask mean value:  tensor(0.5103, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5921  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.080e-16\n",
      "‖w_svm‖₂       : 8.178752578898504e-08\n",
      "‖alpha‖₁       : 0.1799999999999989\n",
      "scores min/max : -1.9299665040261468e-07 4.081123258500005e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.328e-21\n",
      "‖w_svm‖₂       : 0.029479466215549345\n",
      "‖alpha‖₁       : 0.5511614680429573\n",
      "scores min/max : -3.4774931679797154 1.09209505820517\n",
      "Mask mean value:  tensor(0.1482, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1719  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.776e-03\n",
      "‖w_svm‖₂       : 0.005634128544541315\n",
      "‖alpha‖₁       : 0.37999999999999584\n",
      "scores min/max : -0.005725324787749231 0.0060724540589848705\n",
      "Mask mean value:  tensor(0.5155, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9169  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.109e-14\n",
      "‖w_svm‖₂       : 2.079889254443707e-08\n",
      "‖alpha‖₁       : 0.11999999999999454\n",
      "scores min/max : -3.5449085445818804e-08 -2.0775045527634654e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.833e-09\n",
      "‖w_svm‖₂       : 0.00018641235648194482\n",
      "‖alpha‖₁       : 0.8199999999999993\n",
      "scores min/max : -9.496176480313479e-05 -8.062782792167577e-05\n",
      "Mask mean value:  tensor(0.4996, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6964  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.349e-17\n",
      "‖w_svm‖₂       : 0.04585158349916781\n",
      "‖alpha‖₁       : 0.938926301549987\n",
      "scores min/max : -2.46494768681076 1.5840057975594233\n",
      "Mask mean value:  tensor(0.1444, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3981  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.279e-03\n",
      "‖w_svm‖₂       : 2.786603764592106e-07\n",
      "‖alpha‖₁       : 0.6599999999999824\n",
      "scores min/max : -6.96954624666828e-07 -5.45186403025506e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.449e-18\n",
      "‖w_svm‖₂       : 7.305084378828805e-08\n",
      "‖alpha‖₁       : 0.13999999999998067\n",
      "scores min/max : 1.4437397588288969e-08 2.028326493252965e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.547e-08\n",
      "‖w_svm‖₂       : 5.73030907203338e-08\n",
      "‖alpha‖₁       : 0.23999999999998933\n",
      "scores min/max : -5.570643367363067e-08 -4.123116023974834e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.563e-20\n",
      "‖w_svm‖₂       : 0.02067277401441507\n",
      "‖alpha‖₁       : 0.7799999999999999\n",
      "scores min/max : -0.03324188666816311 0.037671570475297234\n",
      "Mask mean value:  tensor(0.3672, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6221  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.200e-15\n",
      "‖w_svm‖₂       : 5.292304530986414e-08\n",
      "‖alpha‖₁       : 0.4399999999999881\n",
      "scores min/max : -1.9466277597019766e-07 -9.069198346190273e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.579e-20\n",
      "‖w_svm‖₂       : 5.3416572513315985e-08\n",
      "‖alpha‖₁       : 0.11999999999998924\n",
      "scores min/max : -9.844850826543916e-08 -6.762274927205609e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.292e-07\n",
      "‖w_svm‖₂       : 1.9686207069258728e-07\n",
      "‖alpha‖₁       : 0.4199999999999695\n",
      "scores min/max : -4.741806312293802e-07 -4.51944694878817e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.877e-19\n",
      "‖w_svm‖₂       : 0.01629493898309864\n",
      "‖alpha‖₁       : 0.5962994128741048\n",
      "scores min/max : -2.893069984927248 2.281327769189685\n",
      "Mask mean value:  tensor(0.9198, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.5106  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.733e-06\n",
      "‖w_svm‖₂       : 0.022018939499755027\n",
      "‖alpha‖₁       : 0.8150969906956019\n",
      "scores min/max : -11.513067342202502 2.033887262698144\n",
      "Mask mean value:  tensor(0.7142, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9241  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.694e-06\n",
      "‖w_svm‖₂       : 0.029386111857926844\n",
      "‖alpha‖₁       : 0.8524075590580287\n",
      "scores min/max : -2.894732405609222 1.5861619859900016\n",
      "Mask mean value:  tensor(0.1660, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9615  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.057e-03\n",
      "‖w_svm‖₂       : 0.07077288338965122\n",
      "‖alpha‖₁       : 0.41916388560264406\n",
      "scores min/max : -1.9116290189866751 2.1746774428953746\n",
      "Mask mean value:  tensor(0.7619, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8968  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.501e-04\n",
      "‖w_svm‖₂       : 6.704620288986703e-07\n",
      "‖alpha‖₁       : 0.39999999999999963\n",
      "scores min/max : -2.71196855822016e-08 2.7896350819012965e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.327e-09\n",
      "‖w_svm‖₂       : 0.0003638711797426153\n",
      "‖alpha‖₁       : 0.7399999999999997\n",
      "scores min/max : -0.00040508780650320693 -0.0003882002684409314\n",
      "Mask mean value:  tensor(0.4980, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0601  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.352e-17\n",
      "‖w_svm‖₂       : 1.0678014084621194e-06\n",
      "‖alpha‖₁       : 0.4999999999999987\n",
      "scores min/max : -2.600280338557055e-07 -6.725452465915392e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.964e-19\n",
      "‖w_svm‖₂       : 0.004450661326544703\n",
      "‖alpha‖₁       : 0.4599999999999995\n",
      "scores min/max : -0.008841361592140756 -0.006679595306714109\n",
      "Mask mean value:  tensor(0.4641, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.0169  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.090e-15\n",
      "‖w_svm‖₂       : 0.00030483908716697676\n",
      "‖alpha‖₁       : 0.4199999999473625\n",
      "scores min/max : 0.00013021264071231566 0.0004445027585653393\n",
      "Mask mean value:  tensor(0.5021, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9997  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.055e-15\n",
      "‖w_svm‖₂       : 0.04424094152102113\n",
      "‖alpha‖₁       : 0.9407430808358168\n",
      "scores min/max : -1.8330801281198106 0.3007481234356175\n",
      "Mask mean value:  tensor(0.4055, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.791e-14\n",
      "‖w_svm‖₂       : 0.016216811528309377\n",
      "‖alpha‖₁       : 0.8599999999999997\n",
      "scores min/max : -0.04517279575109225 -0.03183444348211727\n",
      "Mask mean value:  tensor(0.3304, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0841  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.328e-14\n",
      "‖w_svm‖₂       : 7.591493992212624e-08\n",
      "‖alpha‖₁       : 0.5399999999999288\n",
      "scores min/max : -1.744683765512485e-07 -1.5287161888088442e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.864e-20\n",
      "‖w_svm‖₂       : 0.07279631292237339\n",
      "‖alpha‖₁       : 0.65862736241467\n",
      "scores min/max : -2.0252911150158126 4.288048436716417\n",
      "Mask mean value:  tensor(0.3625, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.1468  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.586e-04\n",
      "‖w_svm‖₂       : 0.027060421423573858\n",
      "‖alpha‖₁       : 0.19668270240164154\n",
      "scores min/max : -2.223745986710774 -0.0013151723195694437\n",
      "Mask mean value:  tensor(0.0423, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2890  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.317e-03\n",
      "‖w_svm‖₂       : 0.029930543817579525\n",
      "‖alpha‖₁       : 0.8985830723283231\n",
      "scores min/max : -0.7355843669120303 2.024599340395655\n",
      "Mask mean value:  tensor(0.8619, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3232  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.210e-15\n",
      "‖w_svm‖₂       : 0.0407965977894985\n",
      "‖alpha‖₁       : 0.6599999999999868\n",
      "scores min/max : -0.22196507718049863 0.1548916154991229\n",
      "Mask mean value:  tensor(0.4693, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8735  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.875e-13\n",
      "‖w_svm‖₂       : 4.28555350154259e-07\n",
      "‖alpha‖₁       : 0.7199999999999993\n",
      "scores min/max : 1.7036289434639885e-07 2.2759329328436756e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.210e-21\n",
      "‖w_svm‖₂       : 1.207553558293367e-06\n",
      "‖alpha‖₁       : 0.31999999999748124\n",
      "scores min/max : -2.6310031752688543e-06 -2.5118567008874086e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4897  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.165e-18\n",
      "‖w_svm‖₂       : 2.2627742113516804e-07\n",
      "‖alpha‖₁       : 0.2599999999999948\n",
      "scores min/max : 1.4623223669385542e-07 1.5901637654107929e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.963e-17\n",
      "‖w_svm‖₂       : 0.00021236625051139903\n",
      "‖alpha‖₁       : 0.619999999994159\n",
      "scores min/max : 2.6691373064535446e-06 1.864882015408273e-05\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.704e-17\n",
      "‖w_svm‖₂       : 1.008443554487789e-06\n",
      "‖alpha‖₁       : 0.5999999999999973\n",
      "scores min/max : -1.1675976167370208e-07 -6.590329847886751e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.627e-19\n",
      "‖w_svm‖₂       : 0.0032312212168439085\n",
      "‖alpha‖₁       : 0.5799999999999969\n",
      "scores min/max : 0.0024494919120305004 0.0037684770324373992\n",
      "Mask mean value:  tensor(0.5175, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3290  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.741e-13\n",
      "‖w_svm‖₂       : 0.07193299161694963\n",
      "‖alpha‖₁       : 0.5799999999999538\n",
      "scores min/max : -2.8096100131468 1.6319534746261601\n",
      "Mask mean value:  tensor(0.2124, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3698  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.495e-04\n",
      "‖w_svm‖₂       : 0.03796121118411792\n",
      "‖alpha‖₁       : 0.9199995618695422\n",
      "scores min/max : -0.28180065644913854 0.2490946663720811\n",
      "Mask mean value:  tensor(0.1879, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9343  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.592e-03\n",
      "‖w_svm‖₂       : 0.04025544332093807\n",
      "‖alpha‖₁       : 0.7294201064055711\n",
      "scores min/max : -0.26078524475159814 2.020007763179893\n",
      "Mask mean value:  tensor(0.6785, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.0713  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.626e-03\n",
      "‖w_svm‖₂       : 4.8567072526147674e-08\n",
      "‖alpha‖₁       : 0.17999999999999683\n",
      "scores min/max : 3.779430358492629e-08 5.249675968232082e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.526e-08\n",
      "‖w_svm‖₂       : 0.015054633600553048\n",
      "‖alpha‖₁       : 0.8599999999999972\n",
      "scores min/max : -0.05108896144745167 -0.0006104256025592156\n",
      "Mask mean value:  tensor(0.3520, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4439  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.052e-05\n",
      "‖w_svm‖₂       : 0.020511144634564278\n",
      "‖alpha‖₁       : 0.3834516122771445\n",
      "scores min/max : -1.9586689811790476 0.24382632275580884\n",
      "Mask mean value:  tensor(0.7189, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1533  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.244e-14\n",
      "‖w_svm‖₂       : 0.1830098411795889\n",
      "‖alpha‖₁       : 0.8831909773278848\n",
      "scores min/max : -3.6334731818220387 6.077672722693265\n",
      "Mask mean value:  tensor(0.0926, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.613e-05\n",
      "‖w_svm‖₂       : 3.787903393530772e-07\n",
      "‖alpha‖₁       : 0.2799999999999937\n",
      "scores min/max : -1.2722653421872285e-06 -1.210460011433608e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.977e-19\n",
      "‖w_svm‖₂       : 0.05161752705648137\n",
      "‖alpha‖₁       : 0.8271820464906854\n",
      "scores min/max : -1.9458637100894984 0.40508168353130203\n",
      "Mask mean value:  tensor(0.5397, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0165  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.595e-02\n",
      "‖w_svm‖₂       : 7.338257480545548e-08\n",
      "‖alpha‖₁       : 0.41999999999988513\n",
      "scores min/max : -6.419583765888064e-08 -1.6465903694761016e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.382e-08\n",
      "‖w_svm‖₂       : 0.008584755503217424\n",
      "‖alpha‖₁       : 0.6076958690528415\n",
      "scores min/max : -1.991640889388934 0.2951352196205578\n",
      "Mask mean value:  tensor(0.5501, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2221  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.386e-15\n",
      "‖w_svm‖₂       : 1.0374483207902577e-07\n",
      "‖alpha‖₁       : 0.23999999999999927\n",
      "scores min/max : 5.555259376172426e-08 6.629895636799365e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.393e-21\n",
      "‖w_svm‖₂       : 0.01978444273222278\n",
      "‖alpha‖₁       : 0.6599999999999998\n",
      "scores min/max : -0.04063933720758822 0.023845750762892852\n",
      "Mask mean value:  tensor(0.4150, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.2622  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.293e-15\n",
      "‖w_svm‖₂       : 0.05494615381514708\n",
      "‖alpha‖₁       : 0.5763274213930979\n",
      "scores min/max : -1.95870797302722 0.869606511171957\n",
      "Mask mean value:  tensor(0.7083, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9244  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.054e-03\n",
      "‖w_svm‖₂       : 0.08194514407704898\n",
      "‖alpha‖₁       : 0.47196116284896994\n",
      "scores min/max : -2.2839321379829594 2.795125557138074\n",
      "Mask mean value:  tensor(0.1180, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.2793  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.442e-02\n",
      "‖w_svm‖₂       : 2.3840715404353744e-07\n",
      "‖alpha‖₁       : 0.5799999999999998\n",
      "scores min/max : -2.865141445238961e-07 -2.7089496559286533e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.129e-19\n",
      "‖w_svm‖₂       : 0.04170417837277805\n",
      "‖alpha‖₁       : 0.8899923314160918\n",
      "scores min/max : -2.013640602275765 0.3342192929147717\n",
      "Mask mean value:  tensor(0.2512, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0588  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.323e-05\n",
      "‖w_svm‖₂       : 0.0001610642923196742\n",
      "‖alpha‖₁       : 0.6399999999999999\n",
      "scores min/max : -0.00013749396735460704 -0.0001275956366792578\n",
      "Mask mean value:  tensor(0.4993, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7634  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.361e-17\n",
      "‖w_svm‖₂       : 0.02057806983518588\n",
      "‖alpha‖₁       : 0.823377215258629\n",
      "scores min/max : -1.9031594489796357 1.4936403705449683\n",
      "Mask mean value:  tensor(0.6891, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.3583  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.151e-05\n",
      "‖w_svm‖₂       : 0.0005120176847440726\n",
      "‖alpha‖₁       : 0.43999999999998585\n",
      "scores min/max : -0.0008901479545224615 -0.0006200294358009806\n",
      "Mask mean value:  tensor(0.4958, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6413  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.318e-16\n",
      "‖w_svm‖₂       : 8.343248093393834e-08\n",
      "‖alpha‖₁       : 0.37999999999999934\n",
      "scores min/max : -1.279084458741485e-09 1.6880946637277275e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.900e-22\n",
      "‖w_svm‖₂       : 0.13950398691244906\n",
      "‖alpha‖₁       : 0.8799999999999584\n",
      "scores min/max : -1.3615829595178677 3.728887952009884\n",
      "Mask mean value:  tensor(0.3777, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2677  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.276e-10\n",
      "‖w_svm‖₂       : 1.6842911890362373e-07\n",
      "‖alpha‖₁       : 0.2399999999999789\n",
      "scores min/max : 2.5966809059404634e-07 2.7525678033990867e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.737e-20\n",
      "‖w_svm‖₂       : 7.525890595681866e-08\n",
      "‖alpha‖₁       : 0.6599999999999977\n",
      "scores min/max : 1.0476343303365658e-07 2.497488526145766e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.063e-08\n",
      "‖w_svm‖₂       : 0.060568931586889606\n",
      "‖alpha‖₁       : 0.8986612609160667\n",
      "scores min/max : -1.6986852721811654 3.714478415701349\n",
      "Mask mean value:  tensor(0.4330, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5830  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.076e-02\n",
      "‖w_svm‖₂       : 1.9487541970466017e-07\n",
      "‖alpha‖₁       : 0.37999999999999956\n",
      "scores min/max : -4.2543208825604184e-08 -6.869334225134555e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.699e-19\n",
      "‖w_svm‖₂       : 1.1449755395922294e-07\n",
      "‖alpha‖₁       : 0.2999999999999937\n",
      "scores min/max : 1.5441076587783783e-07 1.6339881752784048e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.406e-07\n",
      "‖w_svm‖₂       : 0.13788453174620136\n",
      "‖alpha‖₁       : 0.6549563762676178\n",
      "scores min/max : -18.07606918801983 1.8184521991570635\n",
      "Mask mean value:  tensor(0.2556, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2812  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.721e-03\n",
      "‖w_svm‖₂       : 0.09722433429032523\n",
      "‖alpha‖₁       : 0.8733145360090854\n",
      "scores min/max : -12.155877905588868 2.1687538432730546\n",
      "Mask mean value:  tensor(0.5695, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.6871  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.313e-02\n",
      "‖w_svm‖₂       : 0.00013955863236206567\n",
      "‖alpha‖₁       : 0.43999999998363404\n",
      "scores min/max : -0.00029376035867721 -0.00015090107060319858\n",
      "Mask mean value:  tensor(0.4990, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0207  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.912e-17\n",
      "‖w_svm‖₂       : 2.0474880729327753e-07\n",
      "‖alpha‖₁       : 0.37999999999547923\n",
      "scores min/max : -1.27272059784572e-07 -1.0892376981629885e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.764e-07\n",
      "‖w_svm‖₂       : 0.018206168934762592\n",
      "‖alpha‖₁       : 0.6815987901596353\n",
      "scores min/max : -1.9713202397796135 0.058172803929432915\n",
      "Mask mean value:  tensor(0.6165, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2602  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.455e-12\n",
      "‖w_svm‖₂       : 3.940574546611687e-06\n",
      "‖alpha‖₁       : 0.4199999999903918\n",
      "scores min/max : -3.469771563962179e-06 -1.717407664601461e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1229  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.784e-05\n",
      "‖w_svm‖₂       : 0.0064398569124339425\n",
      "‖alpha‖₁       : 0.7005731456046588\n",
      "scores min/max : -2.0023040512856416 0.021115321314985783\n",
      "Mask mean value:  tensor(0.4631, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2593  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.091e-12\n",
      "‖w_svm‖₂       : 1.766192537789843e-05\n",
      "‖alpha‖₁       : 0.3599999999819573\n",
      "scores min/max : 1.3822116465737219e-05 1.493007440222722e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.973e-17\n",
      "‖w_svm‖₂       : 2.3391458968153746e-07\n",
      "‖alpha‖₁       : 0.6399999999999961\n",
      "scores min/max : 4.198184320784502e-07 4.4070605885467104e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.037e-19\n",
      "‖w_svm‖₂       : 1.1101889606151533e-07\n",
      "‖alpha‖₁       : 0.5199999999999962\n",
      "scores min/max : 3.0480740016314477e-07 3.262117688823205e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.925e-19\n",
      "‖w_svm‖₂       : 0.14549344054556135\n",
      "‖alpha‖₁       : 0.7591491419914405\n",
      "scores min/max : -1.763806936089941 2.2138014457451374\n",
      "Mask mean value:  tensor(0.8453, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3568  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.881e-11\n",
      "‖w_svm‖₂       : 0.005772301366807719\n",
      "‖alpha‖₁       : 0.5599999999999996\n",
      "scores min/max : 0.005937684808885464 0.00767202191339548\n",
      "Mask mean value:  tensor(0.5312, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1136  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.976e-05\n",
      "‖w_svm‖₂       : 0.04755018398088363\n",
      "‖alpha‖₁       : 0.9056469786867055\n",
      "scores min/max : -0.7286416732676394 1.8862573567755256\n",
      "Mask mean value:  tensor(0.3608, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6699  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.817e-03\n",
      "‖w_svm‖₂       : 0.08517023170068128\n",
      "‖alpha‖₁       : 0.5779458090765721\n",
      "scores min/max : -2.1259770669363856 5.796305230427127\n",
      "Mask mean value:  tensor(0.2640, dtype=torch.float64)\n",
      "max feasible return = 0.1086  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.9402589322337404e-07\n",
      "‖alpha‖₁       : 0.5799999999999994\n",
      "scores min/max : -3.7302045774789865e-07 -2.822538340886207e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0282926757525878e-07\n",
      "‖alpha‖₁       : 0.29999999999999927\n",
      "scores min/max : 3.563330851043539e-08 4.061087622854337e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 6.78868429265325e-08\n",
      "‖alpha‖₁       : 0.5999999999999727\n",
      "scores min/max : 2.450248870257877e-08 6.1283443734891e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.558293541110363e-08\n",
      "‖alpha‖₁       : 0.379999999999999\n",
      "scores min/max : -8.117668750683327e-09 1.2629137503548614e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 5.717391770130013e-06\n",
      "‖alpha‖₁       : 0.3199999999403813\n",
      "scores min/max : 5.985182558327856e-06 9.71102743314054e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4259  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04886075157218502\n",
      "‖alpha‖₁       : 0.7397148431110665\n",
      "scores min/max : -3.4607611317884275 2.1722256178904797\n",
      "Mask mean value:  tensor(0.8959, dtype=torch.float64)\n",
      "max feasible return = -0.9386  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.027567334058671577\n",
      "‖alpha‖₁       : 0.6151653866211623\n",
      "scores min/max : -1.9326000127786405 1.1965486030551915\n",
      "Mask mean value:  tensor(0.7486, dtype=torch.float64)\n",
      "max feasible return = 2.6137  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.6654431953568856e-07\n",
      "‖alpha‖₁       : 0.4599999999999989\n",
      "scores min/max : 1.715175693327099e-07 7.657196449727159e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.3784103117122573e-07\n",
      "‖alpha‖₁       : 0.519999999999982\n",
      "scores min/max : 8.840043727908303e-08 1.0132012002214793e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.5725011893758014e-07\n",
      "‖alpha‖₁       : 0.5799999999999716\n",
      "scores min/max : -2.387725731555111e-07 -2.0239674749756565e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.04449983759013559\n",
      "‖alpha‖₁       : 0.6878410489446224\n",
      "scores min/max : -0.442519932695168 1.9461167120871303\n",
      "Mask mean value:  tensor(0.3651, dtype=torch.float64)\n",
      "max feasible return = 0.0715  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.033092378512014964\n",
      "‖alpha‖₁       : 0.4222735383658567\n",
      "scores min/max : -3.878031138672563 5.272739657645609\n",
      "Mask mean value:  tensor(0.0979, dtype=torch.float64)\n",
      "max feasible return = 0.6190  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.043218827413023485\n",
      "‖alpha‖₁       : 0.8211261555498046\n",
      "scores min/max : -5.006764528733083 3.162390992424031\n",
      "Mask mean value:  tensor(0.9597, dtype=torch.float64)\n",
      "max feasible return = -3.6639  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.0069546601295113345\n",
      "‖alpha‖₁       : 0.7999999999998681\n",
      "scores min/max : -0.031474814175861995 0.001109261750881916\n",
      "Mask mean value:  tensor(0.4647, dtype=torch.float64)\n",
      "max feasible return = -0.1689  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.0824834918639517e-07\n",
      "‖alpha‖₁       : 0.6199999999999584\n",
      "scores min/max : 7.177595767089141e-08 8.831858819753311e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.7842052732960465e-08\n",
      "‖alpha‖₁       : 0.43999999999999984\n",
      "scores min/max : 5.505414925156055e-09 1.507520468556475e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.3077226339920496e-07\n",
      "‖alpha‖₁       : 0.2799999999999786\n",
      "scores min/max : -2.457858174799914e-07 -2.2785750012443334e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  39 | train 0.005319 | val 0.006639\n",
      "-----------------------------------------Epoch:  40 ----------------------------------------\n",
      "‖w_svm‖₂       : 0.019741403376028602\n",
      "‖alpha‖₁       : 0.6599999999998873\n",
      "scores min/max : -0.039644517688643494 0.02410865609483327\n",
      "Mask mean value:  tensor(0.4190, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.2922  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.258e-15\n",
      "‖w_svm‖₂       : 0.014968267132683467\n",
      "‖alpha‖₁       : 0.8599999999999972\n",
      "scores min/max : -0.05090757940392713 -0.0009696445455719509\n",
      "Mask mean value:  tensor(0.3519, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4434  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.067e-05\n",
      "‖w_svm‖₂       : 1.187963295594107e-06\n",
      "‖alpha‖₁       : 0.3199999999978645\n",
      "scores min/max : -2.636363595719171e-06 -2.5188997132192048e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4897  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.142e-18\n",
      "‖w_svm‖₂       : 7.525306623903048e-08\n",
      "‖alpha‖₁       : 0.6599999999999978\n",
      "scores min/max : 1.047651556281265e-07 2.497423360741317e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.166e-09\n",
      "‖w_svm‖₂       : 0.09686100382603317\n",
      "‖alpha‖₁       : 0.8733149615021846\n",
      "scores min/max : -12.185243321981275 2.138635631222755\n",
      "Mask mean value:  tensor(0.4966, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.4765  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.667e-02\n",
      "‖w_svm‖₂       : 4.8518406466717644e-08\n",
      "‖alpha‖₁       : 0.17999999999999644\n",
      "scores min/max : 4.6615621479063545e-08 6.133278986168985e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.637e-08\n",
      "‖w_svm‖₂       : 0.020632542486399102\n",
      "‖alpha‖₁       : 0.3834567032355282\n",
      "scores min/max : -1.9600999493491624 0.24239501264388486\n",
      "Mask mean value:  tensor(0.7137, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1526  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.509e-14\n",
      "‖w_svm‖₂       : 0.00016082080830371692\n",
      "‖alpha‖₁       : 0.6399999999999799\n",
      "scores min/max : -0.00015415928301257557 -0.00014429083134544218\n",
      "Mask mean value:  tensor(0.4993, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7633  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.397e-17\n",
      "‖w_svm‖₂       : 0.00051697583083132\n",
      "‖alpha‖₁       : 0.4399999999999787\n",
      "scores min/max : -0.0009284548171690477 -0.0006523539091951571\n",
      "Mask mean value:  tensor(0.4956, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6407  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.305e-16\n",
      "‖w_svm‖₂       : 6.519853349603188e-07\n",
      "‖alpha‖₁       : 0.2799999999722579\n",
      "scores min/max : -2.2342716939006904e-06 -2.11367798150862e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1957  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.881e-19\n",
      "‖w_svm‖₂       : 0.051601343600672055\n",
      "‖alpha‖₁       : 0.8271786851448609\n",
      "scores min/max : -1.948043122797021 0.4028920702439656\n",
      "Mask mean value:  tensor(0.5320, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0150  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.642e-02\n",
      "‖w_svm‖₂       : 0.00036369843730559216\n",
      "‖alpha‖₁       : 0.7399999999999991\n",
      "scores min/max : -0.0004136804275856193 -0.000396808924107311\n",
      "Mask mean value:  tensor(0.4980, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0600  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.366e-17\n",
      "‖w_svm‖₂       : 0.020705695406340995\n",
      "‖alpha‖₁       : 0.7799999999999994\n",
      "scores min/max : -0.03414720920579829 0.03542941949844858\n",
      "Mask mean value:  tensor(0.3625, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6015  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.534e-15\n",
      "‖w_svm‖₂       : 0.00013922819699900918\n",
      "‖alpha‖₁       : 0.43999999998911044\n",
      "scores min/max : -0.00029228661666959896 -0.00015026784108914396\n",
      "Mask mean value:  tensor(0.4990, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0208  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.908e-17\n",
      "‖w_svm‖₂       : 0.0008956676343183342\n",
      "‖alpha‖₁       : 0.8199999999999992\n",
      "scores min/max : 0.0009466309015789369 0.0025755155405095703\n",
      "Mask mean value:  tensor(0.5099, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5902  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.088e-16\n",
      "‖w_svm‖₂       : 0.005771297205030921\n",
      "‖alpha‖₁       : 0.5599999999999877\n",
      "scores min/max : 0.005685045166861996 0.007418911060986483\n",
      "Mask mean value:  tensor(0.5300, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1109  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 2.190e-05\n",
      "‖w_svm‖₂       : 7.320860761922391e-08\n",
      "‖alpha‖₁       : 0.42000000000000004\n",
      "scores min/max : -6.576838173837939e-08 -1.8662516391156034e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.414e-08\n",
      "‖w_svm‖₂       : 4.3507526197916936e-07\n",
      "‖alpha‖₁       : 0.7199999999998435\n",
      "scores min/max : 1.6312185761189603e-07 2.2054571819097097e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.272e-21\n",
      "‖w_svm‖₂       : 0.008678281626315365\n",
      "‖alpha‖₁       : 0.6076974761541238\n",
      "scores min/max : -1.991977794412672 0.2948011550439236\n",
      "Mask mean value:  tensor(0.5486, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2215  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.669e-15\n",
      "‖w_svm‖₂       : 0.07077756624889493\n",
      "‖alpha‖₁       : 0.4192291385308874\n",
      "scores min/max : -1.9156880058684882 2.1585780477923016\n",
      "Mask mean value:  tensor(0.7581, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8964  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.454e-04\n",
      "‖w_svm‖₂       : 8.129885847437071e-08\n",
      "‖alpha‖₁       : 0.1799999999999988\n",
      "scores min/max : -1.907011221873238e-07 4.317132677567998e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.355e-21\n",
      "‖w_svm‖₂       : 0.029672025420316276\n",
      "‖alpha‖₁       : 0.8524268606591857\n",
      "scores min/max : -2.890976143074833 1.5897930735125416\n",
      "Mask mean value:  tensor(0.1679, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9629  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.029e-03\n",
      "‖w_svm‖₂       : 0.02068399878622778\n",
      "‖alpha‖₁       : 0.8233794273090759\n",
      "scores min/max : -1.9044275819041643 1.4923105456124746\n",
      "Mask mean value:  tensor(0.6856, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.3502  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.513e-05\n",
      "‖w_svm‖₂       : 0.00021204326370139625\n",
      "‖alpha‖₁       : 0.6199999999923866\n",
      "scores min/max : -2.0246768914475083e-06 1.3905839451603224e-05\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.737e-17\n",
      "‖w_svm‖₂       : 8.379905634480898e-08\n",
      "‖alpha‖₁       : 0.3799999999999992\n",
      "scores min/max : -1.7561775814757564e-09 1.6404617712801738e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.937e-22\n",
      "‖w_svm‖₂       : 0.04169659248942087\n",
      "‖alpha‖₁       : 0.8900060716318685\n",
      "scores min/max : -2.0117095991235545 0.33614950315797987\n",
      "Mask mean value:  tensor(0.2565, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0607  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.937e-05\n",
      "‖w_svm‖₂       : 0.08216832437270952\n",
      "‖alpha‖₁       : 0.4719948921517134\n",
      "scores min/max : -2.29104366251777 2.7885109997054953\n",
      "Mask mean value:  tensor(0.1147, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.2465  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.518e-02\n",
      "‖w_svm‖₂       : 7.563812469397634e-08\n",
      "‖alpha‖₁       : 0.5399999999999978\n",
      "scores min/max : -1.7682893882101036e-07 -1.5528861654217183e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.879e-20\n",
      "‖w_svm‖₂       : 2.3442511320873303e-07\n",
      "‖alpha‖₁       : 0.6399999999999961\n",
      "scores min/max : 4.2485801888149863e-07 4.457444783291172e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.048e-19\n",
      "‖w_svm‖₂       : 1.968930658974825e-07\n",
      "‖alpha‖₁       : 0.41999999999996906\n",
      "scores min/max : -4.6862237327481137e-07 -4.4639663669354285e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.897e-19\n",
      "‖w_svm‖₂       : 0.04579793523066988\n",
      "‖alpha‖₁       : 0.9389254903154023\n",
      "scores min/max : -2.4613120935209087 1.587896213546773\n",
      "Mask mean value:  tensor(0.1469, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4031  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.268e-03\n",
      "‖w_svm‖₂       : 0.029801237429369205\n",
      "‖alpha‖₁       : 0.5511829146452523\n",
      "scores min/max : -3.4696804390660927 1.099806355949543\n",
      "Mask mean value:  tensor(0.1528, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1980  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.051e-03\n",
      "‖w_svm‖₂       : 5.315416491344689e-08\n",
      "‖alpha‖₁       : 0.11999999999999583\n",
      "scores min/max : -9.654114479199818e-08 -6.581725359370729e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.210e-08\n",
      "‖w_svm‖₂       : 6.713842633061842e-07\n",
      "‖alpha‖₁       : 0.39999999999999986\n",
      "scores min/max : -3.7558296034936496e-09 3.0247805667139567e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.338e-09\n",
      "‖w_svm‖₂       : 1.0095495008987678e-06\n",
      "‖alpha‖₁       : 0.5999999999999968\n",
      "scores min/max : -1.2439775403116968e-07 -7.353795695438003e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.639e-19\n",
      "‖w_svm‖₂       : 2.265664225576421e-07\n",
      "‖alpha‖₁       : 0.2599999999999959\n",
      "scores min/max : 1.6535554181285603e-07 1.7812963849174144e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.015e-17\n",
      "‖w_svm‖₂       : 0.054806704147399396\n",
      "‖alpha‖₁       : 0.5763177487699573\n",
      "scores min/max : -1.9594466820040628 0.8688101805790303\n",
      "Mask mean value:  tensor(0.7059, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9175  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.030e-03\n",
      "‖w_svm‖₂       : 0.00651550346272925\n",
      "‖alpha‖₁       : 0.7005740397190146\n",
      "scores min/max : -2.0015264555875563 0.021891060707262905\n",
      "Mask mean value:  tensor(0.4669, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2613  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.422e-13\n",
      "‖w_svm‖₂       : 0.1380292667488137\n",
      "‖alpha‖₁       : 0.6550027016969254\n",
      "scores min/max : -18.080638366181255 1.81472766862552\n",
      "Mask mean value:  tensor(0.2521, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2805  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.954e-03\n",
      "‖w_svm‖₂       : 0.021803121970518827\n",
      "‖alpha‖₁       : 0.8150953455945515\n",
      "scores min/max : -11.51293882315952 2.03333439142293\n",
      "Mask mean value:  tensor(0.7131, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9222  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.890e-06\n",
      "‖w_svm‖₂       : 1.1506621746641602e-07\n",
      "‖alpha‖₁       : 0.29999999999999366\n",
      "scores min/max : 1.600406228557174e-07 1.690286553232532e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.425e-07\n",
      "‖w_svm‖₂       : 0.04020625684733204\n",
      "‖alpha‖₁       : 0.659999999999985\n",
      "scores min/max : -0.2161859502476552 0.14933058639130625\n",
      "Mask mean value:  tensor(0.4657, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.057e-13\n",
      "‖w_svm‖₂       : 0.0162571168181398\n",
      "‖alpha‖₁       : 0.5963015222976551\n",
      "scores min/max : -2.8728268086520354 2.2801163286811623\n",
      "Mask mean value:  tensor(0.9193, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.5101  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.754e-09\n",
      "‖w_svm‖₂       : 1.6910911789839267e-07\n",
      "‖alpha‖₁       : 0.23999999999997743\n",
      "scores min/max : 2.5730083308364637e-07 2.7294397838221646e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.769e-20\n",
      "‖w_svm‖₂       : 0.00018557588064359694\n",
      "‖alpha‖₁       : 0.8199999999999956\n",
      "scores min/max : -9.550884031050597e-05 -8.132612111663967e-05\n",
      "Mask mean value:  tensor(0.4996, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6964  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.370e-17\n",
      "‖w_svm‖₂       : 0.13962843451938564\n",
      "‖alpha‖₁       : 0.8799999999999977\n",
      "scores min/max : -1.3621118493543285 3.7372220354865173\n",
      "Mask mean value:  tensor(0.3820, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2679  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.210e-09\n",
      "‖w_svm‖₂       : 1.1146496795465093e-07\n",
      "‖alpha‖₁       : 0.5199999999999954\n",
      "scores min/max : 3.050631234782377e-07 3.264644889953749e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.959e-19\n",
      "‖w_svm‖₂       : 0.03743144236902078\n",
      "‖alpha‖₁       : 0.9199999999997728\n",
      "scores min/max : -0.276485679694989 0.24016181888105514\n",
      "Mask mean value:  tensor(0.1861, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9269  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.164e-03\n",
      "‖w_svm‖₂       : 2.792772920641073e-07\n",
      "‖alpha‖₁       : 0.6599999999999816\n",
      "scores min/max : -6.862104113018968e-07 -5.344381684694159e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.464e-18\n",
      "‖w_svm‖₂       : 3.8030624963338068e-06\n",
      "‖alpha‖₁       : 0.41999999999152876\n",
      "scores min/max : -3.5122161242460356e-06 -1.8153069224958763e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1229  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.736e-05\n",
      "‖w_svm‖₂       : 0.07213734956711229\n",
      "‖alpha‖₁       : 0.5799999999999416\n",
      "scores min/max : -2.8187620700645026 1.6427054016967186\n",
      "Mask mean value:  tensor(0.2195, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3789  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.359e-04\n",
      "‖w_svm‖₂       : 0.03992268107848165\n",
      "‖alpha‖₁       : 0.7294206899709261\n",
      "scores min/max : -0.26255740054033533 2.018170564505334\n",
      "Mask mean value:  tensor(0.6730, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.0486  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.693e-03\n",
      "‖w_svm‖₂       : 0.047303183593870736\n",
      "‖alpha‖₁       : 0.9056470968433191\n",
      "scores min/max : -0.7279267958035909 1.884042419260327\n",
      "Mask mean value:  tensor(0.3519, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6540  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.369e-03\n",
      "‖w_svm‖₂       : 0.00563566330021465\n",
      "‖alpha‖₁       : 0.3799999999999959\n",
      "scores min/max : -0.005438200621193795 0.00634356648976564\n",
      "Mask mean value:  tensor(0.5169, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9194  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.121e-14\n",
      "‖w_svm‖₂       : 1.0403295666649897e-07\n",
      "‖alpha‖₁       : 0.23999999999999921\n",
      "scores min/max : 5.8985500917406253e-08 6.973406215831138e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.439e-21\n",
      "‖w_svm‖₂       : 0.02976308637699614\n",
      "‖alpha‖₁       : 0.8985832972081382\n",
      "scores min/max : -0.7342252948526842 2.025917183416172\n",
      "Mask mean value:  tensor(0.8636, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3193  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.069e-15\n",
      "‖w_svm‖₂       : 7.23938895787717e-08\n",
      "‖alpha‖₁       : 0.1399999999999983\n",
      "scores min/max : 1.4665387218259989e-08 2.0394679782712778e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.551e-08\n",
      "‖w_svm‖₂       : 5.267304354515583e-08\n",
      "‖alpha‖₁       : 0.4399999999999877\n",
      "scores min/max : -1.9555819914355384e-07 -9.157944824518275e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.602e-20\n",
      "‖w_svm‖₂       : 0.0721869076790184\n",
      "‖alpha‖₁       : 0.6585967517037523\n",
      "scores min/max : -2.0159475500427257 4.310982341429151\n",
      "Mask mean value:  tensor(0.3808, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3156  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.684e-04\n",
      "‖w_svm‖₂       : 2.3939651040732214e-07\n",
      "‖alpha‖₁       : 0.5799999999999998\n",
      "scores min/max : -2.903253044682491e-07 -2.7470728049165565e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.133e-19\n",
      "‖w_svm‖₂       : 5.755563225453917e-08\n",
      "‖alpha‖₁       : 0.23999999999999094\n",
      "scores min/max : -5.639038364908276e-08 -4.1930751334574524e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.588e-20\n",
      "‖w_svm‖₂       : 1.7752878811982047e-05\n",
      "‖alpha‖₁       : 0.35999999998168825\n",
      "scores min/max : 1.3800268543022953e-05 1.4914643746938888e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.047e-17\n",
      "‖w_svm‖₂       : 0.18294585885107825\n",
      "‖alpha‖₁       : 0.8831606704299347\n",
      "scores min/max : -3.637324569737205 6.072265833195175\n",
      "Mask mean value:  tensor(0.0918, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2007  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.483e-03\n",
      "‖w_svm‖₂       : 0.026921916015247613\n",
      "‖alpha‖₁       : 0.19668648913403325\n",
      "scores min/max : -2.2195316651641277 0.002673721519903152\n",
      "Mask mean value:  tensor(0.0453, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3089  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.181e-03\n",
      "‖w_svm‖₂       : 2.0720406401292847e-08\n",
      "‖alpha‖₁       : 0.11999999999999432\n",
      "scores min/max : -3.6668685399336136e-08 -2.1983460209858468e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.844e-09\n",
      "‖w_svm‖₂       : 1.9626129927932056e-07\n",
      "‖alpha‖₁       : 0.37999999999998985\n",
      "scores min/max : -4.371468146422952e-08 -7.96774675020247e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.730e-19\n",
      "‖w_svm‖₂       : 0.01605638480075439\n",
      "‖alpha‖₁       : 0.8599999999999985\n",
      "scores min/max : -0.04579170595378601 -0.03271555629062203\n",
      "Mask mean value:  tensor(0.3269, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0835  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.369e-14\n",
      "‖w_svm‖₂       : 0.0003041344348205909\n",
      "‖alpha‖₁       : 0.4199999999427001\n",
      "scores min/max : 0.00012415674694260916 0.00043694273366777796\n",
      "Mask mean value:  tensor(0.5020, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9996  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.059e-15\n",
      "‖w_svm‖₂       : 0.0032408566223609287\n",
      "‖alpha‖₁       : 0.5799999999999981\n",
      "scores min/max : 0.002986058255772835 0.004311712549503355\n",
      "Mask mean value:  tensor(0.5202, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3308  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.130e-13\n",
      "‖w_svm‖₂       : 0.06054066888425976\n",
      "‖alpha‖₁       : 0.8986643727770287\n",
      "scores min/max : -1.6974782925778138 3.7156191655481265\n",
      "Mask mean value:  tensor(0.4354, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5848  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.038e-02\n",
      "‖w_svm‖₂       : 0.018059996008890647\n",
      "‖alpha‖₁       : 0.6815989168177004\n",
      "scores min/max : -1.970842405418576 0.058318340212180006\n",
      "Mask mean value:  tensor(0.6186, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2619  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.419e-12\n",
      "‖w_svm‖₂       : 0.14585533408671134\n",
      "‖alpha‖₁       : 0.7592625811953649\n",
      "scores min/max : -1.7658592580520687 2.211631387683335\n",
      "Mask mean value:  tensor(0.8422, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3572  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.925e-11\n",
      "‖w_svm‖₂       : 1.0676111822479334e-06\n",
      "‖alpha‖₁       : 0.4999999999999984\n",
      "scores min/max : -2.590216776454164e-07 -6.620536655382061e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.982e-19\n",
      "‖w_svm‖₂       : 2.0414745623637118e-07\n",
      "‖alpha‖₁       : 0.3799999999953996\n",
      "scores min/max : -1.2685637848204428e-07 -1.0853266684384791e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.532e-07\n",
      "‖w_svm‖₂       : 0.044624400179390115\n",
      "‖alpha‖₁       : 0.9407790700186558\n",
      "scores min/max : -1.8326204671103774 0.3012132407126715\n",
      "Mask mean value:  tensor(0.4074, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3241  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.159e-10\n",
      "‖w_svm‖₂       : 0.004461229785332274\n",
      "‖alpha‖₁       : 0.45999999999999536\n",
      "scores min/max : -0.009016877348763441 -0.006861627588229453\n",
      "Mask mean value:  tensor(0.4632, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.0130  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.101e-15\n",
      "‖w_svm‖₂       : 0.08541134184970593\n",
      "‖alpha‖₁       : 0.5780068322333025\n",
      "scores min/max : -2.129777130048914 5.791216319016228\n",
      "Mask mean value:  tensor(0.2572, dtype=torch.float64)\n",
      "max feasible return = 0.1074  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.963869042558129e-07\n",
      "‖alpha‖₁       : 0.579999999999977\n",
      "scores min/max : -3.5242395434373965e-07 -2.6132278665616425e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0291897274767672e-07\n",
      "‖alpha‖₁       : 0.29999999999999954\n",
      "scores min/max : 3.5722758321384573e-08 4.069609275855826e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 6.714245738785044e-08\n",
      "‖alpha‖₁       : 0.5999999999999998\n",
      "scores min/max : 2.0734200502092216e-08 5.744481639199441e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.546056374668529e-08\n",
      "‖alpha‖₁       : 0.37999999999999956\n",
      "scores min/max : -1.0335040959198406e-08 -9.60362136710846e-10\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 5.853602399403064e-06\n",
      "‖alpha‖₁       : 0.3199999999429683\n",
      "scores min/max : 6.347448607101721e-06 1.007071662972358e-05\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4259  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04842368988914804\n",
      "‖alpha‖₁       : 0.7397143791956821\n",
      "scores min/max : -3.456623128245032 2.176228874772566\n",
      "Mask mean value:  tensor(0.8988, dtype=torch.float64)\n",
      "max feasible return = -0.9404  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.027325920255924473\n",
      "‖alpha‖₁       : 0.6151656158265525\n",
      "scores min/max : -1.9365984597611574 1.1923975494281231\n",
      "Mask mean value:  tensor(0.7380, dtype=torch.float64)\n",
      "max feasible return = 2.5759  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.741544299840376e-07\n",
      "‖alpha‖₁       : 0.45999999999972985\n",
      "scores min/max : 2.1138176588789104e-07 8.134025095944763e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.3848606937319398e-07\n",
      "‖alpha‖₁       : 0.5199999999999781\n",
      "scores min/max : 8.387564792184482e-08 9.679777575477804e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.5816285134781835e-07\n",
      "‖alpha‖₁       : 0.5799999999999585\n",
      "scores min/max : -2.4326236639164043e-07 -2.0685080390824392e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.044415192709426225\n",
      "‖alpha‖₁       : 0.6878519846001878\n",
      "scores min/max : -0.4455223672448293 1.943234777587939\n",
      "Mask mean value:  tensor(0.3546, dtype=torch.float64)\n",
      "max feasible return = 0.0808  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.033029142591527456\n",
      "‖alpha‖₁       : 0.42227938973681634\n",
      "scores min/max : -3.8754400097096915 5.2762879999404575\n",
      "Mask mean value:  tensor(0.0996, dtype=torch.float64)\n",
      "max feasible return = 0.6303  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.043299186807460054\n",
      "‖alpha‖₁       : 0.8211330595234347\n",
      "scores min/max : -4.995442531826107 3.173771110793042\n",
      "Mask mean value:  tensor(0.9612, dtype=torch.float64)\n",
      "max feasible return = -3.6718  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.006877289790684274\n",
      "‖alpha‖₁       : 0.7999999999998523\n",
      "scores min/max : -0.031374832201828656 0.00041891854158913975\n",
      "Mask mean value:  tensor(0.4622, dtype=torch.float64)\n",
      "max feasible return = -0.1680  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.089436687321667e-07\n",
      "‖alpha‖₁       : 0.6199999999999588\n",
      "scores min/max : 8.661999711102713e-08 1.0316109440003897e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.807465401885667e-08\n",
      "‖alpha‖₁       : 0.4399999999999998\n",
      "scores min/max : 5.669598526053301e-09 1.5239633374592974e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.3146916719654017e-07\n",
      "‖alpha‖₁       : 0.2799999999999785\n",
      "scores min/max : -2.5265046632891e-07 -2.347214638679446e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  40 | train 0.005320 | val 0.006665\n",
      "-----------------------------------------Epoch:  41 ----------------------------------------\n",
      "‖w_svm‖₂       : 0.00446283601685539\n",
      "‖alpha‖₁       : 0.4599999999999944\n",
      "scores min/max : -0.00902987973030284 -0.006873878072399503\n",
      "Mask mean value:  tensor(0.4632, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.0128  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.101e-15\n",
      "‖w_svm‖₂       : 0.037310575582153484\n",
      "‖alpha‖₁       : 0.9199999999997883\n",
      "scores min/max : -0.27667253297478506 0.2362742399964548\n",
      "Mask mean value:  tensor(0.1824, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9085  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.978e-03\n",
      "‖w_svm‖₂       : 0.05167107693544803\n",
      "‖alpha‖₁       : 0.827192146377447\n",
      "scores min/max : -1.9458354188708826 0.40514202629567103\n",
      "Mask mean value:  tensor(0.5399, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0166  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.595e-02\n",
      "‖w_svm‖₂       : 0.016087629354598034\n",
      "‖alpha‖₁       : 0.859999999999995\n",
      "scores min/max : -0.04676553290576488 -0.03363842123704497\n",
      "Mask mean value:  tensor(0.3228, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0822  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.426e-14\n",
      "‖w_svm‖₂       : 2.785650166081477e-07\n",
      "‖alpha‖₁       : 0.6599999999999919\n",
      "scores min/max : -6.816410131857654e-07 -5.299299339953244e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.461e-18\n",
      "‖w_svm‖₂       : 0.006536660388114239\n",
      "‖alpha‖₁       : 0.7005743200850004\n",
      "scores min/max : -2.001736599617454 0.02168183654152889\n",
      "Mask mean value:  tensor(0.4659, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2607  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.353e-13\n",
      "‖w_svm‖₂       : 1.9611709870617684e-07\n",
      "‖alpha‖₁       : 0.37999999999998946\n",
      "scores min/max : -4.1331296638834555e-08 -5.583010694549005e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.722e-19\n",
      "‖w_svm‖₂       : 5.298732616116078e-08\n",
      "‖alpha‖₁       : 0.11999999999999592\n",
      "scores min/max : -9.610333424996e-08 -6.538072075527759e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.290e-07\n",
      "‖w_svm‖₂       : 1.778213095335946e-05\n",
      "‖alpha‖₁       : 0.35999999998164883\n",
      "scores min/max : 1.3532909301591548e-05 1.4650922730567044e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.060e-17\n",
      "‖w_svm‖₂       : 0.07079752201995979\n",
      "‖alpha‖₁       : 0.4192585613576543\n",
      "scores min/max : -1.915868558374906 2.153677920839142\n",
      "Mask mean value:  tensor(0.7580, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8967  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.486e-04\n",
      "‖w_svm‖₂       : 2.2597036037925002e-07\n",
      "‖alpha‖₁       : 0.25999999999999496\n",
      "scores min/max : 1.6904267465270887e-07 1.818246226683081e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.034e-17\n",
      "‖w_svm‖₂       : 0.07244488727558418\n",
      "‖alpha‖₁       : 0.6586330505763315\n",
      "scores min/max : -2.0268674143372003 4.29955622433015\n",
      "Mask mean value:  tensor(0.3596, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.1204  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.581e-04\n",
      "‖w_svm‖₂       : 2.3979895778787997e-07\n",
      "‖alpha‖₁       : 0.5799999999999763\n",
      "scores min/max : -2.8136688832810914e-07 -2.657035596172142e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.149e-19\n",
      "‖w_svm‖₂       : 0.0822917276501878\n",
      "‖alpha‖₁       : 0.47201890137564206\n",
      "scores min/max : -2.284648836349178 2.7952951924566363\n",
      "Mask mean value:  tensor(0.1178, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.2779  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.465e-02\n",
      "‖w_svm‖₂       : 7.530006055022654e-08\n",
      "‖alpha‖₁       : 0.5399999999999998\n",
      "scores min/max : -1.7660672611658855e-07 -1.5508944780950944e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.881e-20\n",
      "‖w_svm‖₂       : 1.6782804762698294e-07\n",
      "‖alpha‖₁       : 0.23999999999998123\n",
      "scores min/max : 2.449679779021297e-07 2.6051079467577524e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.754e-20\n",
      "‖w_svm‖₂       : 4.2836742264848767e-07\n",
      "‖alpha‖₁       : 0.7199999999999995\n",
      "scores min/max : 1.5364773376695915e-07 2.1082182594600312e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.239e-21\n",
      "‖w_svm‖₂       : 2.0614083560150444e-08\n",
      "‖alpha‖₁       : 0.11999999999999404\n",
      "scores min/max : -3.874421034561685e-08 -2.404495100859924e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.842e-09\n",
      "‖w_svm‖₂       : 1.0645395032594381e-06\n",
      "‖alpha‖₁       : 0.4999999999999981\n",
      "scores min/max : -2.603692196856394e-07 -6.75596792355811e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.957e-19\n",
      "‖w_svm‖₂       : 0.02987108140287796\n",
      "‖alpha‖₁       : 0.852439474444325\n",
      "scores min/max : -2.889651269323541 1.5910336252959352\n",
      "Mask mean value:  tensor(0.1685, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9633  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.033e-03\n",
      "‖w_svm‖₂       : 0.00021282968695007171\n",
      "‖alpha‖₁       : 0.619999999993446\n",
      "scores min/max : -3.952331962096231e-06 1.2096608701222653e-05\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.774e-17\n",
      "‖w_svm‖₂       : 0.0009003491821035081\n",
      "‖alpha‖₁       : 0.8199999999999996\n",
      "scores min/max : 0.0008156091177525075 0.002453526167988256\n",
      "Mask mean value:  tensor(0.5092, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5870  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.098e-16\n",
      "‖w_svm‖₂       : 6.705885378463825e-07\n",
      "‖alpha‖₁       : 0.39999999999999997\n",
      "scores min/max : 4.059112039800814e-09 3.101332929435251e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.337e-09\n",
      "‖w_svm‖₂       : 0.014829757202927014\n",
      "‖alpha‖₁       : 0.8599999999999974\n",
      "scores min/max : -0.051251202079575425 -0.002410404657769486\n",
      "Mask mean value:  tensor(0.3482, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4382  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.417e-05\n",
      "‖w_svm‖₂       : 0.029987467006250643\n",
      "‖alpha‖₁       : 0.5511926222402195\n",
      "scores min/max : -3.4664148158865156 1.1031252699481766\n",
      "Mask mean value:  tensor(0.1549, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.2097  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.699e-03\n",
      "‖w_svm‖₂       : 0.019720587686475374\n",
      "‖alpha‖₁       : 0.6599999999999289\n",
      "scores min/max : -0.038307820529065895 0.02472884426595847\n",
      "Mask mean value:  tensor(0.4249, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.3364  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.222e-15\n",
      "‖w_svm‖₂       : 0.09564662265809228\n",
      "‖alpha‖₁       : 0.8733372662755342\n",
      "scores min/max : -12.155980069718131 2.1669518315506546\n",
      "Mask mean value:  tensor(0.5648, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.6735  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 4.006e-02\n",
      "‖w_svm‖₂       : 3.7765139381806074e-07\n",
      "‖alpha‖₁       : 0.27999999999999386\n",
      "scores min/max : -1.1707651608794105e-06 -1.1089817516564292e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.008e-19\n",
      "‖w_svm‖₂       : 0.0547397874651045\n",
      "‖alpha‖₁       : 0.5763173063626854\n",
      "scores min/max : -1.9588779355792192 0.8694700735577567\n",
      "Mask mean value:  tensor(0.7078, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9229  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.033e-03\n",
      "‖w_svm‖₂       : 0.13901559368423994\n",
      "‖alpha‖₁       : 0.8799999999999972\n",
      "scores min/max : -1.35040992956847 3.7079579802612184\n",
      "Mask mean value:  tensor(0.3838, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2672  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.039e-10\n",
      "‖w_svm‖₂       : 7.338758583023145e-08\n",
      "‖alpha‖₁       : 0.4199999999999982\n",
      "scores min/max : -6.591738482091278e-08 -1.8743270818538995e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.483e-08\n",
      "‖w_svm‖₂       : 0.0001864330743564956\n",
      "‖alpha‖₁       : 0.8199999999999972\n",
      "scores min/max : -9.46915510396256e-05 -8.03726130055839e-05\n",
      "Mask mean value:  tensor(0.4996, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6964  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.346e-17\n",
      "‖w_svm‖₂       : 0.04000466966093489\n",
      "‖alpha‖₁       : 0.7294317957331068\n",
      "scores min/max : -0.26333845320988114 2.017386529719596\n",
      "Mask mean value:  tensor(0.6706, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.0382  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.738e-03\n",
      "‖w_svm‖₂       : 0.03992458669278999\n",
      "‖alpha‖₁       : 0.6599999999999846\n",
      "scores min/max : -0.21539256368799753 0.145460312424333\n",
      "Mask mean value:  tensor(0.4569, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8524  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.671e-13\n",
      "‖w_svm‖₂       : 0.00013981530391515194\n",
      "‖alpha‖₁       : 0.4399999999899607\n",
      "scores min/max : -0.000293186024887482 -0.00015040954134085452\n",
      "Mask mean value:  tensor(0.4990, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0208  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.929e-17\n",
      "‖w_svm‖₂       : 0.02083084798704511\n",
      "‖alpha‖₁       : 0.3834649947078984\n",
      "scores min/max : -1.9597794211360346 0.2427164454109803\n",
      "Mask mean value:  tensor(0.7149, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1528  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.668e-14\n",
      "‖w_svm‖₂       : 3.569059340368831e-06\n",
      "‖alpha‖₁       : 0.41999999999294735\n",
      "scores min/max : -3.301313824144776e-06 -1.6567887780396821e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1229  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.626e-05\n",
      "‖w_svm‖₂       : 0.026892101748676702\n",
      "‖alpha‖₁       : 0.19668930468027482\n",
      "scores min/max : -2.2195488156359793 0.002548417555646519\n",
      "Mask mean value:  tensor(0.0453, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3087  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.187e-03\n",
      "‖w_svm‖₂       : 0.04149431422309169\n",
      "‖alpha‖₁       : 0.889999368468853\n",
      "scores min/max : -2.0116623616887805 0.3361850704639173\n",
      "Mask mean value:  tensor(0.2565, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0602  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.858e-05\n",
      "‖w_svm‖₂       : 0.017975876488734947\n",
      "‖alpha‖₁       : 0.6815987884709456\n",
      "scores min/max : -1.9715247822520192 0.057459103144047396\n",
      "Mask mean value:  tensor(0.6155, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2605  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.581e-14\n",
      "‖w_svm‖₂       : 1.1459521292023644e-07\n",
      "‖alpha‖₁       : 0.29999999999999394\n",
      "scores min/max : 1.6468676100547605e-07 1.7367446324857647e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.439e-07\n",
      "‖w_svm‖₂       : 0.07207349797939509\n",
      "‖alpha‖₁       : 0.5799999999999454\n",
      "scores min/max : -2.8082295859245723 1.6384932259568874\n",
      "Mask mean value:  tensor(0.2227, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3823  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.275e-04\n",
      "‖w_svm‖₂       : 0.02180896585474031\n",
      "‖alpha‖₁       : 0.815093461977745\n",
      "scores min/max : -11.515751972791097 2.0301350830383633\n",
      "Mask mean value:  tensor(0.7063, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.803e-06\n",
      "‖w_svm‖₂       : 7.21763176647151e-08\n",
      "‖alpha‖₁       : 0.1399999999999988\n",
      "scores min/max : 1.3453049224952549e-08 1.9178046762757228e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.497e-08\n",
      "‖w_svm‖₂       : 1.9618781467200955e-07\n",
      "‖alpha‖₁       : 0.4199999999999683\n",
      "scores min/max : -4.808890064710915e-07 -4.5866947555292364e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.935e-19\n",
      "‖w_svm‖₂       : 0.008754161252911183\n",
      "‖alpha‖₁       : 0.607698755944033\n",
      "scores min/max : -1.9922783620546083 0.2944883063870799\n",
      "Mask mean value:  tensor(0.5472, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2209  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.885e-15\n",
      "‖w_svm‖₂       : 0.005676726725085385\n",
      "‖alpha‖₁       : 0.3799999999999999\n",
      "scores min/max : -0.004220984169326675 0.007737355877685877\n",
      "Mask mean value:  tensor(0.5236, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9314  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.028e-14\n",
      "‖w_svm‖₂       : 4.8617393326870214e-08\n",
      "‖alpha‖₁       : 0.1799999999999966\n",
      "scores min/max : 4.7457381356662974e-08 6.216773143723643e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.726e-08\n",
      "‖w_svm‖₂       : 0.045776221885449386\n",
      "‖alpha‖₁       : 0.9389158748878828\n",
      "scores min/max : -2.46357277338148 1.5855178521714042\n",
      "Mask mean value:  tensor(0.1453, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4002  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.240e-03\n",
      "‖w_svm‖₂       : 2.311837675097117e-07\n",
      "‖alpha‖₁       : 0.37999999998793044\n",
      "scores min/max : -1.766365035685962e-07 -1.5482638298838133e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.717e-07\n",
      "‖w_svm‖₂       : 0.00036480087197851953\n",
      "‖alpha‖₁       : 0.7399999999999992\n",
      "scores min/max : -0.0004334948427848958 -0.00041652092931015246\n",
      "Mask mean value:  tensor(0.4979, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0598  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.393e-17\n",
      "‖w_svm‖₂       : 2.3363548567320818e-07\n",
      "‖alpha‖₁       : 0.639999999999997\n",
      "scores min/max : 4.101520375105922e-07 4.3103991707677003e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.040e-19\n",
      "‖w_svm‖₂       : 0.01621672945388415\n",
      "‖alpha‖₁       : 0.5963048424734061\n",
      "scores min/max : -2.8555090689963576 2.2761781626369655\n",
      "Mask mean value:  tensor(0.9175, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.5082  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.032e-13\n",
      "‖w_svm‖₂       : 5.259733261565982e-08\n",
      "‖alpha‖₁       : 0.439999999999987\n",
      "scores min/max : -1.981829868953752e-07 -9.420078702106879e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.661e-20\n",
      "‖w_svm‖₂       : 0.00016097201000272042\n",
      "‖alpha‖₁       : 0.6399999999999998\n",
      "scores min/max : -0.000160687445525474 -0.000150800489779515\n",
      "Mask mean value:  tensor(0.4992, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7632  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.429e-17\n",
      "‖w_svm‖₂       : 0.06072182538925455\n",
      "‖alpha‖₁       : 0.8986822618111532\n",
      "scores min/max : -1.691288976278413 3.7217041931063846\n",
      "Mask mean value:  tensor(0.4481, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.213e-03\n",
      "‖w_svm‖₂       : 1.0623143653641969e-07\n",
      "‖alpha‖₁       : 0.23999999999999044\n",
      "scores min/max : 7.31781593404603e-08 8.407716027238268e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.564e-21\n",
      "‖w_svm‖₂       : 8.06721627529863e-08\n",
      "‖alpha‖₁       : 0.17999999999999916\n",
      "scores min/max : -1.9579032767647728e-07 3.7916589670985486e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.378e-21\n",
      "‖w_svm‖₂       : 8.363659788052371e-08\n",
      "‖alpha‖₁       : 0.37999999999999917\n",
      "scores min/max : 8.87628004718302e-09 2.7036757461025658e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.902e-22\n",
      "‖w_svm‖₂       : 1.0064033556015382e-06\n",
      "‖alpha‖₁       : 0.5999999999999968\n",
      "scores min/max : -1.7756584138335173e-07 -1.2670371389532122e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.684e-19\n",
      "‖w_svm‖₂       : 0.029791860057134478\n",
      "‖alpha‖₁       : 0.8985937848429684\n",
      "scores min/max : -0.7329555662801713 2.027177244830067\n",
      "Mask mean value:  tensor(0.8652, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3160  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.018e-15\n",
      "‖w_svm‖₂       : 1.1222822190134012e-07\n",
      "‖alpha‖₁       : 0.5199999999999683\n",
      "scores min/max : 2.93458188477544e-07 3.14834216134414e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.993e-19\n",
      "‖w_svm‖₂       : 7.484921112546972e-08\n",
      "‖alpha‖₁       : 0.6599999999999984\n",
      "scores min/max : 9.861523330710925e-08 2.4362082325651766e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.190e-09\n",
      "‖w_svm‖₂       : 0.04707658197049645\n",
      "‖alpha‖₁       : 0.9056465153458874\n",
      "scores min/max : -0.7152101738823531 1.888625297117303\n",
      "Mask mean value:  tensor(0.3632, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6753  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.078e-03\n",
      "‖w_svm‖₂       : 0.003279034039150773\n",
      "‖alpha‖₁       : 0.5799999999999982\n",
      "scores min/max : 0.0032665947244264935 0.0046232667597074645\n",
      "Mask mean value:  tensor(0.5217, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3317  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.968e-13\n",
      "‖w_svm‖₂       : 0.020770823022384952\n",
      "‖alpha‖₁       : 0.7799999999999997\n",
      "scores min/max : -0.03450759212339252 0.0340393896937789\n",
      "Mask mean value:  tensor(0.3605, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.5924  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.787e-15\n",
      "‖w_svm‖₂       : 5.740791711066909e-08\n",
      "‖alpha‖₁       : 0.23999999999999067\n",
      "scores min/max : -5.214758835175372e-08 -3.768547018552721e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.590e-20\n",
      "‖w_svm‖₂       : 0.044790893602000566\n",
      "‖alpha‖₁       : 0.9407949245992153\n",
      "scores min/max : -1.8264543797059023 0.3073853251898484\n",
      "Mask mean value:  tensor(0.4317, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3253  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.586e-14\n",
      "‖w_svm‖₂       : 0.18364539582456457\n",
      "‖alpha‖₁       : 0.883411063995484\n",
      "scores min/max : -3.642480827013955 6.0667355890725565\n",
      "Mask mean value:  tensor(0.0909, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1982  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.279e-03\n",
      "‖w_svm‖₂       : 0.020663038941306944\n",
      "‖alpha‖₁       : 0.823379626184507\n",
      "scores min/max : -1.9066723822887794 1.489920582289759\n",
      "Mask mean value:  tensor(0.6792, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.3356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.086e-05\n",
      "‖w_svm‖₂       : 0.00581025558499111\n",
      "‖alpha‖₁       : 0.5599999999999978\n",
      "scores min/max : 0.006019511743168794 0.00777687511493408\n",
      "Mask mean value:  tensor(0.5316, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1145  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 2.025e-05\n",
      "‖w_svm‖₂       : 0.0005201784562835315\n",
      "‖alpha‖₁       : 0.4399999999999433\n",
      "scores min/max : -0.0009304797000727579 -0.0006497276302002788\n",
      "Mask mean value:  tensor(0.4956, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6407  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.329e-16\n",
      "‖w_svm‖₂       : 0.00030411382243425563\n",
      "‖alpha‖₁       : 0.41999999996574655\n",
      "scores min/max : 8.989135434815968e-05 0.0004028710706973542\n",
      "Mask mean value:  tensor(0.5019, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9992  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.077e-15\n",
      "‖w_svm‖₂       : 1.1938450341744695e-06\n",
      "‖alpha‖₁       : 0.31999999999773504\n",
      "scores min/max : -2.618315752302042e-06 -2.500411789925109e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4897  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.192e-18\n",
      "‖w_svm‖₂       : 0.14661482175142643\n",
      "‖alpha‖₁       : 0.7594895995741959\n",
      "scores min/max : -1.7644160128594872 2.2131829008212174\n",
      "Mask mean value:  tensor(0.8443, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3589  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.334e-11\n",
      "‖w_svm‖₂       : 0.13853396378225735\n",
      "‖alpha‖₁       : 0.6551530834558975\n",
      "scores min/max : -18.066422438999787 1.8304294229857239\n",
      "Mask mean value:  tensor(0.2673, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2817  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.282e-03\n",
      "‖w_svm‖₂       : 0.08570899268546643\n",
      "‖alpha‖₁       : 0.5780676002277416\n",
      "scores min/max : -2.1345287827239376 5.785707117621729\n",
      "Mask mean value:  tensor(0.2492, dtype=torch.float64)\n",
      "max feasible return = 0.1060  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.9544018105257197e-07\n",
      "‖alpha‖₁       : 0.5799999999999867\n",
      "scores min/max : -3.5078822754876385e-07 -2.5979399698069143e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0289078598342193e-07\n",
      "‖alpha‖₁       : 0.2999999999999995\n",
      "scores min/max : 3.2568961653892226e-08 3.754328345134161e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 6.738503031485855e-08\n",
      "‖alpha‖₁       : 0.5999999999999746\n",
      "scores min/max : 1.674262248321526e-08 5.352607122478824e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.536888714884274e-08\n",
      "‖alpha‖₁       : 0.37999999999999967\n",
      "scores min/max : -1.3687226491245183e-08 -4.31738221029526e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 5.877852933026311e-06\n",
      "‖alpha‖₁       : 0.319999999941977\n",
      "scores min/max : 5.6481092130050395e-06 9.401371482725986e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4259  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04822027386432952\n",
      "‖alpha‖₁       : 0.7397167614832363\n",
      "scores min/max : -3.451815355924944 2.180912310721796\n",
      "Mask mean value:  tensor(0.9019, dtype=torch.float64)\n",
      "max feasible return = -0.9424  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.027127745567644416\n",
      "‖alpha‖₁       : 0.6151657506425725\n",
      "scores min/max : -1.9365977869600524 1.192294229214646\n",
      "Mask mean value:  tensor(0.7382, dtype=torch.float64)\n",
      "max feasible return = 2.5765  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.7390620386540846e-07\n",
      "‖alpha‖₁       : 0.4599999999997303\n",
      "scores min/max : 1.4468114323632872e-07 7.467245396044221e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.383064588915167e-07\n",
      "‖alpha‖₁       : 0.5199999999999831\n",
      "scores min/max : 9.095930687626896e-08 1.0387809804256776e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.5802358886583324e-07\n",
      "‖alpha‖₁       : 0.5799999999999644\n",
      "scores min/max : -2.2781799334745494e-07 -1.9141784375990186e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.0443620999144247\n",
      "‖alpha‖₁       : 0.6878646780022485\n",
      "scores min/max : -0.4443358235067984 1.9445481652119792\n",
      "Mask mean value:  tensor(0.3583, dtype=torch.float64)\n",
      "max feasible return = 0.0764  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.033199369011197966\n",
      "‖alpha‖₁       : 0.42229293389301636\n",
      "scores min/max : -3.8729037622110156 5.279004364337292\n",
      "Mask mean value:  tensor(0.1011, dtype=torch.float64)\n",
      "max feasible return = 0.6404  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04326543014536529\n",
      "‖alpha‖₁       : 0.821136797423853\n",
      "scores min/max : -4.993118312952042 3.1762688806957335\n",
      "Mask mean value:  tensor(0.9615, dtype=torch.float64)\n",
      "max feasible return = -3.6735  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.006817440189436997\n",
      "‖alpha‖₁       : 0.7999999999998578\n",
      "scores min/max : -0.030738226586425513 0.0005545413806601939\n",
      "Mask mean value:  tensor(0.4635, dtype=torch.float64)\n",
      "max feasible return = -0.1685  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.0865182369901605e-07\n",
      "‖alpha‖₁       : 0.6199999999999601\n",
      "scores min/max : 6.750701145807744e-08 8.404442627404297e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.8017903535482644e-08\n",
      "‖alpha‖₁       : 0.43999999999999984\n",
      "scores min/max : 5.103752151174593e-09 1.467371341332781e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.3140100847347035e-07\n",
      "‖alpha‖₁       : 0.2799999999999781\n",
      "scores min/max : -2.2292385739826912e-07 -2.049907007590693e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  41 | train 0.005312 | val 0.006716\n",
      "-----------------------------------------Epoch:  42 ----------------------------------------\n",
      "‖w_svm‖₂       : 0.04588912926931158\n",
      "‖alpha‖₁       : 0.9389251728694687\n",
      "scores min/max : -2.459743701849321 1.5893901406491553\n",
      "Mask mean value:  tensor(0.1479, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4051  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.187e-03\n",
      "‖w_svm‖₂       : 8.042670439630331e-08\n",
      "‖alpha‖₁       : 0.17999999999999927\n",
      "scores min/max : -1.9299975658327545e-07 4.056874296330171e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.375e-21\n",
      "‖w_svm‖₂       : 7.553652572278509e-08\n",
      "‖alpha‖₁       : 0.5399999999999942\n",
      "scores min/max : -1.8758513538148462e-07 -1.6604196624047833e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.963e-20\n",
      "‖w_svm‖₂       : 0.054747169202353604\n",
      "‖alpha‖₁       : 0.5763347290056036\n",
      "scores min/max : -1.958586664448271 0.8697014989995652\n",
      "Mask mean value:  tensor(0.7087, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9252  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.047e-03\n",
      "‖w_svm‖₂       : 0.005810012338568785\n",
      "‖alpha‖₁       : 0.5599999999999984\n",
      "scores min/max : 0.006077174696299787 0.007834389414415142\n",
      "Mask mean value:  tensor(0.5319, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1151  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 2.006e-05\n",
      "‖w_svm‖₂       : 2.047783632098022e-08\n",
      "‖alpha‖₁       : 0.11999999999999364\n",
      "scores min/max : -3.923484106063279e-08 -2.451904676832761e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.850e-09\n",
      "‖w_svm‖₂       : 8.370768536888844e-08\n",
      "‖alpha‖₁       : 0.3799999999999992\n",
      "scores min/max : 9.95921455733181e-09 2.812040044054836e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.907e-22\n",
      "‖w_svm‖₂       : 0.00030385280730224493\n",
      "‖alpha‖₁       : 0.4199999999645805\n",
      "scores min/max : 9.098575907516017e-05 0.00040341734377795737\n",
      "Mask mean value:  tensor(0.5019, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9992  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.072e-15\n",
      "‖w_svm‖₂       : 0.00571061226492203\n",
      "‖alpha‖₁       : 0.37999999999998035\n",
      "scores min/max : -0.004597872333844913 0.007504065502427916\n",
      "Mask mean value:  tensor(0.5223, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9288  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.091e-14\n",
      "‖w_svm‖₂       : 7.239171018181615e-08\n",
      "‖alpha‖₁       : 0.13999999999999857\n",
      "scores min/max : 1.3192321384578716e-08 1.8918934368292815e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.518e-08\n",
      "‖w_svm‖₂       : 0.08210370338452119\n",
      "‖alpha‖₁       : 0.47199565121945186\n",
      "scores min/max : -2.2652544784737225 2.813973293019646\n",
      "Mask mean value:  tensor(0.1276, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.3759  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.479e-02\n",
      "‖w_svm‖₂       : 5.329518134241507e-08\n",
      "‖alpha‖₁       : 0.11999999999998952\n",
      "scores min/max : -9.761879404967461e-08 -6.679215797125771e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.309e-07\n",
      "‖w_svm‖₂       : 0.030100551707319817\n",
      "‖alpha‖₁       : 0.5512016689470756\n",
      "scores min/max : -3.462046808827346 1.107410027734345\n",
      "Mask mean value:  tensor(0.1577, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.2250  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.271e-03\n",
      "‖w_svm‖₂       : 0.03978454728267705\n",
      "‖alpha‖₁       : 0.6599999999999822\n",
      "scores min/max : -0.21550891553831486 0.14302621547930283\n",
      "Mask mean value:  tensor(0.4501, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8394  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.438e-13\n",
      "‖w_svm‖₂       : 2.7898531903352445e-07\n",
      "‖alpha‖₁       : 0.6599999999999898\n",
      "scores min/max : -6.689524305370527e-07 -5.17224972073566e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.458e-18\n",
      "‖w_svm‖₂       : 1.1493563412265515e-07\n",
      "‖alpha‖₁       : 0.29999999999999405\n",
      "scores min/max : 1.6574568192994253e-07 1.7473311745613792e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.449e-07\n",
      "‖w_svm‖₂       : 0.017909402449540025\n",
      "‖alpha‖₁       : 0.6815989197054582\n",
      "scores min/max : -1.971857739899363 0.056999651687700964\n",
      "Mask mean value:  tensor(0.6140, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2598  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.627e-14\n",
      "‖w_svm‖₂       : 0.0470468231972222\n",
      "‖alpha‖₁       : 0.9056470168024886\n",
      "scores min/max : -0.7181948432246781 1.886866009067361\n",
      "Mask mean value:  tensor(0.3578, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6650  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.630e-03\n",
      "‖w_svm‖₂       : 1.0784469088655206e-07\n",
      "‖alpha‖₁       : 0.23999999999998126\n",
      "scores min/max : 7.372159808845271e-08 8.470471693059417e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.646e-21\n",
      "‖w_svm‖₂       : 0.03002780175709152\n",
      "‖alpha‖₁       : 0.8524453914720517\n",
      "scores min/max : -2.909540451870463 1.5711951315284625\n",
      "Mask mean value:  tensor(0.1584, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9543  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.620e-03\n",
      "‖w_svm‖₂       : 0.13921886703168626\n",
      "‖alpha‖₁       : 0.8799999999999986\n",
      "scores min/max : -1.3632002767549047 3.711267063549308\n",
      "Mask mean value:  tensor(0.3641, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2669  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.541e-07\n",
      "‖w_svm‖₂       : 0.14582767167015898\n",
      "‖alpha‖₁       : 0.7592589535738892\n",
      "scores min/max : -1.767430671083584 2.2100276157276975\n",
      "Mask mean value:  tensor(0.8398, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3571  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.558e-11\n",
      "‖w_svm‖₂       : 0.07212319138048542\n",
      "‖alpha‖₁       : 0.5799999999999484\n",
      "scores min/max : -2.8059455277919407 1.6439948615450866\n",
      "Mask mean value:  tensor(0.2328, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3941  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.882e-04\n",
      "‖w_svm‖₂       : 0.008771363031925414\n",
      "‖alpha‖₁       : 0.6076990311997841\n",
      "scores min/max : -1.9921158039754876 0.29464527426155573\n",
      "Mask mean value:  tensor(0.5479, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2212  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.862e-15\n",
      "‖w_svm‖₂       : 1.1945004756068507e-06\n",
      "‖alpha‖₁       : 0.3199999999977645\n",
      "scores min/max : -2.6350885374822563e-06 -2.5172884200036227e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4897  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.190e-18\n",
      "‖w_svm‖₂       : 0.051622093823858455\n",
      "‖alpha‖₁       : 0.8271939401403564\n",
      "scores min/max : -1.9486801745986753 0.4023431469213759\n",
      "Mask mean value:  tensor(0.5298, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0147  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.660e-02\n",
      "‖w_svm‖₂       : 0.07028920109129362\n",
      "‖alpha‖₁       : 0.4192096545154\n",
      "scores min/max : -1.924992027720307 2.140033997871052\n",
      "Mask mean value:  tensor(0.7489, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8937  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.201e-04\n",
      "‖w_svm‖₂       : 0.07228959717586848\n",
      "‖alpha‖₁       : 0.6586260302271214\n",
      "scores min/max : -2.007132090181967 4.321742583432237\n",
      "Mask mean value:  tensor(0.3986, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.4671  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.767e-04\n",
      "‖w_svm‖₂       : 3.8046069708138504e-07\n",
      "‖alpha‖₁       : 0.27999999999996716\n",
      "scores min/max : -1.224906290929011e-06 -1.1626782629502598e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.091e-19\n",
      "‖w_svm‖₂       : 7.499260989172263e-08\n",
      "‖alpha‖₁       : 0.6599999999999983\n",
      "scores min/max : 9.793806901301642e-08 2.429405858447882e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.190e-09\n",
      "‖w_svm‖₂       : 0.060792410557813543\n",
      "‖alpha‖₁       : 0.8987025812279397\n",
      "scores min/max : -1.6911063546864438 3.721883384047712\n",
      "Mask mean value:  tensor(0.4485, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5947  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.229e-03\n",
      "‖w_svm‖₂       : 5.2622842786754354e-08\n",
      "‖alpha‖₁       : 0.4399999999999866\n",
      "scores min/max : -1.9718283253638422e-07 -9.32186295123491e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.672e-20\n",
      "‖w_svm‖₂       : 7.336821297075177e-08\n",
      "‖alpha‖₁       : 0.4199999999999985\n",
      "scores min/max : -6.609202358636355e-08 -1.8971622236308585e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.546e-08\n",
      "‖w_svm‖₂       : 1.0081763644021464e-06\n",
      "‖alpha‖₁       : 0.5999999999999964\n",
      "scores min/max : -1.680263251724681e-07 -1.1716738595028227e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.699e-19\n",
      "‖w_svm‖₂       : 0.00657739604607255\n",
      "‖alpha‖₁       : 0.7005748063019898\n",
      "scores min/max : -2.002354852386785 0.021064522736791963\n",
      "Mask mean value:  tensor(0.4629, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2591  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.214e-12\n",
      "‖w_svm‖₂       : 4.3469263653800146e-07\n",
      "‖alpha‖₁       : 0.7199999999998468\n",
      "scores min/max : 1.1464948089304913e-07 1.7198830008141587e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.283e-21\n",
      "‖w_svm‖₂       : 1.7797900856114927e-05\n",
      "‖alpha‖₁       : 0.35999999998162435\n",
      "scores min/max : 1.3002950645284763e-05 1.4121318130585901e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.069e-17\n",
      "‖w_svm‖₂       : 0.18375482251781827\n",
      "‖alpha‖₁       : 0.8834577970792751\n",
      "scores min/max : -3.635623848114914 6.071833909954889\n",
      "Mask mean value:  tensor(0.0919, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2010  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.725e-03\n",
      "‖w_svm‖₂       : 1.0677167036038232e-06\n",
      "‖alpha‖₁       : 0.499999999999998\n",
      "scores min/max : -2.7266030507818014e-07 -7.982268684822757e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.006e-19\n",
      "‖w_svm‖₂       : 0.04015724875049275\n",
      "‖alpha‖₁       : 0.7294450481761579\n",
      "scores min/max : -0.263511873164575 2.0172121052431793\n",
      "Mask mean value:  tensor(0.6700, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.0359  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.757e-03\n",
      "‖w_svm‖₂       : 5.7500403994997736e-08\n",
      "‖alpha‖₁       : 0.2399999999999913\n",
      "scores min/max : -5.215256812138846e-08 -3.769727826855545e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.593e-20\n",
      "‖w_svm‖₂       : 0.019638018389291405\n",
      "‖alpha‖₁       : 0.6599999999999566\n",
      "scores min/max : -0.03680610786230443 0.025192757532616954\n",
      "Mask mean value:  tensor(0.4309, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.3814  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.175e-15\n",
      "‖w_svm‖₂       : 0.021891085245860167\n",
      "‖alpha‖₁       : 0.8150943253580888\n",
      "scores min/max : -11.515957235334866 2.0297757798783183\n",
      "Mask mean value:  tensor(0.7055, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.044e-06\n",
      "‖w_svm‖₂       : 2.395953259451984e-07\n",
      "‖alpha‖₁       : 0.58\n",
      "scores min/max : -2.3813787272146227e-07 -2.2252044901545512e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.138e-19\n",
      "‖w_svm‖₂       : 0.0005211130269032796\n",
      "‖alpha‖₁       : 0.43999999999999995\n",
      "scores min/max : -0.0009192896715629389 -0.0006371792254717084\n",
      "Mask mean value:  tensor(0.4956, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6409  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.341e-16\n",
      "‖w_svm‖₂       : 2.2661165990990378e-07\n",
      "‖alpha‖₁       : 0.25999999999999485\n",
      "scores min/max : 2.0865295450659116e-07 2.214348198039919e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.234e-17\n",
      "‖w_svm‖₂       : 2.3278815980515432e-07\n",
      "‖alpha‖₁       : 0.3799999999877558\n",
      "scores min/max : -1.7783455695260325e-07 -1.5590464683721243e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.718e-07\n",
      "‖w_svm‖₂       : 0.020876753636762942\n",
      "‖alpha‖₁       : 0.3834671449530643\n",
      "scores min/max : -1.9616288197484777 0.24086595007804507\n",
      "Mask mean value:  tensor(0.7081, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1518  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.160e-14\n",
      "‖w_svm‖₂       : 0.016289593888232135\n",
      "‖alpha‖₁       : 0.5963066716234984\n",
      "scores min/max : -2.858189564779708 2.274302337880103\n",
      "Mask mean value:  tensor(0.9166, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.5073  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.155e-07\n",
      "‖w_svm‖₂       : 0.04475403534642668\n",
      "‖alpha‖₁       : 0.9407947163265293\n",
      "scores min/max : -1.823845237692602 0.31000602377445097\n",
      "Mask mean value:  tensor(0.4421, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3255  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.460e-15\n",
      "‖w_svm‖₂       : 0.016167071409248213\n",
      "‖alpha‖₁       : 0.8599999999999984\n",
      "scores min/max : -0.05359806581184857 -0.040417924914476995\n",
      "Mask mean value:  tensor(0.2939, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0738  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.863e-14\n",
      "‖w_svm‖₂       : 1.1269695375651487e-07\n",
      "‖alpha‖₁       : 0.5199999999999645\n",
      "scores min/max : 2.8182469847281973e-07 3.0319695275348183e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.009e-19\n",
      "‖w_svm‖₂       : 0.027019753147376046\n",
      "‖alpha‖₁       : 0.19669695475159746\n",
      "scores min/max : -2.221408642856952 0.0007232083779131412\n",
      "Mask mean value:  tensor(0.0440, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2998  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.267e-03\n",
      "‖w_svm‖₂       : 0.014726055417492655\n",
      "‖alpha‖₁       : 0.8599999999999981\n",
      "scores min/max : -0.0503465897384508 -0.002110904861532866\n",
      "Mask mean value:  tensor(0.3512, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4415  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.152e-05\n",
      "‖w_svm‖₂       : 1.681586118508401e-07\n",
      "‖alpha‖₁       : 0.23999999999998517\n",
      "scores min/max : 2.3383810909159304e-07 2.4931961038659396e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.787e-20\n",
      "‖w_svm‖₂       : 1.9619197719256445e-07\n",
      "‖alpha‖₁       : 0.3799999999999907\n",
      "scores min/max : -1.364705030821666e-08 2.209473710409663e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.698e-19\n",
      "‖w_svm‖₂       : 0.13886773484740397\n",
      "‖alpha‖₁       : 0.6552495514658091\n",
      "scores min/max : -18.0753048728603 1.822164289969904\n",
      "Mask mean value:  tensor(0.2592, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2809  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.449e-03\n",
      "‖w_svm‖₂       : 6.665979861987306e-07\n",
      "‖alpha‖₁       : 0.39999999999999936\n",
      "scores min/max : -5.407251209141038e-09 3.0079393348971174e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.314e-09\n",
      "‖w_svm‖₂       : 0.00036361868829620536\n",
      "‖alpha‖₁       : 0.739999999999999\n",
      "scores min/max : -0.0004385288971735786 -0.00042166477309291676\n",
      "Mask mean value:  tensor(0.4979, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0598  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.409e-17\n",
      "‖w_svm‖₂       : 0.020807623889032748\n",
      "‖alpha‖₁       : 0.7799999999999996\n",
      "scores min/max : -0.03697634223268687 0.031787920392919944\n",
      "Mask mean value:  tensor(0.3493, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.5456  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.460e-15\n",
      "‖w_svm‖₂       : 3.560731163781845e-06\n",
      "‖alpha‖₁       : 0.41999999999300397\n",
      "scores min/max : -3.2339368533298933e-06 -1.5970633759760294e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1229  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.624e-05\n",
      "‖w_svm‖₂       : 0.00013922714562594515\n",
      "‖alpha‖₁       : 0.43999999999108336\n",
      "scores min/max : -0.00028596912157182624 -0.0001443710707716848\n",
      "Mask mean value:  tensor(0.4990, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0210  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.906e-17\n",
      "‖w_svm‖₂       : 0.029884644752885676\n",
      "‖alpha‖₁       : 0.8985950380369617\n",
      "scores min/max : -0.7323520724893291 2.027741495133337\n",
      "Mask mean value:  tensor(0.8660, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3147  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.999e-15\n",
      "‖w_svm‖₂       : 1.9687271218226617e-07\n",
      "‖alpha‖₁       : 0.41999999999996873\n",
      "scores min/max : -4.6944117893463255e-07 -4.472276742427831e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.959e-19\n",
      "‖w_svm‖₂       : 0.04166406439379357\n",
      "‖alpha‖₁       : 0.8900156941518156\n",
      "scores min/max : -2.0116105390268846 0.33618082266680305\n",
      "Mask mean value:  tensor(0.2566, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0601  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.833e-05\n",
      "‖w_svm‖₂       : 0.003288637399094867\n",
      "‖alpha‖₁       : 0.5799999999999981\n",
      "scores min/max : 0.003888173808389477 0.005252881838359504\n",
      "Mask mean value:  tensor(0.5248, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3337  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.520e-13\n",
      "‖w_svm‖₂       : 0.09537766221135008\n",
      "‖alpha‖₁       : 0.8733436123028665\n",
      "scores min/max : -12.187083545524324 2.1351796017754365\n",
      "Mask mean value:  tensor(0.4871, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.4498  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.382e-02\n",
      "‖w_svm‖₂       : 0.020704264100018\n",
      "‖alpha‖₁       : 0.8233840184322073\n",
      "scores min/max : -1.9066785390649688 1.4899483514842358\n",
      "Mask mean value:  tensor(0.6792, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.3357  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.105e-05\n",
      "‖w_svm‖₂       : 0.00018536468553405187\n",
      "‖alpha‖₁       : 0.8199999999999934\n",
      "scores min/max : -9.124592273131101e-05 -7.709614569390076e-05\n",
      "Mask mean value:  tensor(0.4996, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6965  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.349e-17\n",
      "‖w_svm‖₂       : 0.000904290741707183\n",
      "‖alpha‖₁       : 0.8199999999999987\n",
      "scores min/max : 0.0008122299818866426 0.002474825103713017\n",
      "Mask mean value:  tensor(0.5093, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5873  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.101e-16\n",
      "‖w_svm‖₂       : 2.3476135659228333e-07\n",
      "‖alpha‖₁       : 0.6399999999999969\n",
      "scores min/max : 4.0926420025783957e-07 4.301522426097765e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.047e-19\n",
      "‖w_svm‖₂       : 0.0044973694843604755\n",
      "‖alpha‖₁       : 0.45999999999998925\n",
      "scores min/max : -0.009521736408608261 -0.007346345917804045\n",
      "Mask mean value:  tensor(0.4608, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.0024  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.116e-15\n",
      "‖w_svm‖₂       : 4.896698187197525e-08\n",
      "‖alpha‖₁       : 0.179999999999997\n",
      "scores min/max : 4.600351545257077e-08 6.070102206355766e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.793e-08\n",
      "‖w_svm‖₂       : 0.00021127420177501077\n",
      "‖alpha‖₁       : 0.6199999999932464\n",
      "scores min/max : -4.888131176466754e-06 1.0927169670233435e-05\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.836e-17\n",
      "‖w_svm‖₂       : 0.036880934588494674\n",
      "‖alpha‖₁       : 0.9199999999999866\n",
      "scores min/max : -0.2719483106685381 0.22968284640985348\n",
      "Mask mean value:  tensor(0.1824, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9099  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.948e-03\n",
      "‖w_svm‖₂       : 0.00015989797775095829\n",
      "‖alpha‖₁       : 0.6399999999999998\n",
      "scores min/max : -0.00014593532112453894 -0.00013617975074466126\n",
      "Mask mean value:  tensor(0.4993, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7633  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.400e-17\n",
      "‖w_svm‖₂       : 0.08564831684333446\n",
      "‖alpha‖₁       : 0.5780711625961511\n",
      "scores min/max : -2.133590431604962 5.785896080267086\n",
      "Mask mean value:  tensor(0.2505, dtype=torch.float64)\n",
      "max feasible return = 0.1062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.967244664332192e-07\n",
      "‖alpha‖₁       : 0.5799999999999892\n",
      "scores min/max : -3.2660868669404104e-07 -2.3563888003518016e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0866953743689017e-07\n",
      "‖alpha‖₁       : 0.29999999999996396\n",
      "scores min/max : 3.374824539404289e-08 3.8921809736863215e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 6.753973123285757e-08\n",
      "‖alpha‖₁       : 0.599999999999901\n",
      "scores min/max : 1.3683483583767996e-08 5.041999111421717e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.5410851869421766e-08\n",
      "‖alpha‖₁       : 0.3799999999999996\n",
      "scores min/max : -1.565603118903332e-08 -6.285418034697236e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 5.81048291860402e-06\n",
      "‖alpha‖₁       : 0.3199999999416927\n",
      "scores min/max : 5.751984354875632e-06 9.488982265887192e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4259  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04798847666697363\n",
      "‖alpha‖₁       : 0.739717153746949\n",
      "scores min/max : -3.4482749670589454 2.18438539234821\n",
      "Mask mean value:  tensor(0.9041, dtype=torch.float64)\n",
      "max feasible return = -0.9438  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.027042442949685455\n",
      "‖alpha‖₁       : 0.6151667979270523\n",
      "scores min/max : -1.9402990635398534 1.1885214474183283\n",
      "Mask mean value:  tensor(0.7280, dtype=torch.float64)\n",
      "max feasible return = 2.5403  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.7103409305237017e-07\n",
      "‖alpha‖₁       : 0.4599999999999572\n",
      "scores min/max : 1.8300679880251028e-07 7.786747185396007e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.3937822986005306e-07\n",
      "‖alpha‖₁       : 0.5199999999999755\n",
      "scores min/max : 8.493586534577261e-08 9.785989524578316e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.587657526398533e-07\n",
      "‖alpha‖₁       : 0.5799999999999808\n",
      "scores min/max : -2.3281663631975442e-07 -1.96480371317037e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.04428509104930627\n",
      "‖alpha‖₁       : 0.6878628432204115\n",
      "scores min/max : -0.44539651560224036 1.9435086691946637\n",
      "Mask mean value:  tensor(0.3545, dtype=torch.float64)\n",
      "max feasible return = 0.0798  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.033180006137775775\n",
      "‖alpha‖₁       : 0.4222988421347908\n",
      "scores min/max : -3.8706744848641343 5.281922796132114\n",
      "Mask mean value:  tensor(0.1026, dtype=torch.float64)\n",
      "max feasible return = 0.6505  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04337197530248464\n",
      "‖alpha‖₁       : 0.8211389594681566\n",
      "scores min/max : -4.978579208620202 3.190712695546235\n",
      "Mask mean value:  tensor(0.9631, dtype=torch.float64)\n",
      "max feasible return = -3.6819  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.006783826804695033\n",
      "‖alpha‖₁       : 0.7999999999999998\n",
      "scores min/max : -0.03087084459701231 4.095595017866195e-05\n",
      "Mask mean value:  tensor(0.4614, dtype=torch.float64)\n",
      "max feasible return = -0.1678  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.107097241672975e-07\n",
      "‖alpha‖₁       : 0.6199999999999595\n",
      "scores min/max : 8.290987163496216e-08 9.944718124531101e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.834541188538903e-08\n",
      "‖alpha‖₁       : 0.43999999999999984\n",
      "scores min/max : 5.312283086706854e-09 1.4882078967397637e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.327724767143766e-07\n",
      "‖alpha‖₁       : 0.2799999999999784\n",
      "scores min/max : -2.3276831943162393e-07 -2.1483773895863942e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  42 | train 0.005317 | val 0.006706\n",
      "-----------------------------------------Epoch:  43 ----------------------------------------\n",
      "‖w_svm‖₂       : 0.026898595337514195\n",
      "‖alpha‖₁       : 0.19669366967042226\n",
      "scores min/max : -2.2195551610887505 0.002530224965229033\n",
      "Mask mean value:  tensor(0.0453, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3087  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.195e-03\n",
      "‖w_svm‖₂       : 0.008822838732075369\n",
      "‖alpha‖₁       : 0.6076999801200262\n",
      "scores min/max : -1.9924362506279567 0.2943404213556472\n",
      "Mask mean value:  tensor(0.5466, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2207  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.162e-14\n",
      "‖w_svm‖₂       : 1.1548797810822909e-07\n",
      "‖alpha‖₁       : 0.29999999999999377\n",
      "scores min/max : 1.6013347125668712e-07 1.6912131140677806e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.449e-07\n",
      "‖w_svm‖₂       : 1.1317137092526846e-07\n",
      "‖alpha‖₁       : 0.51999999999996\n",
      "scores min/max : 2.9652752164924337e-07 3.1789700634841154e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.044e-19\n",
      "‖w_svm‖₂       : 0.0009051087154147723\n",
      "‖alpha‖₁       : 0.8199999999999992\n",
      "scores min/max : 0.0008289170157555585 0.0024917040929821415\n",
      "Mask mean value:  tensor(0.5094, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5877  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.106e-16\n",
      "‖w_svm‖₂       : 0.13837824461917608\n",
      "‖alpha‖₁       : 0.6551301409736358\n",
      "scores min/max : -18.064953398367948 1.8346594956148476\n",
      "Mask mean value:  tensor(0.2716, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2813  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.237e-03\n",
      "‖w_svm‖₂       : 5.310544421159081e-08\n",
      "‖alpha‖₁       : 0.11999999999999701\n",
      "scores min/max : -9.52842241012322e-08 -6.4584845055868e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.088e-08\n",
      "‖w_svm‖₂       : 0.05466448917440888\n",
      "‖alpha‖₁       : 0.5763359840791373\n",
      "scores min/max : -1.9585049443893918 0.8698190256216803\n",
      "Mask mean value:  tensor(0.7090, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9260  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.050e-03\n",
      "‖w_svm‖₂       : 1.049012993374573e-07\n",
      "‖alpha‖₁       : 0.2399999999999984\n",
      "scores min/max : 6.16513383235442e-08 7.242356569051825e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.509e-21\n",
      "‖w_svm‖₂       : 0.01458861831877308\n",
      "‖alpha‖₁       : 0.8599999999999985\n",
      "scores min/max : -0.0501458648633292 -0.002949602294471658\n",
      "Mask mean value:  tensor(0.3501, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4395  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.267e-05\n",
      "‖w_svm‖₂       : 2.047750487408722e-08\n",
      "‖alpha‖₁       : 0.11999999999999361\n",
      "scores min/max : -3.952727997786015e-08 -2.480852841666392e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.845e-09\n",
      "‖w_svm‖₂       : 4.9120773864423656e-08\n",
      "‖alpha‖₁       : 0.17999999999999716\n",
      "scores min/max : 4.271141160226713e-08 5.741397255642436e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.754e-08\n",
      "‖w_svm‖₂       : 1.2043702804142616e-06\n",
      "‖alpha‖₁       : 0.3199999999976182\n",
      "scores min/max : -2.6038982241534647e-06 -2.4856054368534964e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4897  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.217e-18\n",
      "‖w_svm‖₂       : 0.07023617495750709\n",
      "‖alpha‖₁       : 0.4192268981687193\n",
      "scores min/max : -1.9131494478304152 2.148166442986091\n",
      "Mask mean value:  tensor(0.7608, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8980  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.574e-04\n",
      "‖w_svm‖₂       : 1.969827370096009e-07\n",
      "‖alpha‖₁       : 0.41999999999996906\n",
      "scores min/max : -4.5934080249732495e-07 -4.371437624290059e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.938e-19\n",
      "‖w_svm‖₂       : 2.3478408786282526e-07\n",
      "‖alpha‖₁       : 0.6399999999999967\n",
      "scores min/max : 4.225231829767406e-07 4.4341255974955326e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.058e-19\n",
      "‖w_svm‖₂       : 1.0113808890266623e-06\n",
      "‖alpha‖₁       : 0.599999999999996\n",
      "scores min/max : -1.244309662317333e-07 -7.357060129294432e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.683e-19\n",
      "‖w_svm‖₂       : 8.44105345096359e-08\n",
      "‖alpha‖₁       : 0.3799999999999992\n",
      "scores min/max : 3.0540927302325284e-10 1.846688910285383e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.031e-22\n",
      "‖w_svm‖₂       : 1.0708305581210667e-06\n",
      "‖alpha‖₁       : 0.49999999999999645\n",
      "scores min/max : -2.551627574595357e-07 -6.227196151534105e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.057e-19\n",
      "‖w_svm‖₂       : 0.005786948485543728\n",
      "‖alpha‖₁       : 0.5599999999999659\n",
      "scores min/max : 0.005706527520945572 0.007450720610429051\n",
      "Mask mean value:  tensor(0.5301, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1112  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 2.177e-05\n",
      "‖w_svm‖₂       : 0.07210637350966681\n",
      "‖alpha‖₁       : 0.5799999999999523\n",
      "scores min/max : -2.799322076249029 1.6432160545898282\n",
      "Mask mean value:  tensor(0.2377, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3997  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.657e-04\n",
      "‖w_svm‖₂       : 2.798408526336247e-07\n",
      "‖alpha‖₁       : 0.6599999999999832\n",
      "scores min/max : -6.768517914967566e-07 -5.250896097533099e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.479e-18\n",
      "‖w_svm‖₂       : 0.00013915827692566598\n",
      "‖alpha‖₁       : 0.43999999999168676\n",
      "scores min/max : -0.0002956738140868797 -0.0001541247691072814\n",
      "Mask mean value:  tensor(0.4990, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0206  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.954e-17\n",
      "‖w_svm‖₂       : 1.6807882829252353e-07\n",
      "‖alpha‖₁       : 0.23999999999998814\n",
      "scores min/max : 2.4404198594729386e-07 2.594787976191695e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.792e-20\n",
      "‖w_svm‖₂       : 5.7929855386395804e-08\n",
      "‖alpha‖₁       : 0.23999999999999155\n",
      "scores min/max : -5.981446210338632e-08 -4.536222648801661e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.620e-20\n",
      "‖w_svm‖₂       : 1.987477016758846e-07\n",
      "‖alpha‖₁       : 0.3799999999999525\n",
      "scores min/max : -4.541885831175258e-08 -9.509035371524708e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.796e-19\n",
      "‖w_svm‖₂       : 0.030112600942771937\n",
      "‖alpha‖₁       : 0.8524531524079701\n",
      "scores min/max : -2.8901591590958313 1.5905234966029997\n",
      "Mask mean value:  tensor(0.1683, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9631  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.042e-03\n",
      "‖w_svm‖₂       : 3.796116751262589e-07\n",
      "‖alpha‖₁       : 0.27999999999999786\n",
      "scores min/max : -1.1538151923276094e-06 -1.0921157885606113e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.059e-19\n",
      "‖w_svm‖₂       : 0.02105379314813507\n",
      "‖alpha‖₁       : 0.38347501337206114\n",
      "scores min/max : -1.957047215248711 0.245481252468024\n",
      "Mask mean value:  tensor(0.7248, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1542  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.305e-14\n",
      "‖w_svm‖₂       : 0.1460039270114718\n",
      "‖alpha‖₁       : 0.7593160511583301\n",
      "scores min/max : -1.770355519797596 2.2070019884583627\n",
      "Mask mean value:  tensor(0.8354, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3564  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.709e-11\n",
      "‖w_svm‖₂       : 0.00015992467960290266\n",
      "‖alpha‖₁       : 0.6399999999999999\n",
      "scores min/max : -0.00013855754161915337 -0.0001287987453488373\n",
      "Mask mean value:  tensor(0.4993, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7634  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.385e-17\n",
      "‖w_svm‖₂       : 0.07264822300503815\n",
      "‖alpha‖₁       : 0.6586769137447092\n",
      "scores min/max : -2.0154201776280836 4.313314349022762\n",
      "Mask mean value:  tensor(0.3817, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3226  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.733e-04\n",
      "‖w_svm‖₂       : 0.01618654065053015\n",
      "‖alpha‖₁       : 0.596305432978299\n",
      "scores min/max : -2.838869736301655 2.280671689735338\n",
      "Mask mean value:  tensor(0.9195, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.5101  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.526e-13\n",
      "‖w_svm‖₂       : 2.420955118840083e-07\n",
      "‖alpha‖₁       : 0.5799999999999756\n",
      "scores min/max : -2.946674862718703e-07 -2.789945244111945e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.180e-19\n",
      "‖w_svm‖₂       : 0.04139941579387691\n",
      "‖alpha‖₁       : 0.8900045411221069\n",
      "scores min/max : -2.015034992623276 0.3327355442110062\n",
      "Mask mean value:  tensor(0.2468, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0550  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.620e-05\n",
      "‖w_svm‖₂       : 0.006639852042358372\n",
      "‖alpha‖₁       : 0.7005755629005076\n",
      "scores min/max : -2.0014645590020943 0.02195287252765373\n",
      "Mask mean value:  tensor(0.4672, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2615  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.399e-13\n",
      "‖w_svm‖₂       : 2.0985553437703636e-07\n",
      "‖alpha‖₁       : 0.37999999999415823\n",
      "scores min/max : -1.276149552300456e-07 -1.0900085535000139e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.121e-07\n",
      "‖w_svm‖₂       : 3.6153119032950584e-06\n",
      "‖alpha‖₁       : 0.41999999999279625\n",
      "scores min/max : -3.499140018775093e-06 -1.8499495720821212e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1229  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.670e-05\n",
      "‖w_svm‖₂       : 0.0003630598194102249\n",
      "‖alpha‖₁       : 0.7399999999999991\n",
      "scores min/max : -0.00042979964956798753 -0.0004129873072369845\n",
      "Mask mean value:  tensor(0.4979, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0599  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.413e-17\n",
      "‖w_svm‖₂       : 0.045131370243121124\n",
      "‖alpha‖₁       : 0.9408270802226777\n",
      "scores min/max : -1.8343294947778546 0.29950757558420693\n",
      "Mask mean value:  tensor(0.4007, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3236  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.736e-14\n",
      "‖w_svm‖₂       : 0.036633299107331714\n",
      "‖alpha‖₁       : 0.9199999999999817\n",
      "scores min/max : -0.26925393551255805 0.22594750552051468\n",
      "Mask mean value:  tensor(0.1822, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9096  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.922e-03\n",
      "‖w_svm‖₂       : 0.01788454548205204\n",
      "‖alpha‖₁       : 0.6815990997432329\n",
      "scores min/max : -1.9693058293021741 0.05938733757200751\n",
      "Mask mean value:  tensor(0.6255, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2664  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.331e-12\n",
      "‖w_svm‖₂       : 4.3714146780384844e-07\n",
      "‖alpha‖₁       : 0.7199999999998509\n",
      "scores min/max : 1.7655269691906544e-07 2.3388656583144258e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.327e-21\n",
      "‖w_svm‖₂       : 0.08181641915405721\n",
      "‖alpha‖₁       : 0.47195884105272623\n",
      "scores min/max : -2.2376204391997927 2.8417569320924985\n",
      "Mask mean value:  tensor(0.1448, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.5451  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.647e-02\n",
      "‖w_svm‖₂       : 7.421175322552397e-08\n",
      "‖alpha‖₁       : 0.4199999999999957\n",
      "scores min/max : -6.565199290286115e-08 -1.847780787049293e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.617e-08\n",
      "‖w_svm‖₂       : 0.18273413260421556\n",
      "‖alpha‖₁       : 0.8831347418633108\n",
      "scores min/max : -3.6350073620315113 6.0748454477157905\n",
      "Mask mean value:  tensor(0.0924, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2027  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.557e-05\n",
      "‖w_svm‖₂       : 5.240303900214253e-08\n",
      "‖alpha‖₁       : 0.43999999999998696\n",
      "scores min/max : -1.9472207215954875e-07 -9.074995000613492e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.620e-20\n",
      "‖w_svm‖₂       : 0.0159082630941009\n",
      "‖alpha‖₁       : 0.859999999999997\n",
      "scores min/max : -0.04688325305841361 -0.034197120366275036\n",
      "Mask mean value:  tensor(0.3209, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0825  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.450e-14\n",
      "‖w_svm‖₂       : 0.00030371838911942637\n",
      "‖alpha‖₁       : 0.41999999981468483\n",
      "scores min/max : 0.0001275063201690112 0.0004381387594481875\n",
      "Mask mean value:  tensor(0.5020, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9996  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.079e-15\n",
      "‖w_svm‖₂       : 0.030064864632627433\n",
      "‖alpha‖₁       : 0.5512028629356847\n",
      "scores min/max : -3.4614656103481667 1.1078546683989354\n",
      "Mask mean value:  tensor(0.1580, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.2265  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.164e-03\n",
      "‖w_svm‖₂       : 0.0005178198475073789\n",
      "‖alpha‖₁       : 0.4399999999999998\n",
      "scores min/max : -0.0008715557046866521 -0.0005918099518063082\n",
      "Mask mean value:  tensor(0.4959, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6417  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.335e-16\n",
      "‖w_svm‖₂       : 7.279627319071821e-08\n",
      "‖alpha‖₁       : 0.13999999999999635\n",
      "scores min/max : 1.4872354551609444e-08 2.0636864329069168e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.682e-08\n",
      "‖w_svm‖₂       : 0.05141342244330013\n",
      "‖alpha‖₁       : 0.8271776110371916\n",
      "scores min/max : -1.9509265311160056 0.40013207305161796\n",
      "Mask mean value:  tensor(0.5218, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0133  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.717e-02\n",
      "‖w_svm‖₂       : 0.03928880505929038\n",
      "‖alpha‖₁       : 0.6599999999999808\n",
      "scores min/max : -0.2064939255738506 0.14252547931922344\n",
      "Mask mean value:  tensor(0.4665, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8749  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.016e-13\n",
      "‖w_svm‖₂       : 0.04558914730360878\n",
      "‖alpha‖₁       : 0.9389008852607771\n",
      "scores min/max : -2.4737259727904823 1.5756777130077293\n",
      "Mask mean value:  tensor(0.1392, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3879  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.160e-03\n",
      "‖w_svm‖₂       : 7.477431660182805e-08\n",
      "‖alpha‖₁       : 0.6599999999999984\n",
      "scores min/max : 9.734680717759736e-08 2.423575509242152e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.236e-09\n",
      "‖w_svm‖₂       : 0.06038745412252305\n",
      "‖alpha‖₁       : 0.8986600811933892\n",
      "scores min/max : -1.6982911249370325 3.714487913639844\n",
      "Mask mean value:  tensor(0.4334, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5832  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.080e-02\n",
      "‖w_svm‖₂       : 8.042274672660752e-08\n",
      "‖alpha‖₁       : 0.17999999999999872\n",
      "scores min/max : -1.9307771863565615e-07 4.086193392384442e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.378e-21\n",
      "‖w_svm‖₂       : 0.040523362684327616\n",
      "‖alpha‖₁       : 0.7294718890115807\n",
      "scores min/max : -0.2650290231494249 2.015697048226037\n",
      "Mask mean value:  tensor(0.6655, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.0161  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.848e-03\n",
      "‖w_svm‖₂       : 0.005668757562525004\n",
      "‖alpha‖₁       : 0.3799999999999939\n",
      "scores min/max : -0.004458597285571131 0.007429751549451753\n",
      "Mask mean value:  tensor(0.5222, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9289  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.018e-14\n",
      "‖w_svm‖₂       : 0.04673069870855541\n",
      "‖alpha‖₁       : 0.9056470674571658\n",
      "scores min/max : -0.7167926030773287 1.8846915094363146\n",
      "Mask mean value:  tensor(0.3488, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6491  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.200e-03\n",
      "‖w_svm‖₂       : 0.00021182836980790222\n",
      "‖alpha‖₁       : 0.6199999999970147\n",
      "scores min/max : -5.033778384084338e-06 1.0865344505740435e-05\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.844e-17\n",
      "‖w_svm‖₂       : 0.003285323090019065\n",
      "‖alpha‖₁       : 0.5799999999999993\n",
      "scores min/max : 0.0037894544963345037 0.005148785658975384\n",
      "Mask mean value:  tensor(0.5243, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3334  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.569e-13\n",
      "‖w_svm‖₂       : 7.527524306363434e-08\n",
      "‖alpha‖₁       : 0.5399999999999952\n",
      "scores min/max : -1.7565854643136597e-07 -1.5411622409923046e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.896e-20\n",
      "‖w_svm‖₂       : 0.13858243963797662\n",
      "‖alpha‖₁       : 0.879999999999999\n",
      "scores min/max : -1.3425128305028724 3.690242999548442\n",
      "Mask mean value:  tensor(0.3862, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2668  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.969e-11\n",
      "‖w_svm‖₂       : 0.020595729655011837\n",
      "‖alpha‖₁       : 0.7799999999999969\n",
      "scores min/max : -0.039267002384410415 0.026855774594672588\n",
      "Mask mean value:  tensor(0.3380, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.4950  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.126e-15\n",
      "‖w_svm‖₂       : 0.00018537485169966094\n",
      "‖alpha‖₁       : 0.819999999999997\n",
      "scores min/max : -9.501736403455248e-05 -8.143468147831954e-05\n",
      "Mask mean value:  tensor(0.4996, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6964  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.416e-17\n",
      "‖w_svm‖₂       : 0.029747876644108908\n",
      "‖alpha‖₁       : 0.8985937177457547\n",
      "scores min/max : -0.7328719451702713 2.027201084945567\n",
      "Mask mean value:  tensor(0.8652, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3152  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.039e-15\n",
      "‖w_svm‖₂       : 2.2661532739013892e-07\n",
      "‖alpha‖₁       : 0.259999999999996\n",
      "scores min/max : 1.7351264397726355e-07 1.8628576483784057e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.115e-17\n",
      "‖w_svm‖₂       : 0.09403389423683439\n",
      "‖alpha‖₁       : 0.8733507621194122\n",
      "scores min/max : -12.14532298032171 2.1758726075826056\n",
      "Mask mean value:  tensor(0.5850, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7330  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 7.569e-02\n",
      "‖w_svm‖₂       : 0.021637971763871267\n",
      "‖alpha‖₁       : 0.8150932121005129\n",
      "scores min/max : -11.511616678635999 2.033596201402011\n",
      "Mask mean value:  tensor(0.7137, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9245  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.539e-06\n",
      "‖w_svm‖₂       : 6.709631699720356e-07\n",
      "‖alpha‖₁       : 0.3999999999999997\n",
      "scores min/max : 1.4741092139693126e-08 3.2090593939565525e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.350e-09\n",
      "‖w_svm‖₂       : 1.3337527030034495e-05\n",
      "‖alpha‖₁       : 0.3599999999999989\n",
      "scores min/max : 9.510343938763546e-06 1.029690389839038e-05\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.080e-17\n",
      "‖w_svm‖₂       : 0.019553520758348052\n",
      "‖alpha‖₁       : 0.6599999999999836\n",
      "scores min/max : -0.036776529487201726 0.024007191962546255\n",
      "Mask mean value:  tensor(0.4296, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.3699  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.167e-15\n",
      "‖w_svm‖₂       : 0.020693758819887435\n",
      "‖alpha‖₁       : 0.823383070772812\n",
      "scores min/max : -1.907615820999121 1.4889427743546617\n",
      "Mask mean value:  tensor(0.6765, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.3295  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.320e-05\n",
      "‖w_svm‖₂       : 0.004511277336287287\n",
      "‖alpha‖₁       : 0.4599999999999973\n",
      "scores min/max : -0.00966537903959255 -0.007473168213781554\n",
      "Mask mean value:  tensor(0.4602, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9996  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.123e-15\n",
      "‖w_svm‖₂       : 0.085745046876314\n",
      "‖alpha‖₁       : 0.5780989111984256\n",
      "scores min/max : -2.1305784961489196 5.787930086965696\n",
      "Mask mean value:  tensor(0.2555, dtype=torch.float64)\n",
      "max feasible return = 0.1068  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.9464465751002146e-07\n",
      "‖alpha‖₁       : 0.5799999999999912\n",
      "scores min/max : -3.2616547623902154e-07 -2.3521975659223525e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0826235076070919e-07\n",
      "‖alpha‖₁       : 0.2999999999999642\n",
      "scores min/max : 3.283061726640244e-08 3.8002995492544075e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 6.683798388186692e-08\n",
      "‖alpha‖₁       : 0.5999999999999592\n",
      "scores min/max : 1.1733049128853943e-08 4.850764985061563e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.5176609003146455e-08\n",
      "‖alpha‖₁       : 0.37999999999998263\n",
      "scores min/max : -1.7199389379700176e-08 -7.814907675623258e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 5.807437131796536e-06\n",
      "‖alpha‖₁       : 0.3199999999409477\n",
      "scores min/max : 5.351706810000643e-06 9.121021788785324e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4259  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04771179548280785\n",
      "‖alpha‖₁       : 0.7397115338050886\n",
      "scores min/max : -3.4482631234920356 2.1843095288040963\n",
      "Mask mean value:  tensor(0.9041, dtype=torch.float64)\n",
      "max feasible return = -0.9439  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.026836509139955018\n",
      "‖alpha‖₁       : 0.6151657348510474\n",
      "scores min/max : -1.9407398967736116 1.187978937329408\n",
      "Mask mean value:  tensor(0.7270, dtype=torch.float64)\n",
      "max feasible return = 2.5365  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.6820659716617476e-07\n",
      "‖alpha‖₁       : 0.4599999999999915\n",
      "scores min/max : 1.4584882909249635e-07 7.404129040505405e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.390036920395188e-07\n",
      "‖alpha‖₁       : 0.5199999999999785\n",
      "scores min/max : 8.769628964031859e-08 1.00618228975005e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.5872156697029233e-07\n",
      "‖alpha‖₁       : 0.5799999999999612\n",
      "scores min/max : -2.2471535694581748e-07 -1.8830971850159704e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.04417844723906827\n",
      "‖alpha‖₁       : 0.6878736054599628\n",
      "scores min/max : -0.444011734136466 1.9450425182579962\n",
      "Mask mean value:  tensor(0.3589, dtype=torch.float64)\n",
      "max feasible return = 0.0746  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03323576663065525\n",
      "‖alpha‖₁       : 0.4223047220155089\n",
      "scores min/max : -3.868092063004444 5.284699405533928\n",
      "Mask mean value:  tensor(0.1043, dtype=torch.float64)\n",
      "max feasible return = 0.6613  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.0433872797665388\n",
      "‖alpha‖₁       : 0.8211489482063342\n",
      "scores min/max : -4.974932946994787 3.194566287758503\n",
      "Mask mean value:  tensor(0.9635, dtype=torch.float64)\n",
      "max feasible return = -3.6839  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.006726745260942986\n",
      "‖alpha‖₁       : 0.7999999999999999\n",
      "scores min/max : -0.03051013789074357 -0.00010200964912442621\n",
      "Mask mean value:  tensor(0.4613, dtype=torch.float64)\n",
      "max feasible return = -0.1677  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.0926579256737755e-07\n",
      "‖alpha‖₁       : 0.6199999999999618\n",
      "scores min/max : 7.273826705916415e-08 8.92712769744532e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.8254733509989695e-08\n",
      "‖alpha‖₁       : 0.43999999999999984\n",
      "scores min/max : 4.9144614965839625e-09 1.4484307240009616e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.3209456752076687e-07\n",
      "‖alpha‖₁       : 0.2799999999999786\n",
      "scores min/max : -2.1551485549652466e-07 -1.9758482420841503e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  43 | train 0.005316 | val 0.006675\n",
      "-----------------------------------------Epoch:  44 ----------------------------------------\n",
      "‖w_svm‖₂       : 0.09395395027067338\n",
      "‖alpha‖₁       : 0.8733534068658091\n",
      "scores min/max : -12.171527128793674 2.149234451253231\n",
      "Mask mean value:  tensor(0.5216, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.5487  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 4.473e-02\n",
      "‖w_svm‖₂       : 0.0406445697168509\n",
      "‖alpha‖₁       : 0.7294831129891894\n",
      "scores min/max : -0.2650576981705763 2.0156673132007006\n",
      "Mask mean value:  tensor(0.6653, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.0156  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.863e-03\n",
      "‖w_svm‖₂       : 0.019498579724553124\n",
      "‖alpha‖₁       : 0.6599999999999893\n",
      "scores min/max : -0.03641586864058516 0.02385581106681816\n",
      "Mask mean value:  tensor(0.4306, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.3771  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.153e-15\n",
      "‖w_svm‖₂       : 1.96465894836773e-07\n",
      "‖alpha‖₁       : 0.41999999999997045\n",
      "scores min/max : -4.7261187213880674e-07 -4.5043337961560197e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.974e-19\n",
      "‖w_svm‖₂       : 0.061114043630902\n",
      "‖alpha‖₁       : 0.8987384871136314\n",
      "scores min/max : -1.686882952121471 3.7259291102329666\n",
      "Mask mean value:  tensor(0.4572, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6013  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.304e-03\n",
      "‖w_svm‖₂       : 0.04581741237875553\n",
      "‖alpha‖₁       : 0.9389207949889995\n",
      "scores min/max : -2.4630527310179406 1.5863583935786223\n",
      "Mask mean value:  tensor(0.1458, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4015  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.252e-03\n",
      "‖w_svm‖₂       : 0.07167092643620414\n",
      "‖alpha‖₁       : 0.5799999999999672\n",
      "scores min/max : -2.767571275691076 1.6205542372981268\n",
      "Mask mean value:  tensor(0.2328, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3928  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.763e-04\n",
      "‖w_svm‖₂       : 2.704698459423773e-07\n",
      "‖alpha‖₁       : 0.37999999998973244\n",
      "scores min/max : -1.9529432316285845e-07 -1.7420318130245446e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.129e-06\n",
      "‖w_svm‖₂       : 0.03625533895348651\n",
      "‖alpha‖₁       : 0.9199999999999942\n",
      "scores min/max : -0.2678211442119657 0.21701539679130022\n",
      "Mask mean value:  tensor(0.1758, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8782  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 7.227e-03\n",
      "‖w_svm‖₂       : 1.2050509827772302e-06\n",
      "‖alpha‖₁       : 0.3199999999975908\n",
      "scores min/max : -2.6450461763442993e-06 -2.5266547726629046e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4897  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.252e-18\n",
      "‖w_svm‖₂       : 0.020726591406117635\n",
      "‖alpha‖₁       : 0.7799999999999997\n",
      "scores min/max : -0.03783765536926704 0.02851588982734632\n",
      "Mask mean value:  tensor(0.3445, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.5230  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.759e-15\n",
      "‖w_svm‖₂       : 0.0003017816146131128\n",
      "‖alpha‖₁       : 0.4199999999578816\n",
      "scores min/max : 0.00010377337609186093 0.00041188039862919806\n",
      "Mask mean value:  tensor(0.5019, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9993  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.093e-15\n",
      "‖w_svm‖₂       : 0.0009136018618537285\n",
      "‖alpha‖₁       : 0.8199999999999983\n",
      "scores min/max : 0.0008456961485095679 0.002539450488899075\n",
      "Mask mean value:  tensor(0.5096, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5886  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.112e-16\n",
      "‖w_svm‖₂       : 0.000363539115121248\n",
      "‖alpha‖₁       : 0.7399999999999991\n",
      "scores min/max : -0.0004286077285340531 -0.00041175098128320834\n",
      "Mask mean value:  tensor(0.4979, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0599  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.439e-17\n",
      "‖w_svm‖₂       : 0.021620099076684385\n",
      "‖alpha‖₁       : 0.8150953186776675\n",
      "scores min/max : -11.512633313683692 2.0320819913846737\n",
      "Mask mean value:  tensor(0.7106, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9174  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.484e-06\n",
      "‖w_svm‖₂       : 0.0005239622138715755\n",
      "‖alpha‖₁       : 0.4399999999999994\n",
      "scores min/max : -0.0009355186846694266 -0.0006483178635285306\n",
      "Mask mean value:  tensor(0.4956, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6406  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.356e-16\n",
      "‖w_svm‖₂       : 7.399591090097571e-08\n",
      "‖alpha‖₁       : 0.41999999999999993\n",
      "scores min/max : -6.636118593208718e-08 -1.9270159329180595e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.700e-08\n",
      "‖w_svm‖₂       : 0.07321265376668425\n",
      "‖alpha‖₁       : 0.6587607898614413\n",
      "scores min/max : -2.0219116211450583 4.307654522360621\n",
      "Mask mean value:  tensor(0.3690, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.2084  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.716e-04\n",
      "‖w_svm‖₂       : 0.18444062500267303\n",
      "‖alpha‖₁       : 0.8837444803136081\n",
      "scores min/max : -3.644089098766419 6.06588369748005\n",
      "Mask mean value:  tensor(0.0908, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1984  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.173e-03\n",
      "‖w_svm‖₂       : 1.9861477568585765e-07\n",
      "‖alpha‖₁       : 0.3799999999999562\n",
      "scores min/max : -2.3069906912788506e-08 1.2823869414812241e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.776e-19\n",
      "‖w_svm‖₂       : 0.026865821741282717\n",
      "‖alpha‖₁       : 0.19670339986978494\n",
      "scores min/max : -2.221705274227745 5.092636197481792e-06\n",
      "Mask mean value:  tensor(0.0437, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2980  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.283e-03\n",
      "‖w_svm‖₂       : 0.04559884186457337\n",
      "‖alpha‖₁       : 0.9408711720967589\n",
      "scores min/max : -1.8275860073890868 0.3062568159570568\n",
      "Mask mean value:  tensor(0.4272, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3250  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.529e-10\n",
      "‖w_svm‖₂       : 0.13921397459521812\n",
      "‖alpha‖₁       : 0.6553553718945435\n",
      "scores min/max : -18.07161125822346 1.8270379295832562\n",
      "Mask mean value:  tensor(0.2639, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2813  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.884e-03\n",
      "‖w_svm‖₂       : 0.020686907408172226\n",
      "‖alpha‖₁       : 0.823383327165854\n",
      "scores min/max : -1.9083274479229477 1.4880523376997328\n",
      "Mask mean value:  tensor(0.6743, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.3243  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.487e-05\n",
      "‖w_svm‖₂       : 0.00013921870507522518\n",
      "‖alpha‖₁       : 0.43999999999870626\n",
      "scores min/max : -0.0002909737827720175 -0.00014932972646559376\n",
      "Mask mean value:  tensor(0.4990, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0208  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.956e-17\n",
      "‖w_svm‖₂       : 0.008999237267146389\n",
      "‖alpha‖₁       : 0.6077030427733007\n",
      "scores min/max : -1.9930158336375872 0.2937429741260709\n",
      "Mask mean value:  tensor(0.5440, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2196  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.202e-14\n",
      "‖w_svm‖₂       : 0.0045257682908009566\n",
      "‖alpha‖₁       : 0.45999999999999347\n",
      "scores min/max : -0.00984433135837107 -0.007635158238474497\n",
      "Mask mean value:  tensor(0.4593, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9960  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.131e-15\n",
      "‖w_svm‖₂       : 1.0110092976025488e-06\n",
      "‖alpha‖₁       : 0.5999999999999956\n",
      "scores min/max : -1.9345140559880808e-07 -1.4259019521307842e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.737e-19\n",
      "‖w_svm‖₂       : 0.006741177680153788\n",
      "‖alpha‖₁       : 0.7005767206201909\n",
      "scores min/max : -2.002234610578267 0.021188091720183013\n",
      "Mask mean value:  tensor(0.4635, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2594  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.084e-12\n",
      "‖w_svm‖₂       : 0.00015987327527508592\n",
      "‖alpha‖₁       : 0.6399999999999999\n",
      "scores min/max : -0.00015891899517432118 -0.0001491664783050934\n",
      "Mask mean value:  tensor(0.4992, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7632  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.452e-17\n",
      "‖w_svm‖₂       : 7.456975897305473e-08\n",
      "‖alpha‖₁       : 0.659999999999999\n",
      "scores min/max : 9.801797931403739e-08 2.4308029865976826e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.261e-09\n",
      "‖w_svm‖₂       : 0.03052917549490816\n",
      "‖alpha‖₁       : 0.852475236708618\n",
      "scores min/max : -2.90244739072236 1.5781705404114403\n",
      "Mask mean value:  tensor(0.1618, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9575  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.506e-03\n",
      "‖w_svm‖₂       : 0.05206559377790558\n",
      "‖alpha‖₁       : 0.8272418448662882\n",
      "scores min/max : -1.9441975775172846 0.40683967471728844\n",
      "Mask mean value:  tensor(0.5458, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0178  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.576e-02\n",
      "‖w_svm‖₂       : 2.3473650845254877e-07\n",
      "‖alpha‖₁       : 0.6399999999999967\n",
      "scores min/max : 4.1348274371016737e-07 4.343743674902892e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.062e-19\n",
      "‖w_svm‖₂       : 2.421681463254062e-07\n",
      "‖alpha‖₁       : 0.5799999999999768\n",
      "scores min/max : -2.6916720522916513e-07 -2.534994645180269e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.190e-19\n",
      "‖w_svm‖₂       : 0.00021121000735309017\n",
      "‖alpha‖₁       : 0.619999999994187\n",
      "scores min/max : -1.8452402576068258e-06 1.3960585272971297e-05\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.906e-17\n",
      "‖w_svm‖₂       : 1.6943212868707085e-07\n",
      "‖alpha‖₁       : 0.23999999999997898\n",
      "scores min/max : 2.408818118628193e-07 2.565693757022765e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.903e-20\n",
      "‖w_svm‖₂       : 2.0244183604684434e-08\n",
      "‖alpha‖₁       : 0.11999999999999325\n",
      "scores min/max : -4.119431971627365e-08 -2.6459508057436446e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.844e-09\n",
      "‖w_svm‖₂       : 0.01593231050406548\n",
      "‖alpha‖₁       : 0.8599999999999977\n",
      "scores min/max : -0.05263288795929276 -0.04019348850617027\n",
      "Mask mean value:  tensor(0.2958, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0755  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.825e-14\n",
      "‖w_svm‖₂       : 0.005733170864004414\n",
      "‖alpha‖₁       : 0.3799999999999952\n",
      "scores min/max : -0.0035724156877305208 0.007619862968555845\n",
      "Mask mean value:  tensor(0.5267, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9362  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.027e-14\n",
      "‖w_svm‖₂       : 6.68879567931056e-07\n",
      "‖alpha‖₁       : 0.3999999999999997\n",
      "scores min/max : 1.3438094971036944e-08 3.195989161697388e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.341e-09\n",
      "‖w_svm‖₂       : 0.07052067916565889\n",
      "‖alpha‖₁       : 0.4193532017957168\n",
      "scores min/max : -1.9276942102320045 2.1165650631701958\n",
      "Mask mean value:  tensor(0.7464, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8941  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.236e-04\n",
      "‖w_svm‖₂       : 4.374652471194738e-07\n",
      "‖alpha‖₁       : 0.7199999999998568\n",
      "scores min/max : 1.4089606810619985e-07 1.9823654453643382e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.364e-21\n",
      "‖w_svm‖₂       : 5.238494853013937e-08\n",
      "‖alpha‖₁       : 0.43999999999998474\n",
      "scores min/max : -1.972177001846601e-07 -9.32602798946113e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.701e-20\n",
      "‖w_svm‖₂       : 7.993611237937964e-08\n",
      "‖alpha‖₁       : 0.17999999999999738\n",
      "scores min/max : -1.9082033138249694e-07 4.342402492416333e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.463e-21\n",
      "‖w_svm‖₂       : 7.549249140049452e-08\n",
      "‖alpha‖₁       : 0.5399999999999909\n",
      "scores min/max : -1.8516265721352118e-07 -1.6360539329515984e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.991e-20\n",
      "‖w_svm‖₂       : 0.03871999454460841\n",
      "‖alpha‖₁       : 0.6599999999999814\n",
      "scores min/max : -0.20397606184155392 0.13518704599550646\n",
      "Mask mean value:  tensor(0.4517, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8475  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.258e-13\n",
      "‖w_svm‖₂       : 0.08272296092440351\n",
      "‖alpha‖₁       : 0.4721094286242754\n",
      "scores min/max : -2.2851793519615824 2.794747091929776\n",
      "Mask mean value:  tensor(0.1177, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.2758  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.504e-02\n",
      "‖w_svm‖₂       : 0.0547332353865871\n",
      "‖alpha‖₁       : 0.5763644103884876\n",
      "scores min/max : -1.961977626458197 0.8663384186703006\n",
      "Mask mean value:  tensor(0.6980, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.8943  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.929e-04\n",
      "‖w_svm‖₂       : 5.356443922884037e-07\n",
      "‖alpha‖₁       : 0.27999999998405856\n",
      "scores min/max : -1.5785816877260976e-06 -1.480598807810628e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1957  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.866e-19\n",
      "‖w_svm‖₂       : 2.7968126660240625e-07\n",
      "‖alpha‖₁       : 0.6599999999999835\n",
      "scores min/max : -6.518332674475172e-07 -5.00065535371294e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.487e-18\n",
      "‖w_svm‖₂       : 6.206378791929873e-07\n",
      "‖alpha‖₁       : 0.2399999999676212\n",
      "scores min/max : 5.195938878780492e-07 5.637207595816464e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.682e-20\n",
      "‖w_svm‖₂       : 0.0033112534165890277\n",
      "‖alpha‖₁       : 0.579999999999999\n",
      "scores min/max : 0.004238006999937854 0.005619065045683151\n",
      "Mask mean value:  tensor(0.5266, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3349  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.359e-13\n",
      "‖w_svm‖₂       : 0.01612253123186204\n",
      "‖alpha‖₁       : 0.5963091585877607\n",
      "scores min/max : -2.8154160051186525 2.2752887887085946\n",
      "Mask mean value:  tensor(0.9171, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.5076  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.906e-07\n",
      "‖w_svm‖₂       : 0.13763361399581864\n",
      "‖alpha‖₁       : 0.8799999999999558\n",
      "scores min/max : -1.3209441842087721 3.649036645360007\n",
      "Mask mean value:  tensor(0.3980, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2658  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.492e-10\n",
      "‖w_svm‖₂       : 1.0697706125452262e-06\n",
      "‖alpha‖₁       : 0.499999999999995\n",
      "scores min/max : -2.5398277003189735e-07 -6.104874026453529e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.069e-19\n",
      "‖w_svm‖₂       : 1.33245020473222e-05\n",
      "‖alpha‖₁       : 0.35999999999999766\n",
      "scores min/max : 9.318216102934953e-06 1.0102229806821234e-05\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.098e-17\n",
      "‖w_svm‖₂       : 4.9157946127095975e-08\n",
      "‖alpha‖₁       : 0.1799999999999967\n",
      "scores min/max : 4.9575432801001945e-08 6.428946261378224e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.905e-08\n",
      "‖w_svm‖₂       : 7.32221076862634e-08\n",
      "‖alpha‖₁       : 0.13999999999999066\n",
      "scores min/max : 1.3880520973342602e-08 1.9684639524079707e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.722e-08\n",
      "‖w_svm‖₂       : 1.1054142110127454e-07\n",
      "‖alpha‖₁       : 0.5199999999999992\n",
      "scores min/max : 2.7733727785899844e-07 2.9879289191379513e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.002e-19\n",
      "‖w_svm‖₂       : 0.04658073475622724\n",
      "‖alpha‖₁       : 0.9056590922530999\n",
      "scores min/max : -0.704978609165479 1.8896754513126481\n",
      "Mask mean value:  tensor(0.3622, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6746  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.189e-03\n",
      "‖w_svm‖₂       : 0.014323531225456208\n",
      "‖alpha‖₁       : 0.8599999999999997\n",
      "scores min/max : -0.04997148871842421 -0.00448178889793702\n",
      "Mask mean value:  tensor(0.3477, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4357  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.495e-05\n",
      "‖w_svm‖₂       : 0.005828268235735643\n",
      "‖alpha‖₁       : 0.5599999999999996\n",
      "scores min/max : 0.005972806428514961 0.007743307572320221\n",
      "Mask mean value:  tensor(0.5314, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 2.051e-05\n",
      "‖w_svm‖₂       : 8.44664323761903e-08\n",
      "‖alpha‖₁       : 0.3799999999999989\n",
      "scores min/max : 9.610386831544125e-09 2.7774670738981775e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.018e-22\n",
      "‖w_svm‖₂       : 0.14661268055731438\n",
      "‖alpha‖₁       : 0.7595072741874929\n",
      "scores min/max : -1.7689836826487502 2.208454962513165\n",
      "Mask mean value:  tensor(0.8373, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3596  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.283e-11\n",
      "‖w_svm‖₂       : 0.021354204644180707\n",
      "‖alpha‖₁       : 0.3834876637501367\n",
      "scores min/max : -1.9593161557090144 0.24320683240556473\n",
      "Mask mean value:  tensor(0.7166, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1530  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.828e-14\n",
      "‖w_svm‖₂       : 0.04142858676229547\n",
      "‖alpha‖₁       : 0.8900247922176714\n",
      "scores min/max : -2.0130881147455493 0.33467539717456474\n",
      "Mask mean value:  tensor(0.2521, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0566  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.473e-05\n",
      "‖w_svm‖₂       : 2.2726170225780027e-07\n",
      "‖alpha‖₁       : 0.2599999999999951\n",
      "scores min/max : 2.134511390614219e-07 2.2623099293608327e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.296e-17\n",
      "‖w_svm‖₂       : 5.294962311690867e-08\n",
      "‖alpha‖₁       : 0.11999999999999644\n",
      "scores min/max : -9.409796024900412e-08 -6.338722010515095e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.592e-08\n",
      "‖w_svm‖₂       : 0.030408864829860067\n",
      "‖alpha‖₁       : 0.5512244007304308\n",
      "scores min/max : -3.4550629744166947 1.114216080630283\n",
      "Mask mean value:  tensor(0.1623, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.2502  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.172e-03\n",
      "‖w_svm‖₂       : 0.01766531535580928\n",
      "‖alpha‖₁       : 0.6815990496106235\n",
      "scores min/max : -1.9708150001181102 0.05745399552024824\n",
      "Mask mean value:  tensor(0.6186, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2634  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.098e-12\n",
      "‖w_svm‖₂       : 5.7959526525503355e-08\n",
      "‖alpha‖₁       : 0.2399999999999894\n",
      "scores min/max : -5.7445164209640134e-08 -4.29706919269579e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.637e-20\n",
      "‖w_svm‖₂       : 0.029612746859883427\n",
      "‖alpha‖₁       : 0.8985951666121551\n",
      "scores min/max : -0.7311996451159983 2.0288405941697216\n",
      "Mask mean value:  tensor(0.8673, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3108  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.930e-15\n",
      "‖w_svm‖₂       : 1.1548042778370107e-07\n",
      "‖alpha‖₁       : 0.29999999999999405\n",
      "scores min/max : 1.655972414684631e-07 1.7458400255858353e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.480e-07\n",
      "‖w_svm‖₂       : 3.5058692086624097e-06\n",
      "‖alpha‖₁       : 0.4199999999933633\n",
      "scores min/max : -3.3682744194995776e-06 -1.7926002364300537e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1229  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.624e-05\n",
      "‖w_svm‖₂       : 0.00018494781947894562\n",
      "‖alpha‖₁       : 0.8199999999999987\n",
      "scores min/max : -9.384188461637836e-05 -8.033005221224341e-05\n",
      "Mask mean value:  tensor(0.4996, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6964  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.408e-17\n",
      "‖w_svm‖₂       : 0.08557734319205458\n",
      "‖alpha‖₁       : 0.5780882612728013\n",
      "scores min/max : -2.133417585554173 5.783946912584228\n",
      "Mask mean value:  tensor(0.2505, dtype=torch.float64)\n",
      "max feasible return = 0.1060  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.9406161524927667e-07\n",
      "‖alpha‖₁       : 0.5799999999999998\n",
      "scores min/max : -3.2143636180987476e-07 -2.3070063537197986e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.081901144353256e-07\n",
      "‖alpha‖₁       : 0.2999999999999683\n",
      "scores min/max : 3.4103603043845094e-08 3.9261856647050895e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 6.6738822779926e-08\n",
      "‖alpha‖₁       : 0.5999999999999794\n",
      "scores min/max : 9.291010527536947e-09 4.608382473387906e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.5149270372819854e-08\n",
      "‖alpha‖₁       : 0.3799999999999981\n",
      "scores min/max : -1.881342984517423e-08 -9.439928143602645e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 5.946929679410127e-06\n",
      "‖alpha‖₁       : 0.3199999999383405\n",
      "scores min/max : 5.421246285501353e-06 9.287952944314978e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4259  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.047192720459416575\n",
      "‖alpha‖₁       : 0.7397068180648614\n",
      "scores min/max : -3.450139776913139 2.1822514373140747\n",
      "Mask mean value:  tensor(0.9030, dtype=torch.float64)\n",
      "max feasible return = -0.9433  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.026525917108162614\n",
      "‖alpha‖₁       : 0.6151654925059882\n",
      "scores min/max : -1.9412745248750802 1.1872739296309964\n",
      "Mask mean value:  tensor(0.7259, dtype=torch.float64)\n",
      "max feasible return = 2.5323  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.694661834037207e-07\n",
      "‖alpha‖₁       : 0.4599999999999901\n",
      "scores min/max : 1.423006599601866e-07 7.368496540926743e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.395161629662107e-07\n",
      "‖alpha‖₁       : 0.51999999999998\n",
      "scores min/max : 8.900976405756115e-08 1.0193096212425733e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.587090557952263e-07\n",
      "‖alpha‖₁       : 0.5799999999999786\n",
      "scores min/max : -2.2606342155154976e-07 -1.8973587580071048e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.043918470421030746\n",
      "‖alpha‖₁       : 0.6878715077885871\n",
      "scores min/max : -0.4456202362446732 1.9435744463739628\n",
      "Mask mean value:  tensor(0.3531, dtype=torch.float64)\n",
      "max feasible return = 0.0793  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.033128332492725794\n",
      "‖alpha‖₁       : 0.4223070393083394\n",
      "scores min/max : -3.8683525767377103 5.2853149138536475\n",
      "Mask mean value:  tensor(0.1044, dtype=torch.float64)\n",
      "max feasible return = 0.6621  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.043309035338931615\n",
      "‖alpha‖₁       : 0.8211459111090404\n",
      "scores min/max : -4.966043951508819 3.203591280018817\n",
      "Mask mean value:  tensor(0.9643, dtype=torch.float64)\n",
      "max feasible return = -3.6882  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.006634016565044221\n",
      "‖alpha‖₁       : 0.7999999999999998\n",
      "scores min/max : -0.029887564505739997 -0.0003225980147938791\n",
      "Mask mean value:  tensor(0.4613, dtype=torch.float64)\n",
      "max feasible return = -0.1677  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.102250105985481e-07\n",
      "‖alpha‖₁       : 0.619999999999963\n",
      "scores min/max : 7.320804838789108e-08 8.973798648896164e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.843420550641052e-08\n",
      "‖alpha‖₁       : 0.43999999999999984\n",
      "scores min/max : 4.99578450387863e-09 1.4565560127043907e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.3277334523680526e-07\n",
      "‖alpha‖₁       : 0.27999999999997915\n",
      "scores min/max : -2.1446921456870538e-07 -1.9654441273883074e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  44 | train 0.005309 | val 0.006707\n",
      "-----------------------------------------Epoch:  45 ----------------------------------------\n",
      "‖w_svm‖₂       : 1.0100919358220215e-06\n",
      "‖alpha‖₁       : 0.5999999999999955\n",
      "scores min/max : -1.7327198181523036e-07 -1.2240941286075894e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.724e-19\n",
      "‖w_svm‖₂       : 7.408566091030765e-08\n",
      "‖alpha‖₁       : 0.42\n",
      "scores min/max : -6.699990074052135e-08 -1.989651243069051e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.710e-08\n",
      "‖w_svm‖₂       : 0.13804532315764434\n",
      "‖alpha‖₁       : 0.87999999999993\n",
      "scores min/max : -1.3299880466040064 3.672158986304042\n",
      "Mask mean value:  tensor(0.3967, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2664  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.787e-08\n",
      "‖w_svm‖₂       : 2.02913728527707e-08\n",
      "‖alpha‖₁       : 0.1199999999999933\n",
      "scores min/max : -4.214876930800252e-08 -2.7415098293650973e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.848e-09\n",
      "‖w_svm‖₂       : 0.0465180025861333\n",
      "‖alpha‖₁       : 0.9056553736198387\n",
      "scores min/max : -0.7054647486246546 1.8891331844383994\n",
      "Mask mean value:  tensor(0.3603, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6710  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.324e-03\n",
      "‖w_svm‖₂       : 1.0694207646261022e-06\n",
      "‖alpha‖₁       : 0.499999999999995\n",
      "scores min/max : -2.559159516909823e-07 -6.29697164704563e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.066e-19\n",
      "‖w_svm‖₂       : 0.0387367395181485\n",
      "‖alpha‖₁       : 0.6599999999999819\n",
      "scores min/max : -0.2039892705111025 0.13537847584092488\n",
      "Mask mean value:  tensor(0.4522, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8484  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.195e-13\n",
      "‖w_svm‖₂       : 0.030612382794691577\n",
      "‖alpha‖₁       : 0.8524839171052583\n",
      "scores min/max : -2.894555075535269 1.5859917282117502\n",
      "Mask mean value:  tensor(0.1658, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9610  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.152e-03\n",
      "‖w_svm‖₂       : 0.18333694022230396\n",
      "‖alpha‖₁       : 0.8833477236168128\n",
      "scores min/max : -3.6331940681562997 6.074476094167231\n",
      "Mask mean value:  tensor(0.0925, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.588e-05\n",
      "‖w_svm‖₂       : 3.7911472406971785e-07\n",
      "‖alpha‖₁       : 0.2799999999999975\n",
      "scores min/max : -1.114455431637587e-06 -1.0527457038577835e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.142e-19\n",
      "‖w_svm‖₂       : 0.04570142945748114\n",
      "‖alpha‖₁       : 0.9389058214849262\n",
      "scores min/max : -2.4623996146858995 1.5870386395866594\n",
      "Mask mean value:  tensor(0.1463, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4024  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.236e-03\n",
      "‖w_svm‖₂       : 0.041423781430713486\n",
      "‖alpha‖₁       : 0.8900245554708193\n",
      "scores min/max : -2.013286140777804 0.33447114462267785\n",
      "Mask mean value:  tensor(0.2515, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0564  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.944e-05\n",
      "‖w_svm‖₂       : 0.03603629617660172\n",
      "‖alpha‖₁       : 0.9199999999999677\n",
      "scores min/max : -0.2666540407265498 0.21236022581513814\n",
      "Mask mean value:  tensor(0.1727, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8632  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 8.487e-03\n",
      "‖w_svm‖₂       : 0.04547481879330093\n",
      "‖alpha‖₁       : 0.9408623425163489\n",
      "scores min/max : -1.8256029808835823 0.3082509886550957\n",
      "Mask mean value:  tensor(0.4351, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3253  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.962e-10\n",
      "‖w_svm‖₂       : 7.536432536693214e-08\n",
      "‖alpha‖₁       : 0.5399999999999926\n",
      "scores min/max : -1.8519085364458334e-07 -1.6363939216634114e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.983e-20\n",
      "‖w_svm‖₂       : 0.13849775693539532\n",
      "‖alpha‖₁       : 0.6551652214803549\n",
      "scores min/max : -18.067634538896975 1.8323161595399116\n",
      "Mask mean value:  tensor(0.2693, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2809  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.183e-03\n",
      "‖w_svm‖₂       : 0.005820899533842478\n",
      "‖alpha‖₁       : 0.5599999999999996\n",
      "scores min/max : 0.006027960043527463 0.007793863794449182\n",
      "Mask mean value:  tensor(0.5317, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 2.029e-05\n",
      "‖w_svm‖₂       : 7.277186667064105e-08\n",
      "‖alpha‖₁       : 0.13999999999999602\n",
      "scores min/max : 1.3837730283588425e-08 1.9588239382630362e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.685e-08\n",
      "‖w_svm‖₂       : 0.015904452014239824\n",
      "‖alpha‖₁       : 0.8599999999999983\n",
      "scores min/max : -0.05259174427218591 -0.04015601384026807\n",
      "Mask mean value:  tensor(0.2959, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0756  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.820e-14\n",
      "‖w_svm‖₂       : 1.1047935450813006e-07\n",
      "‖alpha‖₁       : 0.5199999999999976\n",
      "scores min/max : 2.7635202676731947e-07 2.9779899930752643e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.011e-19\n",
      "‖w_svm‖₂       : 1.6857254895838595e-07\n",
      "‖alpha‖₁       : 0.239999999999979\n",
      "scores min/max : 2.3159594867511896e-07 2.4722757132780927e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.870e-20\n",
      "‖w_svm‖₂       : 0.01612455383546224\n",
      "‖alpha‖₁       : 0.5963103113833417\n",
      "scores min/max : -2.81155089522997 2.275992517639669\n",
      "Mask mean value:  tensor(0.9174, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.5079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.772e-06\n",
      "‖w_svm‖₂       : 7.442273423219158e-08\n",
      "‖alpha‖₁       : 0.6599999999999988\n",
      "scores min/max : 9.212217936863216e-08 2.3716759508593171e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.253e-09\n",
      "‖w_svm‖₂       : 0.0009115970935103646\n",
      "‖alpha‖₁       : 0.8199999999999981\n",
      "scores min/max : 0.0007699109515904067 0.0024491697839278802\n",
      "Mask mean value:  tensor(0.5091, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5865  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.115e-16\n",
      "‖w_svm‖₂       : 0.07196897473145718\n",
      "‖alpha‖₁       : 0.5799999999999561\n",
      "scores min/max : -2.780629956151098 1.6358312593535083\n",
      "Mask mean value:  tensor(0.2438, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4060  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.262e-04\n",
      "‖w_svm‖₂       : 5.2712188711907855e-08\n",
      "‖alpha‖₁       : 0.11999999999999905\n",
      "scores min/max : -9.430666380218016e-08 -6.365851299262066e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.590e-08\n",
      "‖w_svm‖₂       : 0.004508679044993438\n",
      "‖alpha‖₁       : 0.45999999999999447\n",
      "scores min/max : -0.00983847374459394 -0.007647059013151769\n",
      "Mask mean value:  tensor(0.4593, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.129e-15\n",
      "‖w_svm‖₂       : 0.006756452886636961\n",
      "‖alpha‖₁       : 0.7005770173225981\n",
      "scores min/max : -2.0016449585215463 0.02177570969363562\n",
      "Mask mean value:  tensor(0.4664, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2610  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.305e-13\n",
      "‖w_svm‖₂       : 5.787201764425551e-08\n",
      "‖alpha‖₁       : 0.23999999999998844\n",
      "scores min/max : -5.451321025909716e-08 -4.0028932392825094e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.646e-20\n",
      "‖w_svm‖₂       : 0.09275940701959781\n",
      "‖alpha‖₁       : 0.8733652096921976\n",
      "scores min/max : -12.168035547177993 2.151352625454196\n",
      "Mask mean value:  tensor(0.5265, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.5627  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 4.209e-02\n",
      "‖w_svm‖₂       : 2.7865355348581664e-07\n",
      "‖alpha‖₁       : 0.6599999999999863\n",
      "scores min/max : -6.56847054407264e-07 -5.051024394420478e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.486e-18\n",
      "‖w_svm‖₂       : 6.713739464825499e-07\n",
      "‖alpha‖₁       : 0.3999999999999996\n",
      "scores min/max : 1.717310863844673e-08 3.233865841018377e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.359e-09\n",
      "‖w_svm‖₂       : 0.00030217653368802886\n",
      "‖alpha‖₁       : 0.41999999995928583\n",
      "scores min/max : 9.766742660710821e-05 0.0004065970740114354\n",
      "Mask mean value:  tensor(0.5019, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9993  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.110e-15\n",
      "‖w_svm‖₂       : 2.3385052499901457e-07\n",
      "‖alpha‖₁       : 0.6399999999999966\n",
      "scores min/max : 4.144445186775926e-07 4.3533605462082773e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.062e-19\n",
      "‖w_svm‖₂       : 0.029576373941628032\n",
      "‖alpha‖₁       : 0.8985976461367049\n",
      "scores min/max : -0.7299227366622586 2.030119823109742\n",
      "Mask mean value:  tensor(0.8690, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.800e-15\n",
      "‖w_svm‖₂       : 1.3331309780119613e-05\n",
      "‖alpha‖₁       : 0.359999999999999\n",
      "scores min/max : 9.433391079526764e-06 1.021895795344459e-05\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.129e-17\n",
      "‖w_svm‖₂       : 0.02147837229085068\n",
      "‖alpha‖₁       : 0.3834931089943077\n",
      "scores min/max : -1.9574096581456906 0.24512145641618882\n",
      "Mask mean value:  tensor(0.7235, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1540  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.596e-14\n",
      "‖w_svm‖₂       : 0.019285580563515817\n",
      "‖alpha‖₁       : 0.659999999999995\n",
      "scores min/max : -0.036400646928263225 0.021944520028627505\n",
      "Mask mean value:  tensor(0.4279, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.3538  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.140e-15\n",
      "‖w_svm‖₂       : 0.0003641021475451504\n",
      "‖alpha‖₁       : 0.74\n",
      "scores min/max : -0.00043756662455654517 -0.00042065764333730146\n",
      "Mask mean value:  tensor(0.4979, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0598  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.449e-17\n",
      "‖w_svm‖₂       : 0.05187011543845947\n",
      "‖alpha‖₁       : 0.8272292427425351\n",
      "scores min/max : -1.9412220726196607 0.4098670638362892\n",
      "Mask mean value:  tensor(0.5564, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0201  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.701e-11\n",
      "‖w_svm‖₂       : 1.982062703455612e-07\n",
      "‖alpha‖₁       : 0.3799999999999979\n",
      "scores min/max : -3.5637531295794914e-08 6.743135937123593e-11\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.805e-19\n",
      "‖w_svm‖₂       : 4.329837882375051e-07\n",
      "‖alpha‖₁       : 0.7199999999999995\n",
      "scores min/max : 1.636093295623464e-07 2.207988473689995e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.336e-21\n",
      "‖w_svm‖₂       : 7.977400704448188e-08\n",
      "‖alpha‖₁       : 0.17999999999999752\n",
      "scores min/max : -1.969166069946105e-07 3.7288174218970225e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.491e-21\n",
      "‖w_svm‖₂       : 1.1555702247700039e-07\n",
      "‖alpha‖₁       : 0.29999999999999405\n",
      "scores min/max : 1.6446577648652042e-07 1.7345301729217586e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.491e-07\n",
      "‖w_svm‖₂       : 0.039964715521778396\n",
      "‖alpha‖₁       : 0.7294831244697727\n",
      "scores min/max : -0.2659916497119579 2.015278818594239\n",
      "Mask mean value:  tensor(0.6625, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.0049  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.911e-03\n",
      "‖w_svm‖₂       : 0.01754964197766585\n",
      "‖alpha‖₁       : 0.681598974748303\n",
      "scores min/max : -1.9699744074226426 0.058078400593463836\n",
      "Mask mean value:  tensor(0.6224, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2658  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.550e-14\n",
      "‖w_svm‖₂       : 0.06988762035395035\n",
      "‖alpha‖₁       : 0.4192947197436926\n",
      "scores min/max : -1.9143452806487262 2.124387539591325\n",
      "Mask mean value:  tensor(0.7600, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8990  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.617e-04\n",
      "‖w_svm‖₂       : 3.3288099348173267e-06\n",
      "‖alpha‖₁       : 0.4199999999960538\n",
      "scores min/max : -3.3471007299945294e-06 -1.8446697000985714e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1229  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.556e-05\n",
      "‖w_svm‖₂       : 0.005684083244707854\n",
      "‖alpha‖₁       : 0.3799999999999833\n",
      "scores min/max : -0.004017748735581847 0.0069591227150577264\n",
      "Mask mean value:  tensor(0.5237, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9309  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.062e-14\n",
      "‖w_svm‖₂       : 2.418051001795275e-07\n",
      "‖alpha‖₁       : 0.5799999999999998\n",
      "scores min/max : -2.9028182775190536e-07 -2.7466707385946716e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.214e-19\n",
      "‖w_svm‖₂       : 0.026663269341756213\n",
      "‖alpha‖₁       : 0.19669943229582226\n",
      "scores min/max : -2.2198016378344936 0.0018649633317946793\n",
      "Mask mean value:  tensor(0.0451, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3073  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.208e-03\n",
      "‖w_svm‖₂       : 0.02132978216828571\n",
      "‖alpha‖₁       : 0.8150918437309899\n",
      "scores min/max : -11.511215874795525 2.0329612734214395\n",
      "Mask mean value:  tensor(0.7125, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9225  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.705e-06\n",
      "‖w_svm‖₂       : 5.2230931396827624e-08\n",
      "‖alpha‖₁       : 0.4399999999999856\n",
      "scores min/max : -1.9755200424522915e-07 -9.358538859615977e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.682e-20\n",
      "‖w_svm‖₂       : 0.05437235638188997\n",
      "‖alpha‖₁       : 0.5763372743867707\n",
      "scores min/max : -1.9601973717992138 0.8681008625332003\n",
      "Mask mean value:  tensor(0.7037, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9104  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.014e-03\n",
      "‖w_svm‖₂       : 0.030380834654756543\n",
      "‖alpha‖₁       : 0.5512255759793883\n",
      "scores min/max : -3.454434551597036 1.1147464361265524\n",
      "Mask mean value:  tensor(0.1626, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.2520  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.075e-03\n",
      "‖w_svm‖₂       : 0.00015973070692752134\n",
      "‖alpha‖₁       : 0.6399999999999999\n",
      "scores min/max : -0.00014805667422278666 -0.00013832156624411492\n",
      "Mask mean value:  tensor(0.4993, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7633  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.423e-17\n",
      "‖w_svm‖₂       : 1.9659723182800537e-07\n",
      "‖alpha‖₁       : 0.4199999999999713\n",
      "scores min/max : -4.778361594965576e-07 -4.5568517969799465e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.993e-19\n",
      "‖w_svm‖₂       : 0.020571011781514326\n",
      "‖alpha‖₁       : 0.8233837876512169\n",
      "scores min/max : -1.906841649618315 1.4895545467644724\n",
      "Mask mean value:  tensor(0.6785, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.3337  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.148e-05\n",
      "‖w_svm‖₂       : 0.020483901438823355\n",
      "‖alpha‖₁       : 0.779999999999971\n",
      "scores min/max : -0.04057983857512871 0.024199843161689605\n",
      "Mask mean value:  tensor(0.3316, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.4666  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.605e-15\n",
      "‖w_svm‖₂       : 1.2078315911015135e-06\n",
      "‖alpha‖₁       : 0.31999999999757284\n",
      "scores min/max : -2.6930945661099623e-06 -2.574639635192482e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4897  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.291e-18\n",
      "‖w_svm‖₂       : 0.14661082075769372\n",
      "‖alpha‖₁       : 0.7595091229826847\n",
      "scores min/max : -1.7706088245172098 2.206890257540142\n",
      "Mask mean value:  tensor(0.8348, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3589  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.560e-10\n",
      "‖w_svm‖₂       : 1.1528903560135299e-07\n",
      "‖alpha‖₁       : 0.23999999999987714\n",
      "scores min/max : 7.15221378193443e-08 8.291906827348092e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.001e-21\n",
      "‖w_svm‖₂       : 0.08213674136363164\n",
      "‖alpha‖₁       : 0.47201645002316006\n",
      "scores min/max : -2.2414989578429423 2.8384845526247213\n",
      "Mask mean value:  tensor(0.1424, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.5207  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.672e-02\n",
      "‖w_svm‖₂       : 4.938014094300441e-08\n",
      "‖alpha‖₁       : 0.17999999999999694\n",
      "scores min/max : 4.4985629905695757e-08 5.968321174051198e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.880e-08\n",
      "‖w_svm‖₂       : 8.489929847587851e-08\n",
      "‖alpha‖₁       : 0.3799999999999988\n",
      "scores min/max : 2.5375670551377655e-09 2.0702332864929254e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.118e-22\n",
      "‖w_svm‖₂       : 0.060774108237866414\n",
      "‖alpha‖₁       : 0.8987117674327869\n",
      "scores min/max : -1.6978976614480732 3.7148090660706323\n",
      "Mask mean value:  tensor(0.4342, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5838  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.095e-02\n",
      "‖w_svm‖₂       : 0.014154381481781693\n",
      "‖alpha‖₁       : 0.8600000000000001\n",
      "scores min/max : -0.050184224930655046 -0.005838473828823031\n",
      "Mask mean value:  tensor(0.3446, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4312  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.774e-05\n",
      "‖w_svm‖₂       : 0.00013940845812071708\n",
      "‖alpha‖₁       : 0.4399999999903905\n",
      "scores min/max : -0.00029894977412624267 -0.00015689108557264092\n",
      "Mask mean value:  tensor(0.4990, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0205  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.004e-17\n",
      "‖w_svm‖₂       : 0.0032984514622021654\n",
      "‖alpha‖₁       : 0.5799999999999993\n",
      "scores min/max : 0.0037968418434448327 0.00516585048661394\n",
      "Mask mean value:  tensor(0.5244, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3334  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.623e-13\n",
      "‖w_svm‖₂       : 0.0005206004925678886\n",
      "‖alpha‖₁       : 0.43999999999999706\n",
      "scores min/max : -0.0008955682166217828 -0.0006119981529276461\n",
      "Mask mean value:  tensor(0.4957, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6413  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.371e-16\n",
      "‖w_svm‖₂       : 2.714647813315336e-07\n",
      "‖alpha‖₁       : 0.37999999999057543\n",
      "scores min/max : -1.78927497863574e-07 -1.5792043857598031e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.695e-07\n",
      "‖w_svm‖₂       : 0.009099866959028407\n",
      "‖alpha‖₁       : 0.6077049107361263\n",
      "scores min/max : -1.9937372571818355 0.2930385926930483\n",
      "Mask mean value:  tensor(0.5408, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2183  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.009e-15\n",
      "‖w_svm‖₂       : 2.2748126344061531e-07\n",
      "‖alpha‖₁       : 0.2599999999999972\n",
      "scores min/max : 1.9152944282821967e-07 2.0429169220816833e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.255e-17\n",
      "‖w_svm‖₂       : 0.00018435975761080275\n",
      "‖alpha‖₁       : 0.819999999999933\n",
      "scores min/max : -9.883515999944151e-05 -8.54472270825227e-05\n",
      "Mask mean value:  tensor(0.4996, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6964  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.408e-17\n",
      "‖w_svm‖₂       : 0.00021089747960457408\n",
      "‖alpha‖₁       : 0.6199999999885745\n",
      "scores min/max : 2.715350252300745e-06 1.8476447414407895e-05\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.910e-17\n",
      "‖w_svm‖₂       : 0.07254178227480641\n",
      "‖alpha‖₁       : 0.6587055804738984\n",
      "scores min/max : -2.002649150762304 4.336944411150082\n",
      "Mask mean value:  tensor(0.4087, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.5516  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.808e-04\n",
      "‖w_svm‖₂       : 0.08515108932738835\n",
      "‖alpha‖₁       : 0.5780213273125668\n",
      "scores min/max : -2.118230518958733 5.798526034314151\n",
      "Mask mean value:  tensor(0.2769, dtype=torch.float64)\n",
      "max feasible return = 0.1102  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.9447186000305396e-07\n",
      "‖alpha‖₁       : 0.5799999999999997\n",
      "scores min/max : -3.5469554101600087e-07 -2.639507188478947e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0818895068633062e-07\n",
      "‖alpha‖₁       : 0.29999999999996974\n",
      "scores min/max : 4.4963287464305933e-08 5.0110372998126045e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 6.627489670652249e-08\n",
      "‖alpha‖₁       : 0.5999999999999959\n",
      "scores min/max : 1.913541396284769e-08 5.593016664509146e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.503448639964512e-08\n",
      "‖alpha‖₁       : 0.37999999999999934\n",
      "scores min/max : -1.2027025669821946e-08 -2.6539920921177605e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 6.939701050822644e-06\n",
      "‖alpha‖₁       : 0.31999999991199796\n",
      "scores min/max : 7.2581219792607026e-06 1.2073921837382891e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64)\n",
      "max feasible return = 2.4260  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.0466238025721344\n",
      "‖alpha‖₁       : 0.7396989273526192\n",
      "scores min/max : -3.4527129486307335 2.179566263965014\n",
      "Mask mean value:  tensor(0.9014, dtype=torch.float64)\n",
      "max feasible return = -0.9424  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.026232630267720092\n",
      "‖alpha‖₁       : 0.6151643307590324\n",
      "scores min/max : -1.9431036754364748 1.1852792555402085\n",
      "Mask mean value:  tensor(0.7211, dtype=torch.float64)\n",
      "max feasible return = 2.5150  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.706440052426824e-07\n",
      "‖alpha‖₁       : 0.4599999999999944\n",
      "scores min/max : 2.030622795715894e-07 7.975452450112239e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.402926856711499e-07\n",
      "‖alpha‖₁       : 0.5199999999999771\n",
      "scores min/max : 8.54322168540515e-08 9.83545094716698e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.588768936706887e-07\n",
      "‖alpha‖₁       : 0.5799999999999987\n",
      "scores min/max : -2.550944297158372e-07 -2.1894730664047222e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.04359201763606985\n",
      "‖alpha‖₁       : 0.6878638413091599\n",
      "scores min/max : -0.44376153028796717 1.9455893009669794\n",
      "Mask mean value:  tensor(0.3590, dtype=torch.float64)\n",
      "max feasible return = 0.0726  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03293642433339031\n",
      "‖alpha‖₁       : 0.4223056731992131\n",
      "scores min/max : -3.865136227283952 5.289659300549894\n",
      "Mask mean value:  tensor(0.1067, dtype=torch.float64)\n",
      "max feasible return = 0.6777  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04327943843556656\n",
      "‖alpha‖₁       : 0.821149371510141\n",
      "scores min/max : -4.959737507420156 3.2100566337621457\n",
      "Mask mean value:  tensor(0.9648, dtype=torch.float64)\n",
      "max feasible return = -3.6910  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.006546262904908428\n",
      "‖alpha‖₁       : 0.7999999999999998\n",
      "scores min/max : -0.02945288536528433 -0.0007059287572736793\n",
      "Mask mean value:  tensor(0.4604, dtype=torch.float64)\n",
      "max feasible return = -0.1674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.1115068568264653e-07\n",
      "‖alpha‖₁       : 0.6199999999999648\n",
      "scores min/max : 8.93398928910613e-08 1.0586514647844349e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.87152338997129e-08\n",
      "‖alpha‖₁       : 0.43999999999999984\n",
      "scores min/max : 5.884710965148287e-09 1.545461576389658e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.3354084302632443e-07\n",
      "‖alpha‖₁       : 0.27999999999998043\n",
      "scores min/max : -2.5949036836733417e-07 -2.4157476132106114e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  45 | train 0.005306 | val 0.006613\n",
      "-----------------------------------------Epoch:  46 ----------------------------------------\n",
      "‖w_svm‖₂       : 2.0260687052559074e-08\n",
      "‖alpha‖₁       : 0.11999999999999322\n",
      "scores min/max : -4.0002505718578484e-08 -2.5265291205178265e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.840e-09\n",
      "‖w_svm‖₂       : 0.030402118661275273\n",
      "‖alpha‖₁       : 0.5512269924820383\n",
      "scores min/max : -3.4527265624289867 1.1164601491357389\n",
      "Mask mean value:  tensor(0.1638, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.2586  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.748e-03\n",
      "‖w_svm‖₂       : 0.026532336480135726\n",
      "‖alpha‖₁       : 0.19669330892055376\n",
      "scores min/max : -2.2132180384039675 0.008395714737916123\n",
      "Mask mean value:  tensor(0.0502, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3408  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.263e-04\n",
      "‖w_svm‖₂       : 7.946984012575898e-08\n",
      "‖alpha‖₁       : 0.1799999999999978\n",
      "scores min/max : -1.9897880023274696e-07 3.5166688066447954e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.456e-21\n",
      "‖w_svm‖₂       : 0.13750327430349643\n",
      "‖alpha‖₁       : 0.6548925548709554\n",
      "scores min/max : -18.027932058006673 1.8721614382360598\n",
      "Mask mean value:  tensor(0.3152, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2713  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.915e-03\n",
      "‖w_svm‖₂       : 0.000363102927019128\n",
      "‖alpha‖₁       : 0.7399999999999992\n",
      "scores min/max : -0.00042049875424712726 -0.00040368242010812346\n",
      "Mask mean value:  tensor(0.4980, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0600  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.462e-17\n",
      "‖w_svm‖₂       : 0.014154659951130985\n",
      "‖alpha‖₁       : 0.86\n",
      "scores min/max : -0.05021369078975113 -0.005918436001835505\n",
      "Mask mean value:  tensor(0.3443, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4308  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.794e-05\n",
      "‖w_svm‖₂       : 0.07239383336232195\n",
      "‖alpha‖₁       : 0.5799999999999663\n",
      "scores min/max : -2.815250404297463 1.6576261626615372\n",
      "Mask mean value:  tensor(0.2446, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4081  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.366e-04\n",
      "‖w_svm‖₂       : 0.04109593513073221\n",
      "‖alpha‖₁       : 0.8900067605535975\n",
      "scores min/max : -2.0139082048991503 0.3338893261205172\n",
      "Mask mean value:  tensor(0.2497, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0552  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.275e-05\n",
      "‖w_svm‖₂       : 1.9660666850982648e-07\n",
      "‖alpha‖₁       : 0.41999999999997134\n",
      "scores min/max : -4.896941803496855e-07 -4.6755208777190316e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.997e-19\n",
      "‖w_svm‖₂       : 1.1594812381738905e-07\n",
      "‖alpha‖₁       : 0.29999999999999416\n",
      "scores min/max : 1.6419282780840846e-07 1.7317975994286267e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.495e-07\n",
      "‖w_svm‖₂       : 2.3474694672270523e-07\n",
      "‖alpha‖₁       : 0.6399999999999966\n",
      "scores min/max : 4.26822311164313e-07 4.4771408056674943e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.074e-19\n",
      "‖w_svm‖₂       : 0.03985101192228572\n",
      "‖alpha‖₁       : 0.7294784571644752\n",
      "scores min/max : -0.26583881311601737 2.015515643388145\n",
      "Mask mean value:  tensor(0.6631, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.0074  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.902e-03\n",
      "‖w_svm‖₂       : 0.006812024729629846\n",
      "‖alpha‖₁       : 0.700577580454553\n",
      "scores min/max : -2.0016584733591256 0.02176233993879927\n",
      "Mask mean value:  tensor(0.4663, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2609  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.456e-13\n",
      "‖w_svm‖₂       : 2.2770821903775e-07\n",
      "‖alpha‖₁       : 0.25999999999999707\n",
      "scores min/max : 1.9157586905803194e-07 2.0433827467865537e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.287e-17\n",
      "‖w_svm‖₂       : 0.13918963144097732\n",
      "‖alpha‖₁       : 0.8799999999999998\n",
      "scores min/max : -1.362225865802937 3.7207654801046366\n",
      "Mask mean value:  tensor(0.3716, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2680  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.691e-09\n",
      "‖w_svm‖₂       : 0.021513583745791518\n",
      "‖alpha‖₁       : 0.3834956960830036\n",
      "scores min/max : -1.9550079427980305 0.24757756096921124\n",
      "Mask mean value:  tensor(0.7320, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1553  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.087e-14\n",
      "‖w_svm‖₂       : 1.0128973923121885e-06\n",
      "‖alpha‖₁       : 0.599999999999995\n",
      "scores min/max : -1.8775529765385424e-07 -1.3689193521973452e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.733e-19\n",
      "‖w_svm‖₂       : 2.7180468455843393e-07\n",
      "‖alpha‖₁       : 0.37999999999059264\n",
      "scores min/max : -1.7694304316073175e-07 -1.5593687567281404e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.723e-07\n",
      "‖w_svm‖₂       : 0.04675495520017901\n",
      "‖alpha‖₁       : 0.9056476319116685\n",
      "scores min/max : -0.721210781362997 1.8814805756166417\n",
      "Mask mean value:  tensor(0.3390, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6314  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.984e-03\n",
      "‖w_svm‖₂       : 0.00018407511344447952\n",
      "‖alpha‖₁       : 0.8199999999999337\n",
      "scores min/max : -9.81755470298022e-05 -8.482974918055442e-05\n",
      "Mask mean value:  tensor(0.4996, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6964  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.406e-17\n",
      "‖w_svm‖₂       : 1.1199292415334424e-07\n",
      "‖alpha‖₁       : 0.5199999999999758\n",
      "scores min/max : 3.12969455806459e-07 3.3439054086447306e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.084e-19\n",
      "‖w_svm‖₂       : 0.0033131333323702904\n",
      "‖alpha‖₁       : 0.5799999999999994\n",
      "scores min/max : 0.003810438938079595 0.00519150889418887\n",
      "Mask mean value:  tensor(0.5245, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3335  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.632e-13\n",
      "‖w_svm‖₂       : 1.992181537347625e-07\n",
      "‖alpha‖₁       : 0.37999999999999523\n",
      "scores min/max : -4.084818696319287e-08 -5.131622078061737e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.830e-19\n",
      "‖w_svm‖₂       : 0.05156210000907067\n",
      "‖alpha‖₁       : 0.8271927557920404\n",
      "scores min/max : -1.9525835258449413 0.3984759174190636\n",
      "Mask mean value:  tensor(0.5158, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0122  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.772e-02\n",
      "‖w_svm‖₂       : 1.2109584789850174e-06\n",
      "‖alpha‖₁       : 0.31999999999755285\n",
      "scores min/max : -2.728788496573598e-06 -2.610273454714895e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4897  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.295e-18\n",
      "‖w_svm‖₂       : 0.0694543769771229\n",
      "‖alpha‖₁       : 0.41924014568751977\n",
      "scores min/max : -1.9186502237739336 2.1186955687106352\n",
      "Mask mean value:  tensor(0.7558, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8979  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.489e-04\n",
      "‖w_svm‖₂       : 0.004493500919000945\n",
      "‖alpha‖₁       : 0.45999999999999996\n",
      "scores min/max : -0.009749082369869547 -0.007566978851781599\n",
      "Mask mean value:  tensor(0.4597, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9976  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.131e-15\n",
      "‖w_svm‖₂       : 0.04577052293870402\n",
      "‖alpha‖₁       : 0.9408876217666726\n",
      "scores min/max : -1.8344637711831666 0.2993788175992862\n",
      "Mask mean value:  tensor(0.4002, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3235  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.188e-14\n",
      "‖w_svm‖₂       : 1.6789012430099266e-07\n",
      "‖alpha‖₁       : 0.2399999999999908\n",
      "scores min/max : 2.4187584479870305e-07 2.572645816333511e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.885e-20\n",
      "‖w_svm‖₂       : 0.146272331358148\n",
      "‖alpha‖₁       : 0.7594128511670792\n",
      "scores min/max : -1.771112572176599 2.206440684363429\n",
      "Mask mean value:  tensor(0.8340, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3590  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.659e-11\n",
      "‖w_svm‖₂       : 0.02131524170780465\n",
      "‖alpha‖₁       : 0.8150909702191323\n",
      "scores min/max : -11.510387603659826 2.033713355018985\n",
      "Mask mean value:  tensor(0.7141, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9263  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.204e-06\n",
      "‖w_svm‖₂       : 0.016123205892696387\n",
      "‖alpha‖₁       : 0.5963114381791897\n",
      "scores min/max : -2.797607425892954 2.2800232531703406\n",
      "Mask mean value:  tensor(0.9192, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.5097  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.911e-09\n",
      "‖w_svm‖₂       : 3.595682053947523e-07\n",
      "‖alpha‖₁       : 0.2399999999880209\n",
      "scores min/max : 2.6943532956228763e-07 2.963194590609867e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.558e-20\n",
      "‖w_svm‖₂       : 0.019351561439801986\n",
      "‖alpha‖₁       : 0.659999999999997\n",
      "scores min/max : -0.03630873909538476 0.022242743305134774\n",
      "Mask mean value:  tensor(0.4289, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.3619  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.043e-15\n",
      "‖w_svm‖₂       : 6.732979157211342e-07\n",
      "‖alpha‖₁       : 0.39999999999999963\n",
      "scores min/max : 3.7470645571327734e-08 3.4355459409853007e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.376e-09\n",
      "‖w_svm‖₂       : 0.0005226302935873284\n",
      "‖alpha‖₁       : 0.43999999999997985\n",
      "scores min/max : -0.000852256994980758 -0.0005664485535854932\n",
      "Mask mean value:  tensor(0.4960, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6420  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.372e-16\n",
      "‖w_svm‖₂       : 0.09169046989801943\n",
      "‖alpha‖₁       : 0.873372079086411\n",
      "scores min/max : -12.141122404660196 2.17757452463035\n",
      "Mask mean value:  tensor(0.5884, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7427  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 7.814e-02\n",
      "‖w_svm‖₂       : 0.015735516442305232\n",
      "‖alpha‖₁       : 0.8599999999999945\n",
      "scores min/max : -0.0504882389066335 -0.0384322925462837\n",
      "Mask mean value:  tensor(0.3036, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0785  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.696e-14\n",
      "‖w_svm‖₂       : 0.08220298753169121\n",
      "‖alpha‖₁       : 0.47202348220705875\n",
      "scores min/max : -2.2287810723112065 2.8519741334062854\n",
      "Mask mean value:  tensor(0.1517, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6125  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.709e-02\n",
      "‖w_svm‖₂       : 3.5289929614964953e-06\n",
      "‖alpha‖₁       : 0.41999999999321613\n",
      "scores min/max : -3.4744830315241087e-06 -1.8845066723382196e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1229  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.648e-05\n",
      "‖w_svm‖₂       : 0.02057914860003808\n",
      "‖alpha‖₁       : 0.78\n",
      "scores min/max : -0.042267809280564825 0.023120321037688518\n",
      "Mask mean value:  tensor(0.3245, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.4369  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.014e-14\n",
      "‖w_svm‖₂       : 0.0009161161228375175\n",
      "‖alpha‖₁       : 0.8199999999999983\n",
      "scores min/max : 0.0007106142113362801 0.002411742937805038\n",
      "Mask mean value:  tensor(0.5089, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5853  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.120e-16\n",
      "‖w_svm‖₂       : 7.438710785630978e-08\n",
      "‖alpha‖₁       : 0.42000000000000004\n",
      "scores min/max : -6.759331035007884e-08 -2.05099451562599e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.788e-08\n",
      "‖w_svm‖₂       : 7.287183306656014e-08\n",
      "‖alpha‖₁       : 0.13999999999999846\n",
      "scores min/max : 1.3669081094447288e-08 1.9395873130068797e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.728e-08\n",
      "‖w_svm‖₂       : 0.00021110159246978246\n",
      "‖alpha‖₁       : 0.619999999997922\n",
      "scores min/max : -5.958635653350512e-06 9.831812303990627e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.924e-17\n",
      "‖w_svm‖₂       : 7.538340430523529e-08\n",
      "‖alpha‖₁       : 0.5399999999999998\n",
      "scores min/max : -1.87085255949689e-07 -1.655615843758977e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.027e-20\n",
      "‖w_svm‖₂       : 2.7976897196418485e-07\n",
      "‖alpha‖₁       : 0.6599999999999799\n",
      "scores min/max : -6.486995178838237e-07 -4.969177206623507e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.486e-18\n",
      "‖w_svm‖₂       : 0.182665123590935\n",
      "‖alpha‖₁       : 0.8830981066582059\n",
      "scores min/max : -3.6363723904755654 6.070416232736922\n",
      "Mask mean value:  tensor(0.0918, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2012  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.143e-03\n",
      "‖w_svm‖₂       : 0.009095915163442424\n",
      "‖alpha‖₁       : 0.6077047863444591\n",
      "scores min/max : -1.9933381832759678 0.29342146015280923\n",
      "Mask mean value:  tensor(0.5425, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2190  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.846e-15\n",
      "‖w_svm‖₂       : 5.238064533275758e-07\n",
      "‖alpha‖₁       : 0.27999999997872144\n",
      "scores min/max : -1.5443174090174842e-06 -1.4466259256521696e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1957  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.762e-19\n",
      "‖w_svm‖₂       : 5.2314184726705445e-08\n",
      "‖alpha‖₁       : 0.4399999999999842\n",
      "scores min/max : -1.972617247903157e-07 -9.332175803061802e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.750e-20\n",
      "‖w_svm‖₂       : 0.005844885096825715\n",
      "‖alpha‖₁       : 0.5599999999999985\n",
      "scores min/max : 0.006202900246159496 0.007984009613915688\n",
      "Mask mean value:  tensor(0.5326, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1165  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.965e-05\n",
      "‖w_svm‖₂       : 4.325227471258021e-07\n",
      "‖alpha‖₁       : 0.7199999999999996\n",
      "scores min/max : 1.2809553927661587e-07 1.8526764417389533e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.284e-21\n",
      "‖w_svm‖₂       : 0.03553464789883943\n",
      "‖alpha‖₁       : 0.9199999999999868\n",
      "scores min/max : -0.2640851693354562 0.20179115812401194\n",
      "Mask mean value:  tensor(0.1661, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8308  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 9.641e-03\n",
      "‖w_svm‖₂       : 1.0690998627792472e-06\n",
      "‖alpha‖₁       : 0.4999999999999939\n",
      "scores min/max : -2.562147854389705e-07 -6.32423770981966e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.082e-19\n",
      "‖w_svm‖₂       : 0.05399997490767138\n",
      "‖alpha‖₁       : 0.5763016746438526\n",
      "scores min/max : -1.9572784406616393 0.8711119258640911\n",
      "Mask mean value:  tensor(0.7130, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9371  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.047e-03\n",
      "‖w_svm‖₂       : 0.07235731348069918\n",
      "‖alpha‖₁       : 0.6586734442217437\n",
      "scores min/max : -1.9780665635139805 4.3585814688945685\n",
      "Mask mean value:  tensor(0.4632, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.9565  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.228e-04\n",
      "‖w_svm‖₂       : 0.02051010693370252\n",
      "‖alpha‖₁       : 0.8233827741658485\n",
      "scores min/max : -1.9088334640673261 1.4875231801613376\n",
      "Mask mean value:  tensor(0.6729, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.3209  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.585e-05\n",
      "‖w_svm‖₂       : 5.80008133278209e-08\n",
      "‖alpha‖₁       : 0.23999999999997185\n",
      "scores min/max : -5.464390363368046e-08 -4.004476408443823e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.642e-20\n",
      "‖w_svm‖₂       : 0.0003024135484356397\n",
      "‖alpha‖₁       : 0.41999999995887183\n",
      "scores min/max : 7.306730023297434e-05 0.00038248206105803234\n",
      "Mask mean value:  tensor(0.5018, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9990  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.092e-15\n",
      "‖w_svm‖₂       : 0.030736705514502808\n",
      "‖alpha‖₁       : 0.8524922084055351\n",
      "scores min/max : -2.9097006846934494 1.570705368932854\n",
      "Mask mean value:  tensor(0.1580, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9536  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.678e-03\n",
      "‖w_svm‖₂       : 0.017430357776451\n",
      "‖alpha‖₁       : 0.6815989205754475\n",
      "scores min/max : -1.9726198325831465 0.05525238754379533\n",
      "Mask mean value:  tensor(0.6104, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2595  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.860e-12\n",
      "‖w_svm‖₂       : 0.00016053369137028852\n",
      "‖alpha‖₁       : 0.6399999999999999\n",
      "scores min/max : -0.00017134210936207942 -0.0001615089626057349\n",
      "Mask mean value:  tensor(0.4992, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7631  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.517e-17\n",
      "‖w_svm‖₂       : 2.4116095217137296e-07\n",
      "‖alpha‖₁       : 0.5799999999999793\n",
      "scores min/max : -2.346674457658851e-07 -2.1900539114331714e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.149e-19\n",
      "‖w_svm‖₂       : 0.03815409233230578\n",
      "‖alpha‖₁       : 0.6599999999999837\n",
      "scores min/max : -0.200779228228546 0.12878716587137373\n",
      "Mask mean value:  tensor(0.4407, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8279  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.922e-13\n",
      "‖w_svm‖₂       : 0.005738117660106897\n",
      "‖alpha‖₁       : 0.37999999999999745\n",
      "scores min/max : -0.0022589559724028527 0.008947945043393753\n",
      "Mask mean value:  tensor(0.5333, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9481  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.906e-14\n",
      "‖w_svm‖₂       : 4.8990985991300046e-08\n",
      "‖alpha‖₁       : 0.1799999999999961\n",
      "scores min/max : 5.6306367779122196e-08 7.103643075251349e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.047e-08\n",
      "‖w_svm‖₂       : 0.029219823471308436\n",
      "‖alpha‖₁       : 0.8985844544026432\n",
      "scores min/max : -0.7308293880290615 2.0291842742585087\n",
      "Mask mean value:  tensor(0.8677, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3091  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.845e-15\n",
      "‖w_svm‖₂       : 0.04558197850652454\n",
      "‖alpha‖₁       : 0.9388895819140664\n",
      "scores min/max : -2.4712633851525556 1.5781410322066878\n",
      "Mask mean value:  tensor(0.1407, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3913  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.112e-03\n",
      "‖w_svm‖₂       : 7.428024602572678e-08\n",
      "‖alpha‖₁       : 0.6599999999999986\n",
      "scores min/max : 8.873233809148924e-08 2.3377406982245141e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.216e-09\n",
      "‖w_svm‖₂       : 0.00014029028886924436\n",
      "‖alpha‖₁       : 0.4399999999802165\n",
      "scores min/max : -0.000290703484968306 -0.00014616039463334255\n",
      "Mask mean value:  tensor(0.4990, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0209  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.974e-17\n",
      "‖w_svm‖₂       : 1.3347715062835329e-05\n",
      "‖alpha‖₁       : 0.35999999999999976\n",
      "scores min/max : 8.986458302942822e-06 9.775291179832362e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.111e-17\n",
      "‖w_svm‖₂       : 8.4154505514266e-08\n",
      "‖alpha‖₁       : 0.3799999999999986\n",
      "scores min/max : 1.961487785263681e-08 3.777991611499351e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.976e-22\n",
      "‖w_svm‖₂       : 0.060362024648913735\n",
      "‖alpha‖₁       : 0.8986677639432008\n",
      "scores min/max : -1.6938991183219443 3.718488252039346\n",
      "Mask mean value:  tensor(0.4422, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5898  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.666e-03\n",
      "‖w_svm‖₂       : 5.2731269961342076e-08\n",
      "‖alpha‖₁       : 0.1199999999999963\n",
      "scores min/max : -9.435920969261513e-08 -6.363335597901136e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.623e-08\n",
      "‖w_svm‖₂       : 0.08499051710046648\n",
      "‖alpha‖₁       : 0.5780074916985575\n",
      "scores min/max : -2.1115260272461085 5.804346801986942\n",
      "Mask mean value:  tensor(0.2893, dtype=torch.float64)\n",
      "max feasible return = 0.1123  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.9453578995601216e-07\n",
      "‖alpha‖₁       : 0.5799999999999743\n",
      "scores min/max : -3.441459279614445e-07 -2.530481744579638e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0762122739356058e-07\n",
      "‖alpha‖₁       : 0.29999999999996907\n",
      "scores min/max : 3.667856247082616e-08 4.183371093814387e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 6.619662074254164e-08\n",
      "‖alpha‖₁       : 0.5999999999999955\n",
      "scores min/max : 1.1245725584726698e-08 4.804094714210041e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.4891074211035485e-08\n",
      "‖alpha‖₁       : 0.3799999999999998\n",
      "scores min/max : -1.809544444433191e-08 -8.726752428519311e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 5.964791771660678e-06\n",
      "‖alpha‖₁       : 0.3199999999373721\n",
      "scores min/max : 5.126462053666683e-06 9.036259781124814e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4259  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04654536103993538\n",
      "‖alpha‖₁       : 0.7396856565044249\n",
      "scores min/max : -3.464489951039177 2.1677164203674786\n",
      "Mask mean value:  tensor(0.8933, dtype=torch.float64)\n",
      "max feasible return = -0.9376  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.026121025591159315\n",
      "‖alpha‖₁       : 0.6151626040692307\n",
      "scores min/max : -1.9378927809086526 1.1904740016525377\n",
      "Mask mean value:  tensor(0.7360, dtype=torch.float64)\n",
      "max feasible return = 2.5670  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.6724752036359557e-07\n",
      "‖alpha‖₁       : 0.4599999999999943\n",
      "scores min/max : 9.521792230274991e-08 6.897071495653226e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.3884371222922467e-07\n",
      "‖alpha‖₁       : 0.5199999999999871\n",
      "scores min/max : 9.410892111483918e-08 1.0702432588425958e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.5847137769841676e-07\n",
      "‖alpha‖₁       : 0.5799999999999592\n",
      "scores min/max : -2.2355876270588823e-07 -1.8714542524620797e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.043468638215000216\n",
      "‖alpha‖₁       : 0.6878641925445343\n",
      "scores min/max : -0.4394672447091593 1.949959355383225\n",
      "Mask mean value:  tensor(0.3734, dtype=torch.float64)\n",
      "max feasible return = 0.0580  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03306702649045688\n",
      "‖alpha‖₁       : 0.4223066738733253\n",
      "scores min/max : -3.86664262116526 5.287345897162642\n",
      "Mask mean value:  tensor(0.1055, dtype=torch.float64)\n",
      "max feasible return = 0.6699  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.043298253601446415\n",
      "‖alpha‖₁       : 0.8211566600675773\n",
      "scores min/max : -4.954704186394778 3.215223763583123\n",
      "Mask mean value:  tensor(0.9652, dtype=torch.float64)\n",
      "max feasible return = -3.6931  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.006525287557135789\n",
      "‖alpha‖₁       : 0.7999999999999998\n",
      "scores min/max : -0.02867283787476095 -2.033730019748714e-05\n",
      "Mask mean value:  tensor(0.4639, dtype=torch.float64)\n",
      "max feasible return = -0.1687  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.0830109690517083e-07\n",
      "‖alpha‖₁       : 0.6199999999999666\n",
      "scores min/max : 5.7634697503961375e-08 7.415641256452441e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.826831254210843e-08\n",
      "‖alpha‖₁       : 0.4399999999999999\n",
      "scores min/max : 4.887803723157089e-09 1.4457892103540096e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.316814533818398e-07\n",
      "‖alpha‖₁       : 0.2799999999999804\n",
      "scores min/max : -2.0393897496397166e-07 -1.8602245257065406e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  46 | train 0.005311 | val 0.006619\n",
      "-----------------------------------------Epoch:  47 ----------------------------------------\n",
      "‖w_svm‖₂       : 8.416188081419455e-08\n",
      "‖alpha‖₁       : 0.37999999999999856\n",
      "scores min/max : 1.98837494863297e-08 3.8048625836272295e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.977e-22\n",
      "‖w_svm‖₂       : 1.9692751223869884e-07\n",
      "‖alpha‖₁       : 0.37999999999999545\n",
      "scores min/max : -3.43600856163014e-11 3.568382448264639e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.714e-19\n",
      "‖w_svm‖₂       : 2.4023243381935076e-07\n",
      "‖alpha‖₁       : 0.58\n",
      "scores min/max : -2.306046481255036e-07 -2.1498455345233532e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.144e-19\n",
      "‖w_svm‖₂       : 2.3322114783785775e-07\n",
      "‖alpha‖₁       : 0.6399999999999961\n",
      "scores min/max : 3.9615082094737207e-07 4.170421468289312e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.048e-19\n",
      "‖w_svm‖₂       : 0.13740867621310687\n",
      "‖alpha‖₁       : 0.6548597701945266\n",
      "scores min/max : -17.997846774255308 1.9014484963226141\n",
      "Mask mean value:  tensor(0.3564, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2486  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.317e-03\n",
      "‖w_svm‖₂       : 7.241892119344181e-08\n",
      "‖alpha‖₁       : 0.13999999999999868\n",
      "scores min/max : 1.2613451840320109e-08 1.833829652166838e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.640e-08\n",
      "‖w_svm‖₂       : 2.2619447158851405e-07\n",
      "‖alpha‖₁       : 0.25999999999999546\n",
      "scores min/max : 2.48464078235812e-07 2.612404149200883e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.504e-17\n",
      "‖w_svm‖₂       : 0.0001607042378816145\n",
      "‖alpha‖₁       : 0.6399999999999998\n",
      "scores min/max : -0.00017500803125038635 -0.00016515402174036042\n",
      "Mask mean value:  tensor(0.4992, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7631  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.535e-17\n",
      "‖w_svm‖₂       : 1.6551708420541174e-07\n",
      "‖alpha‖₁       : 0.23999999999999932\n",
      "scores min/max : 2.275041278237191e-07 2.4266540539554405e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.782e-20\n",
      "‖w_svm‖₂       : 0.05148813036738827\n",
      "‖alpha‖₁       : 0.8271899735907611\n",
      "scores min/max : -1.95245577928237 0.3986367291795759\n",
      "Mask mean value:  tensor(0.5163, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0123  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.746e-02\n",
      "‖w_svm‖₂       : 6.662385046813846e-07\n",
      "‖alpha‖₁       : 0.39999999999999974\n",
      "scores min/max : 1.0270237420019139e-08 3.1655007530265495e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.319e-09\n",
      "‖w_svm‖₂       : 0.09171227469806428\n",
      "‖alpha‖₁       : 0.8733798854532417\n",
      "scores min/max : -12.196829696687622 2.1210849741065787\n",
      "Mask mean value:  tensor(0.4494, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3447  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 8.448e-02\n",
      "‖w_svm‖₂       : 3.536475272935268e-06\n",
      "‖alpha‖₁       : 0.4199999999929287\n",
      "scores min/max : -3.4078760538807274e-06 -1.775398790046438e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1229  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.642e-05\n",
      "‖w_svm‖₂       : 0.0005270485244797859\n",
      "‖alpha‖₁       : 0.4399999999999871\n",
      "scores min/max : -0.0009381288063929451 -0.0006474719806641204\n",
      "Mask mean value:  tensor(0.4955, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6406  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.357e-16\n",
      "‖w_svm‖₂       : 1.0062881572153525e-06\n",
      "‖alpha‖₁       : 0.5999999999999949\n",
      "scores min/max : -2.094212864803744e-07 -1.5855481814519067e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.764e-19\n",
      "‖w_svm‖₂       : 1.0652814164526521e-07\n",
      "‖alpha‖₁       : 0.23999999999999105\n",
      "scores min/max : 7.825902102615898e-08 8.915207447604645e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.667e-21\n",
      "‖w_svm‖₂       : 5.820498043684708e-08\n",
      "‖alpha‖₁       : 0.23999999999997157\n",
      "scores min/max : -5.6252681393396275e-08 -4.165185762817146e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.667e-20\n",
      "‖w_svm‖₂       : 0.0465351622497264\n",
      "‖alpha‖₁       : 0.9056556335620158\n",
      "scores min/max : -0.7093196291239776 1.8857917560450703\n",
      "Mask mean value:  tensor(0.3495, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6515  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.087e-03\n",
      "‖w_svm‖₂       : 2.7034693561335517e-07\n",
      "‖alpha‖₁       : 0.3799999999906323\n",
      "scores min/max : -2.054139762106156e-07 -1.8439912688255273e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.780e-07\n",
      "‖w_svm‖₂       : 0.02654360763594568\n",
      "‖alpha‖₁       : 0.19670009541180664\n",
      "scores min/max : -2.212460007738966 0.009026316792283696\n",
      "Mask mean value:  tensor(0.0508, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3449  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.967e-04\n",
      "‖w_svm‖₂       : 7.874905401834154e-08\n",
      "‖alpha‖₁       : 0.17999999999999777\n",
      "scores min/max : -1.921612275356422e-07 4.198380835078917e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.421e-21\n",
      "‖w_svm‖₂       : 2.791958076437328e-07\n",
      "‖alpha‖₁       : 0.6599999999999735\n",
      "scores min/max : -6.504235169035071e-07 -4.986002369679842e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.497e-18\n",
      "‖w_svm‖₂       : 0.030612615029122957\n",
      "‖alpha‖₁       : 0.5512401806844921\n",
      "scores min/max : -3.4490331969130246 1.1201205324191787\n",
      "Mask mean value:  tensor(0.1664, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.2729  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.412e-03\n",
      "‖w_svm‖₂       : 1.0678186529610996e-06\n",
      "‖alpha‖₁       : 0.49999999999999317\n",
      "scores min/max : -2.430041132043116e-07 -4.999724189789568e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.092e-19\n",
      "‖w_svm‖₂       : 7.484810269425485e-08\n",
      "‖alpha‖₁       : 0.419999999999992\n",
      "scores min/max : -6.599068541331238e-08 -1.8775291945114973e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.860e-08\n",
      "‖w_svm‖₂       : 0.005855982783961283\n",
      "‖alpha‖₁       : 0.5599999999999999\n",
      "scores min/max : 0.0059943569294432265 0.007784204937882192\n",
      "Mask mean value:  tensor(0.5315, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1143  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 2.038e-05\n",
      "‖w_svm‖₂       : 0.006906951874574112\n",
      "‖alpha‖₁       : 0.7005788576664282\n",
      "scores min/max : -2.001267505119666 0.022153638341681027\n",
      "Mask mean value:  tensor(0.4682, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2619  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.148e-13\n",
      "‖w_svm‖₂       : 1.1575330946895222e-07\n",
      "‖alpha‖₁       : 0.29999999999999394\n",
      "scores min/max : 1.6372078495891438e-07 1.727075832283694e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.509e-07\n",
      "‖w_svm‖₂       : 5.2686132088653845e-08\n",
      "‖alpha‖₁       : 0.11999999999999722\n",
      "scores min/max : -9.243628661163188e-08 -6.17437188030062e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.556e-08\n",
      "‖w_svm‖₂       : 0.06956883011074079\n",
      "‖alpha‖₁       : 0.41930559737673556\n",
      "scores min/max : -1.9189402816710626 2.109478814059673\n",
      "Mask mean value:  tensor(0.7556, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8984  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.536e-04\n",
      "‖w_svm‖₂       : 0.07203762112461974\n",
      "‖alpha‖₁       : 0.5799999999999563\n",
      "scores min/max : -2.781239648079126 1.6396901799809407\n",
      "Mask mean value:  tensor(0.2491, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4120  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.077e-04\n",
      "‖w_svm‖₂       : 0.01608084558714183\n",
      "‖alpha‖₁       : 0.5963124669661111\n",
      "scores min/max : -2.7837658750842476 2.277760065158516\n",
      "Mask mean value:  tensor(0.9182, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.5086  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.285e-07\n",
      "‖w_svm‖₂       : 0.1466250823274581\n",
      "‖alpha‖₁       : 0.7595229815551826\n",
      "scores min/max : -1.770022509657827 2.207629107698056\n",
      "Mask mean value:  tensor(0.8357, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3609  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.832e-12\n",
      "‖w_svm‖₂       : 0.00021103403178888898\n",
      "‖alpha‖₁       : 0.619999999997386\n",
      "scores min/max : -1.0816398006221865e-05 4.963561691265439e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.967e-17\n",
      "‖w_svm‖₂       : 0.13773942412777432\n",
      "‖alpha‖₁       : 0.879999999999927\n",
      "scores min/max : -1.319991654280695 3.6665589674001278\n",
      "Mask mean value:  tensor(0.4110, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2662  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.024e-08\n",
      "‖w_svm‖₂       : 0.07236183012401529\n",
      "‖alpha‖₁       : 0.6587170739007308\n",
      "scores min/max : -2.0051247304837183 4.343727359838365\n",
      "Mask mean value:  tensor(0.4040, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.5143  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.836e-04\n",
      "‖w_svm‖₂       : 0.005721147558707761\n",
      "‖alpha‖₁       : 0.3799999999999984\n",
      "scores min/max : -0.0028902462870946596 0.00819924363353515\n",
      "Mask mean value:  tensor(0.5298, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9418  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.958e-14\n",
      "‖w_svm‖₂       : 0.0009274519503502564\n",
      "‖alpha‖₁       : 0.819999999999999\n",
      "scores min/max : 0.0006255100206296578 0.0023639908399515804\n",
      "Mask mean value:  tensor(0.5086, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5838  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.136e-16\n",
      "‖w_svm‖₂       : 0.030916382086087688\n",
      "‖alpha‖₁       : 0.8525012813889143\n",
      "scores min/max : -2.884807261750715 1.5956365748583952\n",
      "Mask mean value:  tensor(0.1710, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9649  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.043e-03\n",
      "‖w_svm‖₂       : 4.347949406563554e-07\n",
      "‖alpha‖₁       : 0.7199999999999996\n",
      "scores min/max : 1.4954072982358893e-07 2.0671216668239526e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.380e-21\n",
      "‖w_svm‖₂       : 0.00030114236835862996\n",
      "‖alpha‖₁       : 0.419999999844772\n",
      "scores min/max : 9.604052941453774e-05 0.0004016838798298203\n",
      "Mask mean value:  tensor(0.5019, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9992  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.131e-15\n",
      "‖w_svm‖₂       : 0.00018437807262513888\n",
      "‖alpha‖₁       : 0.82\n",
      "scores min/max : -9.71464749361607e-05 -8.378166277298656e-05\n",
      "Mask mean value:  tensor(0.4996, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6964  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.420e-17\n",
      "‖w_svm‖₂       : 0.02049104961236146\n",
      "‖alpha‖₁       : 0.7799999999999998\n",
      "scores min/max : -0.04437598685682888 0.020420298371316092\n",
      "Mask mean value:  tensor(0.3150, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.3962  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.102e-14\n",
      "‖w_svm‖₂       : 0.03775850245629989\n",
      "‖alpha‖₁       : 0.6599999999999822\n",
      "scores min/max : -0.19273577559711286 0.1290379799562943\n",
      "Mask mean value:  tensor(0.4573, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8633  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.670e-13\n",
      "‖w_svm‖₂       : 4.95247889360748e-08\n",
      "‖alpha‖₁       : 0.17999999999999744\n",
      "scores min/max : 4.44033441855809e-08 5.908109505184825e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.977e-08\n",
      "‖w_svm‖₂       : 1.960654653625383e-07\n",
      "‖alpha‖₁       : 0.4199999999999717\n",
      "scores min/max : -4.4366205466126875e-07 -4.2153811535636925e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.009e-19\n",
      "‖w_svm‖₂       : 0.004506665624630963\n",
      "‖alpha‖₁       : 0.4599999999999935\n",
      "scores min/max : -0.009777201291717219 -0.007603339540662647\n",
      "Mask mean value:  tensor(0.4596, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9969  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.123e-15\n",
      "‖w_svm‖₂       : 0.08224009835389916\n",
      "‖alpha‖₁       : 0.47202653412328643\n",
      "scores min/max : -2.2315326075368622 2.8494171840587597\n",
      "Mask mean value:  tensor(0.1497, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.5927  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.710e-02\n",
      "‖w_svm‖₂       : 0.05428731295895678\n",
      "‖alpha‖₁       : 0.5763271072390357\n",
      "scores min/max : -1.9600415025286635 0.8681991749770789\n",
      "Mask mean value:  tensor(0.7041, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9116  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.006e-03\n",
      "‖w_svm‖₂       : 0.03956411321777116\n",
      "‖alpha‖₁       : 0.7294802925597461\n",
      "scores min/max : -0.2681092192313136 2.013503214058316\n",
      "Mask mean value:  tensor(0.6561, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.9783  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 2.005e-03\n",
      "‖w_svm‖₂       : 3.7896921589749295e-07\n",
      "‖alpha‖₁       : 0.27999999999999914\n",
      "scores min/max : -1.026700620836786e-06 -9.650407862536826e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.172e-19\n",
      "‖w_svm‖₂       : 0.01749582322450833\n",
      "‖alpha‖₁       : 0.6815988382060473\n",
      "scores min/max : -1.969063982041724 0.0587119994408479\n",
      "Mask mean value:  tensor(0.6264, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2684  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.881e-12\n",
      "‖w_svm‖₂       : 0.02112293394210797\n",
      "‖alpha‖₁       : 0.8150895740699452\n",
      "scores min/max : -11.510838205216723 2.032624761944965\n",
      "Mask mean value:  tensor(0.7118, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9216  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.746e-06\n",
      "‖w_svm‖₂       : 7.512633077409458e-08\n",
      "‖alpha‖₁       : 0.5399999999999998\n",
      "scores min/max : -1.7871107978949144e-07 -1.5719668132928097e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.967e-20\n",
      "‖w_svm‖₂       : 0.029318959790980804\n",
      "‖alpha‖₁       : 0.8985912630810969\n",
      "scores min/max : -0.7311268730976218 2.0288676068413105\n",
      "Mask mean value:  tensor(0.8673, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3095  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.911e-15\n",
      "‖w_svm‖₂       : 0.015591488490773683\n",
      "‖alpha‖₁       : 0.8599999999999547\n",
      "scores min/max : -0.04993816349589143 -0.038281091664895475\n",
      "Mask mean value:  tensor(0.3048, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0795  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.675e-14\n",
      "‖w_svm‖₂       : 0.06060878535854137\n",
      "‖alpha‖₁       : 0.8986890425441921\n",
      "scores min/max : -1.697054678183727 3.7152839639607045\n",
      "Mask mean value:  tensor(0.4356, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5847  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.071e-02\n",
      "‖w_svm‖₂       : 0.04545312961625942\n",
      "‖alpha‖₁       : 0.938893437393737\n",
      "scores min/max : -2.472022330391201 1.5778897310792708\n",
      "Mask mean value:  tensor(0.1405, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3910  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.182e-03\n",
      "‖w_svm‖₂       : 0.01399784022181812\n",
      "‖alpha‖₁       : 0.8599999999999925\n",
      "scores min/max : -0.04992823101864655 -0.006750714438101508\n",
      "Mask mean value:  tensor(0.3435, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4291  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.881e-05\n",
      "‖w_svm‖₂       : 7.421968484414066e-08\n",
      "‖alpha‖₁       : 0.6599999999999988\n",
      "scores min/max : 9.097781334933429e-08 2.3605379033298632e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.262e-08\n",
      "‖w_svm‖₂       : 2.0072876127449137e-08\n",
      "‖alpha‖₁       : 0.11999999999999295\n",
      "scores min/max : -4.24234093777726e-08 -2.7675730989715425e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.830e-09\n",
      "‖w_svm‖₂       : 0.02130092600572729\n",
      "‖alpha‖₁       : 0.383494940523762\n",
      "scores min/max : -1.9560248886979177 0.24688800960680396\n",
      "Mask mean value:  tensor(0.7289, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1551  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.117e-14\n",
      "‖w_svm‖₂       : 0.019289240828140514\n",
      "‖alpha‖₁       : 0.6599999999999999\n",
      "scores min/max : -0.03597222032755515 0.021785640262316913\n",
      "Mask mean value:  tensor(0.4297, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.3667  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.018e-15\n",
      "‖w_svm‖₂       : 0.02065848621526128\n",
      "‖alpha‖₁       : 0.8233815931267896\n",
      "scores min/max : -1.9098266842652722 1.4865168715218788\n",
      "Mask mean value:  tensor(0.6700, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.3145  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.790e-05\n",
      "‖w_svm‖₂       : 0.00013950894486673652\n",
      "‖alpha‖₁       : 0.4399999999927391\n",
      "scores min/max : -0.0002986822234480569 -0.00015637906568325604\n",
      "Mask mean value:  tensor(0.4990, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0205  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.020e-17\n",
      "‖w_svm‖₂       : 0.003349866949968997\n",
      "‖alpha‖₁       : 0.5799999999999995\n",
      "scores min/max : 0.004456263973358554 0.005866247241955405\n",
      "Mask mean value:  tensor(0.5278, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.284e-13\n",
      "‖w_svm‖₂       : 0.041097411613607394\n",
      "‖alpha‖₁       : 0.8900223884969901\n",
      "scores min/max : -2.0126072231187786 0.33518605277297914\n",
      "Mask mean value:  tensor(0.2533, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0561  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.396e-05\n",
      "‖w_svm‖₂       : 0.04612439071645991\n",
      "‖alpha‖₁       : 0.9409184407834098\n",
      "scores min/max : -1.8321476057018664 0.30168807201135095\n",
      "Mask mean value:  tensor(0.4092, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3241  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.403e-14\n",
      "‖w_svm‖₂       : 0.009218779652111058\n",
      "‖alpha‖₁       : 0.6077070918561539\n",
      "scores min/max : -1.9942448655703242 0.29253513937093567\n",
      "Mask mean value:  tensor(0.5386, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2173  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.421e-15\n",
      "‖w_svm‖₂       : 1.111262141431658e-07\n",
      "‖alpha‖₁       : 0.5199999999999984\n",
      "scores min/max : 2.909090004513316e-07 3.1236081934325823e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.074e-19\n",
      "‖w_svm‖₂       : 1.2123570010531088e-06\n",
      "‖alpha‖₁       : 0.31999999999753825\n",
      "scores min/max : -2.6820895520925827e-06 -2.5635243838456507e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4897  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.316e-18\n",
      "‖w_svm‖₂       : 0.18279042085162903\n",
      "‖alpha‖₁       : 0.8831482934214283\n",
      "scores min/max : -3.6443531414327652 6.0642049789133985\n",
      "Mask mean value:  tensor(0.0906, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1981  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.318e-03\n",
      "‖w_svm‖₂       : 0.03513828058340677\n",
      "‖alpha‖₁       : 0.9199999999999116\n",
      "scores min/max : -0.2575866780858752 0.19909655351030797\n",
      "Mask mean value:  tensor(0.1709, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8564  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 9.064e-03\n",
      "‖w_svm‖₂       : 0.00036303267122596056\n",
      "‖alpha‖₁       : 0.74\n",
      "scores min/max : -0.00043082467262107235 -0.00041401482310323947\n",
      "Mask mean value:  tensor(0.4979, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0598  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.486e-17\n",
      "‖w_svm‖₂       : 1.3311949510264543e-05\n",
      "‖alpha‖₁       : 0.3599999999999861\n",
      "scores min/max : 8.982797075790838e-06 9.763157181297676e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.138e-17\n",
      "‖w_svm‖₂       : 5.2003282403309195e-08\n",
      "‖alpha‖₁       : 0.43999999999998246\n",
      "scores min/max : -1.9793569771277234e-07 -9.39960892716698e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.696e-20\n",
      "‖w_svm‖₂       : 0.08517670901574435\n",
      "‖alpha‖₁       : 0.5780456422152989\n",
      "scores min/max : -2.1154033597001454 5.799849306884821\n",
      "Mask mean value:  tensor(0.2819, dtype=torch.float64)\n",
      "max feasible return = 0.1109  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.959191922020834e-07\n",
      "‖alpha‖₁       : 0.5799999999999739\n",
      "scores min/max : -3.320238276695594e-07 -2.4098393615283107e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0810815061098459e-07\n",
      "‖alpha‖₁       : 0.29999999999997\n",
      "scores min/max : 4.3252347480588214e-08 4.8391594901502927e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 6.550504295705863e-08\n",
      "‖alpha‖₁       : 0.5999999999999983\n",
      "scores min/max : 1.3331716493788767e-08 5.010081059628796e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.479216781883496e-08\n",
      "‖alpha‖₁       : 0.37999999999999945\n",
      "scores min/max : -1.6200533443551212e-08 -6.828420942370901e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 5.250656193053795e-06\n",
      "‖alpha‖₁       : 0.3199999999411239\n",
      "scores min/max : 5.2542803554120415e-06 8.665089243877341e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4259  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04610487940866692\n",
      "‖alpha‖₁       : 0.7396938186347031\n",
      "scores min/max : -3.4496555022444895 2.1824710004293317\n",
      "Mask mean value:  tensor(0.9034, dtype=torch.float64)\n",
      "max feasible return = -0.9437  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.025925366487794203\n",
      "‖alpha‖₁       : 0.6151642422592685\n",
      "scores min/max : -1.9460830696029303 1.1821244798496005\n",
      "Mask mean value:  tensor(0.7129, dtype=torch.float64)\n",
      "max feasible return = 2.4858  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.709990687272244e-07\n",
      "‖alpha‖₁       : 0.4599999999999965\n",
      "scores min/max : 1.8611430282422236e-07 7.805160328625132e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.407382128788425e-07\n",
      "‖alpha‖₁       : 0.5199999999999749\n",
      "scores min/max : 8.457516838398304e-08 9.749866329990521e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.594257557020888e-07\n",
      "‖alpha‖₁       : 0.5799999999999985\n",
      "scores min/max : -2.463000371538592e-07 -2.1015151106571519e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.043399092928016365\n",
      "‖alpha‖₁       : 0.6878732767913913\n",
      "scores min/max : -0.44297595715897786 1.9465578789129665\n",
      "Mask mean value:  tensor(0.3612, dtype=torch.float64)\n",
      "max feasible return = 0.0692  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03293033723154994\n",
      "‖alpha‖₁       : 0.4223151273053281\n",
      "scores min/max : -3.8612329737029145 5.294489543262591\n",
      "Mask mean value:  tensor(0.1095, dtype=torch.float64)\n",
      "max feasible return = 0.6968  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.043383860814719434\n",
      "‖alpha‖₁       : 0.8211632954907577\n",
      "scores min/max : -4.94639750370044 3.223571461466438\n",
      "Mask mean value:  tensor(0.9658, dtype=torch.float64)\n",
      "max feasible return = -3.6961  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.006453682688255625\n",
      "‖alpha‖₁       : 0.7999999999999998\n",
      "scores min/max : -0.029070527090996293 -0.0011776817257559743\n",
      "Mask mean value:  tensor(0.4591, dtype=torch.float64)\n",
      "max feasible return = -0.1670  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.115048775846686e-07\n",
      "‖alpha‖₁       : 0.6199999999999682\n",
      "scores min/max : 8.875064521308671e-08 1.0526724365845487e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.886648309719644e-08\n",
      "‖alpha‖₁       : 0.4399999999999999\n",
      "scores min/max : 5.420455101877855e-09 1.4990612195903587e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.3393548364902151e-07\n",
      "‖alpha‖₁       : 0.27999999999998165\n",
      "scores min/max : -2.416999901710144e-07 -2.2379252059666764e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  47 | train 0.005331 | val 0.006611\n",
      "-----------------------------------------Epoch:  48 ----------------------------------------\n",
      "‖w_svm‖₂       : 1.9637954429066267e-07\n",
      "‖alpha‖₁       : 0.4199999999999708\n",
      "scores min/max : -4.750803836275417e-07 -4.52962963670073e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.020e-19\n",
      "‖w_svm‖₂       : 2.7124880287397605e-07\n",
      "‖alpha‖₁       : 0.37999999999057427\n",
      "scores min/max : -1.8358280252471357e-07 -1.6256364896576324e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.489e-06\n",
      "‖w_svm‖₂       : 0.14592671255163703\n",
      "‖alpha‖₁       : 0.7593181047313248\n",
      "scores min/max : -1.7682855634862793 2.2093982066759965\n",
      "Mask mean value:  tensor(0.8383, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3620  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.251e-11\n",
      "‖w_svm‖₂       : 5.864876786531351e-08\n",
      "‖alpha‖₁       : 0.23999999999997107\n",
      "scores min/max : -6.160406551774158e-08 -4.700019416835309e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.699e-20\n",
      "‖w_svm‖₂       : 0.020629021971236006\n",
      "‖alpha‖₁       : 0.8233815944218067\n",
      "scores min/max : -1.9102307227724045 1.486063898823342\n",
      "Mask mean value:  tensor(0.6688, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.3117  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.873e-05\n",
      "‖w_svm‖₂       : 0.035065581606826254\n",
      "‖alpha‖₁       : 0.9199999999999897\n",
      "scores min/max : -0.2585432136407216 0.195922062470701\n",
      "Mask mean value:  tensor(0.1668, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8359  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 9.868e-03\n",
      "‖w_svm‖₂       : 0.015664299103319363\n",
      "‖alpha‖₁       : 0.8599999999999404\n",
      "scores min/max : -0.05108695527357937 -0.03933224336347657\n",
      "Mask mean value:  tensor(0.3003, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0780  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.757e-14\n",
      "‖w_svm‖₂       : 0.04106244194628508\n",
      "‖alpha‖₁       : 0.8900204036018882\n",
      "scores min/max : -2.013614570962187 0.3341662468955306\n",
      "Mask mean value:  tensor(0.2504, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0547  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.753e-05\n",
      "‖w_svm‖₂       : 7.835220934608148e-08\n",
      "‖alpha‖₁       : 0.1799999999999979\n",
      "scores min/max : -1.982181972715484e-07 3.589896960187987e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.438e-21\n",
      "‖w_svm‖₂       : 0.05400439141497659\n",
      "‖alpha‖₁       : 0.5763035591608241\n",
      "scores min/max : -1.9570132796818336 0.871220716628962\n",
      "Mask mean value:  tensor(0.7136, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9389  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.057e-03\n",
      "‖w_svm‖₂       : 2.3356858283327593e-07\n",
      "‖alpha‖₁       : 0.639999999999995\n",
      "scores min/max : 4.1746797735287055e-07 4.383703969767157e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.073e-19\n",
      "‖w_svm‖₂       : 0.0058575473753099015\n",
      "‖alpha‖₁       : 0.5599999999999681\n",
      "scores min/max : 0.006198769827326776 0.007990129976913483\n",
      "Mask mean value:  tensor(0.5326, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1165  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.964e-05\n",
      "‖w_svm‖₂       : 0.0003652225944177379\n",
      "‖alpha‖₁       : 0.7400000000000001\n",
      "scores min/max : -0.0004340826020676501 -0.0004170694013110999\n",
      "Mask mean value:  tensor(0.4979, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0598  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.498e-17\n",
      "‖w_svm‖₂       : 0.017387997930799527\n",
      "‖alpha‖₁       : 0.6815987184221464\n",
      "scores min/max : -1.969507899022994 0.05812226679884597\n",
      "Mask mean value:  tensor(0.6244, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2676  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.546e-14\n",
      "‖w_svm‖₂       : 0.03090687814881951\n",
      "‖alpha‖₁       : 0.8524974877974492\n",
      "scores min/max : -2.889014378966048 1.5913831542187227\n",
      "Mask mean value:  tensor(0.1686, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9631  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.076e-03\n",
      "‖w_svm‖₂       : 0.013936784926182569\n",
      "‖alpha‖₁       : 0.8599999999999932\n",
      "scores min/max : -0.05115933100586495 -0.008283032783903887\n",
      "Mask mean value:  tensor(0.3375, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4217  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.376e-05\n",
      "‖w_svm‖₂       : 1.678449012719659e-07\n",
      "‖alpha‖₁       : 0.23999999999997929\n",
      "scores min/max : 2.3643095296173137e-07 2.5203925534570366e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.932e-20\n",
      "‖w_svm‖₂       : 4.333500911966395e-07\n",
      "‖alpha‖₁       : 0.7199999999999995\n",
      "scores min/max : 1.860687972559128e-07 2.432680330535946e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.359e-21\n",
      "‖w_svm‖₂       : 0.1385254111324508\n",
      "‖alpha‖₁       : 0.8799999999999444\n",
      "scores min/max : -1.3430230456103582 3.701639309662782\n",
      "Mask mean value:  tensor(0.3929, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2680  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.405e-08\n",
      "‖w_svm‖₂       : 0.019162384269660528\n",
      "‖alpha‖₁       : 0.6599999999999993\n",
      "scores min/max : -0.03713637532079683 0.019781506644537636\n",
      "Mask mean value:  tensor(0.4226, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.3112  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.040e-15\n",
      "‖w_svm‖₂       : 0.07243238691631375\n",
      "‖alpha‖₁       : 0.5799999999999631\n",
      "scores min/max : -2.8111892401171423 1.6577547930123924\n",
      "Mask mean value:  tensor(0.2486, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4126  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.135e-04\n",
      "‖w_svm‖₂       : 0.0001852762942127665\n",
      "‖alpha‖₁       : 0.8199999999998956\n",
      "scores min/max : -9.98556423656439e-05 -8.635308346274177e-05\n",
      "Mask mean value:  tensor(0.4995, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6963  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.439e-17\n",
      "‖w_svm‖₂       : 8.484021380651852e-08\n",
      "‖alpha‖₁       : 0.3799999999999983\n",
      "scores min/max : 8.56043241928343e-09 2.6727697874728885e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.136e-22\n",
      "‖w_svm‖₂       : 7.457355784895219e-08\n",
      "‖alpha‖₁       : 0.42\n",
      "scores min/max : -6.635879969293405e-08 -1.9255536846030598e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.839e-08\n",
      "‖w_svm‖₂       : 0.03930350231623576\n",
      "‖alpha‖₁       : 0.7294742386367499\n",
      "scores min/max : -0.267721780027539 2.013979473829054\n",
      "Mask mean value:  tensor(0.6572, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.9835  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.990e-03\n",
      "‖w_svm‖₂       : 5.242138873259971e-08\n",
      "‖alpha‖₁       : 0.11999999999999703\n",
      "scores min/max : -9.42038339252973e-08 -6.350692549747863e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.722e-08\n",
      "‖w_svm‖₂       : 0.0005259267065194929\n",
      "‖alpha‖₁       : 0.4399999999999993\n",
      "scores min/max : -0.0009197884833277524 -0.0006299572286447893\n",
      "Mask mean value:  tensor(0.4956, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6409  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.398e-16\n",
      "‖w_svm‖₂       : 0.046427830433331124\n",
      "‖alpha‖₁       : 0.9056493428523394\n",
      "scores min/max : -0.7116159158084758 1.8835167610356383\n",
      "Mask mean value:  tensor(0.3419, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6376  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.489e-03\n",
      "‖w_svm‖₂       : 0.020590981111149487\n",
      "‖alpha‖₁       : 0.7799999999999998\n",
      "scores min/max : -0.044048126465333445 0.021395075945298825\n",
      "Mask mean value:  tensor(0.3167, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.4041  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.098e-14\n",
      "‖w_svm‖₂       : 1.1531292760514638e-07\n",
      "‖alpha‖₁       : 0.2999999999999944\n",
      "scores min/max : 1.6237780696897698e-07 1.713628476380701e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.516e-07\n",
      "‖w_svm‖₂       : 0.04618582840998231\n",
      "‖alpha‖₁       : 0.9409284733266049\n",
      "scores min/max : -1.8325220245133274 0.3013396532835777\n",
      "Mask mean value:  tensor(0.4078, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.653e-14\n",
      "‖w_svm‖₂       : 0.02119795416366998\n",
      "‖alpha‖₁       : 0.8150893697764707\n",
      "scores min/max : -11.512275391948844 2.0311387459684895\n",
      "Mask mean value:  tensor(0.7087, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9141  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.712e-06\n",
      "‖w_svm‖₂       : 3.775665055976623e-07\n",
      "‖alpha‖₁       : 0.2799999999999935\n",
      "scores min/max : -1.1430916911706676e-06 -1.0813023444965864e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.211e-19\n",
      "‖w_svm‖₂       : 2.781716732334933e-07\n",
      "‖alpha‖₁       : 0.6599999999999786\n",
      "scores min/max : -6.730391748620387e-07 -5.212463728052358e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.511e-18\n",
      "‖w_svm‖₂       : 0.0002119907728686101\n",
      "‖alpha‖₁       : 0.619999999997599\n",
      "scores min/max : 9.026362934505452e-07 1.682723084285243e-05\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.014e-17\n",
      "‖w_svm‖₂       : 2.413626444348922e-07\n",
      "‖alpha‖₁       : 0.5799999999999998\n",
      "scores min/max : -2.895247672812876e-07 -2.739084459948501e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.222e-19\n",
      "‖w_svm‖₂       : 0.029237360391654643\n",
      "‖alpha‖₁       : 0.898593994548432\n",
      "scores min/max : -0.7303210685806818 2.029687198477846\n",
      "Mask mean value:  tensor(0.8683, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3075  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.840e-15\n",
      "‖w_svm‖₂       : 1.1028110329695508e-07\n",
      "‖alpha‖₁       : 0.5199999999999988\n",
      "scores min/max : 2.903658658654404e-07 3.1181828645239836e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.091e-19\n",
      "‖w_svm‖₂       : 6.72223362772893e-07\n",
      "‖alpha‖₁       : 0.3999999999999997\n",
      "scores min/max : 3.757337397959926e-08 3.438662287300417e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.380e-09\n",
      "‖w_svm‖₂       : 7.335320339683792e-08\n",
      "‖alpha‖₁       : 0.1399999999999731\n",
      "scores min/max : 1.4751288943199387e-08 2.0614936052772723e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.916e-08\n",
      "‖w_svm‖₂       : 0.0033372168056038237\n",
      "‖alpha‖₁       : 0.5799999999999995\n",
      "scores min/max : 0.004258224306129477 0.005657823012856517\n",
      "Mask mean value:  tensor(0.5268, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3350  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.435e-13\n",
      "‖w_svm‖₂       : 5.1855673776495554e-08\n",
      "‖alpha‖₁       : 0.43999999999998235\n",
      "scores min/max : -1.9959959402130073e-07 -9.569459616897624e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.729e-20\n",
      "‖w_svm‖₂       : 1.1938449856052136e-06\n",
      "‖alpha‖₁       : 0.3199999999977787\n",
      "scores min/max : -2.681527119723957e-06 -2.5637859327405733e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4897  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.316e-18\n",
      "‖w_svm‖₂       : 0.0009248463244259495\n",
      "‖alpha‖₁       : 0.8199999999999965\n",
      "scores min/max : 0.0008155418817740725 0.0025367082640727666\n",
      "Mask mean value:  tensor(0.5095, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5883  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.133e-16\n",
      "‖w_svm‖₂       : 0.030692666158901337\n",
      "‖alpha‖₁       : 0.5512445931066565\n",
      "scores min/max : -3.4476151905969084 1.1215718321996866\n",
      "Mask mean value:  tensor(0.1675, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.2787  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.328e-03\n",
      "‖w_svm‖₂       : 1.0048534386174754e-06\n",
      "‖alpha‖₁       : 0.5999999999999949\n",
      "scores min/max : -1.9926410342573343e-07 -1.483980410637561e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.747e-19\n",
      "‖w_svm‖₂       : 0.00016048983601310929\n",
      "‖alpha‖₁       : 0.6399999999999999\n",
      "scores min/max : -0.00015286041161428112 -0.00014303273188274828\n",
      "Mask mean value:  tensor(0.4993, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7633  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.481e-17\n",
      "‖w_svm‖₂       : 2.2648072634474676e-07\n",
      "‖alpha‖₁       : 0.2599999999999955\n",
      "scores min/max : 2.075736036997173e-07 2.2035030799099275e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.330e-17\n",
      "‖w_svm‖₂       : 0.0057425096669158964\n",
      "‖alpha‖₁       : 0.37999999999999984\n",
      "scores min/max : -0.003309410463328097 0.007877233010280493\n",
      "Mask mean value:  tensor(0.5281, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9386  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.040e-14\n",
      "‖w_svm‖₂       : 0.037570627742095436\n",
      "‖alpha‖₁       : 0.6599999999999835\n",
      "scores min/max : -0.19144792413867595 0.1279380252614828\n",
      "Mask mean value:  tensor(0.4569, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8634  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.704e-13\n",
      "‖w_svm‖₂       : 3.7127606949947024e-07\n",
      "‖alpha‖₁       : 0.23999999998789132\n",
      "scores min/max : 2.8956539806610986e-07 3.1726718810695845e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.609e-20\n",
      "‖w_svm‖₂       : 1.9838339036440882e-07\n",
      "‖alpha‖₁       : 0.37999999999999495\n",
      "scores min/max : -3.093900369417492e-08 4.7821050364232305e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.809e-19\n",
      "‖w_svm‖₂       : 3.5343040135583525e-06\n",
      "‖alpha‖₁       : 0.41999999999288\n",
      "scores min/max : -3.658162558361887e-06 -2.0268766767140917e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1229  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.667e-05\n",
      "‖w_svm‖₂       : 0.06069550802156942\n",
      "‖alpha‖₁       : 0.8986996824490856\n",
      "scores min/max : -1.6927247619451653 3.7197125646902798\n",
      "Mask mean value:  tensor(0.4446, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5916  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.188e-03\n",
      "‖w_svm‖₂       : 7.46908318273445e-08\n",
      "‖alpha‖₁       : 0.5399999999999995\n",
      "scores min/max : -1.8540035945005662e-07 -1.6388131107635526e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.991e-20\n",
      "‖w_svm‖₂       : 0.0455190334087748\n",
      "‖alpha‖₁       : 0.938894089512254\n",
      "scores min/max : -2.4723466376612584 1.5774826899806134\n",
      "Mask mean value:  tensor(0.1402, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3906  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.186e-03\n",
      "‖w_svm‖₂       : 0.09028654360319596\n",
      "‖alpha‖₁       : 0.8733827272666503\n",
      "scores min/max : -12.14440048910279 2.172268555797932\n",
      "Mask mean value:  tensor(0.5762, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7066  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 4.283e-02\n",
      "‖w_svm‖₂       : 4.9290506685143275e-08\n",
      "‖alpha‖₁       : 0.17999999999999622\n",
      "scores min/max : 4.9528861900233914e-08 6.424847497695088e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.053e-08\n",
      "‖w_svm‖₂       : 0.009254092516687585\n",
      "‖alpha‖₁       : 0.6077076631777509\n",
      "scores min/max : -1.9946796282960566 0.2920723737907027\n",
      "Mask mean value:  tensor(0.5366, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2166  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.642e-15\n",
      "‖w_svm‖₂       : 0.0045503657814454255\n",
      "‖alpha‖₁       : 0.4599999999999485\n",
      "scores min/max : -0.010341660483207568 -0.008100583097121292\n",
      "Mask mean value:  tensor(0.4570, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9857  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.154e-15\n",
      "‖w_svm‖₂       : 0.00014071214441507395\n",
      "‖alpha‖₁       : 0.4399999999735499\n",
      "scores min/max : -0.00029987160373546977 -0.00015561866426718499\n",
      "Mask mean value:  tensor(0.4990, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0205  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.059e-17\n",
      "‖w_svm‖₂       : 1.3356755633313891e-05\n",
      "‖alpha‖₁       : 0.3599999999999812\n",
      "scores min/max : 8.886833303991263e-06 9.674609707674072e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.188e-17\n",
      "‖w_svm‖₂       : 1.0651658234123968e-06\n",
      "‖alpha‖₁       : 0.4999999999999927\n",
      "scores min/max : -2.3029020867241562e-07 -3.726892883158295e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.098e-19\n",
      "‖w_svm‖₂       : 0.0693603694515732\n",
      "‖alpha‖₁       : 0.41932484322314284\n",
      "scores min/max : -1.9353494479711375 2.082053188793438\n",
      "Mask mean value:  tensor(0.7385, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8920  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.024e-04\n",
      "‖w_svm‖₂       : 0.051805977196242275\n",
      "‖alpha‖₁       : 0.8272172378499703\n",
      "scores min/max : -1.9477010591570991 0.40335535178198617\n",
      "Mask mean value:  tensor(0.5333, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0154  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.668e-02\n",
      "‖w_svm‖₂       : 0.01602453102096396\n",
      "‖alpha‖₁       : 0.59631724162586\n",
      "scores min/max : -2.759158424380964 2.2776533370506304\n",
      "Mask mean value:  tensor(0.9181, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.5084  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.647e-07\n",
      "‖w_svm‖₂       : 0.1382999766896354\n",
      "‖alpha‖₁       : 0.6551047256962017\n",
      "scores min/max : -18.039869781033218 1.859244856607749\n",
      "Mask mean value:  tensor(0.2992, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2768  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.933e-03\n",
      "‖w_svm‖₂       : 0.021102142764690853\n",
      "‖alpha‖₁       : 0.3834953743989742\n",
      "scores min/max : -1.957780372742215 0.24544977434275472\n",
      "Mask mean value:  tensor(0.7232, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1545  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.551e-14\n",
      "‖w_svm‖₂       : 0.18340191350607715\n",
      "‖alpha‖₁       : 0.8833857467786531\n",
      "scores min/max : -3.6416093605240727 6.066869808555546\n",
      "Mask mean value:  tensor(0.0911, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1996  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.290e-03\n",
      "‖w_svm‖₂       : 7.367131043196504e-08\n",
      "‖alpha‖₁       : 0.659999999999999\n",
      "scores min/max : 9.025878510500484e-08 2.3538825294968646e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.319e-09\n",
      "‖w_svm‖₂       : 1.985438656815566e-08\n",
      "‖alpha‖₁       : 0.11999999999999295\n",
      "scores min/max : -4.3886675175931364e-08 -2.9136413357705772e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.837e-09\n",
      "‖w_svm‖₂       : 0.00030224480536109147\n",
      "‖alpha‖₁       : 0.41999999991060954\n",
      "scores min/max : 8.447935590310011e-05 0.0003927681861171963\n",
      "Mask mean value:  tensor(0.5018, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9991  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.162e-15\n",
      "‖w_svm‖₂       : 0.07214101831432251\n",
      "‖alpha‖₁       : 0.6587163563114278\n",
      "scores min/max : -1.9984510328922926 4.357499752189863\n",
      "Mask mean value:  tensor(0.4186, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.6309  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.829e-04\n",
      "‖w_svm‖₂       : 0.026542648740293384\n",
      "‖alpha‖₁       : 0.1967055398519634\n",
      "scores min/max : -2.218867190976602 0.002442931181520884\n",
      "Mask mean value:  tensor(0.0458, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3115  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.182e-03\n",
      "‖w_svm‖₂       : 0.08233159310616062\n",
      "‖alpha‖₁       : 0.47204158345117614\n",
      "scores min/max : -2.2398774833283275 2.841001781449848\n",
      "Mask mean value:  tensor(0.1437, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.5340  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.710e-02\n",
      "‖w_svm‖₂       : 0.006956402223815007\n",
      "‖alpha‖₁       : 0.7005794916949633\n",
      "scores min/max : -2.002079181584977 0.021347108897046614\n",
      "Mask mean value:  tensor(0.4643, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2598  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.685e-13\n",
      "‖w_svm‖₂       : 0.08557432828607102\n",
      "‖alpha‖₁       : 0.5781212941055326\n",
      "scores min/max : -2.128546421338158 5.786129902602611\n",
      "Mask mean value:  tensor(0.2583, dtype=torch.float64)\n",
      "max feasible return = 0.1069  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.9338521278686645e-07\n",
      "‖alpha‖₁       : 0.5799999999999771\n",
      "scores min/max : -3.252534825666675e-07 -2.3423395334193984e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0759829224265318e-07\n",
      "‖alpha‖₁       : 0.29999999999996985\n",
      "scores min/max : 3.859089489823513e-08 4.373715069087132e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 6.524703003386526e-08\n",
      "‖alpha‖₁       : 0.5999999999999981\n",
      "scores min/max : 9.083859867143252e-09 4.5861128832105266e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.4586148015558785e-08\n",
      "‖alpha‖₁       : 0.3799999999999989\n",
      "scores min/max : -2.006408943469582e-08 -1.068315455865115e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 7.937462721593293e-06\n",
      "‖alpha‖₁       : 0.31999999987836014\n",
      "scores min/max : 6.546984469812008e-06 1.2425829300736942e-05\n",
      "Mask mean value:  tensor(0.5001, dtype=torch.float64)\n",
      "max feasible return = 2.4260  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04604193270189264\n",
      "‖alpha‖₁       : 0.739695240958586\n",
      "scores min/max : -3.451169323685428 2.1808572979210847\n",
      "Mask mean value:  tensor(0.9024, dtype=torch.float64)\n",
      "max feasible return = -0.9432  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.025756089977203005\n",
      "‖alpha‖₁       : 0.6151634344193335\n",
      "scores min/max : -1.943839740881967 1.1843064458877466\n",
      "Mask mean value:  tensor(0.7196, dtype=torch.float64)\n",
      "max feasible return = 2.5089  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.68046954466585e-07\n",
      "‖alpha‖₁       : 0.45999999999999375\n",
      "scores min/max : 1.1897574269088998e-07 7.134890364081027e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.395567972916447e-07\n",
      "‖alpha‖₁       : 0.5199999999999849\n",
      "scores min/max : 8.970185233776255e-08 1.026186829767225e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.5940775655709667e-07\n",
      "‖alpha‖₁       : 0.5799999999999566\n",
      "scores min/max : -2.2693885912008654e-07 -1.9051950766095225e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.04340174006240186\n",
      "‖alpha‖₁       : 0.6878877979537361\n",
      "scores min/max : -0.4442418490174261 1.9453939829282096\n",
      "Mask mean value:  tensor(0.3568, dtype=torch.float64)\n",
      "max feasible return = 0.0729  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03321498997564364\n",
      "‖alpha‖₁       : 0.4223290660820217\n",
      "scores min/max : -3.8600446371746826 5.295161209308535\n",
      "Mask mean value:  tensor(0.1101, dtype=torch.float64)\n",
      "max feasible return = 0.7011  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.043296099991172246\n",
      "‖alpha‖₁       : 0.8211615998716135\n",
      "scores min/max : -4.948850561091958 3.221259349543007\n",
      "Mask mean value:  tensor(0.9656, dtype=torch.float64)\n",
      "max feasible return = -3.6953  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.006409361690355069\n",
      "‖alpha‖₁       : 0.799999999999844\n",
      "scores min/max : -0.028539007859099547 -0.0009418305609432878\n",
      "Mask mean value:  tensor(0.4607, dtype=torch.float64)\n",
      "max feasible return = -0.1675  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.08954870474789e-07\n",
      "‖alpha‖₁       : 0.6199999999999701\n",
      "scores min/max : 7.013617356473318e-08 8.664776472721737e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.850424357981563e-08\n",
      "‖alpha‖₁       : 0.43999999999999984\n",
      "scores min/max : 4.5499431335030625e-09 1.4119835686092196e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.323293880886796e-07\n",
      "‖alpha‖₁       : 0.27999999999998193\n",
      "scores min/max : -2.0545332213604838e-07 -1.875480767549664e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  48 | train 0.005307 | val 0.006659\n",
      "-----------------------------------------Epoch:  49 ----------------------------------------\n",
      "‖w_svm‖₂       : 0.18321137963854378\n",
      "‖alpha‖₁       : 0.8833180922006414\n",
      "scores min/max : -3.637553915736345 6.070014062736062\n",
      "Mask mean value:  tensor(0.0917, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2013  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.240e-03\n",
      "‖w_svm‖₂       : 0.00926663379691344\n",
      "‖alpha‖₁       : 0.607707861736225\n",
      "scores min/max : -1.9945411919914469 0.2922017338545747\n",
      "Mask mean value:  tensor(0.5372, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2168  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.615e-15\n",
      "‖w_svm‖₂       : 0.026475835822304932\n",
      "‖alpha‖₁       : 0.19670220861240417\n",
      "scores min/max : -2.21590536521546 0.005430633425308688\n",
      "Mask mean value:  tensor(0.0480, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3265  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.055e-03\n",
      "‖w_svm‖₂       : 1.153705231975636e-07\n",
      "‖alpha‖₁       : 0.29999999999999444\n",
      "scores min/max : 1.6792809958975075e-07 1.7691191170533133e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.535e-07\n",
      "‖w_svm‖₂       : 0.00021174380267823271\n",
      "‖alpha‖₁       : 0.6199999999984371\n",
      "scores min/max : -6.718350956020714e-06 9.167933653334708e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.047e-17\n",
      "‖w_svm‖₂       : 7.340562133743588e-08\n",
      "‖alpha‖₁       : 0.13999999999997603\n",
      "scores min/max : 1.363339833042185e-08 1.948904807346207e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.868e-08\n",
      "‖w_svm‖₂       : 0.000929193311626288\n",
      "‖alpha‖₁       : 0.8199999999999995\n",
      "scores min/max : 0.0007006125351913706 0.0024451860682966117\n",
      "Mask mean value:  tensor(0.5090, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5858  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.134e-16\n",
      "‖w_svm‖₂       : 0.015790952509764634\n",
      "‖alpha‖₁       : 0.8599999999999964\n",
      "scores min/max : -0.05819109448136209 -0.04624634508749645\n",
      "Mask mean value:  tensor(0.2718, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0694  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.371e-14\n",
      "‖w_svm‖₂       : 2.3335566711699407e-07\n",
      "‖alpha‖₁       : 0.6399999999999952\n",
      "scores min/max : 3.997060552574991e-07 4.20613053043093e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.063e-19\n",
      "‖w_svm‖₂       : 1.5171611057061728e-07\n",
      "‖alpha‖₁       : 0.3799999999989494\n",
      "scores min/max : -1.2958772758793226e-07 -1.1346258077616175e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.854e-07\n",
      "‖w_svm‖₂       : 5.291483285415465e-08\n",
      "‖alpha‖₁       : 0.11999999999998663\n",
      "scores min/max : -9.391092405148922e-08 -6.304594263637394e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.321e-07\n",
      "‖w_svm‖₂       : 8.126157197753968e-08\n",
      "‖alpha‖₁       : 0.17999999999997604\n",
      "scores min/max : -2.0637628487963095e-07 3.083945277185226e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.815e-21\n",
      "‖w_svm‖₂       : 1.1043859600254289e-07\n",
      "‖alpha‖₁       : 0.519999999999999\n",
      "scores min/max : 2.671049230778007e-07 2.8855732419104486e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.063e-19\n",
      "‖w_svm‖₂       : 1.0661835959360279e-06\n",
      "‖alpha‖₁       : 0.49999999999999245\n",
      "scores min/max : -2.3750152295755134e-07 -4.448069954932229e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.100e-19\n",
      "‖w_svm‖₂       : 0.090123909457094\n",
      "‖alpha‖₁       : 0.8733903597964412\n",
      "scores min/max : -12.174146509450779 2.14215691937319\n",
      "Mask mean value:  tensor(0.5026, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.4949  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.112e-02\n",
      "‖w_svm‖₂       : 0.04098627448247265\n",
      "‖alpha‖₁       : 0.8900206849342625\n",
      "scores min/max : -2.013866317097372 0.333887016392306\n",
      "Mask mean value:  tensor(0.2496, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0540  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.383e-05\n",
      "‖w_svm‖₂       : 0.003359659020560341\n",
      "‖alpha‖₁       : 0.5799999999999994\n",
      "scores min/max : 0.004785616496990159 0.006204294769630046\n",
      "Mask mean value:  tensor(0.5295, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.192e-13\n",
      "‖w_svm‖₂       : 0.005771505348454441\n",
      "‖alpha‖₁       : 0.37999999999999834\n",
      "scores min/max : -0.002399501225645806 0.008894136007748129\n",
      "Mask mean value:  tensor(0.5330, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9474  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.946e-14\n",
      "‖w_svm‖₂       : 4.3429129272968456e-07\n",
      "‖alpha‖₁       : 0.7199999999999996\n",
      "scores min/max : 1.5544859034084076e-07 2.1263302699095995e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.362e-21\n",
      "‖w_svm‖₂       : 0.03110274809565462\n",
      "‖alpha‖₁       : 0.8525114104219069\n",
      "scores min/max : -2.8946792373818298 1.5856643594882118\n",
      "Mask mean value:  tensor(0.1655, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9606  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.198e-03\n",
      "‖w_svm‖₂       : 3.56050266917905e-06\n",
      "‖alpha‖₁       : 0.4199999999927196\n",
      "scores min/max : -3.736529740090328e-06 -2.1055173117710166e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1229  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.689e-05\n",
      "‖w_svm‖₂       : 0.020559773205015286\n",
      "‖alpha‖₁       : 0.8233837091280475\n",
      "scores min/max : -1.9103567454255788 1.4858370415139759\n",
      "Mask mean value:  tensor(0.6684, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.3105  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.912e-05\n",
      "‖w_svm‖₂       : 0.0727062702804402\n",
      "‖alpha‖₁       : 0.5799999999999282\n",
      "scores min/max : -2.824057641207081 1.6702958050047292\n",
      "Mask mean value:  tensor(0.2559, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4213  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.854e-04\n",
      "‖w_svm‖₂       : 1.0101924307259097e-06\n",
      "‖alpha‖₁       : 0.5999999999999943\n",
      "scores min/max : -2.0320766840936222e-07 -1.5233898654883415e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.773e-19\n",
      "‖w_svm‖₂       : 0.030726181197496944\n",
      "‖alpha‖₁       : 0.5512500199035346\n",
      "scores min/max : -3.4453255261347544 1.1237189445142068\n",
      "Mask mean value:  tensor(0.1691, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.2872  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.197e-03\n",
      "‖w_svm‖₂       : 0.04633538266627134\n",
      "‖alpha‖₁       : 0.9056548757379853\n",
      "scores min/max : -0.7116095704512182 1.8832852077570932\n",
      "Mask mean value:  tensor(0.3411, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6365  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.523e-03\n",
      "‖w_svm‖₂       : 0.029102928851162647\n",
      "‖alpha‖₁       : 0.8985889193057139\n",
      "scores min/max : -0.7304829237998435 2.02948283715912\n",
      "Mask mean value:  tensor(0.8680, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3073  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.847e-15\n",
      "‖w_svm‖₂       : 0.08157401711283377\n",
      "‖alpha‖₁       : 0.4719340515909706\n",
      "scores min/max : -2.1946477665002306 2.886019487478956\n",
      "Mask mean value:  tensor(0.1829, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9173  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.092e-02\n",
      "‖w_svm‖₂       : 0.000527267645993133\n",
      "‖alpha‖₁       : 0.4399999999999996\n",
      "scores min/max : -0.0008910690545132602 -0.0005996140819604177\n",
      "Mask mean value:  tensor(0.4958, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6414  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.406e-16\n",
      "‖w_svm‖₂       : 0.13702876048240656\n",
      "‖alpha‖₁       : 0.6547887351568205\n",
      "scores min/max : -18.027868089109255 1.8759127894525744\n",
      "Mask mean value:  tensor(0.3202, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2683  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.789e-03\n",
      "‖w_svm‖₂       : 1.9981633975904005e-07\n",
      "‖alpha‖₁       : 0.37999999999999934\n",
      "scores min/max : -3.323195561257813e-08 2.4503109830558358e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.858e-19\n",
      "‖w_svm‖₂       : 0.04525579136587761\n",
      "‖alpha‖₁       : 0.9388723658226197\n",
      "scores min/max : -2.473644388441751 1.5764683422659436\n",
      "Mask mean value:  tensor(0.1396, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.116e-03\n",
      "‖w_svm‖₂       : 0.0003635159350224838\n",
      "‖alpha‖₁       : 0.7399999999999991\n",
      "scores min/max : -0.0004393148380189817 -0.00042246023620929105\n",
      "Mask mean value:  tensor(0.4979, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0598  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.512e-17\n",
      "‖w_svm‖₂       : 2.4341355979964296e-07\n",
      "‖alpha‖₁       : 0.5799999999999998\n",
      "scores min/max : -2.896298605351922e-07 -2.7401381416805925e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.244e-19\n",
      "‖w_svm‖₂       : 0.03703043737746679\n",
      "‖alpha‖₁       : 0.6599999999999826\n",
      "scores min/max : -0.1856506757035178 0.12388752906791967\n",
      "Mask mean value:  tensor(0.4574, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8670  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.668e-13\n",
      "‖w_svm‖₂       : 1.2006628641777575e-06\n",
      "‖alpha‖₁       : 0.31999999999783935\n",
      "scores min/max : -2.7062977924092527e-06 -2.5887934813781503e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4897  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.337e-18\n",
      "‖w_svm‖₂       : 0.06023893045036524\n",
      "‖alpha‖₁       : 0.898659566981\n",
      "scores min/max : -1.6995713791294973 3.7128067078435008\n",
      "Mask mean value:  tensor(0.4305, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5809  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.154e-02\n",
      "‖w_svm‖₂       : 7.403795275294139e-08\n",
      "‖alpha‖₁       : 0.659999999999999\n",
      "scores min/max : 9.314260153528073e-08 2.3823989448401192e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.866e-09\n",
      "‖w_svm‖₂       : 6.74058224788014e-07\n",
      "‖alpha‖₁       : 0.3999999999999998\n",
      "scores min/max : 5.3741294001097846e-08 3.597556167588207e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.391e-09\n",
      "‖w_svm‖₂       : 0.05362813158318107\n",
      "‖alpha‖₁       : 0.5762889446188395\n",
      "scores min/max : -1.9565981987244305 0.8716592210888708\n",
      "Mask mean value:  tensor(0.7150, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9425  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.057e-03\n",
      "‖w_svm‖₂       : 0.00018373212554100666\n",
      "‖alpha‖₁       : 0.8199999999999992\n",
      "scores min/max : -0.00010159778004867624 -8.835160092016494e-05\n",
      "Mask mean value:  tensor(0.4995, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6963  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.439e-17\n",
      "‖w_svm‖₂       : 5.8655706090681784e-08\n",
      "‖alpha‖₁       : 0.23999999999999327\n",
      "scores min/max : -6.392053606337693e-08 -4.9471842873568204e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.725e-20\n",
      "‖w_svm‖₂       : 7.552898633803808e-08\n",
      "‖alpha‖₁       : 0.42\n",
      "scores min/max : -6.657829261110169e-08 -1.946769567201326e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.999e-08\n",
      "‖w_svm‖₂       : 0.0070253755475247846\n",
      "‖alpha‖₁       : 0.7005803432113988\n",
      "scores min/max : -2.0012801773847797 0.02214209491613204\n",
      "Mask mean value:  tensor(0.4681, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2619  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.369e-13\n",
      "‖w_svm‖₂       : 1.9628609141867298e-07\n",
      "‖alpha‖₁       : 0.41999999999996795\n",
      "scores min/max : -4.833323051373409e-07 -4.6121907577196606e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.057e-19\n",
      "‖w_svm‖₂       : 0.017256359654952768\n",
      "‖alpha‖₁       : 0.6815988458168145\n",
      "scores min/max : -1.9686429415869562 0.058632146908462635\n",
      "Mask mean value:  tensor(0.6282, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2703  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.193e-14\n",
      "‖w_svm‖₂       : 1.6845055928304082e-07\n",
      "‖alpha‖₁       : 0.23999999999998134\n",
      "scores min/max : 2.392820116888661e-07 2.5482691729154895e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.977e-20\n",
      "‖w_svm‖₂       : 0.01597198796808769\n",
      "‖alpha‖₁       : 0.5963159645958143\n",
      "scores min/max : -2.7495260680413893 2.2793386189181617\n",
      "Mask mean value:  tensor(0.9189, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.5091  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.234e-08\n",
      "‖w_svm‖₂       : 1.9845366674963264e-08\n",
      "‖alpha‖₁       : 0.11999999999999295\n",
      "scores min/max : -4.3448558669794e-08 -2.8698778905989114e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.824e-09\n",
      "‖w_svm‖₂       : 0.004537199371392396\n",
      "‖alpha‖₁       : 0.45999999999999686\n",
      "scores min/max : -0.010436577739272299 -0.008228139408821711\n",
      "Mask mean value:  tensor(0.4564, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9832  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.163e-15\n",
      "‖w_svm‖₂       : 0.03442881420076151\n",
      "‖alpha‖₁       : 0.9199999999993712\n",
      "scores min/max : -0.2537813806863508 0.18486758101086176\n",
      "Mask mean value:  tensor(0.1622, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8142  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 8.766e-03\n",
      "‖w_svm‖₂       : 0.0685265589326443\n",
      "‖alpha‖₁       : 0.41922783500911176\n",
      "scores min/max : -1.9193510520007593 2.0961603066749506\n",
      "Mask mean value:  tensor(0.7554, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8990  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.521e-04\n",
      "‖w_svm‖₂       : 0.02052486816032483\n",
      "‖alpha‖₁       : 0.779999999999999\n",
      "scores min/max : -0.04522188725843143 0.01907826675163865\n",
      "Mask mean value:  tensor(0.3112, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.3792  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.163e-14\n",
      "‖w_svm‖₂       : 0.07117979111814533\n",
      "‖alpha‖₁       : 0.6586464864776929\n",
      "scores min/max : -1.991088394719902 4.382058770044701\n",
      "Mask mean value:  tensor(0.4360, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.7654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.640e-04\n",
      "‖w_svm‖₂       : 3.7899371130532626e-07\n",
      "‖alpha‖₁       : 0.27999999999999936\n",
      "scores min/max : -1.147491549443103e-06 -1.0858522535904887e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.241e-19\n",
      "‖w_svm‖₂       : 5.183058868158308e-08\n",
      "‖alpha‖₁       : 0.43999999999997963\n",
      "scores min/max : -1.9973102900494178e-07 -9.585287835593077e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.744e-20\n",
      "‖w_svm‖₂       : 7.482537377274517e-08\n",
      "‖alpha‖₁       : 0.5399999999999997\n",
      "scores min/max : -1.8569875197739984e-07 -1.6418113153952315e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.009e-20\n",
      "‖w_svm‖₂       : 0.14523414908107052\n",
      "‖alpha‖₁       : 0.7591241234926306\n",
      "scores min/max : -1.771714616843612 2.20595047928478\n",
      "Mask mean value:  tensor(0.8330, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3610  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.912e-12\n",
      "‖w_svm‖₂       : 0.0001400360164511974\n",
      "‖alpha‖₁       : 0.4399999999958125\n",
      "scores min/max : -0.0003050797884174355 -0.00016167451613007233\n",
      "Mask mean value:  tensor(0.4989, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0203  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.087e-17\n",
      "‖w_svm‖₂       : 0.05144879497711892\n",
      "‖alpha‖₁       : 0.8271846405822524\n",
      "scores min/max : -1.9518761148458348 0.399209636698304\n",
      "Mask mean value:  tensor(0.5184, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.758e-02\n",
      "‖w_svm‖₂       : 0.00015966572825213643\n",
      "‖alpha‖₁       : 0.6399999999999999\n",
      "scores min/max : -0.00015173126405812994 -0.00014200421583248832\n",
      "Mask mean value:  tensor(0.4993, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7633  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.483e-17\n",
      "‖w_svm‖₂       : 0.005896354266491703\n",
      "‖alpha‖₁       : 0.5599999999999656\n",
      "scores min/max : 0.006431023980201943 0.008248681109223473\n",
      "Mask mean value:  tensor(0.5337, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1190  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.912e-05\n",
      "‖w_svm‖₂       : 0.03885801190519727\n",
      "‖alpha‖₁       : 0.7294725435775569\n",
      "scores min/max : -0.2692566122054854 2.012823172133783\n",
      "Mask mean value:  tensor(0.6526, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.9645  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 2.062e-03\n",
      "‖w_svm‖₂       : 3.0417748016482073e-07\n",
      "‖alpha‖₁       : 0.239999999989439\n",
      "scores min/max : 2.2924280985210586e-07 2.520899875069796e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.328e-20\n",
      "‖w_svm‖₂       : 0.0002991605672626706\n",
      "‖alpha‖₁       : 0.41999999999677756\n",
      "scores min/max : 0.00011805787614192656 0.00042125317256911694\n",
      "Mask mean value:  tensor(0.5020, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9994  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.141e-15\n",
      "‖w_svm‖₂       : 0.013737840565879782\n",
      "‖alpha‖₁       : 0.86\n",
      "scores min/max : -0.051110364302236644 -0.009625794324328326\n",
      "Mask mean value:  tensor(0.3351, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4179  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.588e-05\n",
      "‖w_svm‖₂       : 2.2673614461705796e-07\n",
      "‖alpha‖₁       : 0.2599999999999987\n",
      "scores min/max : 2.1298655493689133e-07 2.257326838682445e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.402e-17\n",
      "‖w_svm‖₂       : 0.13827515281738403\n",
      "‖alpha‖₁       : 0.879999999999973\n",
      "scores min/max : -1.3373904155871585 3.6978781136591063\n",
      "Mask mean value:  tensor(0.4004, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2681  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.441e-08\n",
      "‖w_svm‖₂       : 8.534213799575648e-08\n",
      "‖alpha‖₁       : 0.37999999999999634\n",
      "scores min/max : 8.769869660202804e-09 2.6941757680027507e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.210e-22\n",
      "‖w_svm‖₂       : 0.020945629194837615\n",
      "‖alpha‖₁       : 0.3834945279929878\n",
      "scores min/max : -1.9556640763299118 0.24780241052368746\n",
      "Mask mean value:  tensor(0.7310, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1558  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.680e-12\n",
      "‖w_svm‖₂       : 0.021009641104034256\n",
      "‖alpha‖₁       : 0.815088249122839\n",
      "scores min/max : -11.511145977158101 2.0315741245327197\n",
      "Mask mean value:  tensor(0.7096, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9170  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.306e-06\n",
      "‖w_svm‖₂       : 4.9635779322037765e-08\n",
      "‖alpha‖₁       : 0.17999999999999733\n",
      "scores min/max : 4.8601621701099867e-08 6.329556691947475e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.099e-08\n",
      "‖w_svm‖₂       : 0.04653942605915899\n",
      "‖alpha‖₁       : 0.9409598935953083\n",
      "scores min/max : -1.8309488482767264 0.3029048337754945\n",
      "Mask mean value:  tensor(0.4140, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3244  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.246e-14\n",
      "‖w_svm‖₂       : 0.019151934901327525\n",
      "‖alpha‖₁       : 0.6599999999999993\n",
      "scores min/max : -0.03641717558476615 0.020393695259306573\n",
      "Mask mean value:  tensor(0.4260, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.3372  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.029e-15\n",
      "‖w_svm‖₂       : 2.7909622954891995e-07\n",
      "‖alpha‖₁       : 0.6599999999999338\n",
      "scores min/max : -6.55593131786409e-07 -5.036068620448276e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.524e-18\n",
      "‖w_svm‖₂       : 1.4558569555356966e-05\n",
      "‖alpha‖₁       : 0.35999999999592197\n",
      "scores min/max : 8.938049008382755e-06 9.812837949136675e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.744e-17\n",
      "‖w_svm‖₂       : 0.08518671120145839\n",
      "‖alpha‖₁       : 0.5780765779007748\n",
      "scores min/max : -2.122460547252728 5.790727725069846\n",
      "Mask mean value:  tensor(0.2686, dtype=torch.float64)\n",
      "max feasible return = 0.1085  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.9210978743290224e-07\n",
      "‖alpha‖₁       : 0.5799999999999933\n",
      "scores min/max : -3.048886977497127e-07 -2.1400556478403746e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0735088697119915e-07\n",
      "‖alpha‖₁       : 0.29999999999997096\n",
      "scores min/max : 4.1653653710748405e-08 4.6781632581575055e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 6.460264665791881e-08\n",
      "‖alpha‖₁       : 0.599999999999999\n",
      "scores min/max : 6.9089021990175785e-09 4.366239302344098e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.435160974290259e-08\n",
      "‖alpha‖₁       : 0.3799999999999989\n",
      "scores min/max : -2.1187248397448945e-08 -1.1805910849924596e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.8407584175826628e-06\n",
      "‖alpha‖₁       : 0.3199999999808635\n",
      "scores min/max : 2.706259771097399e-06 4.470501409150143e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4258  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04555774221155255\n",
      "‖alpha‖₁       : 0.7396856537130478\n",
      "scores min/max : -3.4490063366184662 2.1829568130782806\n",
      "Mask mean value:  tensor(0.9038, dtype=torch.float64)\n",
      "max feasible return = -0.9441  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.02556991973532104\n",
      "‖alpha‖₁       : 0.6151630094316805\n",
      "scores min/max : -1.9487220863311598 1.1793023181150821\n",
      "Mask mean value:  tensor(0.7055, dtype=torch.float64)\n",
      "max feasible return = 2.4598  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.6891855246692795e-07\n",
      "‖alpha‖₁       : 0.4599999999999935\n",
      "scores min/max : 1.6573698842325064e-07 7.60278766241758e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.4046136266203893e-07\n",
      "‖alpha‖₁       : 0.5199999999999757\n",
      "scores min/max : 8.321491809018997e-08 9.61380187977613e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.586484342454343e-07\n",
      "‖alpha‖₁       : 0.5799999999999985\n",
      "scores min/max : -2.3553238385339306e-07 -1.9938381230644613e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.04316758240101414\n",
      "‖alpha‖₁       : 0.687884180075834\n",
      "scores min/max : -0.44560197405145174 1.9441397682899988\n",
      "Mask mean value:  tensor(0.3518, dtype=torch.float64)\n",
      "max feasible return = 0.0769  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03304116228440592\n",
      "‖alpha‖₁       : 0.4223278052058377\n",
      "scores min/max : -3.857470414253652 5.298745188708649\n",
      "Mask mean value:  tensor(0.1122, dtype=torch.float64)\n",
      "max feasible return = 0.7150  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04337614656410227\n",
      "‖alpha‖₁       : 0.8211648504571633\n",
      "scores min/max : -4.9333066366876075 3.2367920234058754\n",
      "Mask mean value:  tensor(0.9665, dtype=torch.float64)\n",
      "max feasible return = -3.7001  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.00634857894240629\n",
      "‖alpha‖₁       : 0.7999999999999999\n",
      "scores min/max : -0.028825993050424853 -0.001888179535008836\n",
      "Mask mean value:  tensor(0.4567, dtype=torch.float64)\n",
      "max feasible return = -0.1661  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.09686876518132e-07\n",
      "‖alpha‖₁       : 0.6199999999999728\n",
      "scores min/max : 8.807469278086077e-08 1.0457924637452241e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.882251574926125e-08\n",
      "‖alpha‖₁       : 0.43999999999999995\n",
      "scores min/max : 4.746719324156117e-09 1.4317009802995833e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.3305532382617968e-07\n",
      "‖alpha‖₁       : 0.27999999999998304\n",
      "scores min/max : -2.196501640094199e-07 -2.0175381514707794e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  49 | train 0.005308 | val 0.006623\n",
      "-----------------------------------------Epoch:  50 ----------------------------------------\n",
      "‖w_svm‖₂       : 0.00036569138941533545\n",
      "‖alpha‖₁       : 0.739999999999999\n",
      "scores min/max : -0.0004497112366544123 -0.00043265432096117743\n",
      "Mask mean value:  tensor(0.4978, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0596  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.534e-17\n",
      "‖w_svm‖₂       : 5.238623281052688e-08\n",
      "‖alpha‖₁       : 0.11999999999999711\n",
      "scores min/max : -9.178053701832628e-08 -6.108856875926251e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.283e-07\n",
      "‖w_svm‖₂       : 4.3494411625735895e-07\n",
      "‖alpha‖₁       : 0.7199999999999998\n",
      "scores min/max : 1.7299701354602215e-07 2.3016954614311924e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.417e-21\n",
      "‖w_svm‖₂       : 1.0655308187679084e-06\n",
      "‖alpha‖₁       : 0.4999999999999919\n",
      "scores min/max : -2.208902323917087e-07 -2.7837911701065304e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.132e-19\n",
      "‖w_svm‖₂       : 1.157759970372899e-07\n",
      "‖alpha‖₁       : 0.2999999999999943\n",
      "scores min/max : 1.6241463150423146e-07 1.7140029033956344e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.539e-07\n",
      "‖w_svm‖₂       : 0.00016000272768236986\n",
      "‖alpha‖₁       : 0.6399999999999999\n",
      "scores min/max : -0.000149983555822459 -0.00014021545637493238\n",
      "Mask mean value:  tensor(0.4993, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7633  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.489e-17\n",
      "‖w_svm‖₂       : 0.0009405128853712233\n",
      "‖alpha‖₁       : 0.8199999999999991\n",
      "scores min/max : 0.0006418374134469673 0.0024236951012690704\n",
      "Mask mean value:  tensor(0.5088, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5849  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.151e-16\n",
      "‖w_svm‖₂       : 0.017318744202719845\n",
      "‖alpha‖₁       : 0.6815986571324772\n",
      "scores min/max : -1.9685814772113344 0.05878342993694856\n",
      "Mask mean value:  tensor(0.6285, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2703  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.281e-12\n",
      "‖w_svm‖₂       : 1.4518105638193321e-05\n",
      "‖alpha‖₁       : 0.35999999999607213\n",
      "scores min/max : 8.831873035852281e-06 9.704132010179115e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.725e-17\n",
      "‖w_svm‖₂       : 7.287555284983512e-08\n",
      "‖alpha‖₁       : 0.13999999999999843\n",
      "scores min/max : 1.5095197263361354e-08 2.0822323597888476e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.956e-08\n",
      "‖w_svm‖₂       : 3.775923578674553e-07\n",
      "‖alpha‖₁       : 0.27999999999999936\n",
      "scores min/max : -1.0082989612060311e-06 -9.466575622187801e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.239e-19\n",
      "‖w_svm‖₂       : 0.020966559751568678\n",
      "‖alpha‖₁       : 0.38349433102059916\n",
      "scores min/max : -1.9560286702050127 0.24739529065006516\n",
      "Mask mean value:  tensor(0.7296, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1556  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.105e-14\n",
      "‖w_svm‖₂       : 0.07158745060417057\n",
      "‖alpha‖₁       : 0.6586649870521475\n",
      "scores min/max : -1.9936524542200837 4.3697734122387\n",
      "Mask mean value:  tensor(0.4296, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.7165  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.725e-04\n",
      "‖w_svm‖₂       : 0.02070277488135338\n",
      "‖alpha‖₁       : 0.8233845185606421\n",
      "scores min/max : -1.911198861702151 1.485063008606433\n",
      "Mask mean value:  tensor(0.6660, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.3053  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.079e-05\n",
      "‖w_svm‖₂       : 0.04541269413119911\n",
      "‖alpha‖₁       : 0.9388833731399954\n",
      "scores min/max : -2.4681660752578853 1.5817558286100635\n",
      "Mask mean value:  tensor(0.1428, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3961  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.225e-03\n",
      "‖w_svm‖₂       : 0.13782739398248448\n",
      "‖alpha‖₁       : 0.6549661104502674\n",
      "scores min/max : -18.049575552665058 1.8485647475603117\n",
      "Mask mean value:  tensor(0.2869, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2792  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.388e-03\n",
      "‖w_svm‖₂       : 0.02636638677545905\n",
      "‖alpha‖₁       : 0.19670393399065528\n",
      "scores min/max : -2.215012597634196 0.006215907034018579\n",
      "Mask mean value:  tensor(0.0487, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3312  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.017e-03\n",
      "‖w_svm‖₂       : 1.9528269017128767e-07\n",
      "‖alpha‖₁       : 0.4199999999999681\n",
      "scores min/max : -4.5765949183789345e-07 -4.355446022925013e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.060e-19\n",
      "‖w_svm‖₂       : 0.05399440196931154\n",
      "‖alpha‖₁       : 0.5763076657345043\n",
      "scores min/max : -1.9595279370602485 0.8687377497479274\n",
      "Mask mean value:  tensor(0.7058, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9163  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.012e-03\n",
      "‖w_svm‖₂       : 3.5458444092226863e-06\n",
      "‖alpha‖₁       : 0.4199999999925738\n",
      "scores min/max : -3.8550489970518885e-06 -2.2298459955562214e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1229  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.695e-05\n",
      "‖w_svm‖₂       : 0.18269053856755785\n",
      "‖alpha‖₁       : 0.8831040809583394\n",
      "scores min/max : -3.6355245976156874 6.070090158608382\n",
      "Mask mean value:  tensor(0.0919, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2013  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.888e-03\n",
      "‖w_svm‖₂       : 7.75231220719377e-08\n",
      "‖alpha‖₁       : 0.17999999999999888\n",
      "scores min/max : -1.9479305121060748e-07 3.909255649358915e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.463e-21\n",
      "‖w_svm‖₂       : 0.007047797212151215\n",
      "‖alpha‖₁       : 0.7005807149984105\n",
      "scores min/max : -2.0009657797847176 0.022457237307976795\n",
      "Mask mean value:  tensor(0.4697, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2627  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.575e-13\n",
      "‖w_svm‖₂       : 0.02052802016056174\n",
      "‖alpha‖₁       : 0.7799999999999988\n",
      "scores min/max : -0.04673542528087917 0.017753491741516136\n",
      "Mask mean value:  tensor(0.3048, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.3525  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.234e-14\n",
      "‖w_svm‖₂       : 1.9824555926889997e-08\n",
      "‖alpha‖₁       : 0.119999999999993\n",
      "scores min/max : -4.421002907328561e-08 -2.946340749799901e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.824e-09\n",
      "‖w_svm‖₂       : 0.08189685168423522\n",
      "‖alpha‖₁       : 0.4719738469740867\n",
      "scores min/max : -2.2081446387990455 2.873384599607709\n",
      "Mask mean value:  tensor(0.1695, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.7869  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.564e-02\n",
      "‖w_svm‖₂       : 0.005887955957878896\n",
      "‖alpha‖₁       : 0.5599999999999787\n",
      "scores min/max : 0.006331760579866127 0.008143909785156323\n",
      "Mask mean value:  tensor(0.5332, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1179  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.939e-05\n",
      "‖w_svm‖₂       : 0.03891650415009052\n",
      "‖alpha‖₁       : 0.7294734339636733\n",
      "scores min/max : -0.2692759073941209 2.012748612925357\n",
      "Mask mean value:  tensor(0.6525, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.9640  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 2.064e-03\n",
      "‖w_svm‖₂       : 2.3340392898465007e-07\n",
      "‖alpha‖₁       : 0.6399999999999954\n",
      "scores min/max : 4.103269267111271e-07 4.31246394237786e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.079e-19\n",
      "‖w_svm‖₂       : 0.00453755951043675\n",
      "‖alpha‖₁       : 0.45999999999999924\n",
      "scores min/max : -0.010450449856555804 -0.008243079269149444\n",
      "Mask mean value:  tensor(0.4563, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9829  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.160e-15\n",
      "‖w_svm‖₂       : 1.1071786300611082e-07\n",
      "‖alpha‖₁       : 0.519999999999999\n",
      "scores min/max : 2.8072183492156816e-07 3.0217460029104854e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.109e-19\n",
      "‖w_svm‖₂       : 0.00018433424812305552\n",
      "‖alpha‖₁       : 0.8199999999999996\n",
      "scores min/max : -0.00010031812681114748 -8.698121092742513e-05\n",
      "Mask mean value:  tensor(0.4995, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6963  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.449e-17\n",
      "‖w_svm‖₂       : 0.046328879306825804\n",
      "‖alpha‖₁       : 0.9056512292173784\n",
      "scores min/max : -0.7138050878466761 1.8812543315780417\n",
      "Mask mean value:  tensor(0.3346, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6243  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.850e-03\n",
      "‖w_svm‖₂       : 1.0087460787033637e-06\n",
      "‖alpha‖₁       : 0.5999999999999939\n",
      "scores min/max : -2.0763379415105775e-07 -1.567633010283603e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.771e-19\n",
      "‖w_svm‖₂       : 3.293469298018588e-07\n",
      "‖alpha‖₁       : 0.23999999998990848\n",
      "scores min/max : 2.4924396736196134e-07 2.719715077564747e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.438e-20\n",
      "‖w_svm‖₂       : 0.013716811025484511\n",
      "‖alpha‖₁       : 0.8599999999999929\n",
      "scores min/max : -0.05099577656939102 -0.009537715731801143\n",
      "Mask mean value:  tensor(0.3356, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4186  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.542e-05\n",
      "‖w_svm‖₂       : 0.015955438308410796\n",
      "‖alpha‖₁       : 0.5963160391595037\n",
      "scores min/max : -2.750642514466657 2.2770699686260967\n",
      "Mask mean value:  tensor(0.9179, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.5081  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.316e-09\n",
      "‖w_svm‖₂       : 6.729871242373233e-07\n",
      "‖alpha‖₁       : 0.3999999999999998\n",
      "scores min/max : 5.444263808738854e-08 3.6043933897620986e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.391e-09\n",
      "‖w_svm‖₂       : 8.535700936431219e-08\n",
      "‖alpha‖₁       : 0.37999999999999656\n",
      "scores min/max : 1.084630017710776e-08 2.901622533095018e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.202e-22\n",
      "‖w_svm‖₂       : 5.190294803719195e-08\n",
      "‖alpha‖₁       : 0.4399999999999803\n",
      "scores min/max : -1.9959125906411586e-07 -9.568398456094801e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.756e-20\n",
      "‖w_svm‖₂       : 0.005753257170585671\n",
      "‖alpha‖₁       : 0.37999999999999934\n",
      "scores min/max : -0.0029955159652996985 0.007237697151162764\n",
      "Mask mean value:  tensor(0.5291, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9398  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.976e-14\n",
      "‖w_svm‖₂       : 0.08888525040280758\n",
      "‖alpha‖₁       : 0.8733816337884762\n",
      "scores min/max : -12.137790861080859 2.1776797521932867\n",
      "Mask mean value:  tensor(0.5885, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7419  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.893e-02\n",
      "‖w_svm‖₂       : 0.03698715462205859\n",
      "‖alpha‖₁       : 0.6599999999999822\n",
      "scores min/max : -0.1862143574066677 0.12297502896249324\n",
      "Mask mean value:  tensor(0.4538, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8599  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.036e-13\n",
      "‖w_svm‖₂       : 1.5052759689870002e-07\n",
      "‖alpha‖₁       : 0.37999999999902345\n",
      "scores min/max : -1.1777162194370866e-07 -1.0169919365233009e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.783e-07\n",
      "‖w_svm‖₂       : 0.021037668010180668\n",
      "‖alpha‖₁       : 0.8150873133297004\n",
      "scores min/max : -11.511559332004445 2.028933367670931\n",
      "Mask mean value:  tensor(0.7045, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9047  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.578e-06\n",
      "‖w_svm‖₂       : 0.018931704895648105\n",
      "‖alpha‖₁       : 0.6599999999999997\n",
      "scores min/max : -0.0361699765015302 0.019421877539705992\n",
      "Mask mean value:  tensor(0.4248, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.3271  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.011e-15\n",
      "‖w_svm‖₂       : 0.0005289226944900522\n",
      "‖alpha‖₁       : 0.4399999999999996\n",
      "scores min/max : -0.000951765246753034 -0.0006585101314081293\n",
      "Mask mean value:  tensor(0.4955, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6404  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.385e-16\n",
      "‖w_svm‖₂       : 2.7950573667858227e-07\n",
      "‖alpha‖₁       : 0.6599999999999997\n",
      "scores min/max : -6.595477527511731e-07 -5.079112167971709e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.515e-18\n",
      "‖w_svm‖₂       : 0.04672724078842247\n",
      "‖alpha‖₁       : 0.9409842435242455\n",
      "scores min/max : -1.827643082924185 0.30622849623054715\n",
      "Mask mean value:  tensor(0.4271, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3249  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.294e-11\n",
      "‖w_svm‖₂       : 0.07304015045244455\n",
      "‖alpha‖₁       : 0.5799999999999778\n",
      "scores min/max : -2.8433615134044623 1.6924400143271816\n",
      "Mask mean value:  tensor(0.2724, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4395  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.030e-04\n",
      "‖w_svm‖₂       : 1.6603299778878633e-07\n",
      "‖alpha‖₁       : 0.2399999999999994\n",
      "scores min/max : 2.2580877806075553e-07 2.4096859364610604e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.872e-20\n",
      "‖w_svm‖₂       : 7.396109662719644e-08\n",
      "‖alpha‖₁       : 0.659999999999999\n",
      "scores min/max : 9.046406469551171e-08 2.3556399175212979e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.304e-09\n",
      "‖w_svm‖₂       : 0.02886721071236261\n",
      "‖alpha‖₁       : 0.8985789563748583\n",
      "scores min/max : -0.7300704185362522 2.0299059976331586\n",
      "Mask mean value:  tensor(0.8686, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3063  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.725e-15\n",
      "‖w_svm‖₂       : 7.474739672746036e-08\n",
      "‖alpha‖₁       : 0.41999999999999993\n",
      "scores min/max : -6.893958503068869e-08 -2.1825082964081603e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.887e-08\n",
      "‖w_svm‖₂       : 7.505818177045285e-08\n",
      "‖alpha‖₁       : 0.5399999999999998\n",
      "scores min/max : -1.9549686638265135e-07 -1.739769684026436e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.106e-20\n",
      "‖w_svm‖₂       : 2.4333114897739124e-07\n",
      "‖alpha‖₁       : 0.5799999999999812\n",
      "scores min/max : -2.551932372553267e-07 -2.395268099148003e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.197e-19\n",
      "‖w_svm‖₂       : 2.275383300340036e-07\n",
      "‖alpha‖₁       : 0.2599999999999959\n",
      "scores min/max : 2.549521682204189e-07 2.6772553292207024e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.649e-17\n",
      "‖w_svm‖₂       : 0.04072586487480202\n",
      "‖alpha‖₁       : 0.8900027272111564\n",
      "scores min/max : -2.0141823643541557 0.33358482713523047\n",
      "Mask mean value:  tensor(0.2487, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0535  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.991e-05\n",
      "‖w_svm‖₂       : 1.9892476759334617e-07\n",
      "‖alpha‖₁       : 0.3799999999999942\n",
      "scores min/max : -2.7474931168959793e-09 3.29752345932138e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.782e-19\n",
      "‖w_svm‖₂       : 0.13933286419838933\n",
      "‖alpha‖₁       : 0.8799999999999999\n",
      "scores min/max : -1.3598407346418813 3.7518280299383404\n",
      "Mask mean value:  tensor(0.3948, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2692  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.264e-08\n",
      "‖w_svm‖₂       : 5.80927284900931e-08\n",
      "‖alpha‖₁       : 0.23999999999999855\n",
      "scores min/max : -5.555007456277995e-08 -4.1196587583762804e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.693e-20\n",
      "‖w_svm‖₂       : 4.944107893144061e-08\n",
      "‖alpha‖₁       : 0.17999999999999566\n",
      "scores min/max : 5.892997380956293e-08 7.366826173133438e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.204e-08\n",
      "‖w_svm‖₂       : 0.0002998465620321546\n",
      "‖alpha‖₁       : 0.4199999999994166\n",
      "scores min/max : 8.816456616839315e-05 0.0003927862550579936\n",
      "Mask mean value:  tensor(0.5018, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9991  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.109e-15\n",
      "‖w_svm‖₂       : 0.009402763415289084\n",
      "‖alpha‖₁       : 0.6077103324807986\n",
      "scores min/max : -1.994597744056076 0.29213194063707915\n",
      "Mask mean value:  tensor(0.5369, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2167  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.928e-15\n",
      "‖w_svm‖₂       : 0.060054052597291366\n",
      "‖alpha‖₁       : 0.8986393326539104\n",
      "scores min/max : -1.694721730808774 3.7182299698003045\n",
      "Mask mean value:  tensor(0.4409, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5891  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.876e-03\n",
      "‖w_svm‖₂       : 0.051267588143964676\n",
      "‖alpha‖₁       : 0.8271846609006486\n",
      "scores min/max : -1.9524006722218648 0.39899365502498135\n",
      "Mask mean value:  tensor(0.5163, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0125  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.725e-02\n",
      "‖w_svm‖₂       : 1.1562721630664804e-06\n",
      "‖alpha‖₁       : 0.3199999999989168\n",
      "scores min/max : -2.6074083402024858e-06 -2.493847365550496e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4897  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.268e-18\n",
      "‖w_svm‖₂       : 0.03423774523389048\n",
      "‖alpha‖₁       : 0.9199999999908416\n",
      "scores min/max : -0.26060860137398084 0.1531530631261595\n",
      "Mask mean value:  tensor(0.1489, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.7545  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 2.219e-02\n",
      "‖w_svm‖₂       : 0.14511005567562268\n",
      "‖alpha‖₁       : 0.7590823450061015\n",
      "scores min/max : -1.774834164040327 2.202582522733384\n",
      "Mask mean value:  tensor(0.8282, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3578  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.343e-11\n",
      "‖w_svm‖₂       : 0.0001394572089691589\n",
      "‖alpha‖₁       : 0.43999999999048267\n",
      "scores min/max : -0.00029184020350853297 -0.00014978516067229507\n",
      "Mask mean value:  tensor(0.4990, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0208  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.025e-17\n",
      "‖w_svm‖₂       : 0.03110884512416737\n",
      "‖alpha‖₁       : 0.8525114269724771\n",
      "scores min/max : -2.9119570604117833 1.5685506510136789\n",
      "Mask mean value:  tensor(0.1570, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9527  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.709e-03\n",
      "‖w_svm‖₂       : 0.015757555044249208\n",
      "‖alpha‖₁       : 0.8599999999999322\n",
      "scores min/max : -0.061859568516648616 -0.05020022769899259\n",
      "Mask mean value:  tensor(0.2568, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0656  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.830e-14\n",
      "‖w_svm‖₂       : 0.00020974629314242024\n",
      "‖alpha‖₁       : 0.6199999999993959\n",
      "scores min/max : -7.3971572346505626e-06 8.191507501980758e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.008e-17\n",
      "‖w_svm‖₂       : 0.003387584348808917\n",
      "‖alpha‖₁       : 0.5799999999999994\n",
      "scores min/max : 0.00522905442036364 0.006672561553044926\n",
      "Mask mean value:  tensor(0.5318, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3382  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.027e-13\n",
      "‖w_svm‖₂       : 0.06849275813238834\n",
      "‖alpha‖₁       : 0.4192051619445013\n",
      "scores min/max : -1.932727999831927 2.0861923952279344\n",
      "Mask mean value:  tensor(0.7413, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8935  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.071e-04\n",
      "‖w_svm‖₂       : 0.03066045057445596\n",
      "‖alpha‖₁       : 0.551256166641969\n",
      "scores min/max : -3.4417898095258286 1.1268119517489708\n",
      "Mask mean value:  tensor(0.1713, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.2994  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.062e-03\n",
      "‖w_svm‖₂       : 0.08494096564755048\n",
      "‖alpha‖₁       : 0.5780073286744158\n",
      "scores min/max : -2.1164354502759166 5.798918045412824\n",
      "Mask mean value:  tensor(0.2799, dtype=torch.float64)\n",
      "max feasible return = 0.1106  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.9874607308350203e-07\n",
      "‖alpha‖₁       : 0.5799999999999914\n",
      "scores min/max : -3.1328442961279656e-07 -2.2238198670061765e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0883037482803462e-07\n",
      "‖alpha‖₁       : 0.29999999999997035\n",
      "scores min/max : 3.8076355565167766e-08 4.321549015709139e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 6.581994674909532e-08\n",
      "‖alpha‖₁       : 0.5999999999999988\n",
      "scores min/max : 4.573878799068151e-09 4.1331839276536075e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.5131524964941254e-08\n",
      "‖alpha‖₁       : 0.37999999999999906\n",
      "scores min/max : -2.350526266973656e-08 -1.412391128594916e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 6.096627275713448e-06\n",
      "‖alpha‖₁       : 0.31999999992758377\n",
      "scores min/max : 4.770201136906171e-06 8.930133779692269e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4259  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04507298307406332\n",
      "‖alpha‖₁       : 0.7397055288274932\n",
      "scores min/max : -3.4523527794980815 2.1792314189924946\n",
      "Mask mean value:  tensor(0.9017, dtype=torch.float64)\n",
      "max feasible return = -0.9430  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.025164074961032352\n",
      "‖alpha‖₁       : 0.6151682211906389\n",
      "scores min/max : -1.9441604243533614 1.1836401325261579\n",
      "Mask mean value:  tensor(0.7196, dtype=torch.float64)\n",
      "max feasible return = 2.5079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.747941557488012e-07\n",
      "‖alpha‖₁       : 0.4599999999999881\n",
      "scores min/max : 8.90215326203739e-08 6.83693062089428e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.412192333266266e-07\n",
      "‖alpha‖₁       : 0.5199999999999886\n",
      "scores min/max : 9.347474549842109e-08 1.0638829629447408e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.6253771630253073e-07\n",
      "‖alpha‖₁       : 0.5799999999999654\n",
      "scores min/max : -2.2175789404166352e-07 -1.853561597677469e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.042809020504440166\n",
      "‖alpha‖₁       : 0.6878561294316089\n",
      "scores min/max : -0.43813268544789247 1.951646468171275\n",
      "Mask mean value:  tensor(0.3772, dtype=torch.float64)\n",
      "max feasible return = 0.0520  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.0327629189364442\n",
      "‖alpha‖₁       : 0.4223312916745729\n",
      "scores min/max : -3.861321502473918 5.296825193478852\n",
      "Mask mean value:  tensor(0.1103, dtype=torch.float64)\n",
      "max feasible return = 0.7023  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.043109347905657375\n",
      "‖alpha‖₁       : 0.8211661654331892\n",
      "scores min/max : -4.933714720079619 3.2368824489415426\n",
      "Mask mean value:  tensor(0.9665, dtype=torch.float64)\n",
      "max feasible return = -3.7001  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.006244683459857593\n",
      "‖alpha‖₁       : 0.7999999999999998\n",
      "scores min/max : -0.027133251941509035 -0.000839937099079419\n",
      "Mask mean value:  tensor(0.4628, dtype=torch.float64)\n",
      "max feasible return = -0.1683  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.1394526570601964e-07\n",
      "‖alpha‖₁       : 0.6199999999999731\n",
      "scores min/max : 6.458359539308859e-08 8.108600756279909e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.86494292713539e-08\n",
      "‖alpha‖₁       : 0.439999999999914\n",
      "scores min/max : 4.167933520371286e-09 1.376262006115978e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.3496520621002613e-07\n",
      "‖alpha‖₁       : 0.27999999999998476\n",
      "scores min/max : -1.8950239795962698e-07 -1.7162342869204968e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  50 | train 0.005308 | val 0.006611\n",
      "-----------------------------------------Epoch:  51 ----------------------------------------\n",
      "‖w_svm‖₂       : 0.05147325953853736\n",
      "‖alpha‖₁       : 0.8272182197934785\n",
      "scores min/max : -1.94456235019746 0.4071856712712622\n",
      "Mask mean value:  tensor(0.5440, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0179  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.550e-02\n",
      "‖w_svm‖₂       : 0.0472869434101358\n",
      "‖alpha‖₁       : 0.9410416936122059\n",
      "scores min/max : -1.8200122675328796 0.3138594796497155\n",
      "Mask mean value:  tensor(0.4576, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3252  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.893e-15\n",
      "‖w_svm‖₂       : 0.007019418497002871\n",
      "‖alpha‖₁       : 0.7005801266249878\n",
      "scores min/max : -2.002990855875583 0.020438456043493446\n",
      "Mask mean value:  tensor(0.4598, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2574  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.494e-07\n",
      "‖w_svm‖₂       : 0.0033923259736450637\n",
      "‖alpha‖₁       : 0.5799999999999995\n",
      "scores min/max : 0.005526261853383047 0.006974330147063834\n",
      "Mask mean value:  tensor(0.5333, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3391  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.925e-13\n",
      "‖w_svm‖₂       : 0.020513316803667435\n",
      "‖alpha‖₁       : 0.8233891290018206\n",
      "scores min/max : -1.9111150243547752 1.484591464036167\n",
      "Mask mean value:  tensor(0.6658, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.3037  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.092e-05\n",
      "‖w_svm‖₂       : 0.18248519434959964\n",
      "‖alpha‖₁       : 0.8832380556724061\n",
      "scores min/max : -3.644387898134513 6.072155323552316\n",
      "Mask mean value:  tensor(0.0919, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2038  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.473e-05\n",
      "‖w_svm‖₂       : 7.678028104065368e-08\n",
      "‖alpha‖₁       : 0.6599999999999745\n",
      "scores min/max : 8.27993574488036e-08 2.2790892885773538e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.409e-09\n",
      "‖w_svm‖₂       : 5.3595296300147646e-08\n",
      "‖alpha‖₁       : 0.119999999999989\n",
      "scores min/max : -9.197008352020262e-08 -6.113947825010913e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.975e-08\n",
      "‖w_svm‖₂       : 0.00035881800017027087\n",
      "‖alpha‖₁       : 0.7399999999999994\n",
      "scores min/max : -0.00046525331396463456 -0.00044883146164551024\n",
      "Mask mean value:  tensor(0.4977, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0595  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.515e-17\n",
      "‖w_svm‖₂       : 0.021265931422814393\n",
      "‖alpha‖₁       : 0.38351172457937055\n",
      "scores min/max : -1.9621868764450388 0.2413704062211618\n",
      "Mask mean value:  tensor(0.7075, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1525  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.487e-14\n",
      "‖w_svm‖₂       : 1.1332723083478151e-07\n",
      "‖alpha‖₁       : 0.5199999999999726\n",
      "scores min/max : 2.4564917932070533e-07 2.670624951810336e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.117e-19\n",
      "‖w_svm‖₂       : 2.0132105886817868e-07\n",
      "‖alpha‖₁       : 0.3799999999999506\n",
      "scores min/max : 1.89462894297743e-09 3.781366931089709e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.836e-19\n",
      "‖w_svm‖₂       : 0.13794338278698656\n",
      "‖alpha‖₁       : 0.6551374496249398\n",
      "scores min/max : -18.07796046853009 1.8386997137379177\n",
      "Mask mean value:  tensor(0.2757, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2792  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.255e-03\n",
      "‖w_svm‖₂       : 0.08826820740991861\n",
      "‖alpha‖₁       : 0.8733548241245948\n",
      "scores min/max : -12.177755877222088 2.1363574757659243\n",
      "Mask mean value:  tensor(0.4875, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.4523  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.859e-02\n",
      "‖w_svm‖₂       : 4.989831431269666e-08\n",
      "‖alpha‖₁       : 0.17999999999999525\n",
      "scores min/max : 5.5438987264138334e-08 7.018686658870432e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.249e-08\n",
      "‖w_svm‖₂       : 0.0337806098014854\n",
      "‖alpha‖₁       : 0.9199999999965328\n",
      "scores min/max : -0.244235004753818 0.17735104120045964\n",
      "Mask mean value:  tensor(0.1727, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8701  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 7.478e-03\n",
      "‖w_svm‖₂       : 0.0722788051565946\n",
      "‖alpha‖₁       : 0.5799999999999635\n",
      "scores min/max : -2.788273587801279 1.6515768091108844\n",
      "Mask mean value:  tensor(0.2612, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4260  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.087e-04\n",
      "‖w_svm‖₂       : 0.01681989055354574\n",
      "‖alpha‖₁       : 0.6815996205366023\n",
      "scores min/max : -1.9709063423060094 0.055660700461967694\n",
      "Mask mean value:  tensor(0.6180, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2659  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.499e-12\n",
      "‖w_svm‖₂       : 0.07084219441574038\n",
      "‖alpha‖₁       : 0.6587618197481547\n",
      "scores min/max : -1.9912502591729409 4.419046939459621\n",
      "Mask mean value:  tensor(0.4381, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.7819  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.699e-04\n",
      "‖w_svm‖₂       : 0.06085688197402204\n",
      "‖alpha‖₁       : 0.8987580603366889\n",
      "scores min/max : -1.6908698276092777 3.7233757992202605\n",
      "Mask mean value:  tensor(0.4501, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5966  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.893e-03\n",
      "‖w_svm‖₂       : 2.3099260319666474e-07\n",
      "‖alpha‖₁       : 0.25999999999999635\n",
      "scores min/max : 2.5770964595735203e-07 2.7047858960462116e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.736e-17\n",
      "‖w_svm‖₂       : 2.385063126321052e-07\n",
      "‖alpha‖₁       : 0.6399999999999904\n",
      "scores min/max : 3.962527243277365e-07 4.172345186230339e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.109e-19\n",
      "‖w_svm‖₂       : 0.0001378430311788755\n",
      "‖alpha‖₁       : 0.43999999999272055\n",
      "scores min/max : -0.00029131165557060544 -0.00015195962366715613\n",
      "Mask mean value:  tensor(0.4990, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0207  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.032e-17\n",
      "‖w_svm‖₂       : 0.00018145878031388573\n",
      "‖alpha‖₁       : 0.8199999999999998\n",
      "scores min/max : -9.580195127008317e-05 -8.288158506918573e-05\n",
      "Mask mean value:  tensor(0.4996, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6964  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.427e-17\n",
      "‖w_svm‖₂       : 7.804471032312955e-08\n",
      "‖alpha‖₁       : 0.17999999999999927\n",
      "scores min/max : -1.8998001748638476e-07 4.360418983564337e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.566e-21\n",
      "‖w_svm‖₂       : 0.005959436846643241\n",
      "‖alpha‖₁       : 0.559999999999996\n",
      "scores min/max : 0.0062216238802075335 0.008077586818227234\n",
      "Mask mean value:  tensor(0.5327, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1169  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.973e-05\n",
      "‖w_svm‖₂       : 5.877643807322693e-08\n",
      "‖alpha‖₁       : 0.23999999999999438\n",
      "scores min/max : -6.120049347522317e-08 -4.677692799250908e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.710e-20\n",
      "‖w_svm‖₂       : 0.0005356783788916004\n",
      "‖alpha‖₁       : 0.4399999999999998\n",
      "scores min/max : -0.000826412763556358 -0.0005254954299687767\n",
      "Mask mean value:  tensor(0.4961, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6425  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.412e-16\n",
      "‖w_svm‖₂       : 7.478217538007305e-08\n",
      "‖alpha‖₁       : 0.13999999999999327\n",
      "scores min/max : 1.5058721492448303e-08 2.0860402142543385e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 7.224e-08\n",
      "‖w_svm‖₂       : 1.7199089770353108e-07\n",
      "‖alpha‖₁       : 0.2399999999999751\n",
      "scores min/max : 1.9752306407293195e-07 2.1328152952140167e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.069e-20\n",
      "‖w_svm‖₂       : 0.013263522491600151\n",
      "‖alpha‖₁       : 0.8599999999999999\n",
      "scores min/max : -0.046964016121634825 -0.008130717191030045\n",
      "Mask mean value:  tensor(0.3489, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.465e-05\n",
      "‖w_svm‖₂       : 1.4035506800967804e-07\n",
      "‖alpha‖₁       : 0.3799999999996512\n",
      "scores min/max : -1.2341336284365676e-07 -1.0783548875301275e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.370e-07\n",
      "‖w_svm‖₂       : 1.958368086949253e-08\n",
      "‖alpha‖₁       : 0.11999999999999317\n",
      "scores min/max : -4.721321118136376e-08 -3.24734975521097e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.826e-09\n",
      "‖w_svm‖₂       : 0.08217045766110323\n",
      "‖alpha‖₁       : 0.47205202154924863\n",
      "scores min/max : -2.2495845993983075 2.827234365551236\n",
      "Mask mean value:  tensor(0.1364, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.4618  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.639e-02\n",
      "‖w_svm‖₂       : 2.8525980413311406e-07\n",
      "‖alpha‖₁       : 0.6599999999998968\n",
      "scores min/max : -6.357473522949909e-07 -4.835913444583226e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.562e-18\n",
      "‖w_svm‖₂       : 0.1377815048163675\n",
      "‖alpha‖₁       : 0.8799999999999998\n",
      "scores min/max : -1.3126321738664701 3.678757770418225\n",
      "Mask mean value:  tensor(0.4313, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2642  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.114e-07\n",
      "‖w_svm‖₂       : 0.0001569861346179795\n",
      "‖alpha‖₁       : 0.6399999999999998\n",
      "scores min/max : -0.00015891375595622492 -0.00014950999464592024\n",
      "Mask mean value:  tensor(0.4992, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7632  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.467e-17\n",
      "‖w_svm‖₂       : 3.847746979966443e-07\n",
      "‖alpha‖₁       : 0.27999999999999603\n",
      "scores min/max : -9.655773265105934e-07 -9.038371702992016e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.299e-19\n",
      "‖w_svm‖₂       : 0.04664191071777993\n",
      "‖alpha‖₁       : 0.9056661665098769\n",
      "scores min/max : -0.7088046202551092 1.8863047036086034\n",
      "Mask mean value:  tensor(0.3512, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6549  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.009e-03\n",
      "‖w_svm‖₂       : 0.06851541132209121\n",
      "‖alpha‖₁       : 0.4192172810080893\n",
      "scores min/max : -1.920519268269837 2.0974290599940244\n",
      "Mask mean value:  tensor(0.7542, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8986  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.489e-04\n",
      "‖w_svm‖₂       : 1.0856826120552001e-06\n",
      "‖alpha‖₁       : 0.4999999999999899\n",
      "scores min/max : -2.3437923886245795e-07 -4.134611744188103e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.382e-19\n",
      "‖w_svm‖₂       : 0.005822073182596742\n",
      "‖alpha‖₁       : 0.37999999999999984\n",
      "scores min/max : -0.0014312381869925825 0.008647364349806789\n",
      "Mask mean value:  tensor(0.5370, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9526  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.544e-04\n",
      "‖w_svm‖₂       : 1.420298726740277e-05\n",
      "‖alpha‖₁       : 0.35999999999634086\n",
      "scores min/max : 8.421975351004504e-06 9.256265795029528e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.596e-17\n",
      "‖w_svm‖₂       : 0.02085639993094625\n",
      "‖alpha‖₁       : 0.8150927428937749\n",
      "scores min/max : -11.508425085483449 2.032518355934494\n",
      "Mask mean value:  tensor(0.7118, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9235  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.352e-06\n",
      "‖w_svm‖₂       : 0.020611007857634214\n",
      "‖alpha‖₁       : 0.7799999999999995\n",
      "scores min/max : -0.049518777332512885 0.013019499434741899\n",
      "Mask mean value:  tensor(0.2924, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.2976  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.380e-14\n",
      "‖w_svm‖₂       : 4.470164455323087e-07\n",
      "‖alpha‖₁       : 0.7199999999998667\n",
      "scores min/max : 1.3677122714864775e-07 1.9409237145880592e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.524e-21\n",
      "‖w_svm‖₂       : 0.0002946631487792288\n",
      "‖alpha‖₁       : 0.4199999999987506\n",
      "scores min/max : 8.118176292303794e-05 0.00037534782223717114\n",
      "Mask mean value:  tensor(0.5017, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9990  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.099e-15\n",
      "‖w_svm‖₂       : 1.9873389466716928e-07\n",
      "‖alpha‖₁       : 0.4199999999999659\n",
      "scores min/max : -4.723897523287853e-07 -4.502330497989919e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.091e-19\n",
      "‖w_svm‖₂       : 0.0009301688346971688\n",
      "‖alpha‖₁       : 0.8199999999999985\n",
      "scores min/max : 0.00036048818133383766 0.0021326090821838445\n",
      "Mask mean value:  tensor(0.5074, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5776  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.168e-16\n",
      "‖w_svm‖₂       : 1.0275297097410638e-06\n",
      "‖alpha‖₁       : 0.5999999999999939\n",
      "scores min/max : -2.0267450269965094e-07 -1.5181258332021695e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.827e-19\n",
      "‖w_svm‖₂       : 0.004584083405093231\n",
      "‖alpha‖₁       : 0.4599999999999983\n",
      "scores min/max : -0.010650593967918145 -0.008393412270073739\n",
      "Mask mean value:  tensor(0.4555, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9793  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.176e-15\n",
      "‖w_svm‖₂       : 0.04073794321180213\n",
      "‖alpha‖₁       : 0.8900135690662979\n",
      "scores min/max : -2.0123273933083614 0.33544148595109663\n",
      "Mask mean value:  tensor(0.2538, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0555  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.815e-05\n",
      "‖w_svm‖₂       : 0.031015085052582346\n",
      "‖alpha‖₁       : 0.8525076389765726\n",
      "scores min/max : -2.889555113789287 1.591329051378301\n",
      "Mask mean value:  tensor(0.1688, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9637  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.076e-03\n",
      "‖w_svm‖₂       : 6.746608789463105e-07\n",
      "‖alpha‖₁       : 0.3999999999998981\n",
      "scores min/max : 7.013432226819833e-08 3.786612755652934e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.389e-09\n",
      "‖w_svm‖₂       : 0.015463764319229653\n",
      "‖alpha‖₁       : 0.8599999999999868\n",
      "scores min/max : -0.05451282473460079 -0.04351286252980546\n",
      "Mask mean value:  tensor(0.2839, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0744  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.061e-14\n",
      "‖w_svm‖₂       : 7.661144843940928e-08\n",
      "‖alpha‖₁       : 0.41999999999999393\n",
      "scores min/max : -6.565777101555804e-08 -1.8457632214954867e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 7.236e-08\n",
      "‖w_svm‖₂       : 7.525859209299851e-08\n",
      "‖alpha‖₁       : 0.5399999999999295\n",
      "scores min/max : -1.898510389045435e-07 -1.6811986184083973e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.070e-20\n",
      "‖w_svm‖₂       : 0.009711138720660782\n",
      "‖alpha‖₁       : 0.6077161763527833\n",
      "scores min/max : -1.9950152931813387 0.2917215816338241\n",
      "Mask mean value:  tensor(0.5350, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2159  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.062e-14\n",
      "‖w_svm‖₂       : 1.140767812971431e-06\n",
      "‖alpha‖₁       : 0.3199999999992804\n",
      "scores min/max : -2.488099023312713e-06 -2.3776177120630974e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4897  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.229e-18\n",
      "‖w_svm‖₂       : 5.308995758956728e-08\n",
      "‖alpha‖₁       : 0.4399999999999732\n",
      "scores min/max : -2.0299205442469425e-07 -9.919858370312408e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.842e-20\n",
      "‖w_svm‖₂       : 0.026054411012141426\n",
      "‖alpha‖₁       : 0.19669153144347035\n",
      "scores min/max : -2.2055724137583006 0.015527981002262915\n",
      "Mask mean value:  tensor(0.0567, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3838  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.254e-04\n",
      "‖w_svm‖₂       : 2.4682096161931087e-07\n",
      "‖alpha‖₁       : 0.5799999999999998\n",
      "scores min/max : -2.4882551060440497e-07 -2.332087638145655e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.269e-19\n",
      "‖w_svm‖₂       : 0.05336260665684735\n",
      "‖alpha‖₁       : 0.5763268089120687\n",
      "scores min/max : -1.9619884083064711 0.8659743957434717\n",
      "Mask mean value:  tensor(0.6976, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.8921  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.620e-04\n",
      "‖w_svm‖₂       : 0.14560306005665427\n",
      "‖alpha‖₁       : 0.7592201097407378\n",
      "scores min/max : -1.7703844598440392 2.2067855987525613\n",
      "Mask mean value:  tensor(0.8351, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3603  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.588e-11\n",
      "‖w_svm‖₂       : 1.1036761886966596e-07\n",
      "‖alpha‖₁       : 0.23999999999998575\n",
      "scores min/max : 8.286684379804309e-08 9.383383975314131e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.971e-21\n",
      "‖w_svm‖₂       : 1.1771808496773417e-07\n",
      "‖alpha‖₁       : 0.2999999999999952\n",
      "scores min/max : 1.7456490388287263e-07 1.8354443969153964e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.583e-07\n",
      "‖w_svm‖₂       : 0.028810179459013634\n",
      "‖alpha‖₁       : 0.8985760770703234\n",
      "scores min/max : -0.7317372584500625 2.0282119549953608\n",
      "Mask mean value:  tensor(0.8663, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3096  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.872e-15\n",
      "‖w_svm‖₂       : 0.03594317708377594\n",
      "‖alpha‖₁       : 0.6599999999999642\n",
      "scores min/max : -0.17602047518964126 0.11547820982673157\n",
      "Mask mean value:  tensor(0.4538, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8646  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.056e-13\n",
      "‖w_svm‖₂       : 4.469421061492203e-06\n",
      "‖alpha‖₁       : 0.41999999998363\n",
      "scores min/max : -4.8140287388960845e-06 -2.8887848457610516e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1229  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.196e-05\n",
      "‖w_svm‖₂       : 0.030515633281302964\n",
      "‖alpha‖₁       : 0.5512602589055429\n",
      "scores min/max : -3.4390416794910976 1.1290373419191568\n",
      "Mask mean value:  tensor(0.1728, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.029e-03\n",
      "‖w_svm‖₂       : 0.00020735665785734036\n",
      "‖alpha‖₁       : 0.6199999999995386\n",
      "scores min/max : -1.3382905567282011e-05 1.8529032888555267e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.017e-17\n",
      "‖w_svm‖₂       : 0.03766185386103343\n",
      "‖alpha‖₁       : 0.7294947454600686\n",
      "scores min/max : -0.26886181353627575 2.0142923221496947\n",
      "Mask mean value:  tensor(0.6537, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.9724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 2.048e-03\n",
      "‖w_svm‖₂       : 8.625569961858294e-08\n",
      "‖alpha‖₁       : 0.3799999999999924\n",
      "scores min/max : 1.1182782048349334e-08 2.936251107999993e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.318e-22\n",
      "‖w_svm‖₂       : 0.018600584672940065\n",
      "‖alpha‖₁       : 0.6599999999999991\n",
      "scores min/max : -0.032158469826988056 0.021692328300381536\n",
      "Mask mean value:  tensor(0.4409, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.4499  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.968e-15\n",
      "‖w_svm‖₂       : 0.015637641916643876\n",
      "‖alpha‖₁       : 0.5963092276225814\n",
      "scores min/max : -2.7355989224668686 2.2711057187397024\n",
      "Mask mean value:  tensor(0.9151, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.5050  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.124e-12\n",
      "‖w_svm‖₂       : 0.04515467962431017\n",
      "‖alpha‖₁       : 0.9388817135080463\n",
      "scores min/max : -2.4681224677835703 1.5828851293436192\n",
      "Mask mean value:  tensor(0.1434, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3979  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.177e-03\n",
      "‖w_svm‖₂       : 0.08488682361137226\n",
      "‖alpha‖₁       : 0.5780053456435624\n",
      "scores min/max : -2.1190905832146765 5.795811056964651\n",
      "Mask mean value:  tensor(0.2749, dtype=torch.float64)\n",
      "max feasible return = 0.1097  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.017662029079031e-07\n",
      "‖alpha‖₁       : 0.5799999999999981\n",
      "scores min/max : -2.951782308717292e-07 -2.0437256906154438e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0904554134125924e-07\n",
      "‖alpha‖₁       : 0.2999999999999741\n",
      "scores min/max : 3.853272077838383e-08 4.363906641879235e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 6.591370307357622e-08\n",
      "‖alpha‖₁       : 0.5999999999999994\n",
      "scores min/max : -3.3008310139332486e-10 3.641477389165596e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.534276384560284e-08\n",
      "‖alpha‖₁       : 0.3799999999999992\n",
      "scores min/max : -2.6291793755097736e-08 -1.691307583111518e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.493693313437684e-06\n",
      "‖alpha‖₁       : 0.3199999999550205\n",
      "scores min/max : 3.8086976325373233e-06 6.4548048347294385e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4259  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04428938258746045\n",
      "‖alpha‖₁       : 0.7397182913107899\n",
      "scores min/max : -3.439757892101654 2.1915657107694253\n",
      "Mask mean value:  tensor(0.9094, dtype=torch.float64)\n",
      "max feasible return = -0.9478  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.024741867536073554\n",
      "‖alpha‖₁       : 0.6151721968063598\n",
      "scores min/max : -1.9513209904039308 1.1762004699610258\n",
      "Mask mean value:  tensor(0.6991, dtype=torch.float64)\n",
      "max feasible return = 2.4362  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.797978232917312e-07\n",
      "‖alpha‖₁       : 0.4599999999999848\n",
      "scores min/max : 1.2831386489577393e-07 7.230722695426017e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.43032910607857e-07\n",
      "‖alpha‖₁       : 0.5199999999999847\n",
      "scores min/max : 9.148688367807365e-08 1.044037167066083e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.643198151769748e-07\n",
      "‖alpha‖₁       : 0.5799999999999703\n",
      "scores min/max : -2.2939191014453893e-07 -1.9307191924885383e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.04251982400179498\n",
      "‖alpha‖₁       : 0.6878523742255674\n",
      "scores min/max : -0.4406225315392888 1.949301571560681\n",
      "Mask mean value:  tensor(0.3683, dtype=torch.float64)\n",
      "max feasible return = 0.0597  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03247522307345518\n",
      "‖alpha‖₁       : 0.42234118252222086\n",
      "scores min/max : -3.8590456935783797 5.301825865391736\n",
      "Mask mean value:  tensor(0.1127, dtype=torch.float64)\n",
      "max feasible return = 0.7190  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04305169842167868\n",
      "‖alpha‖₁       : 0.8211728088020289\n",
      "scores min/max : -4.922303532589517 3.2485957368142766\n",
      "Mask mean value:  tensor(0.9671, dtype=torch.float64)\n",
      "max feasible return = -3.7030  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.0061119761240932095\n",
      "‖alpha‖₁       : 0.7999999999999997\n",
      "scores min/max : -0.02700199432996428 -0.0018863418567543443\n",
      "Mask mean value:  tensor(0.4590, dtype=torch.float64)\n",
      "max feasible return = -0.1670  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.176742464235242e-07\n",
      "‖alpha‖₁       : 0.6199999999999757\n",
      "scores min/max : 7.986435072157374e-08 9.635727756228722e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.92836191684037e-08\n",
      "‖alpha‖₁       : 0.43999999999992145\n",
      "scores min/max : 4.504774815867244e-09 1.4098321212332943e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.379295524600279e-07\n",
      "‖alpha‖₁       : 0.27999999999997993\n",
      "scores min/max : -1.9945735582150226e-07 -1.815426019016754e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  51 | train 0.005358 | val 0.006612\n",
      "-----------------------------------------Epoch:  52 ----------------------------------------\n",
      "‖w_svm‖₂       : 0.07026583024113563\n",
      "‖alpha‖₁       : 0.6587242308455447\n",
      "scores min/max : -1.9936542709621041 4.428198748365493\n",
      "Mask mean value:  tensor(0.4336, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.7500  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.758e-04\n",
      "‖w_svm‖₂       : 0.03102648582312828\n",
      "‖alpha‖₁       : 0.8525076880912583\n",
      "scores min/max : -2.8877551615332377 1.5931425417436977\n",
      "Mask mean value:  tensor(0.1698, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9645  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.061e-03\n",
      "‖w_svm‖₂       : 7.669983584402056e-08\n",
      "‖alpha‖₁       : 0.6599999999999763\n",
      "scores min/max : 8.631567282491916e-08 2.3146797999798716e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.486e-09\n",
      "‖w_svm‖₂       : 5.31804943451111e-08\n",
      "‖alpha‖₁       : 0.4399999999999738\n",
      "scores min/max : -2.041097939242914e-07 -1.0030284241078774e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.851e-20\n",
      "‖w_svm‖₂       : 1.9905034562216757e-07\n",
      "‖alpha‖₁       : 0.4199999999999659\n",
      "scores min/max : -4.852339640028597e-07 -4.630799374752442e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.096e-19\n",
      "‖w_svm‖₂       : 0.08156931599955142\n",
      "‖alpha‖₁       : 0.47195733321110545\n",
      "scores min/max : -2.2071351390156493 2.8697125531521688\n",
      "Mask mean value:  tensor(0.1689, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.7816  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.525e-02\n",
      "‖w_svm‖₂       : 1.4078958531534064e-05\n",
      "‖alpha‖₁       : 0.3599999999967697\n",
      "scores min/max : 8.58337544557961e-06 9.408019022001742e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.548e-17\n",
      "‖w_svm‖₂       : 0.06037630637284715\n",
      "‖alpha‖₁       : 0.8986951635343252\n",
      "scores min/max : -1.694490918200684 3.7198479859937104\n",
      "Mask mean value:  tensor(0.4426, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5909  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.362e-03\n",
      "‖w_svm‖₂       : 2.859728399293415e-07\n",
      "‖alpha‖₁       : 0.6599999999998973\n",
      "scores min/max : -6.564434633907543e-07 -5.04293067832671e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.569e-18\n",
      "‖w_svm‖₂       : 7.618250759259715e-08\n",
      "‖alpha‖₁       : 0.5399999999999825\n",
      "scores min/max : -1.9465440124195585e-07 -1.730545571357852e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.114e-20\n",
      "‖w_svm‖₂       : 0.04066045679501574\n",
      "‖alpha‖₁       : 0.8900068918087738\n",
      "scores min/max : -2.0117345555790216 0.336053756420868\n",
      "Mask mean value:  tensor(0.2555, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0563  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.336e-05\n",
      "‖w_svm‖₂       : 4.428002524457782e-07\n",
      "‖alpha‖₁       : 0.7199999999999996\n",
      "scores min/max : 1.5881968090536926e-07 2.1599402690675847e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.490e-21\n",
      "‖w_svm‖₂       : 6.744165505638046e-07\n",
      "‖alpha‖₁       : 0.3999999999998959\n",
      "scores min/max : 6.39397819357852e-08 3.724334062990961e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.388e-09\n",
      "‖w_svm‖₂       : 0.00035643567020718706\n",
      "‖alpha‖₁       : 0.739999999999999\n",
      "scores min/max : -0.00044308812802960884 -0.0004268835590977285\n",
      "Mask mean value:  tensor(0.4979, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0597  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.533e-17\n",
      "‖w_svm‖₂       : 0.13899292173859476\n",
      "‖alpha‖₁       : 0.879999999999854\n",
      "scores min/max : -1.3409886493106633 3.739423373508804\n",
      "Mask mean value:  tensor(0.4196, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2669  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.794e-09\n",
      "‖w_svm‖₂       : 5.381322221692977e-08\n",
      "‖alpha‖₁       : 0.11999999999999\n",
      "scores min/max : -9.467531982823481e-08 -6.385711540465062e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.315e-07\n",
      "‖w_svm‖₂       : 3.8588988230708154e-07\n",
      "‖alpha‖₁       : 0.2799999999999936\n",
      "scores min/max : -1.0912135066223154e-06 -1.0294280246258356e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.309e-19\n",
      "‖w_svm‖₂       : 0.015629690861556113\n",
      "‖alpha‖₁       : 0.5963098034984672\n",
      "scores min/max : -2.7342497430444492 2.271333844772256\n",
      "Mask mean value:  tensor(0.9152, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.5051  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.025e-12\n",
      "‖w_svm‖₂       : 0.005823862770516712\n",
      "‖alpha‖₁       : 0.3799999999999997\n",
      "scores min/max : -0.002394765933744665 0.0076911196272677\n",
      "Mask mean value:  tensor(0.5323, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9440  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.955e-14\n",
      "‖w_svm‖₂       : 1.1362548427261191e-06\n",
      "‖alpha‖₁       : 0.3199999999993877\n",
      "scores min/max : -2.5503351555510285e-06 -2.4405014069258028e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4897  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.223e-18\n",
      "‖w_svm‖₂       : 2.3953152777506454e-07\n",
      "‖alpha‖₁       : 0.6399999999999831\n",
      "scores min/max : 4.121106738497027e-07 4.331349155729889e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.125e-19\n",
      "‖w_svm‖₂       : 0.1366896514242025\n",
      "‖alpha‖₁       : 0.6548007429210618\n",
      "scores min/max : -18.041033100080853 1.87652903204464\n",
      "Mask mean value:  tensor(0.3204, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.737e-03\n",
      "‖w_svm‖₂       : 0.02871792732740159\n",
      "‖alpha‖₁       : 0.8985728548964373\n",
      "scores min/max : -0.7333222968315143 2.026644892627837\n",
      "Mask mean value:  tensor(0.8643, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3129  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.971e-15\n",
      "‖w_svm‖₂       : 0.003404609781768082\n",
      "‖alpha‖₁       : 0.5799999999999996\n",
      "scores min/max : 0.005399380319127232 0.006856705529492274\n",
      "Mask mean value:  tensor(0.5327, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3387  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.713e-13\n",
      "‖w_svm‖₂       : 0.035934443406276065\n",
      "‖alpha‖₁       : 0.659999999999958\n",
      "scores min/max : -0.17582763939957058 0.1155840407157529\n",
      "Mask mean value:  tensor(0.4545, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8661  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.973e-13\n",
      "‖w_svm‖₂       : 0.020837072570592686\n",
      "‖alpha‖₁       : 0.8150927098535687\n",
      "scores min/max : -11.508164646864625 2.0328132621083093\n",
      "Mask mean value:  tensor(0.7124, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9250  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.147e-06\n",
      "‖w_svm‖₂       : 0.013223719857156806\n",
      "‖alpha‖₁       : 0.86\n",
      "scores min/max : -0.047042401505515884 -0.00844123464040977\n",
      "Mask mean value:  tensor(0.3481, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.530e-05\n",
      "‖w_svm‖₂       : 0.02064713067880097\n",
      "‖alpha‖₁       : 0.7799999999999996\n",
      "scores min/max : -0.047125887043788776 0.01555890932338072\n",
      "Mask mean value:  tensor(0.3024, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.3405  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.264e-14\n",
      "‖w_svm‖₂       : 5.033790262942292e-08\n",
      "‖alpha‖₁       : 0.17999999999999639\n",
      "scores min/max : 5.536401065053234e-08 7.007724016058893e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.215e-08\n",
      "‖w_svm‖₂       : 0.016768230193501245\n",
      "‖alpha‖₁       : 0.6815993376797929\n",
      "scores min/max : -1.9692830670045556 0.057099098689391226\n",
      "Mask mean value:  tensor(0.6252, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2702  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.353e-12\n",
      "‖w_svm‖₂       : 0.009716679512727342\n",
      "‖alpha‖₁       : 0.6077162684137616\n",
      "scores min/max : -1.99480018025409 0.2919338360460372\n",
      "Mask mean value:  tensor(0.5360, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2163  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.054e-14\n",
      "‖w_svm‖₂       : 0.04511876875314199\n",
      "‖alpha‖₁       : 0.9388785090645764\n",
      "scores min/max : -2.471575339305115 1.5794016058976705\n",
      "Mask mean value:  tensor(0.1413, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3935  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.107e-03\n",
      "‖w_svm‖₂       : 0.015499879268045953\n",
      "‖alpha‖₁       : 0.85999999999999\n",
      "scores min/max : -0.05505175496983878 -0.04403175973831047\n",
      "Mask mean value:  tensor(0.2818, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0737  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.117e-14\n",
      "‖w_svm‖₂       : 0.1449152867974186\n",
      "‖alpha‖₁       : 0.7590184690345647\n",
      "scores min/max : -1.7691454245011562 2.2081693442367216\n",
      "Mask mean value:  tensor(0.8370, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3607  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.037e-11\n",
      "‖w_svm‖₂       : 0.018583324024909666\n",
      "‖alpha‖₁       : 0.659999999999999\n",
      "scores min/max : -0.033360839369409756 0.020414709001024975\n",
      "Mask mean value:  tensor(0.4348, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.4030  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.001e-15\n",
      "‖w_svm‖₂       : 1.141249055132163e-07\n",
      "‖alpha‖₁       : 0.5199999999999736\n",
      "scores min/max : 2.899002489538904e-07 3.1131162301637104e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.180e-19\n",
      "‖w_svm‖₂       : 2.0285732081876375e-07\n",
      "‖alpha‖₁       : 0.3799999999999858\n",
      "scores min/max : -2.227302794918035e-08 1.3500303085831658e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.932e-19\n",
      "‖w_svm‖₂       : 0.046695341841915435\n",
      "‖alpha‖₁       : 0.9056484612968009\n",
      "scores min/max : -0.7220999405321011 1.8784979018085322\n",
      "Mask mean value:  tensor(0.3283, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6118  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.704e-03\n",
      "‖w_svm‖₂       : 2.4831329664003887e-07\n",
      "‖alpha‖₁       : 0.5799999999999831\n",
      "scores min/max : -2.741961053994405e-07 -2.5853385818686356e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.276e-19\n",
      "‖w_svm‖₂       : 0.033351715721758816\n",
      "‖alpha‖₁       : 0.9199999999987262\n",
      "scores min/max : -0.2384124467873301 0.1734620391421895\n",
      "Mask mean value:  tensor(0.1764, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8898  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.330e-03\n",
      "‖w_svm‖₂       : 1.7256847538727364e-07\n",
      "‖alpha‖₁       : 0.2399999999999757\n",
      "scores min/max : 2.2889165052485513e-07 2.446423657859754e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.074e-20\n",
      "‖w_svm‖₂       : 0.004598974272799236\n",
      "‖alpha‖₁       : 0.45999999999999786\n",
      "scores min/max : -0.01095379477185275 -0.008681751122470044\n",
      "Mask mean value:  tensor(0.4541, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9730  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.188e-15\n",
      "‖w_svm‖₂       : 4.342414239398701e-06\n",
      "‖alpha‖₁       : 0.4199999999848751\n",
      "scores min/max : -4.578844478680387e-06 -2.7006291243446825e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1229  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.136e-05\n",
      "‖w_svm‖₂       : 2.3184536547408563e-07\n",
      "‖alpha‖₁       : 0.25999999999999596\n",
      "scores min/max : 2.516411660684268e-07 2.64412735788025e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.741e-17\n",
      "‖w_svm‖₂       : 1.9614754673539664e-08\n",
      "‖alpha‖₁       : 0.11999999999999302\n",
      "scores min/max : -4.547908869823568e-08 -3.073450961663574e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.828e-09\n",
      "‖w_svm‖₂       : 7.491295329141023e-08\n",
      "‖alpha‖₁       : 0.13999999999999319\n",
      "scores min/max : 1.5223544382271705e-08 2.1024560987199883e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 7.281e-08\n",
      "‖w_svm‖₂       : 0.00018105703230909305\n",
      "‖alpha‖₁       : 0.8199999999999994\n",
      "scores min/max : -9.854267976807621e-05 -8.56896021732439e-05\n",
      "Mask mean value:  tensor(0.4996, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6964  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.434e-17\n",
      "‖w_svm‖₂       : 0.0002072437920687457\n",
      "‖alpha‖₁       : 0.6199999999994337\n",
      "scores min/max : -4.585032944418553e-06 1.0634226815295902e-05\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.016e-17\n",
      "‖w_svm‖₂       : 1.1770474267846446e-07\n",
      "‖alpha‖₁       : 0.2999999999999954\n",
      "scores min/max : 1.74781637885538e-07 1.8375961409717866e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.588e-07\n",
      "‖w_svm‖₂       : 7.643151089316088e-08\n",
      "‖alpha‖₁       : 0.4199999999999964\n",
      "scores min/max : -6.611930233681923e-08 -1.895359552386117e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 7.208e-08\n",
      "‖w_svm‖₂       : 0.0009292914483217277\n",
      "‖alpha‖₁       : 0.8199999999999992\n",
      "scores min/max : 0.0006293416418493621 0.0023990769591733883\n",
      "Mask mean value:  tensor(0.5087, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5843  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.156e-16\n",
      "‖w_svm‖₂       : 0.00013777295264754845\n",
      "‖alpha‖₁       : 0.439999999993142\n",
      "scores min/max : -0.00029363371912654343 -0.00015434510574937352\n",
      "Mask mean value:  tensor(0.4990, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0207  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.057e-17\n",
      "‖w_svm‖₂       : 7.795870559522945e-08\n",
      "‖alpha‖₁       : 0.17999999999999933\n",
      "scores min/max : -2.0067862365424714e-07 3.289602344968631e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.609e-21\n",
      "‖w_svm‖₂       : 0.05336078428204448\n",
      "‖alpha‖₁       : 0.5763252508574637\n",
      "scores min/max : -1.9590228226445006 0.8689144416186702\n",
      "Mask mean value:  tensor(0.7070, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9191  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.013e-03\n",
      "‖w_svm‖₂       : 0.1822743301964964\n",
      "‖alpha‖₁       : 0.883203595314279\n",
      "scores min/max : -3.6544098304979373 6.064994550153862\n",
      "Mask mean value:  tensor(0.0906, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2005  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.915e-03\n",
      "‖w_svm‖₂       : 1.085527351246337e-06\n",
      "‖alpha‖₁       : 0.49999999999999\n",
      "scores min/max : -2.2573241173440823e-07 -3.269146372396168e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.388e-19\n",
      "‖w_svm‖₂       : 0.04810801526564459\n",
      "‖alpha‖₁       : 0.9411199712872051\n",
      "scores min/max : -1.8282935754829026 0.30556134284689884\n",
      "Mask mean value:  tensor(0.4245, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3246  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.092e-10\n",
      "‖w_svm‖₂       : 5.8751559177446756e-08\n",
      "‖alpha‖₁       : 0.23999999999999888\n",
      "scores min/max : -6.14946580529342e-08 -4.713799558103693e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.714e-20\n",
      "‖w_svm‖₂       : 0.0375326238262978\n",
      "‖alpha‖₁       : 0.7294917769283047\n",
      "scores min/max : -0.2683103065233341 2.014888063962154\n",
      "Mask mean value:  tensor(0.6554, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.9797  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 2.021e-03\n",
      "‖w_svm‖₂       : 0.02606356857443836\n",
      "‖alpha‖₁       : 0.19669273573828588\n",
      "scores min/max : -2.2022913819937715 0.01875886663855257\n",
      "Mask mean value:  tensor(0.0597, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4038  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.840e-04\n",
      "‖w_svm‖₂       : 0.08662426957452617\n",
      "‖alpha‖₁       : 0.8733466699660863\n",
      "scores min/max : -12.139103854824612 2.1736499295838403\n",
      "Mask mean value:  tensor(0.5792, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7139  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 3.988e-02\n",
      "‖w_svm‖₂       : 0.00015702219046406617\n",
      "‖alpha‖₁       : 0.6399999999999999\n",
      "scores min/max : -0.000161592290091253 -0.00015218418293267897\n",
      "Mask mean value:  tensor(0.4992, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7632  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.476e-17\n",
      "‖w_svm‖₂       : 0.07261043702075726\n",
      "‖alpha‖₁       : 0.5799999999999272\n",
      "scores min/max : -2.809847215537738 1.6709240061248343\n",
      "Mask mean value:  tensor(0.2710, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4372  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.975e-04\n",
      "‖w_svm‖₂       : 0.021056880630597692\n",
      "‖alpha‖₁       : 0.3835149747795663\n",
      "scores min/max : -1.957374077942669 0.24639208276233016\n",
      "Mask mean value:  tensor(0.7245, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1553  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.656e-14\n",
      "‖w_svm‖₂       : 8.594643350104739e-08\n",
      "‖alpha‖₁       : 0.3799999999999917\n",
      "scores min/max : 1.6592550150088703e-08 3.4775013967647567e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.277e-22\n",
      "‖w_svm‖₂       : 0.0005352494595125203\n",
      "‖alpha‖₁       : 0.4399999999999999\n",
      "scores min/max : -0.0009571149418383093 -0.0006569157272248482\n",
      "Mask mean value:  tensor(0.4955, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6403  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.399e-16\n",
      "‖w_svm‖₂       : 2.573026936399064e-07\n",
      "‖alpha‖₁       : 0.379999999991105\n",
      "scores min/max : -2.0947183093638841e-07 -1.891462924218002e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.581e-07\n",
      "‖w_svm‖₂       : 0.05134845005188345\n",
      "‖alpha‖₁       : 0.8272276469249493\n",
      "scores min/max : -1.9457558733251177 0.4065213871483539\n",
      "Mask mean value:  tensor(0.5393, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0172  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.587e-02\n",
      "‖w_svm‖₂       : 1.0669662202963503e-07\n",
      "‖alpha‖₁       : 0.23999999999999935\n",
      "scores min/max : 8.052302812529206e-08 9.127647211257439e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.828e-21\n",
      "‖w_svm‖₂       : 1.026988945036525e-06\n",
      "‖alpha‖₁       : 0.5999999999999952\n",
      "scores min/max : -2.3489377042498568e-07 -1.840379331938801e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.859e-19\n",
      "‖w_svm‖₂       : 0.00029575631947182166\n",
      "‖alpha‖₁       : 0.4199999999998331\n",
      "scores min/max : 8.18632652196847e-05 0.0003782373513017809\n",
      "Mask mean value:  tensor(0.5017, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9990  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.095e-15\n",
      "‖w_svm‖₂       : 0.007141815516394448\n",
      "‖alpha‖₁       : 0.7005816840797245\n",
      "scores min/max : -2.002791626958762 0.020639672756601873\n",
      "Mask mean value:  tensor(0.4608, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.410e-12\n",
      "‖w_svm‖₂       : 0.030569242287847142\n",
      "‖alpha‖₁       : 0.5512630833073651\n",
      "scores min/max : -3.4378668179410097 1.1302295786810606\n",
      "Mask mean value:  tensor(0.1738, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.3130  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.051e-03\n",
      "‖w_svm‖₂       : 0.02040015949801627\n",
      "‖alpha‖₁       : 0.8233895441184161\n",
      "scores min/max : -1.9122502452528192 1.4831831197442507\n",
      "Mask mean value:  tensor(0.6623, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.2953  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.298e-05\n",
      "‖w_svm‖₂       : 0.06818978828328237\n",
      "‖alpha‖₁       : 0.4191937674568753\n",
      "scores min/max : -1.9281594675978413 2.0853710526339295\n",
      "Mask mean value:  tensor(0.7463, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8960  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.242e-04\n",
      "‖w_svm‖₂       : 0.005973527111539949\n",
      "‖alpha‖₁       : 0.5599999999999834\n",
      "scores min/max : 0.006713594525088208 0.008576807060990621\n",
      "Mask mean value:  tensor(0.5352, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1220  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.912e-05\n",
      "‖w_svm‖₂       : 0.0852195080620937\n",
      "‖alpha‖₁       : 0.5780634834525593\n",
      "scores min/max : -2.1194996359565175 5.795378114851463\n",
      "Mask mean value:  tensor(0.2741, dtype=torch.float64)\n",
      "max feasible return = 0.1096  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.020496099174684e-07\n",
      "‖alpha‖₁       : 0.5799999999999977\n",
      "scores min/max : -3.0360685193281135e-07 -2.1274352398311038e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0912770931439457e-07\n",
      "‖alpha‖₁       : 0.299999999999972\n",
      "scores min/max : 3.857886008020152e-08 4.369920108492739e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 6.617604457772843e-08\n",
      "‖alpha‖₁       : 0.5999999999999993\n",
      "scores min/max : 2.9416910227049745e-09 3.968973497376199e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.5426446730588366e-08\n",
      "‖alpha‖₁       : 0.37999999999999956\n",
      "scores min/max : -2.5196774745697034e-08 -1.581990383988146e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.186293896107987e-06\n",
      "‖alpha‖₁       : 0.319999999976131\n",
      "scores min/max : 2.6568865880616767e-06 4.5739930180468185e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4258  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.044153607743447086\n",
      "‖alpha‖₁       : 0.7397189575388907\n",
      "scores min/max : -3.4454941462435373 2.1857031326592358\n",
      "Mask mean value:  tensor(0.9060, dtype=torch.float64)\n",
      "max feasible return = -0.9458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.02454917937498371\n",
      "‖alpha‖₁       : 0.615171674156803\n",
      "scores min/max : -1.9480715733793141 1.179379764864536\n",
      "Mask mean value:  tensor(0.7090, dtype=torch.float64)\n",
      "max feasible return = 2.4702  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.791290358802086e-07\n",
      "‖alpha‖₁       : 0.459999999999973\n",
      "scores min/max : 1.0366369341428436e-07 6.988567781851041e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.422099203238344e-07\n",
      "‖alpha‖₁       : 0.51999999999999\n",
      "scores min/max : 9.166888007546967e-08 1.0458077811608807e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.6417720267990195e-07\n",
      "‖alpha‖₁       : 0.5799999999999721\n",
      "scores min/max : -2.2387724670453313e-07 -1.8749417832535643e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.042504423549884734\n",
      "‖alpha‖₁       : 0.6878597570776132\n",
      "scores min/max : -0.4392918959034823 1.9506961473196252\n",
      "Mask mean value:  tensor(0.3727, dtype=torch.float64)\n",
      "max feasible return = 0.0550  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.0325187768599654\n",
      "‖alpha‖₁       : 0.42234386381197286\n",
      "scores min/max : -3.858934842151461 5.301879879099663\n",
      "Mask mean value:  tensor(0.1128, dtype=torch.float64)\n",
      "max feasible return = 0.7195  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04303323708020557\n",
      "‖alpha‖₁       : 0.8211764036968234\n",
      "scores min/max : -4.923228480028965 3.2478010382419615\n",
      "Mask mean value:  tensor(0.9670, dtype=torch.float64)\n",
      "max feasible return = -3.7029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.006065954381628055\n",
      "‖alpha‖₁       : 0.7999999999999998\n",
      "scores min/max : -0.02622815554745321 -0.0013691590555887177\n",
      "Mask mean value:  tensor(0.4620, dtype=torch.float64)\n",
      "max feasible return = -0.1680  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.165804408945345e-07\n",
      "‖alpha‖₁       : 0.6199999999999772\n",
      "scores min/max : 7.162193776740353e-08 8.811084706999585e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.905812780570103e-08\n",
      "‖alpha‖₁       : 0.4399999999999299\n",
      "scores min/max : 4.1165501384787615e-09 1.3708560780523557e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.3755015416388397e-07\n",
      "‖alpha‖₁       : 0.2799999999999733\n",
      "scores min/max : -1.9108517077955783e-07 -1.7312795815140573e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  52 | train 0.005300 | val 0.006612\n",
      "-----------------------------------------Epoch:  53 ----------------------------------------\n",
      "‖w_svm‖₂       : 8.570980831207272e-08\n",
      "‖alpha‖₁       : 0.37999999999999023\n",
      "scores min/max : 1.9218709353262725e-08 3.740231936253967e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.234e-22\n",
      "‖w_svm‖₂       : 0.13796779653632613\n",
      "‖alpha‖₁       : 0.8799999999999999\n",
      "scores min/max : -1.3198629556967885 3.691138934339287\n",
      "Mask mean value:  tensor(0.4262, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2650  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.978e-08\n",
      "‖w_svm‖₂       : 2.566860139902288e-07\n",
      "‖alpha‖₁       : 0.37999999998932\n",
      "scores min/max : -2.1761663593099428e-07 -1.9698286396782595e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.682e-07\n",
      "‖w_svm‖₂       : 0.007147054444637453\n",
      "‖alpha‖₁       : 0.700581769473136\n",
      "scores min/max : -2.002755807360599 0.02067569322744474\n",
      "Mask mean value:  tensor(0.4610, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2580  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.108e-12\n",
      "‖w_svm‖₂       : 1.1723247981924915e-07\n",
      "‖alpha‖₁       : 0.29999999999999605\n",
      "scores min/max : 1.7298115356579517e-07 1.8195335471636692e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.594e-07\n",
      "‖w_svm‖₂       : 0.021158577803889023\n",
      "‖alpha‖₁       : 0.3835198807891253\n",
      "scores min/max : -1.9598347582874092 0.2439448373322874\n",
      "Mask mean value:  tensor(0.7157, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1541  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.120e-14\n",
      "‖w_svm‖₂       : 0.015603472455966392\n",
      "‖alpha‖₁       : 0.5963121986284673\n",
      "scores min/max : -2.7272162946148777 2.2659294921564523\n",
      "Mask mean value:  tensor(0.9126, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.5017  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.458e-10\n",
      "‖w_svm‖₂       : 0.07023127748700124\n",
      "‖alpha‖₁       : 0.6587558164587106\n",
      "scores min/max : -1.9793774866120915 4.4496751999263635\n",
      "Mask mean value:  tensor(0.4664, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.9814  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.257e-04\n",
      "‖w_svm‖₂       : 0.04669196897335387\n",
      "‖alpha‖₁       : 0.9056772636861492\n",
      "scores min/max : -0.7094899870352233 1.8852347600251909\n",
      "Mask mean value:  tensor(0.3473, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6472  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.207e-03\n",
      "‖w_svm‖₂       : 0.07237897017086739\n",
      "‖alpha‖₁       : 0.579999999999968\n",
      "scores min/max : -2.78897521203 1.658202610840267\n",
      "Mask mean value:  tensor(0.2712, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4369  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.505e-04\n",
      "‖w_svm‖₂       : 0.000535867360444391\n",
      "‖alpha‖₁       : 0.4399999999999991\n",
      "scores min/max : -0.0009129479185998788 -0.0006122639107362026\n",
      "Mask mean value:  tensor(0.4957, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6410  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.414e-16\n",
      "‖w_svm‖₂       : 1.7145067239305893e-07\n",
      "‖alpha‖₁       : 0.2399999999999768\n",
      "scores min/max : 2.0038374999755157e-07 2.160281118058944e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.048e-20\n",
      "‖w_svm‖₂       : 0.004631728347971571\n",
      "‖alpha‖₁       : 0.4599999999999902\n",
      "scores min/max : -0.01128969684021197 -0.008990637331580158\n",
      "Mask mean value:  tensor(0.4525, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9662  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.196e-15\n",
      "‖w_svm‖₂       : 0.02079154298450535\n",
      "‖alpha‖₁       : 0.7799999999999996\n",
      "scores min/max : -0.04843657556308178 0.015085413291710037\n",
      "Mask mean value:  tensor(0.2973, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.3197  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.332e-14\n",
      "‖w_svm‖₂       : 2.0081374526263203e-07\n",
      "‖alpha‖₁       : 0.3799999999999867\n",
      "scores min/max : -3.5718331397313363e-09 3.219836043530265e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.848e-19\n",
      "‖w_svm‖₂       : 5.327312633346639e-08\n",
      "‖alpha‖₁       : 0.43999999999996997\n",
      "scores min/max : -2.020085027683699e-07 -9.821166338218106e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.935e-20\n",
      "‖w_svm‖₂       : 7.854131175458347e-08\n",
      "‖alpha‖₁       : 0.1799999999999994\n",
      "scores min/max : -1.9515684033506448e-07 3.8442668269518613e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.750e-21\n",
      "‖w_svm‖₂       : 0.08196789002734604\n",
      "‖alpha‖₁       : 0.47204294488217363\n",
      "scores min/max : -2.2127647274979267 2.863174456501353\n",
      "Mask mean value:  tensor(0.1635, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.7287  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.673e-02\n",
      "‖w_svm‖₂       : 0.18275956925651435\n",
      "‖alpha‖₁       : 0.8834426347301251\n",
      "scores min/max : -3.6481315610721565 6.072411230569355\n",
      "Mask mean value:  tensor(0.0919, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2050  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.452e-05\n",
      "‖w_svm‖₂       : 2.3088757723198207e-07\n",
      "‖alpha‖₁       : 0.25999999999999623\n",
      "scores min/max : 2.6170697969569366e-07 2.744758507285949e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.854e-17\n",
      "‖w_svm‖₂       : 0.048341277547021194\n",
      "‖alpha‖₁       : 0.9411506384757207\n",
      "scores min/max : -1.8189101296606758 0.31497043426793825\n",
      "Mask mean value:  tensor(0.4621, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3250  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.456e-11\n",
      "‖w_svm‖₂       : 7.447539090910971e-08\n",
      "‖alpha‖₁       : 0.13999999999999507\n",
      "scores min/max : 1.4123682874635752e-08 1.9908032815581246e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 7.176e-08\n",
      "‖w_svm‖₂       : 0.00029630574951975915\n",
      "‖alpha‖₁       : 0.41999999999014187\n",
      "scores min/max : 5.079347496389432e-05 0.0003481570445732496\n",
      "Mask mean value:  tensor(0.5016, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9987  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.104e-15\n",
      "‖w_svm‖₂       : 1.0854929073488628e-06\n",
      "‖alpha‖₁       : 0.49999999999999023\n",
      "scores min/max : -2.417325313912586e-07 -4.870756510725986e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.385e-19\n",
      "‖w_svm‖₂       : 0.040766042962627425\n",
      "‖alpha‖₁       : 0.8900176273370727\n",
      "scores min/max : -2.0119623928624226 0.33578127131636976\n",
      "Mask mean value:  tensor(0.2548, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0558  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.968e-05\n",
      "‖w_svm‖₂       : 1.0272468811737997e-06\n",
      "‖alpha‖₁       : 0.5999999999999954\n",
      "scores min/max : -2.1172863687277728e-07 -1.6087444790092975e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.867e-19\n",
      "‖w_svm‖₂       : 0.0864901401117179\n",
      "‖alpha‖₁       : 0.8733528353165462\n",
      "scores min/max : -12.164311708017232 2.1479783311759024\n",
      "Mask mean value:  tensor(0.5166, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.5344  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.457e-02\n",
      "‖w_svm‖₂       : 5.004675701742888e-08\n",
      "‖alpha‖₁       : 0.17999999999998006\n",
      "scores min/max : 5.5383672141983464e-08 7.029561321059397e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.262e-08\n",
      "‖w_svm‖₂       : 0.14561623993645642\n",
      "‖alpha‖₁       : 0.7592270649105233\n",
      "scores min/max : -1.7729384949424765 2.2039704986876276\n",
      "Mask mean value:  tensor(0.8312, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3592  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.000e-11\n",
      "‖w_svm‖₂       : 0.00015709222941507126\n",
      "‖alpha‖₁       : 0.6399999999999999\n",
      "scores min/max : -0.0001610603680135757 -0.00015164382104654263\n",
      "Mask mean value:  tensor(0.4992, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7632  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.501e-17\n",
      "‖w_svm‖₂       : 0.015497699427523208\n",
      "‖alpha‖₁       : 0.8599999999999951\n",
      "scores min/max : -0.05920289987429017 -0.048253392014628756\n",
      "Mask mean value:  tensor(0.2651, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.544e-14\n",
      "‖w_svm‖₂       : 0.031053499710860678\n",
      "‖alpha‖₁       : 0.8525106470925685\n",
      "scores min/max : -2.8967988556676674 1.5840841069072236\n",
      "Mask mean value:  tensor(0.1649, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9606  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.260e-03\n",
      "‖w_svm‖₂       : 0.00018126189630462394\n",
      "‖alpha‖₁       : 0.8199999999999995\n",
      "scores min/max : -9.635974567803095e-05 -8.346190858228499e-05\n",
      "Mask mean value:  tensor(0.4996, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6964  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.440e-17\n",
      "‖w_svm‖₂       : 0.0354544678050594\n",
      "‖alpha‖₁       : 0.659999999999952\n",
      "scores min/max : -0.172656923069629 0.11131758020219334\n",
      "Mask mean value:  tensor(0.4492, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8575  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.741e-13\n",
      "‖w_svm‖₂       : 5.404151018227289e-08\n",
      "‖alpha‖₁       : 0.11999999999998395\n",
      "scores min/max : -9.335735097484935e-08 -6.245673687481262e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.124e-07\n",
      "‖w_svm‖₂       : 0.05308087875641672\n",
      "‖alpha‖₁       : 0.5763353808473062\n",
      "scores min/max : -1.9624055563458433 0.8656433854707605\n",
      "Mask mean value:  tensor(0.6964, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.8883  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.530e-04\n",
      "‖w_svm‖₂       : 1.9739353668104684e-08\n",
      "‖alpha‖₁       : 0.11999999999999292\n",
      "scores min/max : -4.7827618687155955e-08 -3.3078106445230985e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.853e-09\n",
      "‖w_svm‖₂       : 1.1372685696519953e-07\n",
      "‖alpha‖₁       : 0.5199999999999866\n",
      "scores min/max : 2.6408493119273855e-07 2.854943900939823e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.182e-19\n",
      "‖w_svm‖₂       : 0.0003561781704452988\n",
      "‖alpha‖₁       : 0.7399999999999993\n",
      "scores min/max : -0.0004582995628124393 -0.0004421184270642595\n",
      "Mask mean value:  tensor(0.4978, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0595  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.543e-17\n",
      "‖w_svm‖₂       : 0.032768980240356724\n",
      "‖alpha‖₁       : 0.9199999999999973\n",
      "scores min/max : -0.233677758269384 0.16390864431807217\n",
      "Mask mean value:  tensor(0.1744, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8817  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.704e-03\n",
      "‖w_svm‖₂       : 1.106557812184757e-06\n",
      "‖alpha‖₁       : 0.3199999999998026\n",
      "scores min/max : -2.4088701763244138e-06 -2.3017220485044613e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4898  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.182e-18\n",
      "‖w_svm‖₂       : 2.3986623361511976e-07\n",
      "‖alpha‖₁       : 0.6399999999999502\n",
      "scores min/max : 4.1155535500014013e-07 4.326777881793491e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.136e-19\n",
      "‖w_svm‖₂       : 0.028560019323501017\n",
      "‖alpha‖₁       : 0.8985690914244779\n",
      "scores min/max : -0.731489487834525 2.028430349980408\n",
      "Mask mean value:  tensor(0.8666, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3084  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.820e-15\n",
      "‖w_svm‖₂       : 1.4167869434020789e-05\n",
      "‖alpha‖₁       : 0.35999999999646637\n",
      "scores min/max : 8.426286350950781e-06 9.258017553310056e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.607e-17\n",
      "‖w_svm‖₂       : 3.8505797698176587e-07\n",
      "‖alpha‖₁       : 0.27999999999999514\n",
      "scores min/max : -9.855876105270998e-07 -9.238376658342687e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.309e-19\n",
      "‖w_svm‖₂       : 0.020310326320191953\n",
      "‖alpha‖₁       : 0.823389648488946\n",
      "scores min/max : -1.9109407183400737 1.4844984160801562\n",
      "Mask mean value:  tensor(0.6661, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.3038  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.057e-05\n",
      "‖w_svm‖₂       : 0.025890409617541597\n",
      "‖alpha‖₁       : 0.19669030324870218\n",
      "scores min/max : -2.200435661508569 0.020586640341306906\n",
      "Mask mean value:  tensor(0.0615, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4156  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.623e-04\n",
      "‖w_svm‖₂       : 0.045075102124869953\n",
      "‖alpha‖₁       : 0.9388738024859087\n",
      "scores min/max : -2.4688981570611146 1.582366735089358\n",
      "Mask mean value:  tensor(0.1431, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3975  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.152e-03\n",
      "‖w_svm‖₂       : 0.00020769108624030553\n",
      "‖alpha‖₁       : 0.6199999999759305\n",
      "scores min/max : -1.3325741146405327e-05 1.952655306224741e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.040e-17\n",
      "‖w_svm‖₂       : 4.3563804401120605e-06\n",
      "‖alpha‖₁       : 0.4199999999845986\n",
      "scores min/max : -4.7480602628671885e-06 -2.8621644224704177e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1229  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.164e-05\n",
      "‖w_svm‖₂       : 7.660093221187816e-08\n",
      "‖alpha‖₁       : 0.6599999999999757\n",
      "scores min/max : 8.580597344831232e-08 2.3097390137478616e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.520e-09\n",
      "‖w_svm‖₂       : 0.02061175540026504\n",
      "‖alpha‖₁       : 0.8150899784277544\n",
      "scores min/max : -11.5091719185891 2.03113637739793\n",
      "Mask mean value:  tensor(0.7089, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9173  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.104e-06\n",
      "‖w_svm‖₂       : 2.4709339067782545e-07\n",
      "‖alpha‖₁       : 0.5799999999999877\n",
      "scores min/max : -2.5349886040682196e-07 -2.378465078709719e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.278e-19\n",
      "‖w_svm‖₂       : 7.646185988508674e-08\n",
      "‖alpha‖₁       : 0.5399999999999691\n",
      "scores min/max : -1.9321702245007387e-07 -1.7162693233584417e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.153e-20\n",
      "‖w_svm‖₂       : 2.8440575342026227e-07\n",
      "‖alpha‖₁       : 0.6599999999999997\n",
      "scores min/max : -6.537589217809121e-07 -5.02121872144545e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.576e-18\n",
      "‖w_svm‖₂       : 1.9860438322431678e-07\n",
      "‖alpha‖₁       : 0.41999999999996585\n",
      "scores min/max : -4.74732326204297e-07 -4.5256461994248826e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.112e-19\n",
      "‖w_svm‖₂       : 0.0676139722997789\n",
      "‖alpha‖₁       : 0.41914362062001537\n",
      "scores min/max : -1.9119568071416895 2.0970248705640677\n",
      "Mask mean value:  tensor(0.7628, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9012  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.660e-04\n",
      "‖w_svm‖₂       : 0.13695290056133008\n",
      "‖alpha‖₁       : 0.6549213248468925\n",
      "scores min/max : -18.051006928656264 1.8731881786705966\n",
      "Mask mean value:  tensor(0.3161, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2683  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.799e-03\n",
      "‖w_svm‖₂       : 4.413765030384165e-07\n",
      "‖alpha‖₁       : 0.7199999999999996\n",
      "scores min/max : 1.5083880263777265e-07 2.0801864995816775e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.475e-21\n",
      "‖w_svm‖₂       : 6.701088400703904e-07\n",
      "‖alpha‖₁       : 0.3999999999999999\n",
      "scores min/max : 7.047045628720831e-08 3.767060385779181e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.385e-09\n",
      "‖w_svm‖₂       : 0.0001379647586314561\n",
      "‖alpha‖₁       : 0.4399999999897103\n",
      "scores min/max : -0.000296833451985077 -0.00015754080420379455\n",
      "Mask mean value:  tensor(0.4990, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0205  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.094e-17\n",
      "‖w_svm‖₂       : 0.06018068014400032\n",
      "‖alpha‖₁       : 0.8986937899158802\n",
      "scores min/max : -1.702368950898364 3.7120752919743865\n",
      "Mask mean value:  tensor(0.4265, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5787  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.204e-02\n",
      "‖w_svm‖₂       : 0.016455628143569774\n",
      "‖alpha‖₁       : 0.6815997555369839\n",
      "scores min/max : -1.9700935551949932 0.05583935096730541\n",
      "Mask mean value:  tensor(0.6216, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.038e-12\n",
      "‖w_svm‖₂       : 0.000923045491716671\n",
      "‖alpha‖₁       : 0.8199999999999994\n",
      "scores min/max : 0.0005505329021590552 0.002292686059353826\n",
      "Mask mean value:  tensor(0.5083, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5819  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.165e-16\n",
      "‖w_svm‖₂       : 0.036892849983232306\n",
      "‖alpha‖₁       : 0.7294930007652518\n",
      "scores min/max : -0.2684846881240616 2.015172759858693\n",
      "Mask mean value:  tensor(0.6548, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.9786  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 2.028e-03\n",
      "‖w_svm‖₂       : 0.003390876537758936\n",
      "‖alpha‖₁       : 0.5799999999999783\n",
      "scores min/max : 0.005487400473050571 0.00693320021569924\n",
      "Mask mean value:  tensor(0.5331, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3390  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.931e-13\n",
      "‖w_svm‖₂       : 0.030440847041748136\n",
      "‖alpha‖₁       : 0.5512596683292441\n",
      "scores min/max : -3.4378128420585576 1.1301194618682353\n",
      "Mask mean value:  tensor(0.1736, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.3122  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.036e-03\n",
      "‖w_svm‖₂       : 0.01826518481458124\n",
      "‖alpha‖₁       : 0.6599999999999975\n",
      "scores min/max : -0.032957845862104596 0.018995181548534384\n",
      "Mask mean value:  tensor(0.4334, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.3906  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.017e-15\n",
      "‖w_svm‖₂       : 0.05127425076704239\n",
      "‖alpha‖₁       : 0.8272308018878609\n",
      "scores min/max : -1.942344127693042 0.4101626864638174\n",
      "Mask mean value:  tensor(0.5512, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0197  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.848e-10\n",
      "‖w_svm‖₂       : 0.005813911775288889\n",
      "‖alpha‖₁       : 0.37999999999999934\n",
      "scores min/max : -0.002283693233332759 0.007764018500566118\n",
      "Mask mean value:  tensor(0.5327, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9448  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.951e-14\n",
      "‖w_svm‖₂       : 1.0732732639886187e-07\n",
      "‖alpha‖₁       : 0.23999999999999835\n",
      "scores min/max : 7.819077211833149e-08 8.897043972643445e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.869e-21\n",
      "‖w_svm‖₂       : 0.012929252172673362\n",
      "‖alpha‖₁       : 0.8599999999999979\n",
      "scores min/max : -0.04685167351799534 -0.009935886900524939\n",
      "Mask mean value:  tensor(0.3458, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4283  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.729e-05\n",
      "‖w_svm‖₂       : 0.005958570027893753\n",
      "‖alpha‖₁       : 0.5599999999999895\n",
      "scores min/max : 0.0066125802829642135 0.008468043539887157\n",
      "Mask mean value:  tensor(0.5347, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1210  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.905e-05\n",
      "‖w_svm‖₂       : 7.639536232145237e-08\n",
      "‖alpha‖₁       : 0.4199999999999998\n",
      "scores min/max : -6.549543994751096e-08 -1.837209555076589e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 7.255e-08\n",
      "‖w_svm‖₂       : 5.877436583126139e-08\n",
      "‖alpha‖₁       : 0.23999999999999844\n",
      "scores min/max : -6.127606100098119e-08 -4.696195379271906e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.734e-20\n",
      "‖w_svm‖₂       : 0.009915038692277629\n",
      "‖alpha‖₁       : 0.6077200958727884\n",
      "scores min/max : -1.995561337833137 0.2911588389468162\n",
      "Mask mean value:  tensor(0.5326, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2149  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.124e-14\n",
      "‖w_svm‖₂       : 0.0849250152101639\n",
      "‖alpha‖₁       : 0.5780216861487224\n",
      "scores min/max : -2.1103579563573778 5.803837444212508\n",
      "Mask mean value:  tensor(0.2911, dtype=torch.float64)\n",
      "max feasible return = 0.1125  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.017923559717617e-07\n",
      "‖alpha‖₁       : 0.5799999999999986\n",
      "scores min/max : -3.173092929041049e-07 -2.2652923568728797e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.088846275028666e-07\n",
      "‖alpha‖₁       : 0.2999999999999742\n",
      "scores min/max : 4.417722753986166e-08 4.9278321185215956e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 6.586725727235264e-08\n",
      "‖alpha‖₁       : 0.5999999999999993\n",
      "scores min/max : 6.586793163122897e-09 4.332628778058613e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.531792390473653e-08\n",
      "‖alpha‖₁       : 0.37999999999999967\n",
      "scores min/max : -2.235809507431308e-08 -1.2983329306390141e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.815533601218623e-06\n",
      "‖alpha‖₁       : 0.31999999995510475\n",
      "scores min/max : 4.304040035054099e-06 7.334202372419794e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4259  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04353424275110225\n",
      "‖alpha‖₁       : 0.7397152003348964\n",
      "scores min/max : -3.444200453464196 2.1868746229555955\n",
      "Mask mean value:  tensor(0.9068, dtype=torch.float64)\n",
      "max feasible return = -0.9464  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.024239610316753063\n",
      "‖alpha‖₁       : 0.6151714560160615\n",
      "scores min/max : -1.951344725781608 1.175929919376706\n",
      "Mask mean value:  tensor(0.6998, dtype=torch.float64)\n",
      "max feasible return = 2.4375  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.795571043924261e-07\n",
      "‖alpha‖₁       : 0.4599999999999928\n",
      "scores min/max : 1.4498533902021884e-07 7.394443019715194e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.4293849935302956e-07\n",
      "‖alpha‖₁       : 0.5199999999999902\n",
      "scores min/max : 8.886936401202485e-08 1.0178087814069459e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.6530028087607647e-07\n",
      "‖alpha‖₁       : 0.5799999999999563\n",
      "scores min/max : -2.4201482099016494e-07 -2.0559367858172267e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.04221851560513292\n",
      "‖alpha‖₁       : 0.687854556768911\n",
      "scores min/max : -0.43925835546176983 1.950863888540644\n",
      "Mask mean value:  tensor(0.3725, dtype=torch.float64)\n",
      "max feasible return = 0.0543  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03229505670319603\n",
      "‖alpha‖₁       : 0.42234361798376063\n",
      "scores min/max : -3.8560654790532625 5.306152470020374\n",
      "Mask mean value:  tensor(0.1153, dtype=torch.float64)\n",
      "max feasible return = 0.7365  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.043024835316974076\n",
      "‖alpha‖₁       : 0.8211789408421419\n",
      "scores min/max : -4.916093584591016 3.25504942979689\n",
      "Mask mean value:  tensor(0.9673, dtype=torch.float64)\n",
      "max feasible return = -3.7044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.005970934627158198\n",
      "‖alpha‖₁       : 0.7999999999999998\n",
      "scores min/max : -0.0259726516903908 -0.001961653459693794\n",
      "Mask mean value:  tensor(0.4600, dtype=torch.float64)\n",
      "max feasible return = -0.1674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.174760747124066e-07\n",
      "‖alpha‖₁       : 0.6199999999999777\n",
      "scores min/max : 8.31534629756429e-08 9.964121763830315e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.937865858349762e-08\n",
      "‖alpha‖₁       : 0.43999999999993616\n",
      "scores min/max : 4.7775678270313056e-09 1.4368279424023318e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.3357681844601047e-07\n",
      "‖alpha‖₁       : 0.27999999999999925\n",
      "scores min/max : -2.1432403333578138e-07 -1.9672461799093198e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  53 | train 0.005302 | val 0.006616\n",
      "-----------------------------------------Epoch:  54 ----------------------------------------\n",
      "‖w_svm‖₂       : 0.015386502794452956\n",
      "‖alpha‖₁       : 0.8599999999999979\n",
      "scores min/max : -0.055420061622538826 -0.044751874142989145\n",
      "Mask mean value:  tensor(0.2793, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0735  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.172e-14\n",
      "‖w_svm‖₂       : 0.020386086044965446\n",
      "‖alpha‖₁       : 0.8233877871058646\n",
      "scores min/max : -1.9130133928017965 1.4823557399122507\n",
      "Mask mean value:  tensor(0.6600, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.2900  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.414e-05\n",
      "‖w_svm‖₂       : 2.3962102277465226e-07\n",
      "‖alpha‖₁       : 0.6399999999999455\n",
      "scores min/max : 4.157638561089209e-07 4.3689895048791565e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.139e-19\n",
      "‖w_svm‖₂       : 0.1457037580785033\n",
      "‖alpha‖₁       : 0.7592520911455415\n",
      "scores min/max : -1.7712822672635138 2.2058709056119925\n",
      "Mask mean value:  tensor(0.8337, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3601  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.551e-11\n",
      "‖w_svm‖₂       : 5.0220338414614506e-08\n",
      "‖alpha‖₁       : 0.17999999999999627\n",
      "scores min/max : 5.376678752280369e-08 6.846858098434185e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.237e-08\n",
      "‖w_svm‖₂       : 7.657784819986889e-08\n",
      "‖alpha‖₁       : 0.6599999999999763\n",
      "scores min/max : 9.043391261278959e-08 2.3560480964232387e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.751e-08\n",
      "‖w_svm‖₂       : 0.00018104696500633057\n",
      "‖alpha‖₁       : 0.8199999999999994\n",
      "scores min/max : -9.832299833673031e-05 -8.548314777990975e-05\n",
      "Mask mean value:  tensor(0.4996, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6964  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.454e-17\n",
      "‖w_svm‖₂       : 0.06039485599757226\n",
      "‖alpha‖₁       : 0.8987025936722763\n",
      "scores min/max : -1.6923907474375168 3.7222774207579734\n",
      "Mask mean value:  tensor(0.4473, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5946  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.468e-03\n",
      "‖w_svm‖₂       : 1.0866991527286523e-06\n",
      "‖alpha‖₁       : 0.4999999999999897\n",
      "scores min/max : -2.2480126872755545e-07 -3.17662990961646e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.442e-19\n",
      "‖w_svm‖₂       : 0.0001568031938024794\n",
      "‖alpha‖₁       : 0.6399999999999998\n",
      "scores min/max : -0.00015802743229673547 -0.00014864552621247614\n",
      "Mask mean value:  tensor(0.4992, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7632  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.483e-17\n",
      "‖w_svm‖₂       : 7.530210669606568e-08\n",
      "‖alpha‖₁       : 0.13999999999997595\n",
      "scores min/max : 1.5186990352949503e-08 2.1050327572018728e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 7.416e-08\n",
      "‖w_svm‖₂       : 0.01829029352736089\n",
      "‖alpha‖₁       : 0.6599999999999935\n",
      "scores min/max : -0.0333929898596369 0.01869657049850005\n",
      "Mask mean value:  tensor(0.4316, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.3763  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.056e-15\n",
      "‖w_svm‖₂       : 0.005989719197340791\n",
      "‖alpha‖₁       : 0.5599999999999604\n",
      "scores min/max : 0.006727657708981068 0.008603288616225158\n",
      "Mask mean value:  tensor(0.5353, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1222  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.896e-05\n",
      "‖w_svm‖₂       : 0.030652757053196925\n",
      "‖alpha‖₁       : 0.5512728513143844\n",
      "scores min/max : -3.43407679012649 1.1338553258615862\n",
      "Mask mean value:  tensor(0.1766, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.3282  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.083e-03\n",
      "‖w_svm‖₂       : 2.848974779383694e-07\n",
      "‖alpha‖₁       : 0.6599999999999998\n",
      "scores min/max : -6.681210174864553e-07 -5.164848245356723e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.579e-18\n",
      "‖w_svm‖₂       : 0.06965334874884226\n",
      "‖alpha‖₁       : 0.6587375110232933\n",
      "scores min/max : -1.9956793622046507 4.4506963933505395\n",
      "Mask mean value:  tensor(0.4307, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.7305  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.831e-04\n",
      "‖w_svm‖₂       : 1.9426806167359122e-08\n",
      "‖alpha‖₁       : 0.1199999999999936\n",
      "scores min/max : -4.601501421136883e-08 -3.129829075204194e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.826e-09\n",
      "‖w_svm‖₂       : 0.0002943073914889332\n",
      "‖alpha‖₁       : 0.4199999999998446\n",
      "scores min/max : 0.00010844915625676192 0.000401924908084184\n",
      "Mask mean value:  tensor(0.5019, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9992  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.113e-15\n",
      "‖w_svm‖₂       : 0.04518588859705983\n",
      "‖alpha‖₁       : 0.9388913142428814\n",
      "scores min/max : -2.469064571462731 1.5823756495819827\n",
      "Mask mean value:  tensor(0.1430, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3974  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.161e-03\n",
      "‖w_svm‖₂       : 0.0002071254251255222\n",
      "‖alpha‖₁       : 0.6199999999965542\n",
      "scores min/max : -3.8089349914522644e-06 1.1391292272483044e-05\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.108e-17\n",
      "‖w_svm‖₂       : 0.13822139242859485\n",
      "‖alpha‖₁       : 0.8799999999998563\n",
      "scores min/max : -1.3298882155226006 3.6924389836381\n",
      "Mask mean value:  tensor(0.4097, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2666  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.289e-10\n",
      "‖w_svm‖₂       : 0.007242073119834659\n",
      "‖alpha‖₁       : 0.70058291843146\n",
      "scores min/max : -2.0025244719654665 0.02090921857883927\n",
      "Mask mean value:  tensor(0.4621, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2586  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.374e-13\n",
      "‖w_svm‖₂       : 4.4783499252981626e-07\n",
      "‖alpha‖₁       : 0.7199999999998705\n",
      "scores min/max : 1.8994190759514192e-07 2.4728441325690795e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.557e-21\n",
      "‖w_svm‖₂       : 3.5446463218231793e-06\n",
      "‖alpha‖₁       : 0.41999999999318705\n",
      "scores min/max : -3.8080291767224104e-06 -2.2125482038022148e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1229  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.761e-05\n",
      "‖w_svm‖₂       : 0.0005399038265995466\n",
      "‖alpha‖₁       : 0.43999999999999373\n",
      "scores min/max : -0.0009762497062191596 -0.0006703708790741824\n",
      "Mask mean value:  tensor(0.4954, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6400  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.423e-16\n",
      "‖w_svm‖₂       : 2.3172158241277097e-07\n",
      "‖alpha‖₁       : 0.2599999999999965\n",
      "scores min/max : 2.466534744502118e-07 2.594197890300867e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.818e-17\n",
      "‖w_svm‖₂       : 2.0273124584969274e-07\n",
      "‖alpha‖₁       : 0.3799999999999834\n",
      "scores min/max : -2.093057619079445e-08 1.4855904617268874e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.941e-19\n",
      "‖w_svm‖₂       : 0.032594691785820244\n",
      "‖alpha‖₁       : 0.9199999979571529\n",
      "scores min/max : -0.23262570532753027 0.1608041013615471\n",
      "Mask mean value:  tensor(0.1727, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8732  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 7.176e-03\n",
      "‖w_svm‖₂       : 0.015763739202624066\n",
      "‖alpha‖₁       : 0.596316968204748\n",
      "scores min/max : -2.7207834048297563 2.2717317053423933\n",
      "Mask mean value:  tensor(0.9154, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.5052  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.508e-12\n",
      "‖w_svm‖₂       : 0.026003376484928543\n",
      "‖alpha‖₁       : 0.19669636361893755\n",
      "scores min/max : -2.2044087765016034 0.01662903741619823\n",
      "Mask mean value:  tensor(0.0577, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.170e-04\n",
      "‖w_svm‖₂       : 1.1132727480489615e-06\n",
      "‖alpha‖₁       : 0.3199999999996833\n",
      "scores min/max : -2.5132601562735764e-06 -2.4052590171003937e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4897  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.227e-18\n",
      "‖w_svm‖₂       : 0.028752444282772967\n",
      "‖alpha‖₁       : 0.8985869824384602\n",
      "scores min/max : -0.731986080416658 2.027978987779695\n",
      "Mask mean value:  tensor(0.8660, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3093  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.951e-15\n",
      "‖w_svm‖₂       : 5.3611029974946995e-08\n",
      "‖alpha‖₁       : 0.11999999999998825\n",
      "scores min/max : -9.595510471056633e-08 -6.511571473251108e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.664e-08\n",
      "‖w_svm‖₂       : 0.009922993957942751\n",
      "‖alpha‖₁       : 0.6077202401690577\n",
      "scores min/max : -1.9957096383269874 0.29100540885208537\n",
      "Mask mean value:  tensor(0.5319, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2147  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.132e-14\n",
      "‖w_svm‖₂       : 0.012918604514763352\n",
      "‖alpha‖₁       : 0.8599999999999974\n",
      "scores min/max : -0.0471576858590729 -0.01033444247432315\n",
      "Mask mean value:  tensor(0.3443, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4264  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.859e-05\n",
      "‖w_svm‖₂       : 0.04680820178282273\n",
      "‖alpha‖₁       : 0.905667537662349\n",
      "scores min/max : -0.7128338685083093 1.8828219989945483\n",
      "Mask mean value:  tensor(0.3399, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6338  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.658e-03\n",
      "‖w_svm‖₂       : 7.73209437713827e-08\n",
      "‖alpha‖₁       : 0.17999999999999938\n",
      "scores min/max : -2.0175699955370144e-07 3.185314442877912e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.705e-21\n",
      "‖w_svm‖₂       : 1.0721644245339154e-07\n",
      "‖alpha‖₁       : 0.23999999999999805\n",
      "scores min/max : 7.823256326238583e-08 8.90175241154543e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.855e-21\n",
      "‖w_svm‖₂       : 0.020726162370575486\n",
      "‖alpha‖₁       : 0.7799999999999996\n",
      "scores min/max : -0.04652065522631106 0.014523998341745004\n",
      "Mask mean value:  tensor(0.3042, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.3470  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.277e-14\n",
      "‖w_svm‖₂       : 0.1828049193892191\n",
      "‖alpha‖₁       : 0.8834655910010327\n",
      "scores min/max : -3.6623144817409683 6.061487165860277\n",
      "Mask mean value:  tensor(0.0898, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1990  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.346e-03\n",
      "‖w_svm‖₂       : 0.03514430499565392\n",
      "‖alpha‖₁       : 0.6599999999999496\n",
      "scores min/max : -0.1695277694174736 0.10961229577920817\n",
      "Mask mean value:  tensor(0.4508, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8622  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.481e-13\n",
      "‖w_svm‖₂       : 5.8715666179404715e-08\n",
      "‖alpha‖₁       : 0.23999999999999846\n",
      "scores min/max : -5.918849694507979e-08 -4.487300287828697e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.749e-20\n",
      "‖w_svm‖₂       : 1.9821571614513857e-07\n",
      "‖alpha‖₁       : 0.41999999999996596\n",
      "scores min/max : -4.911805040332511e-07 -4.68985796191719e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.150e-19\n",
      "‖w_svm‖₂       : 5.2216092523799165e-08\n",
      "‖alpha‖₁       : 0.43999999999997563\n",
      "scores min/max : -2.0395510035225818e-07 -1.0010378398277801e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.872e-20\n",
      "‖w_svm‖₂       : 1.173857746050697e-07\n",
      "‖alpha‖₁       : 0.29999999999999616\n",
      "scores min/max : 1.70731354317364e-07 1.7970195454727677e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.609e-07\n",
      "‖w_svm‖₂       : 1.4429595866701767e-05\n",
      "‖alpha‖₁       : 0.35999999999556187\n",
      "scores min/max : 8.536395905543356e-06 9.387757206087845e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.739e-17\n",
      "‖w_svm‖₂       : 0.08509261947479067\n",
      "‖alpha‖₁       : 0.8733493687466374\n",
      "scores min/max : -12.133824225396767 2.177110066859443\n",
      "Mask mean value:  tensor(0.5873, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.539e-02\n",
      "‖w_svm‖₂       : 6.641470373889484e-07\n",
      "‖alpha‖₁       : 0.3999999999999997\n",
      "scores min/max : 5.498697145009485e-08 3.6107695220989567e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.355e-09\n",
      "‖w_svm‖₂       : 0.07230171867931277\n",
      "‖alpha‖₁       : 0.5799999999999412\n",
      "scores min/max : -2.7876815710900362 1.6545275364604748\n",
      "Mask mean value:  tensor(0.2665, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4317  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.959e-04\n",
      "‖w_svm‖₂       : 0.03679211805793356\n",
      "‖alpha‖₁       : 0.7294948380176532\n",
      "scores min/max : -0.268813552276584 2.014932633955456\n",
      "Mask mean value:  tensor(0.6538, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.9745  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 2.049e-03\n",
      "‖w_svm‖₂       : 0.05321499090938154\n",
      "‖alpha‖₁       : 0.5763488789029947\n",
      "scores min/max : -1.9595638977083973 0.8682638047304541\n",
      "Mask mean value:  tensor(0.7052, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9136  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.981e-04\n",
      "‖w_svm‖₂       : 0.0009316753392973287\n",
      "‖alpha‖₁       : 0.8199999999999992\n",
      "scores min/max : 0.0007147030961237441 0.00249384509699739\n",
      "Mask mean value:  tensor(0.5092, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5867  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.167e-16\n",
      "‖w_svm‖₂       : 0.0818349605265493\n",
      "‖alpha‖₁       : 0.47201117085554667\n",
      "scores min/max : -2.204524930730109 2.8710102028853752\n",
      "Mask mean value:  tensor(0.1710, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.8020  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.496e-02\n",
      "‖w_svm‖₂       : 3.9042884189925175e-07\n",
      "‖alpha‖₁       : 0.279999999999645\n",
      "scores min/max : -1.0824588709698797e-06 -1.0189998047834596e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.462e-19\n",
      "‖w_svm‖₂       : 0.02087883693216695\n",
      "‖alpha‖₁       : 0.8150938557473579\n",
      "scores min/max : -11.510413122925538 2.029695491510945\n",
      "Mask mean value:  tensor(0.7059, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9098  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.083e-06\n",
      "‖w_svm‖₂       : 1.0244057945495532e-06\n",
      "‖alpha‖₁       : 0.5999999999999943\n",
      "scores min/max : -2.445251229252413e-07 -1.9366576836127418e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.898e-19\n",
      "‖w_svm‖₂       : 0.02151748336262242\n",
      "‖alpha‖₁       : 0.383535408762598\n",
      "scores min/max : -1.957752509851268 0.24603182207523855\n",
      "Mask mean value:  tensor(0.7232, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1551  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.974e-14\n",
      "‖w_svm‖₂       : 0.003424842206815801\n",
      "‖alpha‖₁       : 0.5799999999999961\n",
      "scores min/max : 0.00543433447152376 0.006910184813970727\n",
      "Mask mean value:  tensor(0.5330, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3389  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.030e-13\n",
      "‖w_svm‖₂       : 1.7045014712904039e-07\n",
      "‖alpha‖₁       : 0.23999999999997906\n",
      "scores min/max : 2.1906773784334188e-07 2.34626887655968e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.074e-20\n",
      "‖w_svm‖₂       : 0.13693924285408435\n",
      "‖alpha‖₁       : 0.6549257518819943\n",
      "scores min/max : -18.023175504829585 1.90200831769427\n",
      "Mask mean value:  tensor(0.3564, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2462  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.050e-03\n",
      "‖w_svm‖₂       : 8.553523598473137e-08\n",
      "‖alpha‖₁       : 0.3799999999999837\n",
      "scores min/max : 2.4022129116487908e-08 4.2215323183670404e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.246e-22\n",
      "‖w_svm‖₂       : 7.625063119937988e-08\n",
      "‖alpha‖₁       : 0.5399999999999827\n",
      "scores min/max : -1.979499637360934e-07 -1.7637615787421914e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.242e-20\n",
      "‖w_svm‖₂       : 0.00013880127189687576\n",
      "‖alpha‖₁       : 0.43999999996188593\n",
      "scores min/max : -0.000290799889898367 -0.00015057019732430917\n",
      "Mask mean value:  tensor(0.4990, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0208  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.085e-17\n",
      "‖w_svm‖₂       : 1.1314682438317223e-07\n",
      "‖alpha‖₁       : 0.5199999999999871\n",
      "scores min/max : 2.748461847383563e-07 2.9625641861483126e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.179e-19\n",
      "‖w_svm‖₂       : 0.016541607993849808\n",
      "‖alpha‖₁       : 0.6816039175797486\n",
      "scores min/max : -1.9711240228018612 0.05478445252846567\n",
      "Mask mean value:  tensor(0.6169, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2665  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.684e-14\n",
      "‖w_svm‖₂       : 7.534034806091009e-08\n",
      "‖alpha‖₁       : 0.4199999999999999\n",
      "scores min/max : -6.863521683669112e-08 -2.1522362235980218e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 7.145e-08\n",
      "‖w_svm‖₂       : 0.004668455485712021\n",
      "‖alpha‖₁       : 0.459999999999991\n",
      "scores min/max : -0.01194696250376926 -0.009609676569963498\n",
      "Mask mean value:  tensor(0.4494, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9526  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.225e-15\n",
      "‖w_svm‖₂       : 0.06734333717212229\n",
      "‖alpha‖₁       : 0.41912660465306906\n",
      "scores min/max : -1.9372943693609384 2.0663055605133307\n",
      "Mask mean value:  tensor(0.7365, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8921  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.922e-04\n",
      "‖w_svm‖₂       : 0.05095542544343188\n",
      "‖alpha‖₁       : 0.827208759758129\n",
      "scores min/max : -1.9515517817636667 0.4012177718526582\n",
      "Mask mean value:  tensor(0.5182, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0135  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.714e-02\n",
      "‖w_svm‖₂       : 0.0003580519705054479\n",
      "‖alpha‖₁       : 0.7399999999999995\n",
      "scores min/max : -0.0004416104598597777 -0.00042525868920427164\n",
      "Mask mean value:  tensor(0.4979, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0597  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.572e-17\n",
      "‖w_svm‖₂       : 1.3290859131887192e-07\n",
      "‖alpha‖₁       : 0.3799999999999686\n",
      "scores min/max : -1.1442886370303792e-07 -9.929147216037386e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.236e-06\n",
      "‖w_svm‖₂       : 0.04872163420318668\n",
      "‖alpha‖₁       : 0.9412016441374402\n",
      "scores min/max : -1.8237506718892007 0.3102145777296763\n",
      "Mask mean value:  tensor(0.4466, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3247  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.920e-14\n",
      "‖w_svm‖₂       : 0.005903222287758931\n",
      "‖alpha‖₁       : 0.3799999999999994\n",
      "scores min/max : -0.0013753167093423571 0.009006241155034985\n",
      "Mask mean value:  tensor(0.5386, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9550  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.897e-14\n",
      "‖w_svm‖₂       : 0.040508976653455736\n",
      "‖alpha‖₁       : 0.8900017856762492\n",
      "scores min/max : -2.0109663346239657 0.3368413815967532\n",
      "Mask mean value:  tensor(0.2577, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0570  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.612e-06\n",
      "‖w_svm‖₂       : 0.03136357583278185\n",
      "‖alpha‖₁       : 0.8525258416742493\n",
      "scores min/max : -2.9157532926485326 1.5651047927667392\n",
      "Mask mean value:  tensor(0.1555, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9512  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.726e-03\n",
      "‖w_svm‖₂       : 2.450740298598054e-07\n",
      "‖alpha‖₁       : 0.58\n",
      "scores min/max : -2.3057394799978687e-07 -2.1495106632882476e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.220e-19\n",
      "‖w_svm‖₂       : 0.0846275670261618\n",
      "‖alpha‖₁       : 0.577965822501602\n",
      "scores min/max : -2.1040987525455397 5.810600171260288\n",
      "Mask mean value:  tensor(0.3036, dtype=torch.float64)\n",
      "max feasible return = 0.1150  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.013439538850322e-07\n",
      "‖alpha‖₁       : 0.5799999999999979\n",
      "scores min/max : -3.1185739858980066e-07 -2.2104163107972042e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0871592625049479e-07\n",
      "‖alpha‖₁       : 0.2999999999999733\n",
      "scores min/max : 4.069495821994754e-08 4.580687462645056e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 6.609053582183664e-08\n",
      "‖alpha‖₁       : 0.5999999999999994\n",
      "scores min/max : 5.988667067947922e-09 4.273218148096705e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.537731425314172e-08\n",
      "‖alpha‖₁       : 0.37999999999999956\n",
      "scores min/max : -2.4235241101006832e-08 -1.4859136836341543e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.157097435961075e-06\n",
      "‖alpha‖₁       : 0.31999999996363965\n",
      "scores min/max : 3.389433099917208e-06 5.83726488357305e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4258  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04345295199039139\n",
      "‖alpha‖₁       : 0.7397120526662931\n",
      "scores min/max : -3.4531336021384136 2.17781773208267\n",
      "Mask mean value:  tensor(0.9012, dtype=torch.float64)\n",
      "max feasible return = -0.9431  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.024086982168802524\n",
      "‖alpha‖₁       : 0.6151709246348698\n",
      "scores min/max : -1.9469005558608203 1.1803355270712486\n",
      "Mask mean value:  tensor(0.7131, dtype=torch.float64)\n",
      "max feasible return = 2.4835  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.776984031991923e-07\n",
      "‖alpha‖₁       : 0.45999999999999214\n",
      "scores min/max : 9.584870212982136e-08 6.903380221090846e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.4183506781345185e-07\n",
      "‖alpha‖₁       : 0.5199999999999902\n",
      "scores min/max : 9.023568615194528e-08 1.031469017381327e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.6282453253943875e-07\n",
      "‖alpha‖₁       : 0.579999999999989\n",
      "scores min/max : -2.2433831386797867e-07 -1.8804123191615433e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.04200893333173261\n",
      "‖alpha‖₁       : 0.6878402604346106\n",
      "scores min/max : -0.43402389165714195 1.9561249180451927\n",
      "Mask mean value:  tensor(0.3908, dtype=torch.float64)\n",
      "max feasible return = 0.0362  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03246378203807282\n",
      "‖alpha‖₁       : 0.42234962405018855\n",
      "scores min/max : -3.8559596379250816 5.305709647063289\n",
      "Mask mean value:  tensor(0.1152, dtype=torch.float64)\n",
      "max feasible return = 0.7359  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04291864570534065\n",
      "‖alpha‖₁       : 0.8211744301842296\n",
      "scores min/max : -4.911938403465249 3.2592992828769995\n",
      "Mask mean value:  tensor(0.9675, dtype=torch.float64)\n",
      "max feasible return = -3.7052  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.005939472549948349\n",
      "‖alpha‖₁       : 0.7999999999999997\n",
      "scores min/max : -0.025130972059385276 -0.0012261326314020621\n",
      "Mask mean value:  tensor(0.4639, dtype=torch.float64)\n",
      "max feasible return = -0.1687  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.158486605722113e-07\n",
      "‖alpha‖₁       : 0.6199999999999779\n",
      "scores min/max : 6.889234931462655e-08 8.537986219065787e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.899226090256103e-08\n",
      "‖alpha‖₁       : 0.43999999999993883\n",
      "scores min/max : 4.137147343885063e-09 1.3727117986250952e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.3550297964525902e-07\n",
      "‖alpha‖₁       : 0.27999999999998915\n",
      "scores min/max : -1.9193559817718985e-07 -1.741070727923993e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  54 | train 0.005304 | val 0.006619\n",
      "-----------------------------------------Epoch:  55 ----------------------------------------\n",
      "‖w_svm‖₂       : 0.05278378287224659\n",
      "‖alpha‖₁       : 0.5763243653901744\n",
      "scores min/max : -1.9590836814401642 0.8688594673307554\n",
      "Mask mean value:  tensor(0.7069, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9182  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.001e-03\n",
      "‖w_svm‖₂       : 7.474347713864108e-08\n",
      "‖alpha‖₁       : 0.13999999999998364\n",
      "scores min/max : 1.3541159096512254e-08 1.93816717881679e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 7.260e-08\n",
      "‖w_svm‖₂       : 1.1293185887988194e-07\n",
      "‖alpha‖₁       : 0.5199999999999966\n",
      "scores min/max : 2.6045751981081206e-07 2.8187500132666964e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.162e-19\n",
      "‖w_svm‖₂       : 0.01801792995109517\n",
      "‖alpha‖₁       : 0.659999999999997\n",
      "scores min/max : -0.03196469760750836 0.018599508135299272\n",
      "Mask mean value:  tensor(0.4357, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.4071  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.953e-15\n",
      "‖w_svm‖₂       : 0.06928247199348492\n",
      "‖alpha‖₁       : 0.6587153525200627\n",
      "scores min/max : -1.9651419130604353 4.48585825911957\n",
      "Mask mean value:  tensor(0.5016, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.2077  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.168e-05\n",
      "‖w_svm‖₂       : 0.13658583436963007\n",
      "‖alpha‖₁       : 0.6548492362083504\n",
      "scores min/max : -18.01806354131002 1.9098756772257242\n",
      "Mask mean value:  tensor(0.3686, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2375  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.767e-03\n",
      "‖w_svm‖₂       : 0.006017166126021978\n",
      "‖alpha‖₁       : 0.5599999999999835\n",
      "scores min/max : 0.007155128856315413 0.00904524021110528\n",
      "Mask mean value:  tensor(0.5374, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1267  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.977e-05\n",
      "‖w_svm‖₂       : 7.658650541832938e-08\n",
      "‖alpha‖₁       : 0.6599999999999762\n",
      "scores min/max : 8.493322394002259e-08 2.301019696004737e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.714e-08\n",
      "‖w_svm‖₂       : 0.04051166993967999\n",
      "‖alpha‖₁       : 0.8900022609547318\n",
      "scores min/max : -2.011310540854314 0.33648179408635054\n",
      "Mask mean value:  tensor(0.2567, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0565  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.302e-05\n",
      "‖w_svm‖₂       : 2.0026751921728002e-07\n",
      "‖alpha‖₁       : 0.379999999999985\n",
      "scores min/max : 6.091660188289392e-09 4.1867919223576476e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.823e-19\n",
      "‖w_svm‖₂       : 0.00015746658285483348\n",
      "‖alpha‖₁       : 0.6399999999999999\n",
      "scores min/max : -0.00016549316388818027 -0.00015603171689801475\n",
      "Mask mean value:  tensor(0.4992, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7632  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.565e-17\n",
      "‖w_svm‖₂       : 0.06004119312289906\n",
      "‖alpha‖₁       : 0.8986807111814884\n",
      "scores min/max : -1.69010079728965 3.7249456503400333\n",
      "Mask mean value:  tensor(0.4524, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5987  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.339e-03\n",
      "‖w_svm‖₂       : 0.0005393103514069329\n",
      "‖alpha‖₁       : 0.43999999999995776\n",
      "scores min/max : -0.0009942488647040279 -0.0006897428087787572\n",
      "Mask mean value:  tensor(0.4953, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6397  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.422e-16\n",
      "‖w_svm‖₂       : 0.02840412781275849\n",
      "‖alpha‖₁       : 0.8985675925862177\n",
      "scores min/max : -0.7308928141986898 2.0290443978480237\n",
      "Mask mean value:  tensor(0.8674, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3068  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.764e-15\n",
      "‖w_svm‖₂       : 6.589194230712972e-07\n",
      "‖alpha‖₁       : 0.3999999999999998\n",
      "scores min/max : 5.26435659963713e-08 3.589801860495567e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.311e-09\n",
      "‖w_svm‖₂       : 0.020896298991898113\n",
      "‖alpha‖₁       : 0.8150905393357851\n",
      "scores min/max : -11.513457478862085 2.026480744859935\n",
      "Mask mean value:  tensor(0.6989, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.8933  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.761e-06\n",
      "‖w_svm‖₂       : 7.519529299727859e-08\n",
      "‖alpha‖₁       : 0.42\n",
      "scores min/max : -6.863660796389764e-08 -2.155175783711493e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 7.144e-08\n",
      "‖w_svm‖₂       : 0.021575563351639723\n",
      "‖alpha‖₁       : 0.38353839045330934\n",
      "scores min/max : -1.958889567122815 0.24490532917283298\n",
      "Mask mean value:  tensor(0.7192, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1545  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.340e-14\n",
      "‖w_svm‖₂       : 0.03147571911353934\n",
      "‖alpha‖₁       : 0.8525340013509319\n",
      "scores min/max : -2.9136275974982584 1.5672181427128882\n",
      "Mask mean value:  tensor(0.1565, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9523  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.735e-03\n",
      "‖w_svm‖₂       : 3.9842603729256875e-06\n",
      "‖alpha‖₁       : 0.41999999998833226\n",
      "scores min/max : -4.1681628627230785e-06 -2.4165147602659106e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1229  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.965e-05\n",
      "‖w_svm‖₂       : 0.00020799946108182967\n",
      "‖alpha‖₁       : 0.6199999999960182\n",
      "scores min/max : -1.1314856423101199e-05 4.016228119685532e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.121e-17\n",
      "‖w_svm‖₂       : 0.04861377612879491\n",
      "‖alpha‖₁       : 0.9412015290333143\n",
      "scores min/max : -1.823625348502011 0.31041115412663123\n",
      "Mask mean value:  tensor(0.4508, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3245  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.618e-09\n",
      "‖w_svm‖₂       : 0.012812490544760406\n",
      "‖alpha‖₁       : 0.8599999999999979\n",
      "scores min/max : -0.046695890525772096 -0.010216324540343016\n",
      "Mask mean value:  tensor(0.3459, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4281  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.723e-05\n",
      "‖w_svm‖₂       : 5.216069369642297e-08\n",
      "‖alpha‖₁       : 0.4399999999999815\n",
      "scores min/max : -2.0102105428977018e-07 -9.710329641861608e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.946e-20\n",
      "‖w_svm‖₂       : 2.391131993455002e-07\n",
      "‖alpha‖₁       : 0.6399999999999519\n",
      "scores min/max : 3.9621659474680816e-07 4.173425040522284e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.122e-19\n",
      "‖w_svm‖₂       : 1.330817580545071e-07\n",
      "‖alpha‖₁       : 0.3799999999999666\n",
      "scores min/max : -1.1875334648970544e-07 -1.0361082351270982e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.619e-07\n",
      "‖w_svm‖₂       : 7.790781025260024e-08\n",
      "‖alpha‖₁       : 0.17999999999999933\n",
      "scores min/max : -1.974013927534469e-07 3.6149350173761464e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.829e-21\n",
      "‖w_svm‖₂       : 0.0002964606291062088\n",
      "‖alpha‖₁       : 0.4199999999999053\n",
      "scores min/max : 5.753648779344145e-05 0.0003553258525893479\n",
      "Mask mean value:  tensor(0.5016, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9988  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.117e-15\n",
      "‖w_svm‖₂       : 0.0001379516530403588\n",
      "‖alpha‖₁       : 0.43999999999124406\n",
      "scores min/max : -0.0002872920100577337 -0.0001480624118554405\n",
      "Mask mean value:  tensor(0.4990, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0209  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.064e-17\n",
      "‖w_svm‖₂       : 1.9304969463064617e-08\n",
      "‖alpha‖₁       : 0.11999999999999364\n",
      "scores min/max : -5.0441657595753426e-08 -3.572508690814881e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.840e-09\n",
      "‖w_svm‖₂       : 0.0009284838153105899\n",
      "‖alpha‖₁       : 0.8199999999999993\n",
      "scores min/max : 0.0005837573778590226 0.0023587716066846374\n",
      "Mask mean value:  tensor(0.5085, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5833  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.171e-16\n",
      "‖w_svm‖₂       : 0.0034419707175336826\n",
      "‖alpha‖₁       : 0.5799999999999995\n",
      "scores min/max : 0.006141021795375801 0.007632592311405589\n",
      "Mask mean value:  tensor(0.5365, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3412  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.806e-13\n",
      "‖w_svm‖₂       : 0.005937838807886684\n",
      "‖alpha‖₁       : 0.3799999999999999\n",
      "scores min/max : -0.0005123317433581957 0.009991460592711643\n",
      "Mask mean value:  tensor(0.5435, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9636  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.952e-04\n",
      "‖w_svm‖₂       : 0.08121266739714712\n",
      "‖alpha‖₁       : 0.47192300987702945\n",
      "scores min/max : -2.164951142928386 2.909847553325409\n",
      "Mask mean value:  tensor(0.2176, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.2552  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.222e-03\n",
      "‖w_svm‖₂       : 0.0003575811909133557\n",
      "‖alpha‖₁       : 0.7399999999999995\n",
      "scores min/max : -0.000452952531705319 -0.00043664371371766235\n",
      "Mask mean value:  tensor(0.4978, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0596  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.581e-17\n",
      "‖w_svm‖₂       : 4.975733263289429e-08\n",
      "‖alpha‖₁       : 0.17999999999999872\n",
      "scores min/max : 6.09019537027878e-08 7.553139694131473e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.394e-08\n",
      "‖w_svm‖₂       : 1.0261950330402172e-06\n",
      "‖alpha‖₁       : 0.5999999999999945\n",
      "scores min/max : -2.50331422174042e-07 -1.994712409484395e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.920e-19\n",
      "‖w_svm‖₂       : 7.64005033177794e-08\n",
      "‖alpha‖₁       : 0.5399999999999884\n",
      "scores min/max : -1.9884439700236212e-07 -1.7728166116692738e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.269e-20\n",
      "‖w_svm‖₂       : 0.03652725191597247\n",
      "‖alpha‖₁       : 0.7294940358780919\n",
      "scores min/max : -0.2672938957735032 2.0165572189459473\n",
      "Mask mean value:  tensor(0.6584, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.9945  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.986e-03\n",
      "‖w_svm‖₂       : 0.030920916266146754\n",
      "‖alpha‖₁       : 0.5512864874270516\n",
      "scores min/max : -3.4315170915243685 1.1364955341134029\n",
      "Mask mean value:  tensor(0.1788, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.3401  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.129e-03\n",
      "‖w_svm‖₂       : 5.8426719972275e-08\n",
      "‖alpha‖₁       : 0.23999999999999128\n",
      "scores min/max : -5.402034208111518e-08 -3.9548729698649665e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.716e-20\n",
      "‖w_svm‖₂       : 0.015683108690698087\n",
      "‖alpha‖₁       : 0.8599999999999952\n",
      "scores min/max : -0.06716767743130615 -0.056241297432172925\n",
      "Mask mean value:  tensor(0.2352, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0604  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.111e-14\n",
      "‖w_svm‖₂       : 0.032369236153512546\n",
      "‖alpha‖₁       : 0.9199999999999997\n",
      "scores min/max : -0.23511519931143632 0.15142217464292185\n",
      "Mask mean value:  tensor(0.1627, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8229  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 9.929e-03\n",
      "‖w_svm‖₂       : 0.00018181241133138077\n",
      "‖alpha‖₁       : 0.8199999999999996\n",
      "scores min/max : -9.331427706054816e-05 -8.031581229185397e-05\n",
      "Mask mean value:  tensor(0.4996, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6964  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.457e-17\n",
      "‖w_svm‖₂       : 3.978489517848833e-07\n",
      "‖alpha‖₁       : 0.27999999999939784\n",
      "scores min/max : -1.0842436954123584e-06 -1.0193676549560648e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.565e-19\n",
      "‖w_svm‖₂       : 2.8399059973716054e-07\n",
      "‖alpha‖₁       : 0.6599999999999998\n",
      "scores min/max : -6.400934430527947e-07 -4.884587319893918e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.560e-18\n",
      "‖w_svm‖₂       : 0.08535670708900829\n",
      "‖alpha‖₁       : 0.8733521427867927\n",
      "scores min/max : -12.173135441797116 2.137333769644451\n",
      "Mask mean value:  tensor(0.4895, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.4579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.838e-02\n",
      "‖w_svm‖₂       : 1.0660266185954407e-07\n",
      "‖alpha‖₁       : 0.2399999999999992\n",
      "scores min/max : 8.634657038492298e-08 9.71053025416781e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.879e-21\n",
      "‖w_svm‖₂       : 0.045169299390715026\n",
      "‖alpha‖₁       : 0.9388800836651888\n",
      "scores min/max : -2.4722542768761886 1.579096924004234\n",
      "Mask mean value:  tensor(0.1410, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3935  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.074e-03\n",
      "‖w_svm‖₂       : 1.1013594424909185e-06\n",
      "‖alpha‖₁       : 0.3199999999997848\n",
      "scores min/max : -2.454102748893109e-06 -2.346959373222299e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4897  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.224e-18\n",
      "‖w_svm‖₂       : 1.0838170904264805e-06\n",
      "‖alpha‖₁       : 0.49999999999998945\n",
      "scores min/max : -2.2576664778019173e-07 -3.271915402082223e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.428e-19\n",
      "‖w_svm‖₂       : 0.14523561132411722\n",
      "‖alpha‖₁       : 0.7591128641194765\n",
      "scores min/max : -1.7772718825272855 2.1997921692020723\n",
      "Mask mean value:  tensor(0.8244, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3551  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.362e-11\n",
      "‖w_svm‖₂       : 0.020807527236153006\n",
      "‖alpha‖₁       : 0.7799999999999996\n",
      "scores min/max : -0.049185647510257294 0.0121036843023547\n",
      "Mask mean value:  tensor(0.2933, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.3005  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.405e-14\n",
      "‖w_svm‖₂       : 5.356933449954807e-08\n",
      "‖alpha‖₁       : 0.11999999999999145\n",
      "scores min/max : -9.533940557269408e-08 -6.454502617441572e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.916e-08\n",
      "‖w_svm‖₂       : 0.03472513981932736\n",
      "‖alpha‖₁       : 0.6599999999999648\n",
      "scores min/max : -0.16689507401880166 0.10613941347825429\n",
      "Mask mean value:  tensor(0.4466, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8556  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.285e-13\n",
      "‖w_svm‖₂       : 0.025795331153194042\n",
      "‖alpha‖₁       : 0.19668833791634271\n",
      "scores min/max : -2.201332296777136 0.019707650631684604\n",
      "Mask mean value:  tensor(0.0606, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4096  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.723e-04\n",
      "‖w_svm‖₂       : 0.07280377839745888\n",
      "‖alpha‖₁       : 0.5799999999999702\n",
      "scores min/max : -2.817486306785883 1.6847612403263195\n",
      "Mask mean value:  tensor(0.2862, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4531  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.315e-04\n",
      "‖w_svm‖₂       : 0.016016251075280652\n",
      "‖alpha‖₁       : 0.5963255435606202\n",
      "scores min/max : -2.720800883525432 2.2729509515197166\n",
      "Mask mean value:  tensor(0.9160, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.5059  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.520e-08\n",
      "‖w_svm‖₂       : 1.4408644573386514e-05\n",
      "‖alpha‖₁       : 0.3599999999956489\n",
      "scores min/max : 8.480508370986543e-06 9.330822826929786e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.755e-17\n",
      "‖w_svm‖₂       : 0.010063047096130665\n",
      "‖alpha‖₁       : 0.6077229306691885\n",
      "scores min/max : -1.9959605728123244 0.2907269418827991\n",
      "Mask mean value:  tensor(0.5307, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2142  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.176e-14\n",
      "‖w_svm‖₂       : 0.016695756486359865\n",
      "‖alpha‖₁       : 0.6816082035601603\n",
      "scores min/max : -1.970013013449295 0.05589259449480821\n",
      "Mask mean value:  tensor(0.6219, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2692  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.629e-14\n",
      "‖w_svm‖₂       : 0.051108495565872686\n",
      "‖alpha‖₁       : 0.8272265870142295\n",
      "scores min/max : -1.9460184147264903 0.40690169683427546\n",
      "Mask mean value:  tensor(0.5378, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0172  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.603e-02\n",
      "‖w_svm‖₂       : 0.020319328852250025\n",
      "‖alpha‖₁       : 0.8233876456891133\n",
      "scores min/max : -1.9145151953435304 1.4805932643380393\n",
      "Mask mean value:  tensor(0.6554, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.2791  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.651e-05\n",
      "‖w_svm‖₂       : 0.007310264821711873\n",
      "‖alpha‖₁       : 0.7005838856475898\n",
      "scores min/max : -2.002437821150556 0.020998577960065705\n",
      "Mask mean value:  tensor(0.4625, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2588  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.677e-12\n",
      "‖w_svm‖₂       : 2.4675032967676973e-07\n",
      "‖alpha‖₁       : 0.5799999999999903\n",
      "scores min/max : -2.713577324542657e-07 -2.5570949420707814e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.297e-19\n",
      "‖w_svm‖₂       : 0.004641154691456201\n",
      "‖alpha‖₁       : 0.45999999999999885\n",
      "scores min/max : -0.011479511970870273 -0.009165050889793094\n",
      "Mask mean value:  tensor(0.4517, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9623  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.216e-15\n",
      "‖w_svm‖₂       : 0.04662558886481867\n",
      "‖alpha‖₁       : 0.9056654011472038\n",
      "scores min/max : -0.7126551129725479 1.8824208914152827\n",
      "Mask mean value:  tensor(0.3382, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6304  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.707e-03\n",
      "‖w_svm‖₂       : 1.1724003092069675e-07\n",
      "‖alpha‖₁       : 0.29999999999999727\n",
      "scores min/max : 1.7206820373559147e-07 1.8102839336597201e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.631e-07\n",
      "‖w_svm‖₂       : 0.06800219919654635\n",
      "‖alpha‖₁       : 0.41921984489737385\n",
      "scores min/max : -1.9275455574238038 2.075348288737035\n",
      "Mask mean value:  tensor(0.7471, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8969  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.306e-04\n",
      "‖w_svm‖₂       : 8.595012639285381e-08\n",
      "‖alpha‖₁       : 0.3799999999999728\n",
      "scores min/max : 1.8375376781778123e-08 3.657966649623675e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.366e-22\n",
      "‖w_svm‖₂       : 0.13801908455982498\n",
      "‖alpha‖₁       : 0.8799999999999999\n",
      "scores min/max : -1.3175029714037478 3.6964829330967395\n",
      "Mask mean value:  tensor(0.4337, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.918e-10\n",
      "‖w_svm‖₂       : 1.7100272165859417e-07\n",
      "‖alpha‖₁       : 0.23999999999997793\n",
      "scores min/max : 2.1626776180712403e-07 2.3185228392242785e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.167e-20\n",
      "‖w_svm‖₂       : 0.18239893046981695\n",
      "‖alpha‖₁       : 0.8834347203052639\n",
      "scores min/max : -3.6583650005237685 6.069598255160335\n",
      "Mask mean value:  tensor(0.0912, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2043  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.326e-05\n",
      "‖w_svm‖₂       : 4.4618244124236006e-07\n",
      "‖alpha‖₁       : 0.7199999999998747\n",
      "scores min/max : 1.7501269009953462e-07 2.3235864248957895e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.569e-21\n",
      "‖w_svm‖₂       : 1.985537393598293e-07\n",
      "‖alpha‖₁       : 0.4199999999999667\n",
      "scores min/max : -4.899123232414021e-07 -4.6770397105133024e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.181e-19\n",
      "‖w_svm‖₂       : 2.3122683549540752e-07\n",
      "‖alpha‖₁       : 0.259999999999996\n",
      "scores min/max : 2.59477191350718e-07 2.7224758331506464e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.864e-17\n",
      "‖w_svm‖₂       : 0.08528386431451852\n",
      "‖alpha‖₁       : 0.5780806933865968\n",
      "scores min/max : -2.125894003985002 5.788511450047623\n",
      "Mask mean value:  tensor(0.2627, dtype=torch.float64)\n",
      "max feasible return = 0.1077  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.0207734663912317e-07\n",
      "‖alpha‖₁       : 0.5799999999999974\n",
      "scores min/max : -3.169685990080867e-07 -2.2612857239164298e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0871213570653279e-07\n",
      "‖alpha‖₁       : 0.29999999999997373\n",
      "scores min/max : 4.504790039300276e-08 5.0147567594934526e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 6.581145135428932e-08\n",
      "‖alpha‖₁       : 0.5999999999999994\n",
      "scores min/max : 4.67761414116348e-09 4.1416186050418675e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.5285324602513444e-08\n",
      "‖alpha‖₁       : 0.3799999999999995\n",
      "scores min/max : -2.3909420779806292e-08 -1.4532128019986882e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.145434833110711e-06\n",
      "‖alpha‖₁       : 0.3199999999608145\n",
      "scores min/max : 3.6080224290620756e-06 5.995858863067765e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4258  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.043000446052271164\n",
      "‖alpha‖₁       : 0.7397292662406636\n",
      "scores min/max : -3.4378181515069786 2.192995480331647\n",
      "Mask mean value:  tensor(0.9105, dtype=torch.float64)\n",
      "max feasible return = -0.9488  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.023798807128084628\n",
      "‖alpha‖₁       : 0.6151731011094309\n",
      "scores min/max : -1.9535366996638053 1.173510104693545\n",
      "Mask mean value:  tensor(0.6937, dtype=torch.float64)\n",
      "max feasible return = 2.4160  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.79180833779024e-07\n",
      "‖alpha‖₁       : 0.4599999999999915\n",
      "scores min/max : 1.2569671509396747e-07 7.201973149199999e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.4265990275415793e-07\n",
      "‖alpha‖₁       : 0.5199999999999907\n",
      "scores min/max : 8.919067707791463e-08 1.0210096207129082e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.6443098460601524e-07\n",
      "‖alpha‖₁       : 0.5799999999999781\n",
      "scores min/max : -2.3832848534775785e-07 -2.0196672284147167e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.04203732617172785\n",
      "‖alpha‖₁       : 0.6878583166789858\n",
      "scores min/max : -0.4426003916550707 1.9476582592968212\n",
      "Mask mean value:  tensor(0.3609, dtype=torch.float64)\n",
      "max feasible return = 0.0649  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03245115137652139\n",
      "‖alpha‖₁       : 0.42236576669787784\n",
      "scores min/max : -3.852912249743599 5.310421446892366\n",
      "Mask mean value:  tensor(0.1180, dtype=torch.float64)\n",
      "max feasible return = 0.7549  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.042925049827725506\n",
      "‖alpha‖₁       : 0.8211777464334165\n",
      "scores min/max : -4.911152417947522 3.2601841183786657\n",
      "Mask mean value:  tensor(0.9675, dtype=torch.float64)\n",
      "max feasible return = -3.7054  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.005841111611656267\n",
      "‖alpha‖₁       : 0.7999999999999999\n",
      "scores min/max : -0.02532776105638959 -0.0022843714681754093\n",
      "Mask mean value:  tensor(0.4597, dtype=torch.float64)\n",
      "max feasible return = -0.1672  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.1697419851066636e-07\n",
      "‖alpha‖₁       : 0.6199999999999777\n",
      "scores min/max : 7.839527980800706e-08 9.488387282346074e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.9327974962315976e-08\n",
      "‖alpha‖₁       : 0.43999999999994155\n",
      "scores min/max : 4.483704998729124e-09 1.407288978900313e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.3337155825637058e-07\n",
      "‖alpha‖₁       : 0.279999999999999\n",
      "scores min/max : -2.0623254218437856e-07 -1.8862354190751018e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  55 | train 0.005350 | val 0.006637\n",
      "-----------------------------------------Epoch:  56 ----------------------------------------\n",
      "‖w_svm‖₂       : 7.613528243269557e-08\n",
      "‖alpha‖₁       : 0.42000000000000004\n",
      "scores min/max : -6.547300290232889e-08 -1.835480816534557e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 7.322e-08\n",
      "‖w_svm‖₂       : 0.03162012377344661\n",
      "‖alpha‖₁       : 0.8525400076459941\n",
      "scores min/max : -2.8869832867370713 1.5939190945508015\n",
      "Mask mean value:  tensor(0.1702, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9648  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.081e-03\n",
      "‖w_svm‖₂       : 0.0001378573409321898\n",
      "‖alpha‖₁       : 0.4399999999939742\n",
      "scores min/max : -0.0002925239803549778 -0.0001531992384042832\n",
      "Mask mean value:  tensor(0.4990, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0207  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.135e-17\n",
      "‖w_svm‖₂       : 1.085214793830556e-06\n",
      "‖alpha‖₁       : 0.49999999999998945\n",
      "scores min/max : -2.2196932092964051e-07 -2.892472945037226e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.467e-19\n",
      "‖w_svm‖₂       : 2.3861160492550275e-07\n",
      "‖alpha‖₁       : 0.639999999999999\n",
      "scores min/max : 4.0903030860715624e-07 4.2986743306234594e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.149e-19\n",
      "‖w_svm‖₂       : 0.007332628905475852\n",
      "‖alpha‖₁       : 0.7005842294595341\n",
      "scores min/max : -2.0017887291685117 0.021646693838083268\n",
      "Mask mean value:  tensor(0.4657, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2605  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.163e-13\n",
      "‖w_svm‖₂       : 1.1726798852122467e-07\n",
      "‖alpha‖₁       : 0.29999999999999727\n",
      "scores min/max : 1.7217045372827815e-07 1.8112909376520292e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.632e-07\n",
      "‖w_svm‖₂       : 0.1378320854685682\n",
      "‖alpha‖₁       : 0.6552116303605957\n",
      "scores min/max : -18.07896853628716 1.8516175545465294\n",
      "Mask mean value:  tensor(0.2896, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2762  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.342e-03\n",
      "‖w_svm‖₂       : 7.793524112377578e-08\n",
      "‖alpha‖₁       : 0.1799999999999994\n",
      "scores min/max : -2.0302880701227597e-07 3.05828498521213e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.940e-21\n",
      "‖w_svm‖₂       : 0.01674430485558751\n",
      "‖alpha‖₁       : 0.6816097707045148\n",
      "scores min/max : -1.9695581708553096 0.05634581698341645\n",
      "Mask mean value:  tensor(0.6240, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2704  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.494e-14\n",
      "‖w_svm‖₂       : 0.02063405117485947\n",
      "‖alpha‖₁       : 0.78\n",
      "scores min/max : -0.05115597169892595 0.008539130773068282\n",
      "Mask mean value:  tensor(0.2846, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.2616  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.530e-14\n",
      "‖w_svm‖₂       : 1.3116351358603793e-07\n",
      "‖alpha‖₁       : 0.3799999999999991\n",
      "scores min/max : -1.1830067028322973e-07 -1.0334078250400181e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.596e-07\n",
      "‖w_svm‖₂       : 0.031892294699398624\n",
      "‖alpha‖₁       : 0.9199999999999786\n",
      "scores min/max : -0.22834043982825247 0.14791995482062525\n",
      "Mask mean value:  tensor(0.1675, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8487  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 9.299e-03\n",
      "‖w_svm‖₂       : 0.036178503660421406\n",
      "‖alpha‖₁       : 0.7294997419168365\n",
      "scores min/max : -0.26949138100570685 2.014711592162144\n",
      "Mask mean value:  tensor(0.6516, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.9667  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 2.084e-03\n",
      "‖w_svm‖₂       : 0.04065761748320935\n",
      "‖alpha‖₁       : 0.8900232434633364\n",
      "scores min/max : -2.01228433063402 0.3355024416038297\n",
      "Mask mean value:  tensor(0.2538, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0548  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.775e-05\n",
      "‖w_svm‖₂       : 7.45962609146994e-08\n",
      "‖alpha‖₁       : 0.13999999999999707\n",
      "scores min/max : 1.5119724214944745e-08 2.089145833903516e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 7.462e-08\n",
      "‖w_svm‖₂       : 1.7085811279679422e-07\n",
      "‖alpha‖₁       : 0.23999999999997834\n",
      "scores min/max : 2.0840110269352105e-07 2.2397249164168923e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.171e-20\n",
      "‖w_svm‖₂       : 0.015393719395148433\n",
      "‖alpha‖₁       : 0.8599999999999912\n",
      "scores min/max : -0.05743980629968731 -0.0469758931131886\n",
      "Mask mean value:  tensor(0.2707, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0713  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.402e-14\n",
      "‖w_svm‖₂       : 0.021897927097250088\n",
      "‖alpha‖₁       : 0.38355190418864005\n",
      "scores min/max : -1.9549384284932234 0.2488466634430238\n",
      "Mask mean value:  tensor(0.7331, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1565  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.364e-10\n",
      "‖w_svm‖₂       : 3.8673088359825317e-07\n",
      "‖alpha‖₁       : 0.279999999999933\n",
      "scores min/max : -1.0244296761923226e-06 -9.61964876072965e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.440e-19\n",
      "‖w_svm‖₂       : 5.0023779187959514e-08\n",
      "‖alpha‖₁       : 0.1799999999999949\n",
      "scores min/max : 5.5026041832328266e-08 6.980941888916591e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.299e-08\n",
      "‖w_svm‖₂       : 1.0261487181310285e-06\n",
      "‖alpha‖₁       : 0.5999999999999945\n",
      "scores min/max : -2.458823053233433e-07 -1.950248476748718e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.886e-19\n",
      "‖w_svm‖₂       : 0.010103485560197427\n",
      "‖alpha‖₁       : 0.607723733745419\n",
      "scores min/max : -1.9964355553406876 0.2902486834604687\n",
      "Mask mean value:  tensor(0.5286, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2133  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.205e-14\n",
      "‖w_svm‖₂       : 0.020675199042242035\n",
      "‖alpha‖₁       : 0.8150937275268746\n",
      "scores min/max : -11.507668891356795 2.0318601209554363\n",
      "Mask mean value:  tensor(0.7106, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9215  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.537e-06\n",
      "‖w_svm‖₂       : 0.005857993766780165\n",
      "‖alpha‖₁       : 0.3799999999999992\n",
      "scores min/max : -0.0018438008967435748 0.008364566162176599\n",
      "Mask mean value:  tensor(0.5356, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9497  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.956e-14\n",
      "‖w_svm‖₂       : 0.0009225847714928158\n",
      "‖alpha‖₁       : 0.8199999999999992\n",
      "scores min/max : 0.00048263244530834075 0.002225163828891979\n",
      "Mask mean value:  tensor(0.5079, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5802  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.187e-16\n",
      "‖w_svm‖₂       : 1.0926134513957304e-07\n",
      "‖alpha‖₁       : 0.23999999999998906\n",
      "scores min/max : 8.47838671782198e-08 9.56979241910757e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.977e-21\n",
      "‖w_svm‖₂       : 0.028528512568923244\n",
      "‖alpha‖₁       : 0.8985809551084848\n",
      "scores min/max : -0.727973948940235 2.031959939326147\n",
      "Mask mean value:  tensor(0.8711, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3005  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.459e-15\n",
      "‖w_svm‖₂       : 2.4569114420553836e-07\n",
      "‖alpha‖₁       : 0.58\n",
      "scores min/max : -2.541149979225591e-07 -2.384937733975405e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.299e-19\n",
      "‖w_svm‖₂       : 4.446783875328584e-07\n",
      "‖alpha‖₁       : 0.7199999999998761\n",
      "scores min/max : 1.6806545281709262e-07 2.2538067156700192e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.555e-21\n",
      "‖w_svm‖₂       : 3.5717805828543042e-06\n",
      "‖alpha‖₁       : 0.41999999999317894\n",
      "scores min/max : -3.924481785902523e-06 -2.3263852851884498e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1229  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.799e-05\n",
      "‖w_svm‖₂       : 0.05295067744247876\n",
      "‖alpha‖₁       : 0.5763584678116372\n",
      "scores min/max : -1.9623452547907787 0.8655109305456457\n",
      "Mask mean value:  tensor(0.6964, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.8879  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.532e-04\n",
      "‖w_svm‖₂       : 0.051309591570618804\n",
      "‖alpha‖₁       : 0.827258307158679\n",
      "scores min/max : -1.9382303226840458 0.4148245572884117\n",
      "Mask mean value:  tensor(0.5652, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0228  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.039e-10\n",
      "‖w_svm‖₂       : 1.1288711088493696e-07\n",
      "‖alpha‖₁       : 0.5199999999999967\n",
      "scores min/max : 2.682524380483947e-07 2.896711653193712e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.268e-19\n",
      "‖w_svm‖₂       : 0.046666254715281706\n",
      "‖alpha‖₁       : 0.9056778285544538\n",
      "scores min/max : -0.7100538156811644 1.8847436983916959\n",
      "Mask mean value:  tensor(0.3458, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.327e-03\n",
      "‖w_svm‖₂       : 5.3465032120565194e-08\n",
      "‖alpha‖₁       : 0.11999999999999318\n",
      "scores min/max : -9.394058183304631e-08 -6.317804398300742e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.003e-07\n",
      "‖w_svm‖₂       : 1.4422775949648756e-05\n",
      "‖alpha‖₁       : 0.359999999995634\n",
      "scores min/max : 8.135343081252903e-06 8.987665242065102e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3079  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.785e-17\n",
      "‖w_svm‖₂       : 0.025871085624855582\n",
      "‖alpha‖₁       : 0.1966942190028965\n",
      "scores min/max : -2.2086829886915527 0.012355784466031628\n",
      "Mask mean value:  tensor(0.0539, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.651e-04\n",
      "‖w_svm‖₂       : 0.00020821792576985082\n",
      "‖alpha‖₁       : 0.6199999999936312\n",
      "scores min/max : -8.609913838064762e-06 6.75196338478135e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.172e-17\n",
      "‖w_svm‖₂       : 2.0101967820546865e-07\n",
      "‖alpha‖₁       : 0.3799999999999835\n",
      "scores min/max : -1.4736008106788358e-08 2.1048230311318057e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.904e-19\n",
      "‖w_svm‖₂       : 5.837187417932791e-08\n",
      "‖alpha‖₁       : 0.2399999999999993\n",
      "scores min/max : -5.7455489662509805e-08 -4.313593801145536e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.771e-20\n",
      "‖w_svm‖₂       : 0.000358035916353522\n",
      "‖alpha‖₁       : 0.7399999999999997\n",
      "scores min/max : -0.0004552083712822086 -0.0004388580749976724\n",
      "Mask mean value:  tensor(0.4978, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0596  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.593e-17\n",
      "‖w_svm‖₂       : 7.636672085657466e-08\n",
      "‖alpha‖₁       : 0.6599999999999768\n",
      "scores min/max : 8.410809793946175e-08 2.2928069778012654e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 9.922e-09\n",
      "‖w_svm‖₂       : 2.8368504504082735e-07\n",
      "‖alpha‖₁       : 0.6599999999999998\n",
      "scores min/max : -6.512096175003314e-07 -4.995705041810963e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.600e-18\n",
      "‖w_svm‖₂       : 2.3046794141556481e-07\n",
      "‖alpha‖₁       : 0.25999999999999635\n",
      "scores min/max : 2.603631755218642e-07 2.7313140055241616e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 7.845e-17\n",
      "‖w_svm‖₂       : 0.0034047104703650495\n",
      "‖alpha‖₁       : 0.5799999999999995\n",
      "scores min/max : 0.005723408653963129 0.007182103904646289\n",
      "Mask mean value:  tensor(0.5343, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3398  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.963e-13\n",
      "‖w_svm‖₂       : 8.564318958308362e-08\n",
      "‖alpha‖₁       : 0.37999999999997375\n",
      "scores min/max : 1.9035844200078025e-08 3.723836259795282e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.350e-22\n",
      "‖w_svm‖₂       : 0.00018199326673716575\n",
      "‖alpha‖₁       : 0.8199999999999996\n",
      "scores min/max : -9.799178964391868e-05 -8.499458455025674e-05\n",
      "Mask mean value:  tensor(0.4996, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6964  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.496e-17\n",
      "‖w_svm‖₂       : 0.0002963959462385979\n",
      "‖alpha‖₁       : 0.4199999999992255\n",
      "scores min/max : 8.435742015523305e-05 0.0003820067690328824\n",
      "Mask mean value:  tensor(0.5018, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9990  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.174e-15\n",
      "‖w_svm‖₂       : 1.9818027992764147e-07\n",
      "‖alpha‖₁       : 0.4199999999999662\n",
      "scores min/max : -4.798621803977668e-07 -4.5766093139354027e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.192e-19\n",
      "‖w_svm‖₂       : 0.06912351914102403\n",
      "‖alpha‖₁       : 0.6587632619678084\n",
      "scores min/max : -1.9776748061831295 4.491012336614861\n",
      "Mask mean value:  tensor(0.4730, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.0270  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.227e-04\n",
      "‖w_svm‖₂       : 0.004636478088279267\n",
      "‖alpha‖₁       : 0.45999999999999835\n",
      "scores min/max : -0.011439339400260177 -0.009132640093488782\n",
      "Mask mean value:  tensor(0.4518, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9631  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.211e-15\n",
      "‖w_svm‖₂       : 0.04513571734893664\n",
      "‖alpha‖₁       : 0.9388867439492131\n",
      "scores min/max : -2.467912992346901 1.583845950246407\n",
      "Mask mean value:  tensor(0.1439, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3995  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.201e-03\n",
      "‖w_svm‖₂       : 0.06065589856136462\n",
      "‖alpha‖₁       : 0.898754196582203\n",
      "scores min/max : -1.691695143186471 3.7235135043926553\n",
      "Mask mean value:  tensor(0.4492, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.5964  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.124e-03\n",
      "‖w_svm‖₂       : 0.14631838702678565\n",
      "‖alpha‖₁       : 0.7594301243495708\n",
      "scores min/max : -1.7786759911249412 2.198280746015184\n",
      "Mask mean value:  tensor(0.8222, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3543  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.566e-11\n",
      "‖w_svm‖₂       : 0.020320145902918044\n",
      "‖alpha‖₁       : 0.8233918221038927\n",
      "scores min/max : -1.9132634079008695 1.4818151066963348\n",
      "Mask mean value:  tensor(0.6590, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.2873  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.480e-05\n",
      "‖w_svm‖₂       : 1.94861762376615e-08\n",
      "‖alpha‖₁       : 0.11999999999999299\n",
      "scores min/max : -4.737349135815157e-08 -3.262608881608483e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.852e-09\n",
      "‖w_svm‖₂       : 0.030981220197734305\n",
      "‖alpha‖₁       : 0.5512943724589611\n",
      "scores min/max : -3.42953949760235 1.1383308471356777\n",
      "Mask mean value:  tensor(0.1802, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.3479  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.138e-03\n",
      "‖w_svm‖₂       : 0.0005381426938277269\n",
      "‖alpha‖₁       : 0.4399999999999928\n",
      "scores min/max : -0.0009449916194066653 -0.0006415101137888018\n",
      "Mask mean value:  tensor(0.4955, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6405  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.469e-16\n",
      "‖w_svm‖₂       : 0.0839425592896285\n",
      "‖alpha‖₁       : 0.8733528333814693\n",
      "scores min/max : -12.128537244743125 2.180928155393085\n",
      "Mask mean value:  tensor(0.5963, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7620  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 8.610e-02\n",
      "‖w_svm‖₂       : 0.1375534705835516\n",
      "‖alpha‖₁       : 0.8799999999999999\n",
      "scores min/max : -1.3075185034011574 3.670741146891126\n",
      "Mask mean value:  tensor(0.4352, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2634  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.802e-07\n",
      "‖w_svm‖₂       : 0.08201566824533689\n",
      "‖alpha‖₁       : 0.4720545532246708\n",
      "scores min/max : -2.215181480316504 2.85922853005686\n",
      "Mask mean value:  tensor(0.1609, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.7036  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.755e-02\n",
      "‖w_svm‖₂       : 0.00015832122109522815\n",
      "‖alpha‖₁       : 0.6399999999999999\n",
      "scores min/max : -0.0001645881257061989 -0.00015502386534274734\n",
      "Mask mean value:  tensor(0.4992, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7632  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.571e-17\n",
      "‖w_svm‖₂       : 0.18256286519150855\n",
      "‖alpha‖₁       : 0.8834956120744801\n",
      "scores min/max : -3.6609968434285234 6.067102210092551\n",
      "Mask mean value:  tensor(0.0908, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2031  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.537e-03\n",
      "‖w_svm‖₂       : 7.575180936692137e-08\n",
      "‖alpha‖₁       : 0.5399999999999963\n",
      "scores min/max : -1.9867878257420885e-07 -1.771463226126357e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.243e-20\n",
      "‖w_svm‖₂       : 0.01614614515857343\n",
      "‖alpha‖₁       : 0.5963297305266712\n",
      "scores min/max : -2.724032852666106 2.2698706275238782\n",
      "Mask mean value:  tensor(0.9145, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.5042  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.425e-12\n",
      "‖w_svm‖₂       : 0.034575424594263146\n",
      "‖alpha‖₁       : 0.65999999999995\n",
      "scores min/max : -0.16791165647934758 0.10348462236375422\n",
      "Mask mean value:  tensor(0.4374, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8374  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 8.726e-13\n",
      "‖w_svm‖₂       : 0.006036500782641298\n",
      "‖alpha‖₁       : 0.5599999999999745\n",
      "scores min/max : 0.007246003113783292 0.009148302845232301\n",
      "Mask mean value:  tensor(0.5379, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1277  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 2.002e-05\n",
      "‖w_svm‖₂       : 5.148094230197954e-08\n",
      "‖alpha‖₁       : 0.4399999999999861\n",
      "scores min/max : -2.005345218321851e-07 -9.659505197357784e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.949e-20\n",
      "‖w_svm‖₂       : 6.558141766812998e-07\n",
      "‖alpha‖₁       : 0.39999999999999986\n",
      "scores min/max : 4.6352577188109413e-08 3.5247426532686635e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.317e-09\n",
      "‖w_svm‖₂       : 0.048062809864930625\n",
      "‖alpha‖₁       : 0.941202736722441\n",
      "scores min/max : -1.825782050066703 0.30864292098559964\n",
      "Mask mean value:  tensor(0.4618, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3231  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.005e-13\n",
      "‖w_svm‖₂       : 0.0178324333687462\n",
      "‖alpha‖₁       : 0.6599999999999838\n",
      "scores min/max : -0.03231828968711614 0.01702288711868697\n",
      "Mask mean value:  tensor(0.4320, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.3774  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.036e-15\n",
      "‖w_svm‖₂       : 0.012651436481797737\n",
      "‖alpha‖₁       : 0.8599999999999968\n",
      "scores min/max : -0.047637293326942436 -0.012016190462750535\n",
      "Mask mean value:  tensor(0.3401, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4207  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.200e-05\n",
      "‖w_svm‖₂       : 1.0893895234685713e-06\n",
      "‖alpha‖₁       : 0.3199999999997232\n",
      "scores min/max : -2.276756414563479e-06 -2.1692515939017316e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4898  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.215e-18\n",
      "‖w_svm‖₂       : 0.06787240288417534\n",
      "‖alpha‖₁       : 0.41920538485759573\n",
      "scores min/max : -1.9438855066044605 2.058033009453266\n",
      "Mask mean value:  tensor(0.7291, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8880  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.766e-04\n",
      "‖w_svm‖₂       : 0.07275345001674902\n",
      "‖alpha‖₁       : 0.5799999999999267\n",
      "scores min/max : -2.808920324854707 1.6855835480586112\n",
      "Mask mean value:  tensor(0.2965, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4632  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.600e-04\n",
      "‖w_svm‖₂       : 0.08482485568277097\n",
      "‖alpha‖₁       : 0.577991804672053\n",
      "scores min/max : -2.1116328576825087 5.803480382084428\n",
      "Mask mean value:  tensor(0.2889, dtype=torch.float64)\n",
      "max feasible return = 0.1122  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.9543655335361634e-07\n",
      "‖alpha‖₁       : 0.5799999999999981\n",
      "scores min/max : -2.936766291908838e-07 -2.0285418163965993e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0713715985376648e-07\n",
      "‖alpha‖₁       : 0.2999999999999733\n",
      "scores min/max : 3.595094839474176e-08 4.1062423822883455e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 6.53744382697862e-08\n",
      "‖alpha‖₁       : 0.5999999999999994\n",
      "scores min/max : 2.6734859093280376e-09 3.9416420986541234e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.480710569163566e-08\n",
      "‖alpha‖₁       : 0.3799999999999996\n",
      "scores min/max : -2.6571936335594282e-08 -1.7194597867798182e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 5.9839040074880005e-06\n",
      "‖alpha‖₁       : 0.3199999999233356\n",
      "scores min/max : 3.989349653800922e-06 8.044296625960739e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4259  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04315820151756178\n",
      "‖alpha‖₁       : 0.7397182451651215\n",
      "scores min/max : -3.4517614120281532 2.179065114346602\n",
      "Mask mean value:  tensor(0.9021, dtype=torch.float64)\n",
      "max feasible return = -0.9438  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.023816254634900128\n",
      "‖alpha‖₁       : 0.6151709268435133\n",
      "scores min/max : -1.947554308152578 1.1795516447837613\n",
      "Mask mean value:  tensor(0.7115, dtype=torch.float64)\n",
      "max feasible return = 2.4776  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.7071715560221276e-07\n",
      "‖alpha‖₁       : 0.4599999999999914\n",
      "scores min/max : 5.776345168847127e-08 6.522839915433716e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.3968129458319764e-07\n",
      "‖alpha‖₁       : 0.5199999999999907\n",
      "scores min/max : 8.349061708437134e-08 9.640024167942222e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.5876106832370685e-07\n",
      "‖alpha‖₁       : 0.5799999999999932\n",
      "scores min/max : -2.0173168399695731e-07 -1.654826638295317e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.04189531534838354\n",
      "‖alpha‖₁       : 0.687842685524128\n",
      "scores min/max : -0.4357343736388909 1.954501676377685\n",
      "Mask mean value:  tensor(0.3847, dtype=torch.float64)\n",
      "max feasible return = 0.0417  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03262092755660808\n",
      "‖alpha‖₁       : 0.422363948891059\n",
      "scores min/max : -3.8529087776438278 5.309147147055187\n",
      "Mask mean value:  tensor(0.1175, dtype=torch.float64)\n",
      "max feasible return = 0.7518  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04288610085217726\n",
      "‖alpha‖₁       : 0.8211771171988158\n",
      "scores min/max : -4.9109355961025605 3.26042434321203\n",
      "Mask mean value:  tensor(0.9675, dtype=torch.float64)\n",
      "max feasible return = -3.7054  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.005872718270246993\n",
      "‖alpha‖₁       : 0.7999999999999308\n",
      "scores min/max : -0.024828119551754624 -0.0014227904177712535\n",
      "Mask mean value:  tensor(0.4636, dtype=torch.float64)\n",
      "max feasible return = -0.1686  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.101361078746065e-07\n",
      "‖alpha‖₁       : 0.6199999999999786\n",
      "scores min/max : 5.71222731265362e-08 7.360974244856101e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.838533919310871e-08\n",
      "‖alpha‖₁       : 0.43999999999994377\n",
      "scores min/max : 3.1707920195040013e-09 1.2759475154419403e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.340293105763227e-07\n",
      "‖alpha‖₁       : 0.27999999999997405\n",
      "scores min/max : -1.6568600364907664e-07 -1.477177156387813e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  56 | train 0.005302 | val 0.006612\n",
      "-----------------------------------------Epoch:  57 ----------------------------------------\n",
      "‖w_svm‖₂       : 1.0884109320805262e-06\n",
      "‖alpha‖₁       : 0.3199999999997209\n",
      "scores min/max : -2.259729993122245e-06 -2.1522073375451484e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -5.4898  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.211e-18\n",
      "‖w_svm‖₂       : 4.910979984894261e-08\n",
      "‖alpha‖₁       : 0.17999999999999425\n",
      "scores min/max : 5.795073366588312e-08 7.276226644070821e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.2458  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 6.367e-08\n",
      "‖w_svm‖₂       : 0.031062116778242627\n",
      "‖alpha‖₁       : 0.5512954329322401\n",
      "scores min/max : -3.4285836659956943 1.139434893787973\n",
      "Mask mean value:  tensor(0.1812, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.3532  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.154e-03\n",
      "‖w_svm‖₂       : 4.3287055606775146e-07\n",
      "‖alpha‖₁       : 0.7199999999999998\n",
      "scores min/max : 1.1386816937241215e-07 1.7109890396814734e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6356  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.372e-21\n",
      "‖w_svm‖₂       : 0.003428904385492545\n",
      "‖alpha‖₁       : 0.5799999999999984\n",
      "scores min/max : 0.005663404391304553 0.007143833140759379\n",
      "Mask mean value:  tensor(0.5341, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3396  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.502e-12\n",
      "‖w_svm‖₂       : 0.017823924069137548\n",
      "‖alpha‖₁       : 0.6599999999999198\n",
      "scores min/max : -0.032132155444786814 0.01719587637347865\n",
      "Mask mean value:  tensor(0.4329, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 3.3839  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.024e-15\n",
      "‖w_svm‖₂       : 0.00726154897532605\n",
      "‖alpha‖₁       : 0.7005833976001014\n",
      "scores min/max : -2.004034945435069 0.01940637294193028\n",
      "Mask mean value:  tensor(0.4548, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2546  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.129e-05\n",
      "‖w_svm‖₂       : 3.849035766293136e-06\n",
      "‖alpha‖₁       : 0.4199999999893495\n",
      "scores min/max : -3.870987156514248e-06 -2.1487696267229335e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -3.1229  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.888e-05\n",
      "‖w_svm‖₂       : 0.01666157001918607\n",
      "‖alpha‖₁       : 0.6816099924938176\n",
      "scores min/max : -1.972547600139964 0.05328974109099166\n",
      "Mask mean value:  tensor(0.6105, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.2631  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.682e-14\n",
      "‖w_svm‖₂       : 0.03466498019404618\n",
      "‖alpha‖₁       : 0.6599999999999319\n",
      "scores min/max : -0.17110182000818142 0.10221990648640755\n",
      "Mask mean value:  tensor(0.4277, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.8171  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.457e-04\n",
      "‖w_svm‖₂       : 0.0812614239423899\n",
      "‖alpha‖₁       : 0.47193688327147254\n",
      "scores min/max : -2.1647677404439514 2.90917327068792\n",
      "Mask mean value:  tensor(0.2175, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.2546  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.239e-03\n",
      "‖w_svm‖₂       : 0.015718623724690736\n",
      "‖alpha‖₁       : 0.8599999999999907\n",
      "scores min/max : -0.06905061283874739 -0.058147129870255804\n",
      "Mask mean value:  tensor(0.2285, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0585  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.695e-14\n",
      "‖w_svm‖₂       : 0.00030167972592127617\n",
      "‖alpha‖₁       : 0.41999999999749094\n",
      "scores min/max : 2.591324531553028e-06 0.00031092852287170554\n",
      "Mask mean value:  tensor(0.5014, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9983  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.178e-15\n",
      "‖w_svm‖₂       : 1.0087438884577999e-06\n",
      "‖alpha‖₁       : 0.5999999999999968\n",
      "scores min/max : -2.328379769105596e-07 -1.8198176111399526e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.1903  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.911e-19\n",
      "‖w_svm‖₂       : 1.1471473623474557e-07\n",
      "‖alpha‖₁       : 0.2999999999999985\n",
      "scores min/max : 1.5524501234344198e-07 1.6418503580159556e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.2723  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 1.607e-07\n",
      "‖w_svm‖₂       : 0.05266154307771257\n",
      "‖alpha‖₁       : 0.5763308762236553\n",
      "scores min/max : -1.9592847416248933 0.8685495923238498\n",
      "Mask mean value:  tensor(0.7062, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9159  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 9.953e-04\n",
      "‖w_svm‖₂       : 0.03200476384131092\n",
      "‖alpha‖₁       : 0.9199999999999644\n",
      "scores min/max : -0.23676271919538383 0.13996417187436616\n",
      "Mask mean value:  tensor(0.1506, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.7623  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 5.743e-03\n",
      "‖w_svm‖₂       : 0.02182312630107986\n",
      "‖alpha‖₁       : 0.3835502698476475\n",
      "scores min/max : -1.9595454738399525 0.24427148902930274\n",
      "Mask mean value:  tensor(0.7169, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1541  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 7.455e-14\n",
      "‖w_svm‖₂       : 0.060169243343281206\n",
      "‖alpha‖₁       : 0.898689406449427\n",
      "scores min/max : -1.684519484057255 3.731082090391598\n",
      "Mask mean value:  tensor(0.4648, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6083  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.998e-03\n",
      "‖w_svm‖₂       : 0.02098442237466861\n",
      "‖alpha‖₁       : 0.7799999999999996\n",
      "scores min/max : -0.048179869172216325 0.013887117837433904\n",
      "Mask mean value:  tensor(0.2978, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.3210  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.359e-14\n",
      "‖w_svm‖₂       : 0.04801816077111552\n",
      "‖alpha‖₁       : 0.9412029142173883\n",
      "scores min/max : -1.8234046467783873 0.3110502588136652\n",
      "Mask mean value:  tensor(0.4725, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.3228  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.542e-10\n",
      "‖w_svm‖₂       : 0.050965400563951636\n",
      "‖alpha‖₁       : 0.8272255438969658\n",
      "scores min/max : -1.9472054713209779 0.4059947159400637\n",
      "Mask mean value:  tensor(0.5334, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.0165  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.618e-02\n",
      "‖w_svm‖₂       : 0.01006977139314658\n",
      "‖alpha‖₁       : 0.6077229301228755\n",
      "scores min/max : -1.9956504371970374 0.2909865511927657\n",
      "Mask mean value:  tensor(0.5320, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2148  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.162e-14\n",
      "‖w_svm‖₂       : 0.016220436278286978\n",
      "‖alpha‖₁       : 0.5963322279707048\n",
      "scores min/max : -2.7296505931975164 2.2645462945828085\n",
      "Mask mean value:  tensor(0.9119, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.5008  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.178e-11\n",
      "‖w_svm‖₂       : 1.6768080467973455e-07\n",
      "‖alpha‖₁       : 0.23999999999998575\n",
      "scores min/max : 1.7911998747962186e-07 1.9456878772531682e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.6741  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.980e-20\n",
      "‖w_svm‖₂       : 0.0001458967617142701\n",
      "‖alpha‖₁       : 0.43999999971226034\n",
      "scores min/max : -0.00030299002929407666 -0.0001461277453793673\n",
      "Mask mean value:  tensor(0.4990, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0208  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.233e-17\n",
      "‖w_svm‖₂       : 1.972713019361816e-07\n",
      "‖alpha‖₁       : 0.37999999999998324\n",
      "scores min/max : 2.900051543993596e-08 6.478688594014286e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2320  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.709e-19\n",
      "‖w_svm‖₂       : 8.422813381848236e-08\n",
      "‖alpha‖₁       : 0.37999999999997036\n",
      "scores min/max : 3.581686827789663e-08 5.402322281718863e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4944  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.114e-22\n",
      "‖w_svm‖₂       : 1.1140213397954929e-07\n",
      "‖alpha‖₁       : 0.5199999999999907\n",
      "scores min/max : 2.167795459723193e-07 2.381979094239515e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.2579  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.130e-19\n",
      "‖w_svm‖₂       : 1.0747160743881194e-07\n",
      "‖alpha‖₁       : 0.23999999999998822\n",
      "scores min/max : 9.333476178541619e-08 1.0424762282160702e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5062  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.909e-21\n",
      "‖w_svm‖₂       : 5.103186226804652e-08\n",
      "‖alpha‖₁       : 0.43999999999999295\n",
      "scores min/max : -1.9609861326133345e-07 -9.20946046781106e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1740  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.950e-20\n",
      "‖w_svm‖₂       : 0.03596561631539171\n",
      "‖alpha‖₁       : 0.7294984175355679\n",
      "scores min/max : -0.2682285432468547 2.0159972678754157\n",
      "Mask mean value:  tensor(0.6554, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.9830  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 2.034e-03\n",
      "‖w_svm‖₂       : 0.028430953703179974\n",
      "‖alpha‖₁       : 0.8985785978185554\n",
      "scores min/max : -0.727799835160994 2.032155137193831\n",
      "Mask mean value:  tensor(0.8714, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3001  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 5.421e-15\n",
      "‖w_svm‖₂       : 0.13724837608928694\n",
      "‖alpha‖₁       : 0.6550724145133361\n",
      "scores min/max : -18.04192789521656 1.8914920679028633\n",
      "Mask mean value:  tensor(0.3407, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2554  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.983e-03\n",
      "‖w_svm‖₂       : 0.025845375262453518\n",
      "‖alpha‖₁       : 0.19668947619884536\n",
      "scores min/max : -2.2026745701970336 0.01836867465432296\n",
      "Mask mean value:  tensor(0.0593, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4011  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 6.921e-04\n",
      "‖w_svm‖₂       : 7.534563180314713e-08\n",
      "‖alpha‖₁       : 0.5399999999999736\n",
      "scores min/max : -1.931985129427353e-07 -1.715843970490212e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.0905  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.283e-20\n",
      "‖w_svm‖₂       : 0.032013074412228344\n",
      "‖alpha‖₁       : 0.8525655802363508\n",
      "scores min/max : -2.9149160025498193 1.5659622769372872\n",
      "Mask mean value:  tensor(0.1559, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.9517  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.789e-03\n",
      "‖w_svm‖₂       : 0.0009245445195240177\n",
      "‖alpha‖₁       : 0.8199999999999992\n",
      "scores min/max : 0.00045985659589409256 0.002207820038602314\n",
      "Mask mean value:  tensor(0.5078, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 2.5797  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.179e-16\n",
      "‖w_svm‖₂       : 0.00021064570117739078\n",
      "‖alpha‖₁       : 0.6199999999999951\n",
      "scores min/max : -2.00645107337925e-05 -4.341995658098684e-06\n",
      "Mask mean value:  tensor(0.4999, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.1044  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.250e-17\n",
      "‖w_svm‖₂       : 2.3655290397224245e-07\n",
      "‖alpha‖₁       : 0.6399999999999337\n",
      "scores min/max : 3.672300910954755e-07 3.884133799079986e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3654  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.104e-19\n",
      "‖w_svm‖₂       : 0.06894183044591247\n",
      "‖alpha‖₁       : 0.6587673177725485\n",
      "scores min/max : -1.9666359770552924 4.508538729873642\n",
      "Mask mean value:  tensor(0.4996, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1952  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.947e-05\n",
      "‖w_svm‖₂       : 0.004734741203131024\n",
      "‖alpha‖₁       : 0.4599999999999997\n",
      "scores min/max : -0.012443406758695142 -0.010044994642907304\n",
      "Mask mean value:  tensor(0.4472, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.9428  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.234e-15\n",
      "‖w_svm‖₂       : 0.006070185718365294\n",
      "‖alpha‖₁       : 0.5599999999999987\n",
      "scores min/max : 0.007402226302710948 0.009324402524705426\n",
      "Mask mean value:  tensor(0.5387, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1294  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 2.076e-05\n",
      "‖w_svm‖₂       : 3.9164051830519434e-07\n",
      "‖alpha‖₁       : 0.2799999999993994\n",
      "scores min/max : -8.546230061883531e-07 -7.900564532058209e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 6.1958  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.532e-19\n",
      "‖w_svm‖₂       : 0.18191319694189928\n",
      "‖alpha‖₁       : 0.8833215689510108\n",
      "scores min/max : -3.662848303566436 6.0679944866117665\n",
      "Mask mean value:  tensor(0.0909, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2041  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.975e-05\n",
      "‖w_svm‖₂       : 7.385730276653523e-08\n",
      "‖alpha‖₁       : 0.13999999999998325\n",
      "scores min/max : 1.0940473248236967e-08 1.6782368277889768e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.4342  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 7.151e-08\n",
      "‖w_svm‖₂       : 0.13843723281691434\n",
      "‖alpha‖₁       : 0.8799999999999999\n",
      "scores min/max : -1.3267331834313503 3.713806599876447\n",
      "Mask mean value:  tensor(0.4282, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2650  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.277e-10\n",
      "‖w_svm‖₂       : 1.3092493227092778e-07\n",
      "‖alpha‖₁       : 0.3799999999999928\n",
      "scores min/max : -1.2137867415840458e-07 -1.063354186380681e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.7972  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.509e-07\n",
      "‖w_svm‖₂       : 0.045242223077983965\n",
      "‖alpha‖₁       : 0.9388912533248981\n",
      "scores min/max : -2.4622371252282624 1.5894720242977347\n",
      "Mask mean value:  tensor(0.1476, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4066  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.088e-03\n",
      "‖w_svm‖₂       : 0.040606400265412006\n",
      "‖alpha‖₁       : 0.8900084326601252\n",
      "scores min/max : -2.008218422007836 0.3396544853599415\n",
      "Mask mean value:  tensor(0.2658, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0611  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.362e-13\n",
      "‖w_svm‖₂       : 2.2802863929047168e-07\n",
      "‖alpha‖₁       : 0.25999999999999707\n",
      "scores min/max : 2.6829454249833716e-07 2.810571520724796e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.2140  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.039e-17\n",
      "‖w_svm‖₂       : 7.601647453265336e-08\n",
      "‖alpha‖₁       : 0.6599999999999768\n",
      "scores min/max : 7.190820923458178e-08 2.1706175254001502e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5361  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 4.468e-09\n",
      "‖w_svm‖₂       : 0.07258475930154812\n",
      "‖alpha‖₁       : 0.5799999999999454\n",
      "scores min/max : -2.794310527220386 1.6778896585584528\n",
      "Mask mean value:  tensor(0.2989, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4651  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.706e-04\n",
      "‖w_svm‖₂       : 1.9690114781584684e-07\n",
      "‖alpha‖₁       : 0.41999999999996557\n",
      "scores min/max : -4.2894726141258875e-07 -4.0673532136333306e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.1029  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.214e-19\n",
      "‖w_svm‖₂       : 1.0742586543085108e-06\n",
      "‖alpha‖₁       : 0.49999999999999006\n",
      "scores min/max : -2.5085460580242647e-07 -5.778436512260128e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0240  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.340e-19\n",
      "‖w_svm‖₂       : 7.400063160508053e-08\n",
      "‖alpha‖₁       : 0.42000000000000004\n",
      "scores min/max : -6.757908705814783e-08 -2.0462749108579253e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -2.3724  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 7.105e-08\n",
      "‖w_svm‖₂       : 7.816073235584328e-08\n",
      "‖alpha‖₁       : 0.17999999999999933\n",
      "scores min/max : -1.97134097256572e-07 3.6556091279608685e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 5.3028  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 8.024e-21\n",
      "‖w_svm‖₂       : 2.419930419057388e-07\n",
      "‖alpha‖₁       : 0.5799999999999998\n",
      "scores min/max : -1.664237783430156e-07 -1.5080833681971652e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.0311  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.194e-19\n",
      "‖w_svm‖₂       : 0.046791937076480154\n",
      "‖alpha‖₁       : 0.9056678544087586\n",
      "scores min/max : -0.7086581036456667 1.8863201230324886\n",
      "Mask mean value:  tensor(0.3509, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.6533  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.042e-03\n",
      "‖w_svm‖₂       : 0.0001591341836464799\n",
      "‖alpha‖₁       : 0.6399999999999999\n",
      "scores min/max : -0.00016472309641616322 -0.0001550603937635189\n",
      "Mask mean value:  tensor(0.4992, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.7632  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.646e-17\n",
      "‖w_svm‖₂       : 1.933471945244123e-08\n",
      "‖alpha‖₁       : 0.11999999999999308\n",
      "scores min/max : -5.093214262839378e-08 -3.6189408830405325e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.5262  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.873e-09\n",
      "‖w_svm‖₂       : 5.326229782714834e-08\n",
      "‖alpha‖₁       : 0.11999999999999109\n",
      "scores min/max : -9.0099840170465e-08 -5.929677977400094e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9146  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.368e-07\n",
      "‖w_svm‖₂       : 0.0843783605748141\n",
      "‖alpha‖₁       : 0.8733462435294659\n",
      "scores min/max : -12.186051932657518 2.122918728420176\n",
      "Mask mean value:  tensor(0.4522, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3536  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "grad‖embed‖ = 8.197e-02\n",
      "‖w_svm‖₂       : 0.068105901079978\n",
      "‖alpha‖₁       : 0.4192375407737471\n",
      "scores min/max : -1.946457987886711 2.0553150641022158\n",
      "Mask mean value:  tensor(0.7261, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.8862  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.699e-04\n",
      "‖w_svm‖₂       : 0.020903890605845004\n",
      "‖alpha‖₁       : 0.8150954145956752\n",
      "scores min/max : -11.512760015673592 2.0264251291172113\n",
      "Mask mean value:  tensor(0.6989, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.8938  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 5.811e-06\n",
      "‖w_svm‖₂       : 1.4628394968555184e-05\n",
      "‖alpha‖₁       : 0.35999999999509624\n",
      "scores min/max : 7.1536967024228896e-06 8.026281521299566e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.3078  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 6.805e-17\n",
      "‖w_svm‖₂       : 0.02026208962236679\n",
      "‖alpha‖₁       : 0.8233922098586162\n",
      "scores min/max : -1.9147462851123587 1.4800424156453729\n",
      "Mask mean value:  tensor(0.6545, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.2764  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.721e-05\n",
      "‖w_svm‖₂       : 2.8183235359441717e-07\n",
      "‖alpha‖₁       : 0.6599999999999999\n",
      "scores min/max : -6.021773456950106e-07 -4.5054223554298493e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3331  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 1.559e-18\n",
      "‖w_svm‖₂       : 0.0001833430981777019\n",
      "‖alpha‖₁       : 0.8199999999999998\n",
      "scores min/max : -8.821001660366768e-05 -7.499240571117696e-05\n",
      "Mask mean value:  tensor(0.4996, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6965  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.494e-17\n",
      "‖w_svm‖₂       : 0.14625114068816492\n",
      "‖alpha‖₁       : 0.7594014127539739\n",
      "scores min/max : -1.778626572647164 2.198219922912891\n",
      "Mask mean value:  tensor(0.8223, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.3529  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.400e-11\n",
      "‖w_svm‖₂       : 0.0059159118306083575\n",
      "‖alpha‖₁       : 0.379999999999972\n",
      "scores min/max : -0.0014656147163629243 0.008957922261815209\n",
      "Mask mean value:  tensor(0.5384, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -0.9545  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 2.951e-14\n",
      "‖w_svm‖₂       : 0.0003598866166405495\n",
      "‖alpha‖₁       : 0.7399999999999995\n",
      "scores min/max : -0.0004621228796740075 -0.0004456032038610806\n",
      "Mask mean value:  tensor(0.4978, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = -1.0595  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "grad‖embed‖ = 3.619e-17\n",
      "‖w_svm‖₂       : 6.575474105627823e-07\n",
      "‖alpha‖₁       : 0.3999999999999997\n",
      "scores min/max : 5.521126188528982e-08 3.6177899352079736e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.1894  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 2.334e-09\n",
      "‖w_svm‖₂       : 0.012388359465058084\n",
      "‖alpha‖₁       : 0.8599999999999961\n",
      "scores min/max : -0.04559464640763339 -0.011506258439355677\n",
      "Mask mean value:  tensor(0.3464, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 0.4274  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 4.703e-05\n",
      "‖w_svm‖₂       : 0.000540397977279515\n",
      "‖alpha‖₁       : 0.4399999999999945\n",
      "scores min/max : -0.0009877106111009684 -0.0006819187377722462\n",
      "Mask mean value:  tensor(0.4953, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 1.6398  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 3.464e-16\n",
      "‖w_svm‖₂       : 5.794912258733677e-08\n",
      "‖alpha‖₁       : 0.23999999999999894\n",
      "scores min/max : -4.46704701979778e-08 -3.038077828300382e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "max feasible return = 4.0127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "grad‖embed‖ = 1.735e-20\n",
      "‖w_svm‖₂       : 0.08526953713698518\n",
      "‖alpha‖₁       : 0.5780671555203912\n",
      "scores min/max : -2.1265213723962377 5.788674068763569\n",
      "Mask mean value:  tensor(0.2617, dtype=torch.float64)\n",
      "max feasible return = 0.1076  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.997935305287811e-07\n",
      "‖alpha‖₁       : 0.5799999999999875\n",
      "scores min/max : -2.6816661354363945e-07 -1.7726287404054067e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0778369098642697e-07\n",
      "‖alpha‖₁       : 0.2999999999999733\n",
      "scores min/max : 3.395799251043201e-08 3.905730883217293e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 6.569059019972565e-08\n",
      "‖alpha‖₁       : 0.5999999999999994\n",
      "scores min/max : 3.9965384182860693e-10 3.7138353411898604e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.509397540992752e-08\n",
      "‖alpha‖₁       : 0.37999999999999856\n",
      "scores min/max : -2.804159624690068e-08 -1.8660145965282697e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.995599524632559e-06\n",
      "‖alpha‖₁       : 0.31999999996979\n",
      "scores min/max : 2.179423289784808e-06 4.201702214376847e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4258  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04245029378527736\n",
      "‖alpha‖₁       : 0.7397393172607717\n",
      "scores min/max : -3.4364011268023136 2.1941624410438982\n",
      "Mask mean value:  tensor(0.9113, dtype=torch.float64)\n",
      "max feasible return = -0.9495  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.02337489112674682\n",
      "‖alpha‖₁       : 0.6151749587692243\n",
      "scores min/max : -1.9532098815278265 1.1736228077721196\n",
      "Mask mean value:  tensor(0.6953, dtype=torch.float64)\n",
      "max feasible return = 2.4207  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 4.752969253973085e-07\n",
      "‖alpha‖₁       : 0.4599999999999916\n",
      "scores min/max : 1.0374847413641882e-07 6.982529576994557e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 1.4118084333010797e-07\n",
      "‖alpha‖₁       : 0.5199999999999907\n",
      "scores min/max : 7.754284626328462e-08 9.045168172139943e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.613368254645593e-07\n",
      "‖alpha‖₁       : 0.5799999999999903\n",
      "scores min/max : -2.0368169945784423e-07 -1.6740611363542752e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.041764736561898656\n",
      "‖alpha‖₁       : 0.6878494183455235\n",
      "scores min/max : -0.44107273130225444 1.9492890302643469\n",
      "Mask mean value:  tensor(0.3660, dtype=torch.float64)\n",
      "max feasible return = 0.0593  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03238411782352349\n",
      "‖alpha‖₁       : 0.42237622051434354\n",
      "scores min/max : -3.8517370892843132 5.3129674252659305\n",
      "Mask mean value:  tensor(0.1193, dtype=torch.float64)\n",
      "max feasible return = 0.7645  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 0.04281555408925786\n",
      "‖alpha‖₁       : 0.8211814372156163\n",
      "scores min/max : -4.907667756561305 3.2639602543939232\n",
      "Mask mean value:  tensor(0.9677, dtype=torch.float64)\n",
      "max feasible return = -3.7061  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.00573239159460667\n",
      "‖alpha‖₁       : 0.7999999999999997\n",
      "scores min/max : -0.024366732555606924 -0.002091018062305952\n",
      "Mask mean value:  tensor(0.4616, dtype=torch.float64)\n",
      "max feasible return = -0.1679  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.135666250263454e-07\n",
      "‖alpha‖₁       : 0.6199999999999786\n",
      "scores min/max : 7.514487097276298e-08 9.16330751843885e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.893044525372986e-08\n",
      "‖alpha‖₁       : 0.4399999999999513\n",
      "scores min/max : 3.230846788370524e-09 1.2817310252540985e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "‖w_svm‖₂       : 2.3256225374823085e-07\n",
      "‖alpha‖₁       : 0.27999999999999703\n",
      "scores min/max : -1.686053880460673e-07 -1.5090572553229226e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = tensor([0.0900], dtype=torch.float64)\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "epoch  57 | train 0.005314 | val 0.006638\n",
      "Early stop: no val improvement in 10 epochs\n"
     ]
    }
   ],
   "source": [
    "#######################################################################\n",
    "# 0. imports & helper  ###############################################\n",
    "#######################################################################\n",
    "import copy, torch, math\n",
    "from qpth.qp import QPFunction\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# EXTRA ➋   classify assets by SVM sign -------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "def classify_assets(model, X, y, C, names=None):\n",
    "    \"\"\"returns two Python lists: invest, not_invest\"\"\"\n",
    "    alpha, w_svm, _ = solve_svm(model, X, y, C)\n",
    "    scores = model.embed(X.double()) @ w_svm            # (n,)\n",
    "    invest = (scores > 0).nonzero(as_tuple=True)[0]     # long side\n",
    "    avoid  = (scores <= 0).nonzero(as_tuple=True)[0]    # short / 0\n",
    "    if names is not None:\n",
    "        invest = [names[i] for i in invest.cpu().numpy()]\n",
    "        avoid  = [names[i] for i in avoid.cpu().numpy()]\n",
    "    else:\n",
    "        invest = invest.cpu().tolist()\n",
    "        avoid  = avoid.cpu().tolist()\n",
    "    return invest, avoid\n",
    "\n",
    "def make_spd(M, eps=1e-6):\n",
    "    \"\"\"Add minimal diagonal jitter until Cholesky succeeds.\"\"\"\n",
    "    I = torch.eye(M.size(0), device=M.device, dtype=M.dtype)\n",
    "    jitter = eps\n",
    "    for _ in range(10):\n",
    "        try:\n",
    "            torch.linalg.cholesky(M)\n",
    "            return M\n",
    "        except RuntimeError:\n",
    "            M = (M + M.t()) * 0.5 + jitter * I\n",
    "            jitter *= 10\n",
    "    raise RuntimeError(\"Unable to make SPD matrix\")\n",
    "\n",
    "def solve_svm(model, X_feat, y, C):\n",
    "    \"\"\"\n",
    "    Runs *just* the SVM part of EndToEndSVM_MVO_Sigmoid and\n",
    "    returns (alpha, w_svm, support_index_tensor)\n",
    "    \"\"\"\n",
    "    Xp = model.embed(X_feat.double())            # (n,d)\n",
    "    y  = y.view(-1).double()                     # (n,)\n",
    "\n",
    "    K      = Xp @ Xp.t()\n",
    "    Q_svm  = (y[:,None] * y[None,:]) * K\n",
    "    Q_svm  = make_spd(Q_svm)\n",
    "\n",
    "    n      = Xp.size(0)\n",
    "    p_svm  = -torch.ones(n, dtype=Xp.dtype, device=Xp.device)\n",
    "    G_svm  = torch.cat([-torch.eye(n, dtype=Xp.dtype, device=Xp.device),\n",
    "                         torch.eye(n, dtype=Xp.dtype,  device=Xp.device)], 0)\n",
    "    h_svm  = torch.cat([torch.zeros(n, dtype=Xp.dtype, device=Xp.device),\n",
    "                        C*torch.ones(n, dtype=Xp.dtype, device=Xp.device)], 0)\n",
    "\n",
    "    if (y == y[0]).all():              # single-class edge case\n",
    "        A_svm = torch.empty(0, n, dtype=Xp.dtype, device=Xp.device)\n",
    "        b_svm = torch.empty(0,    dtype=Xp.dtype, device=Xp.device)\n",
    "    else:\n",
    "        A_svm = y.unsqueeze(0)\n",
    "        b_svm = torch.zeros(1, dtype=Xp.dtype, device=Xp.device)\n",
    "\n",
    "    alpha = QPFunction(verbose=False)(Q_svm, p_svm, G_svm, h_svm, A_svm, b_svm)\n",
    "    alpha = torch.clamp(alpha, 0, C).view(-1)\n",
    "\n",
    "    w_svm = Xp.t().mv(alpha * y)       # weight vector in embedded space\n",
    "    sv    = (alpha > 1e-6)             # boolean mask of support vectors\n",
    "    #sv = (alpha > 1e-6)\n",
    "    return alpha, w_svm, sv.nonzero(as_tuple=True)[0]\n",
    "\n",
    "C_svm = 0.01\n",
    "tau = 0.05\n",
    "#######################################################################\n",
    "# 1. create model & *save* the initial weights  #######################\n",
    "#######################################################################\n",
    "device   = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model    = EndToEndSVM_MVO_Sigmoid(in_features=10, C_svm=C_svm,\n",
    "                                   eps=1e-6, tau=tau).to(device)\n",
    "\n",
    "model_init = copy.deepcopy(model).to(device)   # frozen copy for later\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "# 2. … your usual training loop here …\n",
    "#######################################################################\n",
    "# (use early-stopping or fixed epochs – whatever you prefer)\n",
    "# after training 'model' contains the *trained* embed weights\n",
    "# --------------------------------------------------------------------\n",
    "return_goals = [0.09]       # test values\n",
    "\n",
    "train_set = SnapshotDataset(train_snaps, return_goal=return_goals[0])\n",
    "val_set   = SnapshotDataset(val_snaps,   return_goal=return_goals[0])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=1, shuffle=True)\n",
    "val_loader   = DataLoader(val_set,   batch_size=1, shuffle=False)\n",
    "\n",
    "n_features = results[0][\"X_feat\"].shape[1]\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model  = EndToEndSVM_MVO_Sigmoid(\n",
    "            in_features=n_features,\n",
    "            C_svm=C_svm,\n",
    "            eps=1e-6,\n",
    "            tau=tau).to(device)\n",
    "\n",
    "optim  = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "patience   = 10         # stop if no progress for 10 epochs\n",
    "min_delta  = 1e-6      # what counts as “progress”\n",
    "#min_delta  = 1e-7      # what counts as “progress”\n",
    "max_epochs = 150      # hard cap (safety)\n",
    "best_val   = math.inf\n",
    "wait       = 0\n",
    "loss_hist, val_hist = [], []\n",
    "λ_hinge    = 0.0\n",
    "\n",
    "for epoch in range(1, max_epochs+1):\n",
    "    # ---- TRAIN --------------------------------------------------------\n",
    "    print(\"-----------------------------------------Epoch: \", epoch, \"----------------------------------------\")\n",
    "    model.train()\n",
    "    train_loss, n_batches = 0.0, 0\n",
    "    for X, y, mu, Sigma, goal in train_loader:\n",
    "        if torch.unique(y).numel() < 2:        # ← all +1 or all –1\n",
    "            continue                           # skip this batch entirely\n",
    "        X, y, mu, Sigma = (t.squeeze(0).to(device) for t in (X, y, mu, Sigma))\n",
    "\n",
    "        w, hinge     = model(X, y, mu, Sigma, goal)\n",
    "        \n",
    "        var_loss = torch.dot(w, Sigma @ w)\n",
    "        loss  = var_loss + λ_hinge * hinge\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        # put this inside the training loop, **after** loss.backward()\n",
    "        gnorm = model.embed.weight.grad.norm()\n",
    "        print(f\"grad‖embed‖ = {gnorm.item():.3e}\")   # should not be 0\n",
    "        optim.step()\n",
    "\n",
    "        train_loss += loss.item();  n_batches += 1\n",
    "\n",
    "    train_loss /= n_batches\n",
    "    loss_hist.append(train_loss)\n",
    "\n",
    "    # ---- VALIDATE -----------------------------------------------------\n",
    "    model.eval();  val_loss, n_batches = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y, mu, Sigma, goal in val_loader:\n",
    "            if torch.unique(y).numel() < 2:        # ← all +1 or all –1\n",
    "                continue                           # skip this batch entirely\n",
    "            X, y, mu, Sigma = (t.squeeze(0).to(device) for t in (X, y, mu, Sigma))\n",
    "            w, hinge = model(X, y, mu, Sigma, goal)\n",
    "            var_val_loss = torch.dot(w, Sigma @ w)\n",
    "            loss = var_val_loss + λ_hinge * hinge\n",
    "            val_loss += loss.item(); n_batches += 1\n",
    "    val_loss /= n_batches\n",
    "    val_hist.append(val_loss)\n",
    "\n",
    "    print(f\"epoch {epoch:3d} | train {train_loss:.6f} | val {val_loss:.6f}\")\n",
    "\n",
    "    # ---- EARLY-STOPPING LOGIC ----------------------------------------\n",
    "    if val_loss < best_val - min_delta:\n",
    "        best_val = val_loss\n",
    "        wait     = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")   # checkpoint\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(f\"Early stop: no val improvement in {patience} epochs\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Comparing INITIAL vs TRAINED SVM support vectors ===\n",
      "snapshot date: 2023-12-31 00:00:00\n",
      "\n",
      "INIT  | #SV = 59 | indices: [ 4  5  7  8  9 10 12 13 14 16 17 18 19 20 23 25 26 27 28 29 33 34 35 37\n",
      " 38 39 41 43 46 48 50 53 55 56 58 59 62 63 65 66 69 70 72 74 75 76 77 78\n",
      " 79 80 81 83 84 87 90 91 94 95 97]\n",
      "TRAIN | #SV = 59 | indices: [ 4  5  7  8  9 10 12 13 14 16 17 18 19 20 23 25 26 27 28 29 33 34 35 37\n",
      " 38 39 41 43 46 48 50 53 55 56 58 59 62 63 65 66 69 70 72 74 75 76 77 78\n",
      " 79 80 81 83 84 87 90 91 94 95 97]\n"
     ]
    }
   ],
   "source": [
    "# pick ONE snapshot to test on – e.g. the last in validation set\n",
    "test_snap   = val_set[0]                # (X, y, mu, Sigma, goal)\n",
    "\n",
    "X_t, y_t, *_ = test_snap\n",
    "X_t, y_t     = X_t.to(device), y_t.to(device)\n",
    "\n",
    "print(\"=== Comparing INITIAL vs TRAINED SVM support vectors ===\")\n",
    "print(f\"snapshot date: {val_snaps[6]['date']}\\n\")\n",
    "\n",
    "# -------- initial model ---------------------------------------------\n",
    "alpha0, w0, sv0 = solve_svm(model_init, X_t, y_t, C_svm)\n",
    "print(f\"INIT  | #SV = {len(sv0):2d} | indices:\", sv0.cpu().numpy())\n",
    "\n",
    "# -------- trained model ---------------------------------------------\n",
    "alpha1, w1, sv1 = solve_svm(model,       X_t, y_t, C_svm)\n",
    "print(f\"TRAIN | #SV = {len(sv1):2d} | indices:\", sv1.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INIT  | #SV = 59 | ['ABG', 'ACAD', 'ACLS', 'ADUS', 'AEIS', 'AEO', 'AGYS', 'AIN', 'AIR', 'ALG', 'ALGT', 'AMD', 'AMSF', 'AMWD', 'ANIP', 'APD', 'APOG', 'ARWR', 'ATEN', 'AVA', 'BCPC', 'BGS', 'BHE', 'BKE', 'BLMN', 'BRK-B', 'CCOI', 'CENT', 'COP', 'CPK', 'CVX', 'DIS', 'ECL', 'EOG', 'FIZZ', 'FMC', 'GOGO', 'GOOG', 'HON', 'JNJ', 'LLY', 'LOW', 'MCD', 'META', 'MGEE', 'MRK', 'MSEX', 'MSFT', 'NEE', 'NFLX', 'NVDA', 'ORCL', 'OTTR', 'RTX', 'TMUS', 'TSLA', 'V', 'VMC', 'XOM']\n",
      "TRAIN | #SV = 59 | ['ABG', 'ACAD', 'ACLS', 'ADUS', 'AEIS', 'AEO', 'AGYS', 'AIN', 'AIR', 'ALG', 'ALGT', 'AMD', 'AMSF', 'AMWD', 'ANIP', 'APD', 'APOG', 'ARWR', 'ATEN', 'AVA', 'BCPC', 'BGS', 'BHE', 'BKE', 'BLMN', 'BRK-B', 'CCOI', 'CENT', 'COP', 'CPK', 'CVX', 'DIS', 'ECL', 'EOG', 'FIZZ', 'FMC', 'GOGO', 'GOOG', 'HON', 'JNJ', 'LLY', 'LOW', 'MCD', 'META', 'MGEE', 'MRK', 'MSEX', 'MSFT', 'NEE', 'NFLX', 'NVDA', 'ORCL', 'OTTR', 'RTX', 'TMUS', 'TSLA', 'V', 'VMC', 'XOM']\n"
     ]
    }
   ],
   "source": [
    "init_sv_tics  = [tickers[i] for i in sv0.cpu().numpy()]\n",
    "train_sv_tics = [tickers[i] for i in sv1.cpu().numpy()]\n",
    "\n",
    "print(\"INIT  | #SV =\", len(init_sv_tics),  \"|\", init_sv_tics)\n",
    "print(\"TRAIN | #SV =\", len(train_sv_tics), \"|\", train_sv_tics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Δ‖embed‖₂ = tensor(1.5102, dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "#SV init = 59  | #SV train = 59\n",
      "init SV indices : [ 4  5  7  8  9 10 12 13 14 16 17 18 19 20 23]\n",
      "train SV indices: [ 4  5  7  8  9 10 12 13 14 16 17 18 19 20 23]\n",
      "cos( w₀ , w₁ ) = 0.9142729607175512\n"
     ]
    }
   ],
   "source": [
    "import copy, torch, numpy as np\n",
    "\n",
    "\n",
    "# 1) forward once, grab alphas and weight vector\n",
    "alpha0, w0, sv0 = solve_svm(model_init, X_t, y_t, model.C)\n",
    "alpha1, w1, sv1 = solve_svm(model,       X_t, y_t, model.C)\n",
    "\n",
    "print(\"Δ‖embed‖₂ =\", torch.norm(model.embed.weight - model_init.embed.weight))\n",
    "print(\"#SV init =\", len(sv0), \" | #SV train =\", len(sv1))\n",
    "print(\"init SV indices :\", sv0.cpu().numpy()[:15])\n",
    "print(\"train SV indices:\", sv1.cpu().numpy()[:15])\n",
    "\n",
    "# cosine similarity between the two hyper-planes\n",
    "cos = (w0 @ w1) / (w0.norm() * w1.norm())\n",
    "print(\"cos( w₀ , w₁ ) =\", cos.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================  return_goal = 9.0e-02  ================\n",
      "Initial SVM\n",
      "  invest : AAP, AAPL, ABBV, ABCB, ABG, ACAD, ACIW, ACLS, ADUS, AEIS, AEO, AEP, AGYS, AIN, AIR, AL, ALG, ALGT, AMD, AMSF, AMWD, AMZN, ANDE, ANIP, APAM, APD, APOG, ARWR, ATEN, AVA, AWR, AXL, BANC, BCPC, BGS, BHE, BJRI, BKE, BLMN, BRK-B, CAT, CE, CENT, CENX, CNK, COP, COST, CPK, CVI, CVX, CWT, D, DIS, DUK, ECL, EOG, FCX, FIZZ, FMC, FUL, GE, GOOG, HD, HON, JNJ, JPM, KO, LLY, MA, MDU, META, MGEE, MRK, MSEX, MSFT, NEE, NFLX, NVDA, NWN, OTTR, PEP, PG, RTX, SHW, SO, TMUS, TSLA, UNH, UNP, V, VMC, WMT, XOM\n",
      "  avoid  : CCOI, GOGO, LOW, MCD, ORCL\n",
      "Trained SVM\n",
      "  invest : AAPL, ABBV, ABCB, AMZN, APAM, AXL, BANC, BLMN, CAT, CNK, CVI, GE, HD, JPM, LLY, MA, PEP, SHW, UNP\n",
      "  avoid  : AAP, ABG, ACAD, ACIW, ACLS, ADUS, AEIS, AEO, AEP, AGYS, AIN, AIR, AL, ALG, ALGT, AMD, AMSF, AMWD, ANDE, ANIP, APD, APOG, ARWR, ATEN, AVA, AWR, BCPC, BGS, BHE, BJRI, BKE, BRK-B, CCOI, CE, CENT, CENX, COP, COST, CPK, CVX, CWT, D, DIS, DUK, ECL, EOG, FCX, FIZZ, FMC, FUL, GOGO, GOOG, HON, JNJ, KO, LOW, MCD, MDU, META, MGEE, MRK, MSEX, MSFT, NEE, NFLX, NVDA, NWN, ORCL, OTTR, PG, RTX, SO, TMUS, TSLA, UNH, V, VMC, WMT, XOM\n",
      "Δ support:\n",
      "  ➖ moved to avoid  : AAP, ABG, ACAD, ACIW, ACLS, ADUS, AEIS, AEO, AEP, AGYS, AIN, AIR, AL, ALG, ALGT, AMD, AMSF, AMWD, ANDE, ANIP, APD, APOG, ARWR, ATEN, AVA, AWR, BCPC, BGS, BHE, BJRI, BKE, BRK-B, CE, CENT, CENX, COP, COST, CPK, CVX, CWT, D, DIS, DUK, ECL, EOG, FCX, FIZZ, FMC, FUL, GOOG, HON, JNJ, KO, MDU, META, MGEE, MRK, MSEX, MSFT, NEE, NFLX, NVDA, NWN, OTTR, PG, RTX, SO, TMUS, TSLA, UNH, V, VMC, WMT, XOM\n"
     ]
    }
   ],
   "source": [
    "# convert tickers list to numpy array\n",
    "\n",
    "tick_vec = np.array(tickers)\n",
    "\n",
    "Xsnap = val_snaps[0]      \n",
    "X_test = to_tensor(Xsnap[\"X_feat\"]).to(device)\n",
    "y_test = to_tensor(Xsnap[\"y\"]).to(device)\n",
    "\n",
    "\n",
    "for g in return_goals:\n",
    "    print(f\"\\n================  return_goal = {g:.1e}  ================\")\n",
    "\n",
    "    inv0, avo0 = classify_assets(model_init,  X_test, y_test, model_init.C,  tick_vec)\n",
    "    inv1, avo1 = classify_assets(model, X_test, y_test, model.C, tick_vec)\n",
    "\n",
    "    # ---- pretty print ------------------------------------------------\n",
    "    def _fmt(lst): return \", \".join(str(x) for x in lst) if lst else \"—\"\n",
    "\n",
    "    print(\"Initial SVM\")\n",
    "    print(\"  invest :\", _fmt(inv0))\n",
    "    print(\"  avoid  :\", _fmt(avo0))\n",
    "\n",
    "    print(\"Trained SVM\")\n",
    "    print(\"  invest :\", _fmt(inv1))\n",
    "    print(\"  avoid  :\", _fmt(avo1))\n",
    "\n",
    "    # ---- simple diff -------------------------------------------------\n",
    "    added   = sorted(set(inv1) - set(inv0))\n",
    "    removed = sorted(set(inv0) - set(inv1))\n",
    "    if added or removed:\n",
    "        print(\"Δ support:\")\n",
    "        if added:   print(\"  ➕ moved to invest :\", _fmt(added))\n",
    "        if removed: print(\"  ➖ moved to avoid  :\", _fmt(removed))\n",
    "    else:\n",
    "        print(\"No change in invest/avoid sets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksg\\AppData\\Local\\Temp\\ipykernel_18580\\2802520318.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pt\"))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "plt.plot(loss_hist, label=\"train\")\n",
    "plt.plot(val_hist,  label=\"val\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Mean loss\")\n",
    "plt.legend(); plt.grid(True); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training support vectors = 6545\n",
      "‖w_svm‖₂       : 0.08517670901574435\n",
      "‖alpha‖₁       : 0.5780456422152989\n",
      "scores min/max : -2.1154033597001454 5.799849306884821\n",
      "Mask mean value:  tensor(0.2819, dtype=torch.float64)\n",
      "max feasible return = 0.1109  |  goal = 0.09\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 3.959191922020834e-07\n",
      "‖alpha‖₁       : 0.5799999999999739\n",
      "scores min/max : -3.320238276695594e-07 -2.4098393615283107e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.7446  |  goal = 0.09\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 1.0810815061098459e-07\n",
      "‖alpha‖₁       : 0.29999999999997\n",
      "scores min/max : 4.3252347480588214e-08 4.8391594901502927e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.4127  |  goal = 0.09\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 6.550504295705863e-08\n",
      "‖alpha‖₁       : 0.5999999999999983\n",
      "scores min/max : 1.3331716493788767e-08 5.010081059628796e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.8674  |  goal = 0.09\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 4.479216781883496e-08\n",
      "‖alpha‖₁       : 0.37999999999999945\n",
      "scores min/max : -1.6200533443551212e-08 -6.828420942370901e-09\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.6690  |  goal = 0.09\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 5.250656193053795e-06\n",
      "‖alpha‖₁       : 0.3199999999411239\n",
      "scores min/max : 5.2542803554120415e-06 8.665089243877341e-06\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.4259  |  goal = 0.09\n",
      "‖w_svm‖₂       : 0.04610487940866692\n",
      "‖alpha‖₁       : 0.7396938186347031\n",
      "scores min/max : -3.4496555022444895 2.1824710004293317\n",
      "Mask mean value:  tensor(0.9034, dtype=torch.float64)\n",
      "max feasible return = -0.9437  |  goal = 0.09\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.025925366487794203\n",
      "‖alpha‖₁       : 0.6151642422592685\n",
      "scores min/max : -1.9460830696029303 1.1821244798496005\n",
      "Mask mean value:  tensor(0.7129, dtype=torch.float64)\n",
      "max feasible return = 2.4858  |  goal = 0.09\n",
      "‖w_svm‖₂       : 4.709990687272244e-07\n",
      "‖alpha‖₁       : 0.4599999999999965\n",
      "scores min/max : 1.8611430282422236e-07 7.805160328625132e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 2.6367  |  goal = 0.09\n",
      "‖w_svm‖₂       : 1.407382128788425e-07\n",
      "‖alpha‖₁       : 0.5199999999999749\n",
      "scores min/max : 8.457516838398304e-08 9.749866329990521e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5353  |  goal = 0.09\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 2.594257557020888e-07\n",
      "‖alpha‖₁       : 0.5799999999999985\n",
      "scores min/max : -2.463000371538592e-07 -2.1015151106571519e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.4919  |  goal = 0.09\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "‖w_svm‖₂       : 0.043399092928016365\n",
      "‖alpha‖₁       : 0.6878732767913913\n",
      "scores min/max : -0.44297595715897786 1.9465578789129665\n",
      "Mask mean value:  tensor(0.3612, dtype=torch.float64)\n",
      "max feasible return = 0.0692  |  goal = 0.09\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.03293033723154994\n",
      "‖alpha‖₁       : 0.4223151273053281\n",
      "scores min/max : -3.8612329737029145 5.294489543262591\n",
      "Mask mean value:  tensor(0.1095, dtype=torch.float64)\n",
      "max feasible return = 0.6968  |  goal = 0.09\n",
      "‖w_svm‖₂       : 0.043383860814719434\n",
      "‖alpha‖₁       : 0.8211632954907577\n",
      "scores min/max : -4.94639750370044 3.223571461466438\n",
      "Mask mean value:  tensor(0.9658, dtype=torch.float64)\n",
      "max feasible return = -3.6961  |  goal = 0.09\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 0.006453682688255625\n",
      "‖alpha‖₁       : 0.7999999999999998\n",
      "scores min/max : -0.029070527090996293 -0.0011776817257559743\n",
      "Mask mean value:  tensor(0.4591, dtype=torch.float64)\n",
      "max feasible return = -0.1670  |  goal = 0.09\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 4.115048775846686e-07\n",
      "‖alpha‖₁       : 0.6199999999999682\n",
      "scores min/max : 8.875064521308671e-08 1.0526724365845487e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -1.2035  |  goal = 0.09\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "‖w_svm‖₂       : 3.886648309719644e-08\n",
      "‖alpha‖₁       : 0.4399999999999999\n",
      "scores min/max : 5.420455101877855e-09 1.4990612195903587e-08\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = 1.0643  |  goal = 0.09\n",
      "‖w_svm‖₂       : 2.3393548364902151e-07\n",
      "‖alpha‖₁       : 0.27999999999998165\n",
      "scores min/max : -2.416999901710144e-07 -2.2379252059666764e-07\n",
      "Mask mean value:  tensor(0.5000, dtype=torch.float64)\n",
      "max feasible return = -0.5319  |  goal = 0.09\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "Warning: all labels identical, no SVM hyperplane constructed.\n",
      "‖w_svm‖₂       : 0.1546849862682939\n",
      "‖alpha‖₁       : 0.023934227422660194\n",
      "scores min/max : -1.0550037607924798 -0.9993217652190413\n",
      "Mask mean value:  tensor(1.5609e-09, dtype=torch.float64)\n",
      "max feasible return = 0.0000  |  goal = 0.09\n",
      "⚠️  Target return is infeasible for this mask snapshot.\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "          var_nn    var_svm\n",
      "count  19.000000  19.000000\n",
      "mean    0.006356   0.009030\n",
      "std     0.005062   0.011580\n",
      "min     0.001049   0.001049\n",
      "25%     0.002645   0.002819\n",
      "50%     0.003874   0.003375\n",
      "75%     0.009660   0.010162\n",
      "max     0.019501   0.049434\n",
      "feasible ratio NN  : 0.631578947368421\n",
      "feasible ratio SVM : 0.8421052631578947\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC                     # keeps support_ info\n",
    "import numpy as np, torch\n",
    "\n",
    "# ---------- gather pooled (X, y) ------------------------------------\n",
    "X_rows, y_rows = [], []\n",
    "for snap in train_snaps:\n",
    "    X = snap[\"X_feat\"]          # NumPy already (n,d)\n",
    "    y = snap[\"y\"]               # (n,)\n",
    "    mask = np.isfinite(X).all(axis=1) & np.isfinite(y)\n",
    "    if mask.sum() >= 2:         # need ≥2 classes inside the batch\n",
    "        X_rows.append(X[mask])\n",
    "        y_rows.append(y[mask])\n",
    "\n",
    "X_train = np.vstack(X_rows)\n",
    "y_train = np.concatenate(y_rows)\n",
    "\n",
    "# ---------- build & fit ---------------------------------------------\n",
    "C_svm = 0.01\n",
    "svm_clf = make_pipeline(\n",
    "    StandardScaler(),                       # per-feature z-score\n",
    "    SVC(kernel=\"linear\", C=C_svm)\n",
    ")\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"# training support vectors =\", svm_clf[-1].support_.size)\n",
    "\n",
    "def mvo_from_scores(scores, mu, Sigma, tau, goal):\n",
    "    \"\"\"\n",
    "    Turn `scores` → mask → solve the identical QP used in your NN forward.\n",
    "\n",
    "    scores, mu : 1-D torch tensors   (n,)\n",
    "    Sigma      : 2-D torch tensor    (n,n)\n",
    "    \"\"\"\n",
    "    mask = torch.sigmoid(scores / tau)           # gate\n",
    "    n    = len(scores)\n",
    "\n",
    "    # ----- inequalities ------------------------------------------------\n",
    "    G_box = torch.cat([-torch.eye(n),  torch.eye(n)], 0)\n",
    "    h_box = torch.cat([torch.zeros(n), mask], 0)\n",
    "\n",
    "    G_ret = -mu.unsqueeze(0)\n",
    "    h_ret = -torch.tensor([goal], dtype=mu.dtype)\n",
    "\n",
    "    G = torch.cat([G_box, G_ret], 0)\n",
    "    h = torch.cat([h_box, h_ret], 0)\n",
    "\n",
    "    # ----- equality ----------------------------------------------------\n",
    "    A = torch.ones(1, n, dtype=mu.dtype)\n",
    "    b = torch.tensor([1.0], dtype=mu.dtype)\n",
    "\n",
    "    w = QPFunction(verbose=False)(\n",
    "            Sigma, torch.zeros(n, dtype=mu.dtype), G, h, A, b\n",
    "        )\n",
    "    return w, mask\n",
    "\n",
    "nn_stats  = []          # (var, feasible?, mean mask)\n",
    "svm_stats = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for snap in val_snaps:\n",
    "        # ------------ tensors -----------------------------------------\n",
    "        X  = torch.as_tensor(snap[\"X_feat\"],  dtype=torch.float64)\n",
    "        y  = torch.as_tensor(snap[\"y\"],       dtype=torch.float64)\n",
    "        mu = torch.as_tensor(snap[\"mu_fore\"], dtype=torch.float64)\n",
    "        S  = torch.as_tensor(snap[\"Sigma_fore\"], dtype=torch.float64)\n",
    "        goal = 0.09\n",
    "\n",
    "        # ------------ neural network ----------------------------------\n",
    "        w_nn, _ = model(X, y, mu, S, goal)\n",
    "        var_nn  = (w_nn @ (S @ w_nn)).item()\n",
    "        feas_nn = (mu @ w_nn >= goal - 1e-8).item()\n",
    "        nn_stats.append((var_nn, feas_nn))\n",
    "\n",
    "        # ------------ SVM pipeline ------------------------------------\n",
    "        #  a) get standardised features identical to training pipe\n",
    "        X_std   = torch.as_tensor(\n",
    "                     svm_clf[:-1].transform(X.numpy()), dtype=torch.float64)\n",
    "        #  b) linear scores:  X_std @ w + b\n",
    "        coef    = torch.as_tensor(svm_clf[-1].coef_.ravel(),\n",
    "                                  dtype=torch.float64)\n",
    "        bias    = torch.as_tensor(svm_clf[-1].intercept_, dtype=torch.float64)\n",
    "        scores  = X_std @ coef + bias\n",
    "\n",
    "        w_svm, mask = mvo_from_scores(scores, mu, S, tau=0.1, goal=goal)\n",
    "        var_svm = (w_svm @ S @ w_svm.T).item()\n",
    "        feas_svm = (mu @ w_svm.T >= goal - 1e-8).item()\n",
    "        svm_stats.append((var_svm, feas_svm))\n",
    "\n",
    "# ------------- aggregate ----------------------------------------------\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "    \"var_nn\"  : [v for v,_ in nn_stats],\n",
    "    \"var_svm\" : [v for v,_ in svm_stats],\n",
    "    \"feas_nn\" : [f for _,f in nn_stats],\n",
    "    \"feas_svm\": [f for _,f in svm_stats]\n",
    "})\n",
    "print(df.describe())\n",
    "print(\"feasible ratio NN  :\", df.feas_nn.mean())\n",
    "print(\"feasible ratio SVM :\", df.feas_svm.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----- ASSUMPTIONS ---------------------------------------------------\n",
    "# 1) df is a pandas DataFrame already in memory\n",
    "# 2) It has four columns named exactly:\n",
    "#        'var_nn', 'var_svm', 'feas_nn', 'feas_svm'\n",
    "#    where\n",
    "#        • var_*     : portfolio variance (float)\n",
    "#        • feas_*    : boolean or 0/1 feasible flag\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "try:\n",
    "    df  # noqa: F401\n",
    "except NameError:\n",
    "    raise RuntimeError(\"DataFrame 'df' with columns var_nn, var_svm, feas_nn, feas_svm was not found.\")\n",
    "\n",
    "# -------- Scatter: variance NN vs traditional SVM --------------------\n",
    "plt.figure()\n",
    "plt.scatter(df[\"var_svm\"], df[\"var_nn\"])\n",
    "max_val = max(df[\"var_svm\"].max(), df[\"var_nn\"].max()) * 1.05\n",
    "plt.plot([0, max_val], [0, max_val], linestyle='--')   # y = x reference\n",
    "plt.xlabel(\"Traditional SVM portfolio variance\")\n",
    "plt.ylabel(\"Neural‑net portfolio variance\")\n",
    "plt.title(\"Snapshot comparison\\n(lower‑left → NN outperforms SVM)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------- Bar: feasibility ratio -------------------------------------\n",
    "plt.figure()\n",
    "feas_rates = [df[\"feas_svm\"].mean(), df[\"feas_nn\"].mean()]\n",
    "labels     = [\"Traditional SVM\", \"Neural Net\"]\n",
    "plt.bar(labels, feas_rates)\n",
    "plt.ylabel(\"Fraction of snapshots feasible\")\n",
    "plt.title(\"Return‑target feasibility\")\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis=\"y\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training matrix: (7448, 10)\n",
      "Val points inside margin (would-be SV) : 95 / 98\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 0.  Collect one big (X, y) matrix from the training snapshots\n",
    "# ------------------------------------------------------------\n",
    "import numpy as np\n",
    "\n",
    "X_rows, y_rows = [], []\n",
    "for snap in train_snaps:          # ← the earlier-date part of `snapshots`\n",
    "    X  = snap[\"X_feat\"]           # (n_assets, d_features)  – NumPy already\n",
    "    y  = snap[\"y\"]                # (n_assets,)            – ±1 labels\n",
    "\n",
    "    # keep only rows with finite numbers\n",
    "    mask = np.isfinite(X).all(axis=1)\n",
    "    if mask.sum() < 2:            # need at least 2 distinct labels\n",
    "        continue\n",
    "    X_rows.append(X[mask])\n",
    "    y_rows.append(y[mask])\n",
    "\n",
    "X_train = np.vstack(X_rows)       # shape (N_total_assets , d)\n",
    "y_train = np.concatenate(y_rows)  # shape (N_total_assets ,)\n",
    "\n",
    "print(\"training matrix:\", X_train.shape)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1.  Build a pipeline:  Standardise  →  Linear SVM\n",
    "# ------------------------------------------------------------\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC        # faster than SVC for linear kernel\n",
    "\n",
    "C_svm = 0.01          # use the same box-constraint as your end-to-end net\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_clf = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    SVC(kernel=\"linear\", C=C_svm)   # hinge loss, linear kernel\n",
    ")\n",
    "\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "svc_core = svm_clf[-1]            # the SVC estimator inside the pipeline\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2.  Example: predict on the first validation snapshot\n",
    "# ------------------------------------------------------------\n",
    "# pick one validation snapshot\n",
    "X_val, y_val = val_snaps[0][\"X_feat\"], val_snaps[0][\"y\"]\n",
    "\n",
    "# need the *standardised* version the SVM sees\n",
    "X_val_std = svm_clf[-2].transform(X_val)           # pipeline[-2] is StandardScaler\n",
    "decision  = X_val_std.dot(svc_core.coef_.ravel()) + svc_core.intercept_[0]\n",
    "margin    = y_val * decision                       # y_i * f(x_i)\n",
    "\n",
    "inside    = margin <= 1 + 1e-12                    # boolean mask\n",
    "print(\"Val points inside margin (would-be SV) :\", inside.sum(), \"/\", len(y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ep        train          val\n",
      "    1 0.0001858359 0.0001533872\n",
      "    2 0.0001851501 0.0001532505\n",
      "    3 0.0001846446 0.0001531439\n",
      "    4 0.0001841351 0.0001535411\n",
      "    5 0.0001836239 0.0001529683\n",
      "    6 0.0001831475 0.0001529549\n",
      "    7 0.0001826758 0.0001529642\n",
      "    8 0.0001822643 0.0001528081\n",
      "    9 0.0001818951 0.0001532138\n",
      "   10 0.0001817936 0.0001527923\n",
      "   11 0.0001811010 0.0001525588\n",
      "   12 0.0001805994 0.0001530073\n",
      "   13 0.0001801700 0.0001530777\n",
      "   14 0.0001797316 0.0001527838\n",
      "   15 0.0001794381 0.0001525519\n",
      "   16 0.0001789892 0.0001529037\n",
      "   17 0.0001786542 0.0001530188\n",
      "   18 0.0001784510 0.0001524405\n",
      "   19 0.0001782596 0.0001522422\n",
      "   20 0.0001782144 0.0001520858\n",
      "   21 0.0001780297 0.0001521821\n",
      "   22 0.0001778554 0.0001521854\n",
      "   23 0.0001776945 0.0001520087\n",
      "   24 0.0001776006 0.0001518551\n",
      "   25 0.0001774663 0.0001516577\n",
      "   26 0.0001772819 0.0001514638\n",
      "   27 0.0001771631 0.0001512537\n",
      "   28 0.0001770675 0.0001508963\n",
      "   29 0.0001770136 0.0001509384\n",
      "   30 0.0001769376 0.0001501443\n",
      "   31 0.0001767737 0.0001495662\n",
      "   32 0.0001766647 0.0001491062\n",
      "   33 0.0001765265 0.0001485925\n",
      "   34 0.0001764069 0.0001483056\n",
      "   35 0.0001766251 0.0001486455\n",
      "   36 0.0001763367 0.0001482275\n",
      "   37 0.0001762450 0.0001481987\n",
      "   38 0.0001761312 0.0001478194\n",
      "   39 0.0001760778 0.0001471372\n",
      "   40 0.0001758991 0.0001469990\n",
      "   41 0.0001758754 0.0001468038\n",
      "   42 0.0001759020 0.0001473654\n",
      "   43 0.0001759311 0.0001470628\n",
      "   44 0.0001757736 0.0001469940\n",
      "   45 0.0001756256 0.0001462574\n",
      "   46 0.0001754945 0.0001460467\n",
      "   47 0.0001755490 0.0001457113\n",
      "   48 0.0001755974 0.0001463498\n",
      "   49 0.0001756786 0.0001464713\n",
      "   50 0.0001754221 0.0001463422\n",
      "   51 0.0001752801 0.0001459520\n",
      "   52 0.0001751896 0.0001453728\n",
      "   53 0.0001751558 0.0001449519\n",
      "   54 0.0001751062 0.0001444073\n",
      "   55 0.0001751233 0.0001443857\n",
      "   56 0.0001750573 0.0001442480\n",
      "   57 0.0001750622 0.0001441078\n",
      "   58 0.0001750022 0.0001439359\n",
      "   59 0.0001749999 0.0001440544\n",
      "   60 0.0001749645 0.0001438898\n",
      "   61 0.0001749352 0.0001439004\n",
      "   62 0.0001749546 0.0001436978\n",
      "   63 0.0001749202 0.0001437419\n",
      "   64 0.0001748991 0.0001436185\n",
      "   65 0.0001748847 0.0001431190\n",
      "   66 0.0001748586 0.0001430790\n",
      "   67 0.0001749022 0.0001428672\n",
      "   68 0.0001748279 0.0001425540\n",
      "   69 0.0001748149 0.0001423842\n",
      "   70 0.0001747573 0.0001416908\n",
      "   71 0.0001747376 0.0001416534\n",
      "   72 0.0001746993 0.0001405071\n",
      "   73 0.0001746631 0.0001407776\n",
      "   74 0.0001746122 0.0001399696\n",
      "   75 0.0001745648 0.0001390990\n",
      "   76 0.0001745192 0.0001388211\n",
      "   77 0.0001745126 0.0001378879\n",
      "   78 0.0001744299 0.0001375980\n",
      "   79 0.0001743988 0.0001376539\n",
      "   80 0.0001743847 0.0001375265\n",
      "   81 0.0001743888 0.0001377906\n",
      "   82 0.0001743431 0.0001383043\n",
      "   83 0.0001744711 0.0001368812\n",
      "   84 0.0001743881 0.0001381016\n",
      "   85 0.0001743417 0.0001381091\n",
      "   86 0.0001743278 0.0001385617\n",
      "   87 0.0001744132 0.0001380908\n",
      "   88 0.0001743181 0.0001382421\n",
      "   89 0.0001743154 0.0001384619\n",
      "   90 0.0001742987 0.0001386684\n",
      "   91 0.0001742926 0.0001388682\n",
      "   92 0.0001742914 0.0001380918\n",
      "   93 0.0001742765 0.0001390023\n"
     ]
    }
   ],
   "source": [
    "print(\"{:>5} {:>12} {:>12}\".format(\"ep\",\"train\",\"val\"))\n",
    "for e,(t,v) in enumerate(zip(loss_hist, val_hist),1):\n",
    "    print(f\"{e:5d} {t:.10f} {v:.10f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
