{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ticker</th>\n",
       "      <th>pe_exi</th>\n",
       "      <th>de_ratio</th>\n",
       "      <th>close_price</th>\n",
       "      <th>exp_ret</th>\n",
       "      <th>vol</th>\n",
       "      <th>mom</th>\n",
       "      <th>Mkt-RF</th>\n",
       "      <th>SMB</th>\n",
       "      <th>HML</th>\n",
       "      <th>RMW</th>\n",
       "      <th>CMA</th>\n",
       "      <th>RF</th>\n",
       "      <th>ret_excess</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-02-29</td>\n",
       "      <td>AAP</td>\n",
       "      <td>16.706458</td>\n",
       "      <td>3.311468</td>\n",
       "      <td>74.872002</td>\n",
       "      <td>0.017554</td>\n",
       "      <td>0.067338</td>\n",
       "      <td>0.203171</td>\n",
       "      <td>4.42</td>\n",
       "      <td>-1.71</td>\n",
       "      <td>0.43</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.017554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-03-31</td>\n",
       "      <td>AAP</td>\n",
       "      <td>17.332681</td>\n",
       "      <td>3.311468</td>\n",
       "      <td>77.731010</td>\n",
       "      <td>0.028689</td>\n",
       "      <td>0.071533</td>\n",
       "      <td>0.367164</td>\n",
       "      <td>3.11</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>1.14</td>\n",
       "      <td>-0.54</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.028689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-04-30</td>\n",
       "      <td>AAP</td>\n",
       "      <td>17.964775</td>\n",
       "      <td>3.311468</td>\n",
       "      <td>80.565750</td>\n",
       "      <td>0.027881</td>\n",
       "      <td>0.071351</td>\n",
       "      <td>0.354526</td>\n",
       "      <td>-0.85</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.027881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-05-31</td>\n",
       "      <td>AAP</td>\n",
       "      <td>13.118705</td>\n",
       "      <td>3.447840</td>\n",
       "      <td>64.013748</td>\n",
       "      <td>0.031124</td>\n",
       "      <td>0.070729</td>\n",
       "      <td>0.407354</td>\n",
       "      <td>-6.19</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-1.07</td>\n",
       "      <td>2.08</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.031024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-06-30</td>\n",
       "      <td>AAP</td>\n",
       "      <td>12.269784</td>\n",
       "      <td>3.447840</td>\n",
       "      <td>59.923603</td>\n",
       "      <td>0.018280</td>\n",
       "      <td>0.096397</td>\n",
       "      <td>0.178720</td>\n",
       "      <td>3.89</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.62</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.018280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17980</th>\n",
       "      <td>2024-08-31</td>\n",
       "      <td>XOM</td>\n",
       "      <td>14.107656</td>\n",
       "      <td>0.771855</td>\n",
       "      <td>114.883614</td>\n",
       "      <td>0.012588</td>\n",
       "      <td>0.052356</td>\n",
       "      <td>0.144777</td>\n",
       "      <td>1.61</td>\n",
       "      <td>-3.65</td>\n",
       "      <td>-1.13</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.007788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17981</th>\n",
       "      <td>2024-09-30</td>\n",
       "      <td>XOM</td>\n",
       "      <td>14.021531</td>\n",
       "      <td>0.771855</td>\n",
       "      <td>114.182266</td>\n",
       "      <td>0.009021</td>\n",
       "      <td>0.051371</td>\n",
       "      <td>0.097898</td>\n",
       "      <td>1.74</td>\n",
       "      <td>-1.02</td>\n",
       "      <td>-2.59</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.005021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17982</th>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>XOM</td>\n",
       "      <td>13.968900</td>\n",
       "      <td>0.771855</td>\n",
       "      <td>113.753670</td>\n",
       "      <td>0.003723</td>\n",
       "      <td>0.049151</td>\n",
       "      <td>0.031893</td>\n",
       "      <td>-0.97</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>0.89</td>\n",
       "      <td>-1.38</td>\n",
       "      <td>1.03</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>-0.000177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17983</th>\n",
       "      <td>2024-11-30</td>\n",
       "      <td>XOM</td>\n",
       "      <td>14.689913</td>\n",
       "      <td>0.737662</td>\n",
       "      <td>115.847267</td>\n",
       "      <td>0.011724</td>\n",
       "      <td>0.037114</td>\n",
       "      <td>0.141942</td>\n",
       "      <td>6.51</td>\n",
       "      <td>4.78</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-2.62</td>\n",
       "      <td>-2.17</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.007724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17984</th>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>XOM</td>\n",
       "      <td>13.396015</td>\n",
       "      <td>0.737662</td>\n",
       "      <td>103.865776</td>\n",
       "      <td>0.014966</td>\n",
       "      <td>0.035716</td>\n",
       "      <td>0.187306</td>\n",
       "      <td>-3.17</td>\n",
       "      <td>-3.87</td>\n",
       "      <td>-2.95</td>\n",
       "      <td>1.82</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.011266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17985 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date ticker     pe_exi  de_ratio  close_price   exp_ret  \\\n",
       "0      2012-02-29    AAP  16.706458  3.311468    74.872002  0.017554   \n",
       "1      2012-03-31    AAP  17.332681  3.311468    77.731010  0.028689   \n",
       "2      2012-04-30    AAP  17.964775  3.311468    80.565750  0.027881   \n",
       "3      2012-05-31    AAP  13.118705  3.447840    64.013748  0.031124   \n",
       "4      2012-06-30    AAP  12.269784  3.447840    59.923603  0.018280   \n",
       "...           ...    ...        ...       ...          ...       ...   \n",
       "17980  2024-08-31    XOM  14.107656  0.771855   114.883614  0.012588   \n",
       "17981  2024-09-30    XOM  14.021531  0.771855   114.182266  0.009021   \n",
       "17982  2024-10-31    XOM  13.968900  0.771855   113.753670  0.003723   \n",
       "17983  2024-11-30    XOM  14.689913  0.737662   115.847267  0.011724   \n",
       "17984  2024-12-31    XOM  13.396015  0.737662   103.865776  0.014966   \n",
       "\n",
       "            vol       mom  Mkt-RF   SMB   HML   RMW   CMA      RF  ret_excess  \n",
       "0      0.067338  0.203171    4.42 -1.71  0.43 -0.48 -0.01  0.0000    0.017554  \n",
       "1      0.071533  0.367164    3.11 -0.47  1.14 -0.54  0.74  0.0000    0.028689  \n",
       "2      0.071351  0.354526   -0.85 -0.55 -0.78  1.30  0.65  0.0000    0.027881  \n",
       "3      0.070729  0.407354   -6.19 -0.12 -1.07  2.08  2.31  0.0001    0.031024  \n",
       "4      0.096397  0.178720    3.89  0.84  0.62 -1.10  0.46  0.0000    0.018280  \n",
       "...         ...       ...     ...   ...   ...   ...   ...     ...         ...  \n",
       "17980  0.052356  0.144777    1.61 -3.65 -1.13  0.85  0.86  0.0048    0.007788  \n",
       "17981  0.051371  0.097898    1.74 -1.02 -2.59  0.04 -0.26  0.0040    0.005021  \n",
       "17982  0.049151  0.031893   -0.97 -0.88  0.89 -1.38  1.03  0.0039   -0.000177  \n",
       "17983  0.037114  0.141942    6.51  4.78 -0.05 -2.62 -2.17  0.0040    0.007724  \n",
       "17984  0.035716  0.187306   -3.17 -3.87 -2.95  1.82 -1.10  0.0037    0.011266  \n",
       "\n",
       "[17985 rows x 15 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"feature_data/data.csv\")\n",
    "\n",
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test sets\n",
    "#train_df = df[df['date'] < '2024-01-01']\n",
    "#test_df = df[df['date'] >= '2024-01-01']\n",
    "\n",
    "#train_df = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate returns & Covariances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "#import mgarch\n",
    "factor_cols = [\"Mkt-RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\"]\n",
    "\n",
    "\n",
    "# ----- helper #1 :   β̂  and  F̂  --------------------------------\n",
    "def fit_beta(X: np.ndarray, Y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        # 1) strip rows that contain any non-finite value\n",
    "    row_mask = np.isfinite(X).all(axis=1) & np.isfinite(Y).all(axis=1)\n",
    "    Xc, Yc   = X[row_mask], Y[row_mask]\n",
    "\n",
    "    beta, *_ = np.linalg.lstsq(Xc, Yc, rcond=None)               # (d_x, d_y)\n",
    "    resid    = Yc - Xc @ beta                                    # (n, d_y)\n",
    "    F_hat    = np.diag(resid.var(axis=0, ddof=1))              # (d_y, d_y)\n",
    "    return beta, F_hat\n",
    "\n",
    "\n",
    "# ----- helper #2 :   one-step mean & cov ------------------------\n",
    "def forecast_one_step(x_i, W_i, beta, F_hat):\n",
    "    y_hat = x_i @ beta                                         # (d_y,)\n",
    "    V_hat = beta.T @ W_i @ beta + F_hat                        # (d_y, d_y)\n",
    "    return y_hat, V_hat\n",
    "\n",
    "# ---- helper #3 : DCC-GARCH forecast of factor covariance ------\n",
    "#def dcc_garch_cov(X, ndays=1):\n",
    "    #dist = 't'\n",
    "    #vol = mgarch.mgarch(dist)\n",
    "    #vol.fit(X)\n",
    "    #W_t = vol.predict(ndays)['cov']\n",
    "    #return W_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df.groupby('ticker')['close_price'].shift(-1).gt(df['close_price']).astype(int) * 2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ret'] = df.groupby('ticker')['close_price'].pct_change().shift(-1).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ret_excess'] = df['ret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label counts:\n",
      "label\n",
      " 1    10093\n",
      "-1     7892\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# get the how many 1s and 0s in the label column\n",
    "label_counts = df['label'].value_counts()\n",
    "print(\"Label counts:\")\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ticker</th>\n",
       "      <th>pe_exi</th>\n",
       "      <th>de_ratio</th>\n",
       "      <th>close_price</th>\n",
       "      <th>exp_ret</th>\n",
       "      <th>vol</th>\n",
       "      <th>mom</th>\n",
       "      <th>Mkt-RF</th>\n",
       "      <th>SMB</th>\n",
       "      <th>HML</th>\n",
       "      <th>RMW</th>\n",
       "      <th>CMA</th>\n",
       "      <th>RF</th>\n",
       "      <th>ret_excess</th>\n",
       "      <th>label</th>\n",
       "      <th>ret</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-02-29</td>\n",
       "      <td>AAP</td>\n",
       "      <td>16.706458</td>\n",
       "      <td>3.311468</td>\n",
       "      <td>74.872002</td>\n",
       "      <td>0.017554</td>\n",
       "      <td>0.067338</td>\n",
       "      <td>0.203171</td>\n",
       "      <td>4.42</td>\n",
       "      <td>-1.71</td>\n",
       "      <td>0.43</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.038185</td>\n",
       "      <td>1</td>\n",
       "      <td>0.038185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-03-31</td>\n",
       "      <td>AAP</td>\n",
       "      <td>17.332681</td>\n",
       "      <td>3.311468</td>\n",
       "      <td>77.731010</td>\n",
       "      <td>0.028689</td>\n",
       "      <td>0.071533</td>\n",
       "      <td>0.367164</td>\n",
       "      <td>3.11</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>1.14</td>\n",
       "      <td>-0.54</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.036469</td>\n",
       "      <td>1</td>\n",
       "      <td>0.036469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-04-30</td>\n",
       "      <td>AAP</td>\n",
       "      <td>17.964775</td>\n",
       "      <td>3.311468</td>\n",
       "      <td>80.565750</td>\n",
       "      <td>0.027881</td>\n",
       "      <td>0.071351</td>\n",
       "      <td>0.354526</td>\n",
       "      <td>-0.85</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.205447</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.205447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-05-31</td>\n",
       "      <td>AAP</td>\n",
       "      <td>13.118705</td>\n",
       "      <td>3.447840</td>\n",
       "      <td>64.013748</td>\n",
       "      <td>0.031124</td>\n",
       "      <td>0.070729</td>\n",
       "      <td>0.407354</td>\n",
       "      <td>-6.19</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-1.07</td>\n",
       "      <td>2.08</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.063895</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.063895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-06-30</td>\n",
       "      <td>AAP</td>\n",
       "      <td>12.269784</td>\n",
       "      <td>3.447840</td>\n",
       "      <td>59.923603</td>\n",
       "      <td>0.018280</td>\n",
       "      <td>0.096397</td>\n",
       "      <td>0.178720</td>\n",
       "      <td>3.89</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.62</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.028291</td>\n",
       "      <td>1</td>\n",
       "      <td>0.028291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17980</th>\n",
       "      <td>2024-08-31</td>\n",
       "      <td>XOM</td>\n",
       "      <td>14.107656</td>\n",
       "      <td>0.771855</td>\n",
       "      <td>114.883614</td>\n",
       "      <td>0.012588</td>\n",
       "      <td>0.052356</td>\n",
       "      <td>0.144777</td>\n",
       "      <td>1.61</td>\n",
       "      <td>-3.65</td>\n",
       "      <td>-1.13</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>-0.006105</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.006105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17981</th>\n",
       "      <td>2024-09-30</td>\n",
       "      <td>XOM</td>\n",
       "      <td>14.021531</td>\n",
       "      <td>0.771855</td>\n",
       "      <td>114.182266</td>\n",
       "      <td>0.009021</td>\n",
       "      <td>0.051371</td>\n",
       "      <td>0.097898</td>\n",
       "      <td>1.74</td>\n",
       "      <td>-1.02</td>\n",
       "      <td>-2.59</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>-0.003754</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.003754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17982</th>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>XOM</td>\n",
       "      <td>13.968900</td>\n",
       "      <td>0.771855</td>\n",
       "      <td>113.753670</td>\n",
       "      <td>0.003723</td>\n",
       "      <td>0.049151</td>\n",
       "      <td>0.031893</td>\n",
       "      <td>-0.97</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>0.89</td>\n",
       "      <td>-1.38</td>\n",
       "      <td>1.03</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.018405</td>\n",
       "      <td>1</td>\n",
       "      <td>0.018405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17983</th>\n",
       "      <td>2024-11-30</td>\n",
       "      <td>XOM</td>\n",
       "      <td>14.689913</td>\n",
       "      <td>0.737662</td>\n",
       "      <td>115.847267</td>\n",
       "      <td>0.011724</td>\n",
       "      <td>0.037114</td>\n",
       "      <td>0.141942</td>\n",
       "      <td>6.51</td>\n",
       "      <td>4.78</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-2.62</td>\n",
       "      <td>-2.17</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>-0.103425</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.103425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17984</th>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>XOM</td>\n",
       "      <td>13.396015</td>\n",
       "      <td>0.737662</td>\n",
       "      <td>103.865776</td>\n",
       "      <td>0.014966</td>\n",
       "      <td>0.035716</td>\n",
       "      <td>0.187306</td>\n",
       "      <td>-3.17</td>\n",
       "      <td>-3.87</td>\n",
       "      <td>-2.95</td>\n",
       "      <td>1.82</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17985 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date ticker     pe_exi  de_ratio  close_price   exp_ret  \\\n",
       "0      2012-02-29    AAP  16.706458  3.311468    74.872002  0.017554   \n",
       "1      2012-03-31    AAP  17.332681  3.311468    77.731010  0.028689   \n",
       "2      2012-04-30    AAP  17.964775  3.311468    80.565750  0.027881   \n",
       "3      2012-05-31    AAP  13.118705  3.447840    64.013748  0.031124   \n",
       "4      2012-06-30    AAP  12.269784  3.447840    59.923603  0.018280   \n",
       "...           ...    ...        ...       ...          ...       ...   \n",
       "17980  2024-08-31    XOM  14.107656  0.771855   114.883614  0.012588   \n",
       "17981  2024-09-30    XOM  14.021531  0.771855   114.182266  0.009021   \n",
       "17982  2024-10-31    XOM  13.968900  0.771855   113.753670  0.003723   \n",
       "17983  2024-11-30    XOM  14.689913  0.737662   115.847267  0.011724   \n",
       "17984  2024-12-31    XOM  13.396015  0.737662   103.865776  0.014966   \n",
       "\n",
       "            vol       mom  Mkt-RF   SMB   HML   RMW   CMA      RF  ret_excess  \\\n",
       "0      0.067338  0.203171    4.42 -1.71  0.43 -0.48 -0.01  0.0000    0.038185   \n",
       "1      0.071533  0.367164    3.11 -0.47  1.14 -0.54  0.74  0.0000    0.036469   \n",
       "2      0.071351  0.354526   -0.85 -0.55 -0.78  1.30  0.65  0.0000   -0.205447   \n",
       "3      0.070729  0.407354   -6.19 -0.12 -1.07  2.08  2.31  0.0001   -0.063895   \n",
       "4      0.096397  0.178720    3.89  0.84  0.62 -1.10  0.46  0.0000    0.028291   \n",
       "...         ...       ...     ...   ...   ...   ...   ...     ...         ...   \n",
       "17980  0.052356  0.144777    1.61 -3.65 -1.13  0.85  0.86  0.0048   -0.006105   \n",
       "17981  0.051371  0.097898    1.74 -1.02 -2.59  0.04 -0.26  0.0040   -0.003754   \n",
       "17982  0.049151  0.031893   -0.97 -0.88  0.89 -1.38  1.03  0.0039    0.018405   \n",
       "17983  0.037114  0.141942    6.51  4.78 -0.05 -2.62 -2.17  0.0040   -0.103425   \n",
       "17984  0.035716  0.187306   -3.17 -3.87 -2.95  1.82 -1.10  0.0037    0.000000   \n",
       "\n",
       "       label       ret  \n",
       "0          1  0.038185  \n",
       "1          1  0.036469  \n",
       "2         -1 -0.205447  \n",
       "3         -1 -0.063895  \n",
       "4          1  0.028291  \n",
       "...      ...       ...  \n",
       "17980     -1 -0.006105  \n",
       "17981     -1 -0.003754  \n",
       "17982      1  0.018405  \n",
       "17983     -1 -0.103425  \n",
       "17984     -1  0.000000  \n",
       "\n",
       "[17985 rows x 17 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗑  Dropping 55 tickers with missing returns:\n",
      "ABBV, ABM, ACA, ACT, ADEA, ADMA, AESI, AHCO, AL, ALRM, AMN, AMPH, AMR, ANGI, ANIP, AORT, APAM, ARLO, AROC, ASIX, ASO, ATEN, ATGE, AUB, AX, BAC, BANC, BH, BL, BLMN, BTU, CABO, CALM, CARG, CARS, CC, CHEF, CLB, CNR, CRC, CRGY, CRK, CWEN, ENR, EPC, ESI, GOGO, GOOG, HCC, IAC, LUMN, MA, META, MSGS, TMUS\n",
      "✅  Y_df is now NaN-free and has 86 tickers.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# 1.  Build monthly factor (X_df) and asset-return (Y_df) tables\n",
    "\n",
    "Y_df = (df\n",
    "        .pivot(index='date', columns='ticker', values='ret_excess') # ret_excess can be used or ret\n",
    "        .sort_index())\n",
    "\n",
    "X_df = (df[['date'] + factor_cols]\n",
    "        .drop_duplicates('date')\n",
    "        .set_index('date')\n",
    "        .sort_index())\n",
    "# create a label_df\n",
    "label_df = (df\n",
    "            .pivot(index='date', columns='ticker', values='label')\n",
    "            .sort_index())\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1.  Find ticker columns with *any* NaNs in Y_df\n",
    "# ------------------------------------------------------------------\n",
    "bad_tickers = Y_df.columns[Y_df.isna().any()]\n",
    "\n",
    "print(f\"🗑  Dropping {len(bad_tickers)} tickers with missing returns:\")\n",
    "print(\", \".join(map(str, bad_tickers)))\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2.  Drop them from Y_df  (axis=1 ⇒ columns)\n",
    "# ------------------------------------------------------------------\n",
    "Y_df = Y_df.drop(columns=bad_tickers)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3.  OPTIONAL: keep your other pivot tables in sync\n",
    "#     (uncomment if you have X_df, label_df, etc. with same columns)\n",
    "# ------------------------------------------------------------------\n",
    "# X_df      = X_df.drop(columns=bad_tickers, errors=\"ignore\")\n",
    "label_df  = label_df.drop(columns=bad_tickers, errors=\"ignore\")\n",
    "# Sigma_fore = Sigma_fore[np.ix_(good_mask, good_mask)]  # inside loop\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4.  Verify — there should be zero NaNs left\n",
    "# ------------------------------------------------------------------\n",
    "assert not Y_df.isna().any().any(), \"Still NaNs lurking in Y_df!\"\n",
    "print(\"✅  Y_df is now NaN-free and has\", Y_df.shape[1], \"tickers.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) make sure every index really *is* a DatetimeIndex\n",
    "X_df.index      = pd.to_datetime(X_df.index)\n",
    "Y_df.index      = pd.to_datetime(Y_df.index)\n",
    "label_df.index  = pd.to_datetime(label_df.index)\n",
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every row is month-end already ⇒ just iterate over the index\n",
    "month_ends = X_df.index.sort_values()\n",
    "\n",
    "lookback = 12          # e.g. use the past 12 months\n",
    "month_ends = month_ends[lookback:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2013-02-28', '2013-03-31', '2013-04-30', '2013-05-31',\n",
       "               '2013-06-30', '2013-07-31', '2013-08-31', '2013-09-30',\n",
       "               '2013-10-31', '2013-11-30',\n",
       "               ...\n",
       "               '2024-03-31', '2024-04-30', '2024-05-31', '2024-06-30',\n",
       "               '2024-07-31', '2024-08-31', '2024-09-30', '2024-10-31',\n",
       "               '2024-11-30', '2024-12-31'],\n",
       "              dtype='datetime64[ns]', name='date', length=143, freq=None)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "month_ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_spd(M, eps=1e-6):\n",
    "    M = (M + M.T) * 0.5  # enforce symmetry\n",
    "    jitter = eps\n",
    "    I = np.eye(M.shape[0])\n",
    "    for _ in range(10):\n",
    "        try:\n",
    "            # This will error if M is not SPD\n",
    "            np.linalg.cholesky(M)\n",
    "            return M\n",
    "        except np.linalg.LinAlgError:\n",
    "            M = M + jitter * I\n",
    "            jitter *= 10\n",
    "    raise RuntimeError(\"Unable to make SPD matrix\")\n",
    "\n",
    "results = []\n",
    "tickers = Y_df.columns.unique()\n",
    "for me_date in month_ends:\n",
    "\n",
    "    # ---------- 1) pick the estimation window ------------\n",
    "    win_mask  = (X_df.index <= me_date)                     & \\\n",
    "                (X_df.index >  me_date - pd.offsets.MonthEnd(lookback))\n",
    "\n",
    "    X_window  = X_df.loc[win_mask]\n",
    "    Y_window  = Y_df.loc[win_mask]\n",
    "\n",
    "    if len(X_window) < 2:\n",
    "        continue   # still not enough data – skip\n",
    "\n",
    "    # ---------- 2)  β̂ , F̂  from the window --------------\n",
    "    beta_hat, F_hat = fit_beta(X_window.values, Y_window.values)\n",
    "\n",
    "    # ---------- 3)  Σ̂ (covariance)  ----------------------\n",
    "    # but most people just use a sample/Exp-Wtd cov here:\n",
    "    W_hat = np.cov(X_window.values.T, ddof=1)\n",
    "    #W_hat = dcc_garch_cov(X_window.values)\n",
    "    \n",
    "    x_today      = X_df.loc[me_date].values\n",
    "    mu_fore, Sigma_fore = forecast_one_step(x_today, W_hat, beta_hat, F_hat)\n",
    "    #  └─ make sure your forecast function is set up for “+1 month”,\n",
    "    #     not “+1 trading day”.\n",
    "\n",
    "    W = 12\n",
    "    R_hist = (Y_df.loc[Y_df.index <= me_date]\n",
    "                .tail(W)         # 12 rows = 12 months\n",
    "                .T.values)       # (n_assets × W)\n",
    "               \n",
    "    # 5) features ----------------------------------------\n",
    "    feature_list = ['exp_ret', 'vol','mom', 'pe_exi', 'de_ratio', 'Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']\n",
    "\n",
    "    monthly = (df\n",
    "               .set_index(['date','ticker'])\n",
    "               .loc[me_date]\n",
    "               .reindex(tickers))\n",
    "    X_feat = monthly[feature_list].values\n",
    "\n",
    "    # Standardize X_feat now\n",
    "    mean = X_feat.mean(axis=0, keepdims=True)\n",
    "    std = X_feat.std(axis=0, keepdims=True) + 1e-8  # to avoid division by zero\n",
    "    X_feat = (X_feat - mean) / std\n",
    "    \n",
    "    # 6) labels ------------------------------------------\n",
    "    y_vec = label_df.loc[me_date, tickers].values\n",
    "\n",
    "    # 7) get row of Y_df for the current month-end\n",
    "    real_returns = Y_df.loc[me_date, tickers].values\n",
    "\n",
    "    # 8) get the real covariance matrix for the current month-end\n",
    "    real_sigma = np.cov(Y_df.loc[Y_df.index <= me_date].tail(W).T, ddof=1)\n",
    "    real_sigma = make_spd(real_sigma)\n",
    "    # 9) store snapshot ----------------------------------\n",
    "    results.append(dict(date         = me_date,\n",
    "                        X_feat       = X_feat,\n",
    "                        returns_hist = R_hist,\n",
    "                        y            = y_vec.astype(np.double),\n",
    "                        mu_fore      = mu_fore.astype(np.double),\n",
    "                        Sigma_fore   = Sigma_fore.astype(np.double),\n",
    "                        beta         = beta_hat,\n",
    "                        F_hat        = F_hat,\n",
    "                        W_hat        = W_hat,\n",
    "                        real_returns = real_returns.astype(np.double),\n",
    "                        real_sigma   = real_sigma.astype(np.double)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Σ̂ SPD check: 143/143 pass, 0 fail\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- helper --------------------------------------------------------\n",
    "def is_spd(mat: np.ndarray, tol: float = 1e-8) -> bool:\n",
    "    \"\"\"\n",
    "    Return True if `mat` is symmetric-positive-definite.\n",
    "    • First check symmetry (fast) – avoids false negatives from tiny asymmetry\n",
    "    • Then try a Cholesky factorisation.  If it succeeds, SPD.\n",
    "    • `tol` lets you ignore round-off noise on symmetry test.\n",
    "    \"\"\"\n",
    "    if not np.allclose(mat, mat.T, atol=tol, rtol=0):\n",
    "        return False\n",
    "    try:\n",
    "        np.linalg.cholesky(mat)\n",
    "        return True\n",
    "    except np.linalg.LinAlgError:\n",
    "        return False\n",
    "\n",
    "# --- scan the results list ----------------------------------------\n",
    "bad = []                 # collect (index, date) for matrices that fail\n",
    "for i, snap in enumerate(results):\n",
    "    if not is_spd(snap[\"Sigma_fore\"]):\n",
    "        bad.append((i, snap[\"date\"]))\n",
    "    if not is_spd(snap[\"real_sigma\"]):\n",
    "        bad.append((i, snap[\"date\"]))\n",
    "# --- quick report --------------------------------------------------\n",
    "total = len(results)\n",
    "print(f\"Σ̂ SPD check: {total - len(bad)}/{total} pass, {len(bad)} fail\")\n",
    "\n",
    "if bad:\n",
    "    print(\"First few failures:\")\n",
    "    for idx, d in bad[:10]:\n",
    "        print(f\"  #{idx:3d}  {d}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from qpth.qp import QPFunction\n",
    "torch.manual_seed(99)\n",
    "    \n",
    "def sanitise_spd(M: torch.Tensor,\n",
    "                 name: str = \"Q\",\n",
    "                 eps: float = 1e-6) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    1. reports NaN/Inf, duplicate rows, condition number, min/max eigenvalues\n",
    "    2. drops exact duplicate rows/cols (they kill Cholesky)\n",
    "    3. adds minimal jitter to make SPD\n",
    "    \"\"\"\n",
    "    #def _report(tag, val):  # compact helper\n",
    "        #print(f\"[{name}] {tag}: {val}\")\n",
    "\n",
    "    # 0) NaN / Inf check -------------------------------------------------\n",
    "    if torch.isnan(M).any() or torch.isinf(M).any():\n",
    "        raise ValueError(f\"[{name}] contains NaN or Inf – cannot factorise\")\n",
    "\n",
    "    # 1) deduplicate rows/cols ------------------------------------------\n",
    "    #    (X rows that are numerically identical give zero-variance directions)\n",
    "    # NOTE: this assumes you constructed M as  y yᵀ ⊙ X Xᵀ\n",
    "    #       If that is not the case just delete the dedup block.\n",
    "    with torch.no_grad():\n",
    "        diag = torch.diag(M)\n",
    "        mask_nonzero = diag > 0      # rows with all-zero features have 0 on diag\n",
    "        if mask_nonzero.sum() < len(mask_nonzero):\n",
    "            #_report(\"duplicate/zero rows removed\",\n",
    "                    #nt(len(mask_nonzero) - mask_nonzero.sum()))\n",
    "            M = M[mask_nonzero][:, mask_nonzero]\n",
    "\n",
    "    # 2) minimal jitter --------------------------------------------------\n",
    "    eigvals = torch.linalg.eigvalsh(M)\n",
    "    λmin    = eigvals.min().item()\n",
    "    λmax    = eigvals.max().item()\n",
    "    cond    = λmax / max(λmin, eps)\n",
    "    #_report(\"λ_min\",  λmin)\n",
    "    #_report(\"λ_max\",  λmax)\n",
    "    #_report(\"cond\",   cond)\n",
    "\n",
    "    jitter  = max(eps, -λmin + eps)\n",
    "    M       = M + jitter * torch.eye(M.size(0), device=M.device, dtype=M.dtype)\n",
    "\n",
    "    # final safety: Cholesky must succeed now\n",
    "    try:\n",
    "        torch.linalg.cholesky(M)\n",
    "    except RuntimeError as e:\n",
    "        raise RuntimeError(f\"[{name}] still not SPD even after jitter = {jitter}\") from e\n",
    "\n",
    "    return M\n",
    "\n",
    "class EndToEndSVM_MVO_Sigmoid(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 C_svm_init: float,\n",
    "                 eps: float,\n",
    "                 tau_init: float,\n",
    "                 lambda_hinge_init: float\n",
    "            ):\n",
    "        \"\"\"\n",
    "        in_features : number of raw features per asset\n",
    "        C_svm_init       : SVM dual box-constraint\n",
    "        eps         : jitter to ensure all Q-matrices are SPD\n",
    "        tau_init         : sigmoid temperature for soft gating\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # 1)  make the learnable projection W (d × d, no bias)\n",
    "        self.embed = nn.Linear(in_features, in_features, bias=False).double()\n",
    "\n",
    "        # 2)  start it as an identity matrix instead of tiny random numbers\n",
    "        nn.init.eye_(self.embed.weight)          # ← this line\n",
    "        with torch.no_grad():\n",
    "            self.embed.weight += 0.01 * torch.randn_like(self.embed.weight)\n",
    "            \n",
    "        self.log_C = nn.Parameter(torch.log(torch.tensor([C_svm_init], dtype=torch.double)))\n",
    "        # self.log_tau = nn.Parameter(torch.log(torch.tensor([tau_init], dtype=torch.double)))\n",
    "        self.tau = tau_init\n",
    "        self.eps = eps\n",
    "        # self.log_lambda_hinge = nn.Parameter(torch.log(torch.tensor([lambda_hinge_init], dtype=torch.double)))\n",
    "        self.lambda_hinge = lambda_hinge_init \n",
    "\n",
    "    def forward(self,\n",
    "                X_feat: torch.Tensor,     # (n,d)\n",
    "                y: torch.Tensor,          # ±1\n",
    "                mu_est: torch.Tensor,     # (n,)  \n",
    "                Sigma_est: torch.Tensor,  # (n,n) \n",
    "                return_goal: float\n",
    "               ) -> torch.Tensor:\n",
    "        n, d = X_feat.shape\n",
    "        # A. raw input\n",
    "        #_check(\"X_feat\", X_feat)\n",
    "\n",
    "\n",
    "        # 1) feature embedding\n",
    "        Xp = self.embed(X_feat.double())\n",
    "        #_check(\"Xp\", Xp)\n",
    "        y  = y.view(-1).double()           # now y is float64 ±1\n",
    "\n",
    "        # 2) SVM dual QP\n",
    "        K      = Xp @ Xp.t()                             # (n, n)\n",
    "        #_check(\"K\", K)\n",
    "\n",
    "        yy    = y.unsqueeze(1) * y.unsqueeze(0)  # (n,n) float64\n",
    "        Q_svm  = sanitise_spd((y[:,None] * y[None,:]) * K, name=\"Q_svm\", eps=self.eps)\n",
    "        #_check(\"Q_svm\", Q_svm)\n",
    "        p_svm  = -torch.ones(n, device=Xp.device, dtype=Xp.dtype)\n",
    "\n",
    "        G_svm  = torch.cat([\n",
    "            -torch.eye(n, device=Xp.device, dtype=Xp.dtype),\n",
    "             torch.eye(n, device=Xp.device, dtype=Xp.dtype)\n",
    "        ], dim=0)                                        # (2n, n)\n",
    "\n",
    "        C_svm = torch.exp(self.log_C)\n",
    "\n",
    "        h_svm  = torch.cat([\n",
    "            torch.zeros(n, device=Xp.device, dtype=Xp.dtype),\n",
    "            C_svm * torch.ones(n, device=Xp.device, dtype=Xp.dtype)\n",
    "        ], dim=0)                                        # (2n,)\n",
    "\n",
    "        A_svm  = y.view(1,-1)#.to(Xp)                     # (1, n)\n",
    "        b_svm  = torch.zeros(1, device=Xp.device, dtype=Xp.dtype)\n",
    "        # ---------------------------------------------------------------\n",
    "        # handle single-class case (all +1  *or*  all –1)\n",
    "        # ---------------------------------------------------------------\n",
    "        if (y == y[0]).all():              # every label identical\n",
    "            A_svm = torch.empty(0, n, device=Xp.device, dtype=Xp.dtype)  # shape (0, n)\n",
    "            b_svm = torch.empty(0,       device=Xp.device, dtype=Xp.dtype)  # shape (0,)\n",
    "            print(\"Warning: all labels identical, no SVM hyperplane constructed.\")\n",
    "        else:\n",
    "            A_svm = y.view(1, -1)                                           # (1, n)\n",
    "            b_svm = torch.zeros(1, device=Xp.device, dtype=Xp.dtype)        # (1,)\n",
    "\n",
    "        alpha      = QPFunction(verbose=False)(\n",
    "                    Q_svm, p_svm, G_svm, h_svm, A_svm, b_svm\n",
    "                 )                                  # (n,)\n",
    "        alpha = torch.clamp(alpha, min=0.0, max=C_svm.item())\n",
    "        # 3) build hyperplane and score\n",
    "        # after solving for alpha\n",
    "        # construct w_svm properly:\n",
    "        # make sure alphas is a 1-D tensor of length \n",
    "        alpha = alpha.view(-1)               \n",
    "\n",
    "        # ensure y is double or double to match alphas dtype\n",
    "        y = y.to(alpha.dtype)            \n",
    "\n",
    "        # elementwise product alpha_i * y_i\n",
    "        alpha_y = alpha * y                \n",
    "        w_svm = Xp.t().mv(alpha_y)                   # or torch.matmul(X.t(), alpha_y)\n",
    "\n",
    "        scores = Xp @ w_svm                 # (n_assets,)\n",
    "\n",
    "        # diagnostic\n",
    "        #with torch.no_grad():\n",
    "            #print(\"‖w_svm‖₂       :\", w_svm.norm().item())\n",
    "            #print(\"‖alpha‖₁       :\", alpha.abs().sum().item())\n",
    "            #print(\"scores min/max :\", scores.min().item(), scores.max().item())\n",
    "            \n",
    "        hinge = torch.clamp(1.0 - y * scores, min=0.0).mean()\n",
    "        # Dual SVM loss: -1ᵗα + ½ αᵗQα\n",
    "        #hinge = -alpha.sum() + 0.5 * alpha @ (Q_svm @ alpha)\n",
    "\n",
    "        # 4) differentiable sigmoid gate\n",
    "        tau = self.tau\n",
    "        mask = torch.sigmoid(scores / tau)\n",
    "        #print(\"Mask mean value: \", mask.mean())\n",
    "\n",
    "\n",
    "\n",
    "        # 5) MVO QP *over all assets* with w_i ≤ mask_i\n",
    "        #    compute moments for every asset\n",
    "        # ---------- 2) MVO QP using *forecast* μ, Σ ----------\n",
    "        mu     = mu_est                             # (n,)\n",
    "        Sigma  = Sigma_est\n",
    "        \n",
    "        # Diagnostics before solving MVO QP\n",
    "       # with torch.no_grad():\n",
    "            #print(f\"[Diag] μ.min(): {mu.min().item():.4f}, μ.max(): {mu.max().item():.4f}, μ.mean(): {mu.mean().item():.4f}\")\n",
    "           # print(f\"[Diag] return_goal: {return_goal}\")\n",
    "            #print(f\"[Diag] mask.min(): {mask.min().item():.4f}, mask.max(): {mask.max().item():.4f}, mask.mean(): {mask.mean().item():.4f}\")\n",
    "\n",
    "        P_mvo  = Sigma\n",
    "        q_mvo  = torch.zeros(n, device=Sigma.device, dtype=Sigma.dtype)\n",
    "\n",
    "        # box constraints: 0 ≤ w ≤ mask\n",
    "        G_box  = torch.cat([\n",
    "            -torch.eye(n, device=Sigma.device, dtype=Sigma.dtype),  # -w ≤ 0\n",
    "             torch.eye(n, device=Sigma.device, dtype=Sigma.dtype)   #  w ≤ mask\n",
    "        ], dim=0)                                                   # (2n, n)\n",
    "        h_box  = torch.cat([\n",
    "            torch.zeros(n, device=Sigma.device, dtype=Sigma.dtype),\n",
    "            mask\n",
    "        ], dim=0)                                                   # (2n,)\n",
    "\n",
    "        # ------------------ NEW inequality: μᵀw ≥ return_goal -------------\n",
    "        G_ret = -mu.unsqueeze(0)                                     # (1, n)\n",
    "        h_ret = -torch.tensor([return_goal],\n",
    "                            device=Sigma.device, dtype=Sigma.dtype)\n",
    "\n",
    "        # concat all inequalities\n",
    "        G_ineq = torch.cat([G_box, G_ret], dim=0)                    # (2n+1, n)\n",
    "        h_ineq = torch.cat([h_box, h_ret], dim=0)                    # (2n+1,)\n",
    "\n",
    "        # equality: sum(w)=1\n",
    "        A_eq = torch.ones(1, n, device=Sigma.device, dtype=Sigma.dtype)  # (1, n)                                             \n",
    "        b_eq = torch.tensor([1.0], device=Sigma.device, dtype=Sigma.dtype)\n",
    "\n",
    "        w_opt  = QPFunction(verbose=False)(\n",
    "                    P_mvo, q_mvo, G_ineq, h_ineq, A_eq, b_eq\n",
    "                 )\n",
    "                                       \n",
    "        # 2. Check return target vs feasible region\n",
    "        #print(\"mask_min/max:\", mask.min().item(), mask.max().item())\n",
    "        #print(\"mu_min/max:\",   mu.min().item(),   mu.max().item(), \"goal:\", return_goal)\n",
    "        \n",
    "        return w_opt.view(-1), mask, hinge, C_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "snapshots = sorted(results, key=lambda s: s[\"date\"])   # ensure sorted\n",
    "# drop the last 1 snapshot, it is incomplete\n",
    "snapshots = snapshots[:-1]  # drop the last snapshot, it is incomplete\n",
    "\n",
    "#val_fraction = 0.2\n",
    "#n_total      = len(snapshots)\n",
    "#n_val        = int(n_total * val_fraction)\n",
    "n_val = 12  # e.g. second last 18 months for validation\n",
    "n_test = 11  # e.g. last 12 months for testing\n",
    "train_snaps  = snapshots[:-(n_val + n_test)]          # earlier dates\n",
    "val_snaps    = snapshots[-(n_val + n_test):-n_test]          # second last 12 months\n",
    "test_snaps   = snapshots[-n_test:]               # last 12 months\n",
    "\n",
    "# ---------- 1.  tiny helper --------------------------------------------------\n",
    "def to_tensor(x, *, dtype=torch.float64):\n",
    "    \"\"\"\n",
    "    NumPy → torch, replace NaN/Inf with finite numbers\n",
    "    (you can swap 'nan=0.0' for any imputation of your choice).\n",
    "    \"\"\"\n",
    "    x = np.nan_to_num(x, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "    return torch.as_tensor(x, dtype=dtype)\n",
    "\n",
    "\n",
    "# ---------- 2.  custom Dataset ----------------------------------------------\n",
    "class SnapshotDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Each item is one time-snapshot:\n",
    "        X_feat, y, mu_fore, Sigma_fore, return_goal\n",
    "    Shapes:\n",
    "        X_feat      (n, d)   – features\n",
    "        y           (n,)     – ±1 labels\n",
    "        mu_fore     (n,)\n",
    "        Sigma_fore  (n, n)\n",
    "    \"\"\"\n",
    "    def __init__(self, results, return_goal):\n",
    "        self.data = []\n",
    "\n",
    "        for snap in results:\n",
    "            # normalize the X_feat\n",
    "            X_feat = snap[\"X_feat\"]  # (n, d)\n",
    "            #X_feat = (X_feat - X_feat.mean(axis=0)) / (X_feat.std(axis=0) + 1e-8)\n",
    "            X  = to_tensor(X_feat)      # (n, d)\n",
    "            y  = to_tensor(snap[\"y\"]).view(-1)  # (n,)\n",
    "            mu = to_tensor(snap[\"mu_fore\"])     # (n,)\n",
    "            S  = to_tensor(snap[\"Sigma_fore\"])  # (n, n)\n",
    "            real_mu = to_tensor(snap[\"real_returns\"])\n",
    "            real_sigma = to_tensor(snap[\"real_sigma\"])\n",
    "\n",
    "            # ---------- basic sanity: drop rows that are still all-zero ------\n",
    "            # (happens if the original had only NaNs)\n",
    "            keep = (X.abs().sum(dim=1) > 0)\n",
    "            if keep.sum() < 2:                  # need ≥2 points for an SVM\n",
    "                continue                         # skip this snapshot\n",
    "\n",
    "            X, y, mu, real_mu = X[keep], y[keep], mu[keep], real_mu[keep]\n",
    "            real_sigma = real_sigma[keep][:, keep]  # (n, n)\n",
    "            S        = S[keep][:, keep]\n",
    "\n",
    "            self.data.append((X, y, mu, S, real_mu, real_sigma, float(return_goal)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]          # batch = tuple of 5 tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119, 12, 11)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_snaps), len(val_snaps), len(test_snaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# 0. imports & helper  ###############################################\n",
    "#######################################################################\n",
    "import copy, torch, math\n",
    "from qpth.qp import QPFunction\n",
    "import pathlib, datetime\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# EXTRA ➋   classify assets by SVM sign -------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "def classify_assets(model, X, y, C, names=None):\n",
    "    \"\"\"returns two Python lists: invest, not_invest\"\"\"\n",
    "    alpha, w_svm, _ = solve_svm(model, X, y, C)\n",
    "    scores = model.embed(X.double()) @ w_svm            # (n,)\n",
    "    invest = (scores > 0).nonzero(as_tuple=True)[0]     # long side\n",
    "    avoid  = (scores <= 0).nonzero(as_tuple=True)[0]    # short / 0\n",
    "    if names is not None:\n",
    "        invest = [names[i] for i in invest.cpu().numpy()]\n",
    "        avoid  = [names[i] for i in avoid.cpu().numpy()]\n",
    "    else:\n",
    "        invest = invest.cpu().tolist()\n",
    "        avoid  = avoid.cpu().tolist()\n",
    "    return invest, avoid\n",
    "\n",
    "def make_spd(M, eps=1e-6):\n",
    "    \"\"\"Add minimal diagonal jitter until Cholesky succeeds.\"\"\"\n",
    "    I = torch.eye(M.size(0), device=M.device, dtype=M.dtype)\n",
    "    jitter = eps\n",
    "    for _ in range(10):\n",
    "        try:\n",
    "            torch.linalg.cholesky(M)\n",
    "            return M\n",
    "        except RuntimeError:\n",
    "            M = (M + M.t()) * 0.5 + jitter * I\n",
    "            jitter *= 10\n",
    "    raise RuntimeError(\"Unable to make SPD matrix\")\n",
    "\n",
    "def solve_svm(model, X_feat, y, C):\n",
    "    \"\"\"\n",
    "    Runs *just* the SVM part of EndToEndSVM_MVO_Sigmoid and\n",
    "    returns (alpha, w_svm, support_index_tensor)\n",
    "    \"\"\"\n",
    "    Xp = model.embed(X_feat.double())            # (n,d)\n",
    "    y  = y.view(-1).double()                     # (n,)\n",
    "\n",
    "    K      = Xp @ Xp.t()\n",
    "    Q_svm  = (y[:,None] * y[None,:]) * K\n",
    "    Q_svm  = make_spd(Q_svm)\n",
    "\n",
    "    n      = Xp.size(0)\n",
    "    p_svm  = -torch.ones(n, dtype=Xp.dtype, device=Xp.device)\n",
    "    G_svm  = torch.cat([-torch.eye(n, dtype=Xp.dtype, device=Xp.device),\n",
    "                         torch.eye(n, dtype=Xp.dtype,  device=Xp.device)], 0)\n",
    "    h_svm  = torch.cat([torch.zeros(n, dtype=Xp.dtype, device=Xp.device),\n",
    "                        C*torch.ones(n, dtype=Xp.dtype, device=Xp.device)], 0)\n",
    "\n",
    "    if (y == y[0]).all():              # single-class edge case\n",
    "        A_svm = torch.empty(0, n, dtype=Xp.dtype, device=Xp.device)\n",
    "        b_svm = torch.empty(0,    dtype=Xp.dtype, device=Xp.device)\n",
    "    else:\n",
    "        A_svm = y.unsqueeze(0)\n",
    "        b_svm = torch.zeros(1, dtype=Xp.dtype, device=Xp.device)\n",
    "\n",
    "    alpha = QPFunction(verbose=False)(Q_svm, p_svm, G_svm, h_svm, A_svm, b_svm)\n",
    "    alpha = torch.clamp(alpha, min=0.0, max=C.item()).view(-1)\n",
    "\n",
    "    w_svm = Xp.t().mv(alpha * y)       # weight vector in embedded space\n",
    "    sv    = (alpha > 1e-6)             # boolean mask of support vectors\n",
    "    #sv = (alpha > 1e-6)\n",
    "    return alpha, w_svm, sv.nonzero(as_tuple=True)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------Epoch:  1 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch   1 | train 0.975662 | val 0.985219\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  2 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch   2 | train 0.976085 | val 0.981947\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  3 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch   3 | train 0.971326 | val 0.978489\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  4 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch   4 | train 0.966128 | val 0.975221\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  5 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch   5 | train 0.964759 | val 0.972924\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  6 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch   6 | train 0.961176 | val 0.970119\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  7 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch   7 | train 0.955751 | val 0.968740\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  8 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch   8 | train 0.956561 | val 0.967053\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  9 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch   9 | train 0.951395 | val 0.965266\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  10 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  10 | train 0.953485 | val 0.964208\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  11 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  11 | train 0.952771 | val 0.963398\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  12 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  12 | train 0.944448 | val 0.962533\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  13 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  13 | train 0.948100 | val 0.961543\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  14 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  14 | train 0.942901 | val 0.960441\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  15 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  15 | train 0.945242 | val 0.958954\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  16 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  16 | train 0.943791 | val 0.958113\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  17 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  17 | train 0.940388 | val 0.957091\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  18 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  18 | train 0.945037 | val 0.956688\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  19 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  19 | train 0.944049 | val 0.955531\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  20 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  20 | train 0.940090 | val 0.953977\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  21 ----------------------------------------\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  21 | train 0.940725 | val 0.952610\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  22 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  22 | train 0.940804 | val 0.951717\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  23 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  23 | train 0.940401 | val 0.951014\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  24 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  24 | train 0.943175 | val 0.950419\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  25 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  25 | train 0.937315 | val 0.949359\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  26 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  26 | train 0.936692 | val 0.948696\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  27 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  27 | train 0.937583 | val 0.947688\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  28 ----------------------------------------\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  28 | train 0.938220 | val 0.946181\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  29 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  29 | train 0.936044 | val 0.945682\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  30 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  30 | train 0.939296 | val 0.945525\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  31 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  31 | train 0.938187 | val 0.945258\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  32 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  32 | train 0.934738 | val 0.945027\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  33 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  33 | train 0.939007 | val 0.944854\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  34 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  34 | train 0.932068 | val 0.944753\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  35 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  35 | train 0.937073 | val 0.944651\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  36 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  36 | train 0.936846 | val 0.944535\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  37 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  37 | train 0.931841 | val 0.944403\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  38 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  38 | train 0.930154 | val 0.944262\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  39 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  39 | train 0.934138 | val 0.944262\n",
      "-----------------------------------------Epoch:  40 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  40 | train 0.930899 | val 0.944186\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  41 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  41 | train 0.931037 | val 0.944054\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  42 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  42 | train 0.938044 | val 0.943901\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  43 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  43 | train 0.935119 | val 0.943832\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  44 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  44 | train 0.934980 | val 0.943772\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  45 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  45 | train 0.936214 | val 0.943606\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  46 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  46 | train 0.929150 | val 0.943389\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  47 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  47 | train 0.933490 | val 0.943159\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  48 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  48 | train 0.931682 | val 0.943092\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  49 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  49 | train 0.925190 | val 0.942881\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  50 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  50 | train 0.931887 | val 0.942643\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  51 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  51 | train 0.932361 | val 0.942496\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  52 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  52 | train 0.931585 | val 0.942201\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  53 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  53 | train 0.932553 | val 0.941965\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  54 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  54 | train 0.929618 | val 0.941802\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  55 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  55 | train 0.929869 | val 0.941750\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  56 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  56 | train 0.930363 | val 0.941675\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  57 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  57 | train 0.927559 | val 0.941496\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  58 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  58 | train 0.926810 | val 0.941547\n",
      "-----------------------------------------Epoch:  59 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  59 | train 0.924783 | val 0.941498\n",
      "-----------------------------------------Epoch:  60 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  60 | train 0.923150 | val 0.941580\n",
      "-----------------------------------------Epoch:  61 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  61 | train 0.920433 | val 0.941447\n",
      "✓ checkpoint saved to checkpoints/goal_0_050/best_model.pt\n",
      "-----------------------------------------Epoch:  62 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  62 | train 0.930822 | val 0.941480\n",
      "-----------------------------------------Epoch:  63 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  63 | train 0.931827 | val 0.941486\n",
      "-----------------------------------------Epoch:  64 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  64 | train 0.930761 | val 0.941508\n",
      "-----------------------------------------Epoch:  65 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  65 | train 0.922947 | val 0.941523\n",
      "-----------------------------------------Epoch:  66 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  66 | train 0.923224 | val 0.941543\n",
      "-----------------------------------------Epoch:  67 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  67 | train 0.923547 | val 0.941629\n",
      "-----------------------------------------Epoch:  68 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  68 | train 0.927431 | val 0.941691\n",
      "-----------------------------------------Epoch:  69 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  69 | train 0.925320 | val 0.941758\n",
      "-----------------------------------------Epoch:  70 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  70 | train 0.928514 | val 0.941817\n",
      "-----------------------------------------Epoch:  71 ----------------------------------------\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0404)\n",
      "Skipping snapshot: return_goal=tensor([0.0500], dtype=torch.float64) not feasible (max mu = 0.0319)\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "haha, skipping this batch\n",
      "epoch  71 | train 0.925422 | val 0.941830\n",
      "Early stop: no val improvement in 10 epochs\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "C_svm_init , tau_init, lambda_hinge_init= 0.01, 0.01, 1.0\n",
    "# For storing masks each epoch\n",
    "epoch_mask_values = []\n",
    "\n",
    "#######################################################################\n",
    "# 1. create model & *save* the initial weights  #######################\n",
    "#######################################################################\n",
    "device   = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model    = EndToEndSVM_MVO_Sigmoid(in_features=10, C_svm_init=C_svm_init,\n",
    "                                   eps=1e-6, tau_init=tau_init, lambda_hinge_init=lambda_hinge_init).to(device)\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "# 2. … your usual training loop here …\n",
    "#######################################################################\n",
    "# (use early-stopping or fixed epochs – whatever you prefer)\n",
    "# after training 'model' contains the *trained* embed weights\n",
    "# --------------------------------------------------------------------\n",
    "return_goals = [0.05]       # test values\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 0.  choose a unique, file-system-safe tag for this goal\n",
    "# --------------------------------------------------------------------\n",
    "goal_tag = f\"{return_goals[0]:.3f}\".replace(\".\", \"_\")   # e.g. 0.15 -> \"0_150\"\n",
    "\n",
    "# you might also want a dedicated sub-folder:\n",
    "\n",
    "ckpt_dir = pathlib.Path(\"checkpoints\") / f\"goal_{goal_tag}\"\n",
    "ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ckpt_path = ckpt_dir / \"best_model.pt\"     # goal-specific file\n",
    "\n",
    "train_set = SnapshotDataset(train_snaps, return_goal=return_goals[0])\n",
    "val_set   = SnapshotDataset(val_snaps,   return_goal=return_goals[0])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=1, shuffle=True)\n",
    "val_loader   = DataLoader(val_set,   batch_size=1, shuffle=False)\n",
    "\n",
    "n_features = results[0][\"X_feat\"].shape[1]\n",
    "\n",
    "optim  = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "patience   = 10         # stop if no progress for 10 epochs\n",
    "min_delta  = 1e-6      # what counts as “progress”\n",
    "#min_delta  = 1e-6      # what counts as “progress”\n",
    "max_epochs = 500      # hard cap (safety)\n",
    "best_val   = math.inf\n",
    "wait       = 0\n",
    "loss_hist, val_hist = [], []\n",
    "\n",
    "\n",
    "for epoch in range(1, max_epochs+1):\n",
    "    # ---- TRAIN --------------------------------------------------------\n",
    "    print(\"-----------------------------------------Epoch: \", epoch, \"----------------------------------------\")\n",
    "    model.train()\n",
    "    train_loss, n_batches = 0.0, 0\n",
    "    for X, y, mu, Sigma, real_mu, real_sigma, goal in train_loader:\n",
    "        if torch.unique(y).numel() < 2:        # ← all +1 or all –1\n",
    "            continue                           # skip this batch entirely\n",
    "        if mu.max().item() < goal:\n",
    "            print(f\"Skipping snapshot: return_goal={goal} not feasible (max mu = {mu.max().item():.4f})\")\n",
    "            continue\n",
    "        \n",
    "        X, y, mu, Sigma, real_mu, real_sigma = (t.squeeze(0).to(device) for t in (X, y, mu, Sigma, real_mu, real_sigma))\n",
    "        if (mu * mask).max().item() < goal:\n",
    "            continue\n",
    "        w, mask, hinge, C_svm    = model(X, y, mu, Sigma, goal)\n",
    "\n",
    "        # Collect masks for histogram\n",
    "        epoch_mask_values.append(mask.detach().cpu().numpy())\n",
    "        \n",
    "       # print(f\"#assets selected (mask > 0.5): {(mask > 0.5).sum().item()} / {mask.numel()}\")\n",
    "\n",
    "        var_loss = torch.dot(w, real_sigma @ w)\n",
    "        #lambda_hinge = torch.exp(model.log_lambda_hinge)\n",
    "        lambda_hinge = lambda_hinge_init\n",
    "        loss  = var_loss + lambda_hinge * hinge\n",
    "        \n",
    "        #mu_p = torch.dot(mu, w)\n",
    "        #var_p = torch.dot(w, Sigma @ w)\n",
    "        #sharpe = mu_p / (var_p.sqrt() + 1e-8)\n",
    "        #lambda_hinge = lambda_hinge_init \n",
    "        #loss = -sharpe + lambda_hinge * hinge\n",
    "        #loss = -(mu_p - 0.5 * var_p) + lambda_hinge * hinge\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        # put this inside the training loop, **after** loss.backward()\n",
    "        gnorm = model.embed.weight.grad.norm()\n",
    "       # print(f\"grad‖embed‖ = {gnorm.item():.3e}\")   # should not be 0\n",
    "        optim.step()\n",
    "\n",
    "        train_loss += loss.item();  n_batches += 1\n",
    "\n",
    "    train_loss /= n_batches\n",
    "    loss_hist.append(train_loss)\n",
    "\n",
    "    # ---- VALIDATE -----------------------------------------------------\n",
    "    model.eval();  val_loss, n_batches = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y, mu, Sigma, real_mu, real_sigma, goal in val_loader:\n",
    "            if torch.unique(y).numel() < 2:        # ← all +1 or all –1\n",
    "                continue                           # skip this batch entirely\n",
    "            if mu.max().item() < goal:\n",
    "                print(f\"Skipping snapshot: return_goal={goal} not feasible (max mu = {mu.max().item():.4f})\")\n",
    "                continue\n",
    "\n",
    "            X, y, mu, Sigma, real_mu, real_sigma = (t.squeeze(0).to(device) for t in (X, y, mu, Sigma, real_mu, real_sigma))\n",
    "            w, mask, hinge, C_svm = model(X, y, mu, Sigma, goal)\n",
    "            if (mu * mask).max().item() < goal:\n",
    "                print(\"haha, skipping this batch\")\n",
    "                continue\n",
    "            var_val_loss = torch.dot(w, real_sigma @ w)\n",
    "            #lambda_hinge = torch.exp(model.log_lambda_hinge)\n",
    "            lambda_hinge = lambda_hinge_init\n",
    "            loss = var_val_loss + lambda_hinge * hinge\n",
    "\n",
    "            #mu_p = torch.dot(mu, w)\n",
    "            #var_p = torch.dot(w, Sigma @ w)\n",
    "            #sharpe = mu_p / (var_p.sqrt() + 1e-8)\n",
    "            #lambda_hinge = lambda_hinge_init\n",
    "            #loss = -sharpe + lambda_hinge * hinge\n",
    "            #loss = -(mu_p - 0.5 * var_p) + lambda_hinge * hinge\n",
    "            \n",
    "            val_loss += loss.item(); n_batches += 1\n",
    "    val_loss /= n_batches\n",
    "    val_hist.append(val_loss)\n",
    "\n",
    "    print(f\"epoch {epoch:3d} | train {train_loss:.6f} | val {val_loss:.6f}\")\n",
    "\n",
    "\n",
    "\n",
    "    # ---- EARLY-STOPPING LOGIC ----------------------------------------\n",
    "    if val_loss < best_val - min_delta:\n",
    "        best_val = val_loss\n",
    "        wait     = 0\n",
    "        torch.save(model.state_dict(), ckpt_path)\n",
    "        print(f\"✓ checkpoint saved to {ckpt_path}\")\n",
    "        # Clear for next epoch\n",
    "        epoch_mask_values = []\n",
    "        \n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(f\"Early stop: no val improvement in {patience} epochs\")\n",
    "            flat_mask = np.concatenate(epoch_mask_values)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---- PLOT MASK HISTOGRAM ------------------------------------------\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(flat_mask, bins=20, range=(0, 1), color='blue', edgecolor='black')\n",
    "plt.title(f\"Mask Value Distribution — Epoch {epoch}\")\n",
    "plt.xlabel(\"mask value\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_svm = 0.8468935700907977\n",
      "tau = 0.1\n",
      "λ_hinge = 1.0\n"
     ]
    }
   ],
   "source": [
    "C_svm = torch.exp(model.log_C).detach()\n",
    "print(\"C_svm =\", C_svm.item())\n",
    "\n",
    "tau = model.tau\n",
    "print(\"tau =\", tau)\n",
    "\n",
    "lambda_hinge = model.lambda_hinge\n",
    "print(\"λ_hinge =\", lambda_hinge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 2.  load later (anywhere in your code)\n",
    "# --------------------------------------------------------------------\n",
    "def load_goal_model(goal, in_features, C, eps, tau, device=\"cpu\"):\n",
    "    tag   = f\"{goal:.3f}\".replace(\".\", \"_\")\n",
    "    path  = pathlib.Path(\"checkpoints\") / f\"goal_{tag}\" / \"best_model.pt\"\n",
    "    mdl   = EndToEndSVM_MVO_Sigmoid(in_features, C, eps, tau).to(device)\n",
    "    mdl.load_state_dict(torch.load(path, map_location=device))\n",
    "    mdl.eval()\n",
    "    return mdl\n",
    "\n",
    "# example usage  \n",
    "#model = load_goal_model(return_goals[0], in_features=10, C=C_svm, eps=1e-6, tau=tau)\n",
    "\n",
    "plt.plot(loss_hist, label=\"train\")\n",
    "plt.plot(val_hist,  label=\"val\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Mean loss\")\n",
    "plt.legend(); plt.grid(True); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (10234, 10)\n",
      "y_train shape: (10234,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=0.8468935700907977, kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;SVC<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>SVC(C=0.8468935700907977, kernel=&#x27;linear&#x27;)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=0.8468935700907977, kernel='linear')"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "import cvxpy as cp\n",
    "\n",
    "def solve_mvo(mask, mu, Sigma, return_goal):\n",
    "    # mask: length-n (upper bounds, or 1 for selected assets)\n",
    "    import cvxpy as cp\n",
    "    n = len(mu)\n",
    "    w = cp.Variable(n)\n",
    "    constraints = [\n",
    "        cp.sum(w) == 1,\n",
    "        mu @ w >= return_goal,\n",
    "        w >= 0,\n",
    "        w <= mask\n",
    "    ]\n",
    "    prob = cp.Problem(cp.Minimize(cp.quad_form(w, Sigma)), constraints)\n",
    "    try:\n",
    "        prob.solve(solver=cp.OSQP)\n",
    "        if w.value is not None:\n",
    "            return np.array(w.value).flatten()\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# 1. Pre-train SVM classifier once, on all historical training data\n",
    "\n",
    "\n",
    "X_train = np.vstack([s[\"X_feat\"] for s in train_snaps])\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "y_train = np.hstack([s[\"y\"] for s in train_snaps])\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "svm_clf = SVC(kernel='linear', C=C_svm.item())\n",
    "svm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping snapshot: return_goal=0.05 not feasible (max mu = 0.0404)\n",
      "Skipping snapshot: return_goal=0.05 not feasible (max mu = 0.0319)\n"
     ]
    }
   ],
   "source": [
    "rets_nn, vars_nn = [], []\n",
    "rets_svm, vars_svm = [], []\n",
    "\n",
    "\n",
    "for snap in train_snaps:\n",
    "    X, y, mu, Sigma, real_ret, real_sigma = snap[\"X_feat\"], snap[\"y\"], snap[\"mu_fore\"], snap[\"Sigma_fore\"], snap[\"real_returns\"], snap[\"real_sigma\"]\n",
    "    \n",
    "    # Check if SVM has more than one class\n",
    "    if len(np.unique(y)) < 2:\n",
    "        continue\n",
    "\n",
    "    # check if mu_fore is feasible\n",
    "    if mu.max() < return_goals[0]:\n",
    "        print(f\"Skipping snapshot: return_goal={return_goals[0]} not feasible (max mu = {mu.max():.4f})\")\n",
    "        continue\n",
    "    # ---- NN Efficient Frontier ----\n",
    "    X_t, y_t, mu_t, Sigma_t = map(lambda x: torch.tensor(x).double(), (X, y, mu, Sigma))\n",
    "\n",
    "    mu_np = np.asarray(mu)\n",
    "    Sigma_np = np.asarray(Sigma)\n",
    "\n",
    "    real_mu_np = np.asarray(real_ret)\n",
    "    real_sigma_np = np.asarray(real_sigma)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w_nn, mask, _, _ = model(X_t, y_t, mu_t, Sigma_t, return_goal=return_goals[0])\n",
    "    w_nn = w_nn.cpu().numpy()\n",
    "    ret_nn = real_mu_np @ w_nn\n",
    "    var_nn = w_nn @ real_sigma_np @ w_nn\n",
    "\n",
    "    # ---- SVM then MVO ----\n",
    "    scores = svm_clf.decision_function(X)\n",
    "    selected = scores > 0\n",
    "    if selected.sum() < 2:\n",
    "        continue  # skip this snapshot\n",
    "\n",
    "    mu_sel = mu_np[selected]\n",
    "    Sigma_sel = Sigma_np[np.ix_(selected, selected)]\n",
    "    real_mu_sel = real_mu_np[selected]\n",
    "    real_sigma_sel = real_sigma_np[np.ix_(selected, selected)]\n",
    "\n",
    "    w_svm = solve_mvo(np.ones(len(mu_sel)), mu_sel, Sigma_sel, return_goals[0])\n",
    "    if w_svm is None:\n",
    "        continue\n",
    "\n",
    "    ret_svm = real_mu_sel @ w_svm\n",
    "    var_svm = w_svm @ real_sigma_sel @ w_svm\n",
    "\n",
    "    rets_nn.append(ret_nn)\n",
    "    vars_nn.append(var_nn)\n",
    "    rets_svm.append(ret_svm)\n",
    "    vars_svm.append(var_svm)\n",
    "\n",
    "mean_ret_nn = np.mean(rets_nn)\n",
    "mean_var_nn = np.mean(vars_nn)\n",
    "sharpe_nn = mean_ret_nn / (np.std(rets_nn))\n",
    "\n",
    "mean_ret_svm = np.mean(rets_svm)\n",
    "mean_var_svm = np.mean(vars_svm)\n",
    "sharpe_svm = mean_ret_svm / (np.std(rets_svm))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN: Mean Return = 0.0567, Variance = 0.0014, Sharpe = 1.6070\n",
      "SVM: Mean Return = 0.0554, Variance = 0.0013, Sharpe = 1.6328\n"
     ]
    }
   ],
   "source": [
    "print(f\"NN: Mean Return = {mean_ret_nn:.4f}, Variance = {mean_var_nn:.4f}, Sharpe = {sharpe_nn:.4f}\")\n",
    "print(f\"SVM: Mean Return = {mean_ret_svm:.4f}, Variance = {mean_var_svm:.4f}, Sharpe = {sharpe_svm:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## not using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# n_pts = 30\n",
    "# sigmas_nn, mus_nn = [], []\n",
    "# sigmas_svm, mus_svm = [], []\n",
    "\n",
    "# for snap in train_snaps:\n",
    "#     X, y, mu, Sigma, real_ret, real_sigma = snap[\"X_feat\"], snap[\"y\"], snap[\"mu_fore\"], snap[\"Sigma_fore\"], snap[\"real_returns\"], snap[\"real_sigma\"]\n",
    "    \n",
    "#     # Check if SVM has more than one class\n",
    "#     if len(np.unique(y)) < 2:\n",
    "#         continue\n",
    "\n",
    "#     # ---- NN Efficient Frontier ----\n",
    "#     X_t, y_t, mu_t, Sigma_t = map(lambda x: torch.tensor(x).double(), (X, y, mu, Sigma))\n",
    "    \n",
    "#     mu_np = np.asarray(mu)\n",
    "#     Sigma_np = np.asarray(Sigma)\n",
    "\n",
    "#     real_mu_np = np.asarray(real_ret)\n",
    "#     real_sigma_np = np.asarray(real_sigma)\n",
    "\n",
    "#     sigma_nn, mu_nn = [], []\n",
    "#     for goal in np.linspace(mu_np.min(), mu_np.max(), n_pts):\n",
    "#         with torch.no_grad():\n",
    "#             w, mask, _, _ = model(X_t, y_t, mu_t, Sigma_t, return_goal=goal)\n",
    "#         w = w.cpu().numpy()  # convert to numpy array\n",
    "#         if w is not None:\n",
    "#             sigma_nn.append(np.sqrt(w @ real_sigma_np @ w))\n",
    "#             mu_nn.append(real_mu_np @ w)\n",
    "#     if len(sigma_nn) == n_pts:\n",
    "#         sigmas_nn.append(sigma_nn)\n",
    "#         mus_nn.append(mu_nn)\n",
    "\n",
    "#     # ---- SVM+MVO Efficient Frontier ----\n",
    "\n",
    "#     scores = svm_clf.decision_function(X)\n",
    "#     selected = scores > 0\n",
    "#     if selected.sum() < 2:\n",
    "#         continue\n",
    "#     mu_sel = mu_np[selected]\n",
    "#     real_mu_sel = real_mu_np[selected]\n",
    "\n",
    "#     Sigma_sel = Sigma_np[np.ix_(selected, selected)]\n",
    "#     real_sigma_np = real_sigma_np[np.ix_(selected, selected)]\n",
    "\n",
    "#     sigma_svm, mu_svm = [], []\n",
    "#     for goal in np.linspace(mu_sel.min(), mu_sel.max(), n_pts):\n",
    "#         w = solve_mvo(np.ones(mu_sel.shape), mu_sel, Sigma_sel, goal)\n",
    "#         if w is not None:\n",
    "#             sigma_svm.append(np.sqrt(w @ real_sigma_np @ w))\n",
    "#             mu_svm.append(real_mu_sel @ w)\n",
    "#     if len(sigma_svm) == n_pts:\n",
    "#         sigmas_svm.append(sigma_svm)\n",
    "#         mus_svm.append(mu_svm)\n",
    "\n",
    "# # Convert to arrays for easier averaging\n",
    "# sigmas_nn = np.array(sigmas_nn)\n",
    "# mus_nn = np.array(mus_nn)\n",
    "# sigmas_svm = np.array(sigmas_svm)\n",
    "# mus_svm = np.array(mus_svm)\n",
    "\n",
    "# # Compute averages\n",
    "# mean_sig_nn, mean_mu_nn = sigmas_nn.mean(axis=0), mus_nn.mean(axis=0)\n",
    "# mean_sig_svm, mean_mu_svm = sigmas_svm.mean(axis=0), mus_svm.mean(axis=0)\n",
    "\n",
    "# # Plot\n",
    "# plt.figure(figsize=(8,5))\n",
    "# plt.plot(mean_sig_nn, mean_mu_nn, label=\"End-to-End NN\", marker='o')\n",
    "# plt.plot(mean_sig_svm, mean_mu_svm, label=\"SVM+MVO\", marker='x')\n",
    "# plt.xlabel(\"Portfolio Risk (Std Dev)\")\n",
    "# plt.ylabel(\"Portfolio Expected Return\")\n",
    "# plt.title(\"Average Efficient Frontiers Across All Valid Snapshots\")\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out-of-sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_spd_np(M, eps=1e-6):\n",
    "    M = (M + M.T) * 0.5\n",
    "    I = np.eye(M.shape[0], dtype=M.dtype)\n",
    "    jitter = eps\n",
    "    for _ in range(10):\n",
    "        try:\n",
    "            np.linalg.cholesky(M)\n",
    "            return M\n",
    "        except np.linalg.LinAlgError:\n",
    "            M = M + jitter * I\n",
    "            jitter *= 10\n",
    "    raise RuntimeError(\"Unable to make SPD matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 58\n",
      "21 65\n",
      "63 23\n",
      "24 62\n",
      "56 30\n",
      "20 66\n",
      "37 49\n",
      "36 50\n",
      "63 23\n",
      "18 68\n",
      "75 11\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(test_snaps)):\n",
    "    print((test_snaps[i]['y'] == -1).sum(), (test_snaps[i]['y'] == 1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aligned on 12 snapshots.\n",
      "NN Mean Return: 0.051060, Mean Variance: 0.003056\n",
      "SVM+MVO Mean Return: 0.050972, Mean Variance: 0.003050\n",
      "NN Sharpe: 1.094111\n",
      "SVM+MVO Sharpe: 1.084861\n"
     ]
    }
   ],
   "source": [
    "results_aligned = []\n",
    "\n",
    "for snap in val_snaps:\n",
    "    X = snap[\"X_feat\"]\n",
    "    y = snap[\"y\"]\n",
    "    mu = snap[\"mu_fore\"]\n",
    "    Sigma = snap[\"Sigma_fore\"]\n",
    "    real_returns = snap[\"real_returns\"]\n",
    "    real_sigma = snap[\"real_sigma\"]\n",
    "\n",
    "    # If your snapshots include tickers:\n",
    "    asset_names = np.asarray(snap.get(\"tickers\", np.arange(len(X))))\n",
    "\n",
    "    try:\n",
    "        # ================== NN Portfolio ====================\n",
    "        X_t, y_t, mu_t, Sigma_t = map(lambda x: torch.tensor(x).double(), (X, y, mu, Sigma))\n",
    "        with torch.no_grad():\n",
    "            w_nn, mask, _, _ = model(X_t, y_t, mu_t, Sigma_t, return_goal=return_goals[0])\n",
    "        w_nn = w_nn.cpu().numpy()\n",
    "        mask_np = mask.cpu().numpy()\n",
    "\n",
    "        realized_nn = real_returns\n",
    "        port_ret_nn = (w_nn * realized_nn).sum()\n",
    "        port_var_nn = w_nn @ real_sigma @ w_nn\n",
    "        port_sharpe_nn = port_ret_nn / (np.sqrt(port_var_nn) + 1e-8)\n",
    "\n",
    "        nn_selected_idx = np.where(w_nn > 1e-6)[0]\n",
    "        nn_selected_names = asset_names[nn_selected_idx]\n",
    "\n",
    "        # ================== SVM+MVO Portfolio ====================        \n",
    "        scores = svm_clf.decision_function(X)\n",
    "        selected = scores > 0\n",
    "\n",
    "        if selected.sum() < 2:\n",
    "            print(f\"Skipping {snap['date']} (SVM selected too few assets)\")\n",
    "            continue\n",
    "\n",
    "        mu_sel = mu[selected]\n",
    "        Sigma_sel = Sigma[np.ix_(selected, selected)]\n",
    "        Sigma_sel = make_spd_np(Sigma_sel)  # Ensure SPD\n",
    "\n",
    "        w_svm = solve_mvo(np.ones(mu_sel.shape), mu_sel, Sigma_sel, return_goal=return_goals[0])\n",
    "        if w_svm is None:\n",
    "            print(f\"Skipping {snap['date']} (SVM+MVO infeasible)\")\n",
    "            continue\n",
    "\n",
    "        realized_svm = real_returns[selected]\n",
    "        real_sigma_svm = real_sigma[np.ix_(selected, selected)]\n",
    "        port_ret_svm = (w_svm * realized_svm).sum()\n",
    "        port_var_svm = w_svm @ real_sigma_svm @ w_svm\n",
    "        port_sharpe_svm = port_ret_svm / (np.sqrt(port_var_svm) + 1e-8)\n",
    "\n",
    "        svm_selected_idx = np.where(w_svm > 1e-6)[0]\n",
    "        svm_selected_names = asset_names[svm_selected_idx]\n",
    "\n",
    "        # ================== Append aligned results ====================\n",
    "        results_aligned.append({\n",
    "            \"date\": snap[\"date\"],\n",
    "            \"nn_return\": port_ret_nn,\n",
    "            \"nn_variance\": port_var_nn,\n",
    "            \"nn_sharpe\": port_sharpe_nn,\n",
    "            \"nn_selected_idx\": nn_selected_idx.tolist(),\n",
    "            \"nn_selected_names\": nn_selected_names.tolist(),\n",
    "            \"svm_return\": port_ret_svm,\n",
    "            \"svm_variance\": port_var_svm,\n",
    "            \"svm_sharpe\": port_sharpe_svm,\n",
    "            \"svm_selected_idx\": svm_selected_idx.tolist(),\n",
    "            \"svm_selected_names\": svm_selected_names.tolist(),\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {snap['date']} due to error: {e}\")\n",
    "        continue\n",
    "\n",
    "# ================== Compute aligned comparisons ====================\n",
    "\n",
    "rets_nn = [r[\"nn_return\"] for r in results_aligned]\n",
    "rets_svm = [r[\"svm_return\"] for r in results_aligned]\n",
    "vars_nn = [r[\"nn_variance\"] for r in results_aligned]\n",
    "vars_svm = [r[\"svm_variance\"] for r in results_aligned]\n",
    "sharpe_nn = [r[\"nn_sharpe\"] for r in results_aligned]\n",
    "sharpe_svm = [r[\"svm_sharpe\"] for r in results_aligned]\n",
    "\n",
    "print(f\"\\nAligned on {len(results_aligned)} snapshots.\")\n",
    "print(f\"NN Mean Return: {np.mean(rets_nn):.6f}, Mean Variance: {np.mean(vars_nn):.6f}\")\n",
    "print(f\"SVM+MVO Mean Return: {np.mean(rets_svm):.6f}, Mean Variance: {np.mean(vars_svm):.6f}\")\n",
    "print(f\"NN Sharpe: {np.mean(sharpe_nn):.6f}\")\n",
    "print(f\"SVM+MVO Sharpe: {np.mean(sharpe_svm):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN Out-of-Sample: Mean Return: 0.051060116767464525 Mean Variance: 0.0030558455996507056\n",
      "SVM+MVO Out-of-Sample: Mean Return: 0.05097160822378177 Mean Variance: 0.0030502105858067987\n",
      "NN Sharpe: 1.0941108012711978\n",
      "SVM+MVO Sharpe: 1.0848607594033801\n",
      "NN outperforms SVM+MVO in terms of mean return.\n",
      "SVM+MVO outperforms NN in terms of variance.\n",
      "NN outperforms SVM+MVO in terms of Sharpe ratio.\n"
     ]
    }
   ],
   "source": [
    "print(\"NN Out-of-Sample: Mean Return:\", np.mean(rets_nn), \"Mean Variance:\", np.mean(vars_nn))\n",
    "print(\"SVM+MVO Out-of-Sample: Mean Return:\", np.mean(rets_svm), \"Mean Variance:\", np.mean(vars_svm))\n",
    "print(\"NN Sharpe:\", np.mean(sharpe_nn))\n",
    "print(\"SVM+MVO Sharpe:\", np.mean(sharpe_svm))\n",
    "\n",
    "if np.mean(rets_nn) > np.mean(rets_svm):\n",
    "    print(\"NN outperforms SVM+MVO in terms of mean return.\")\n",
    "else:\n",
    "    print(\"SVM+MVO outperforms NN in terms of mean return.\")\n",
    "if np.mean(vars_nn) < np.mean(vars_svm):\n",
    "    print(\"NN outperforms SVM+MVO in terms of variance.\")\n",
    "else:\n",
    "    print(\"SVM+MVO outperforms NN in terms of variance.\")\n",
    "if np.mean(sharpe_nn)  > np.mean(sharpe_svm):\n",
    "    print(\"NN outperforms SVM+MVO in terms of Sharpe ratio.\")\n",
    "else:\n",
    "    print(\"SVM+MVO outperforms NN in terms of Sharpe ratio.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2023-01-31 00:00:00\n",
      "  NN Return: 0.018253, Variance: 0.003448, Selected Assets: 84\n",
      "  SVM+MVO Return: 0.027837, Variance: 0.003071, Selected Assets: 27\n",
      "Date: 2023-02-28 00:00:00\n",
      "  NN Return: 0.039045, Variance: 0.005724, Selected Assets: 63\n",
      "  SVM+MVO Return: 0.039045, Variance: 0.005724, Selected Assets: 63\n",
      "Date: 2023-03-31 00:00:00\n",
      "  NN Return: 0.062792, Variance: 0.001681, Selected Assets: 34\n",
      "  SVM+MVO Return: 0.063646, Variance: 0.001778, Selected Assets: 35\n",
      "Date: 2023-04-30 00:00:00\n",
      "  NN Return: 0.026228, Variance: 0.002998, Selected Assets: 84\n",
      "  SVM+MVO Return: 0.025155, Variance: 0.003011, Selected Assets: 20\n",
      "Date: 2023-05-31 00:00:00\n",
      "  NN Return: 0.102031, Variance: 0.003550, Selected Assets: 21\n",
      "  SVM+MVO Return: 0.102031, Variance: 0.003550, Selected Assets: 21\n",
      "Date: 2023-06-30 00:00:00\n",
      "  NN Return: 0.082103, Variance: 0.002824, Selected Assets: 14\n",
      "  SVM+MVO Return: 0.082103, Variance: 0.002824, Selected Assets: 14\n",
      "Date: 2023-07-31 00:00:00\n",
      "  NN Return: 0.012610, Variance: 0.007781, Selected Assets: 7\n",
      "  SVM+MVO Return: 0.012610, Variance: 0.007781, Selected Assets: 7\n",
      "Date: 2023-08-31 00:00:00\n",
      "  NN Return: -0.002733, Variance: 0.002143, Selected Assets: 23\n",
      "  SVM+MVO Return: -0.002711, Variance: 0.002143, Selected Assets: 85\n",
      "Date: 2023-09-30 00:00:00\n",
      "  NN Return: 0.081123, Variance: 0.001520, Selected Assets: 11\n",
      "  SVM+MVO Return: 0.080988, Variance: 0.001509, Selected Assets: 12\n",
      "Date: 2023-10-31 00:00:00\n",
      "  NN Return: 0.050128, Variance: 0.001269, Selected Assets: 9\n",
      "  SVM+MVO Return: 0.049053, Variance: 0.001297, Selected Assets: 10\n",
      "Date: 2023-11-30 00:00:00\n",
      "  NN Return: 0.072381, Variance: 0.001189, Selected Assets: 59\n",
      "  SVM+MVO Return: 0.072434, Variance: 0.001183, Selected Assets: 59\n",
      "Date: 2023-12-31 00:00:00\n",
      "  NN Return: 0.068760, Variance: 0.002543, Selected Assets: 13\n",
      "  SVM+MVO Return: 0.059468, Variance: 0.002730, Selected Assets: 12\n"
     ]
    }
   ],
   "source": [
    "# use results_aligned\n",
    "\n",
    "for r in results_aligned:\n",
    "    print(f\"Date: {r['date']}\")\n",
    "    print(f\"  NN Return: {r['nn_return']:.6f}, Variance: {r['nn_variance']:.6f}, Selected Assets: {len(r['nn_selected_names'])}\")\n",
    "    print(f\"  SVM+MVO Return: {r['svm_return']:.6f}, Variance: {r['svm_variance']:.6f}, Selected Assets: {len(r['svm_selected_names'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------\n",
      "qpth warning: Returning an inaccurate and potentially incorrect solution.\n",
      "\n",
      "Some residual is large.\n",
      "Your problem may be infeasible or difficult.\n",
      "\n",
      "You can try using the CVXPY solver to see if your problem is feasible\n",
      "and you can use the verbose option to check the convergence status of\n",
      "our solver while increasing the number of iterations.\n",
      "\n",
      "Advanced users:\n",
      "You can also try to enable iterative refinement in the solver:\n",
      "https://github.com/locuslab/qpth/issues/6\n",
      "--------\n",
      "\n",
      "Skipping 2024-04-30 00:00:00 (SVM+MVO infeasible)\n",
      "\n",
      "Aligned on 10 snapshots.\n",
      "NN Mean Return: 0.064086, Mean Variance: 0.001707\n",
      "SVM+MVO Mean Return: 0.058675, Mean Variance: 0.001613\n",
      "NN Sharpe: 1.865631\n",
      "SVM+MVO Sharpe: 1.610391\n"
     ]
    }
   ],
   "source": [
    "results_aligned = []\n",
    "\n",
    "for snap in test_snaps:\n",
    "    X = snap[\"X_feat\"]\n",
    "    y = snap[\"y\"]\n",
    "    mu = snap[\"mu_fore\"]\n",
    "    Sigma = snap[\"Sigma_fore\"]\n",
    "    real_returns = snap[\"real_returns\"]\n",
    "    real_sigma = snap[\"real_sigma\"]\n",
    "\n",
    "    # If your snapshots include tickers:\n",
    "    asset_names = np.asarray(snap.get(\"tickers\", np.arange(len(X))))\n",
    "\n",
    "    try:\n",
    "        # ================== NN Portfolio ====================\n",
    "        X_t, y_t, mu_t, Sigma_t = map(lambda x: torch.tensor(x).double(), (X, y, mu, Sigma))\n",
    "        with torch.no_grad():\n",
    "            w_nn, mask, _, _ = model(X_t, y_t, mu_t, Sigma_t, return_goal=return_goals[0])\n",
    "        w_nn = w_nn.cpu().numpy()\n",
    "        mask_np = mask.cpu().numpy()\n",
    "\n",
    "        realized_nn = real_returns[:len(w_nn)]\n",
    "        port_ret_nn = (w_nn * realized_nn).sum()\n",
    "        port_var_nn = w_nn @ real_sigma[:len(w_nn), :len(w_nn)] @ w_nn\n",
    "\n",
    "        nn_selected_idx = np.where(w_nn > 1e-5)[0]\n",
    "        nn_selected_names = asset_names[nn_selected_idx]\n",
    "\n",
    "        # ================== SVM+MVO Portfolio ====================\n",
    "        scores = svm_clf.decision_function(X)\n",
    "        selected = scores > 0\n",
    "\n",
    "        if selected.sum() < 2:\n",
    "            print(f\"Skipping {snap['date']} (SVM selected too few assets)\")\n",
    "            continue\n",
    "\n",
    "        mu_sel = mu[selected]\n",
    "        Sigma_sel = Sigma[np.ix_(selected, selected)]\n",
    "        Sigma_sel = make_spd_np(Sigma_sel)  # Ensure SPD\n",
    "\n",
    "        w_svm = solve_mvo(np.ones(mu_sel.shape), mu_sel, Sigma_sel, return_goal=return_goals[0])\n",
    "        if w_svm is None:\n",
    "            print(f\"Skipping {snap['date']} (SVM+MVO infeasible)\")\n",
    "            continue\n",
    "\n",
    "        realized_svm = real_returns[selected]\n",
    "        real_sigma_svm = real_sigma[np.ix_(selected, selected)]\n",
    "        port_ret_svm = (w_svm * realized_svm).sum()\n",
    "        port_var_svm = w_svm @ real_sigma_svm @ w_svm\n",
    "\n",
    "        svm_selected_idx = np.where(w_svm > 1e-5)[0]\n",
    "        svm_selected_names = asset_names[svm_selected_idx]\n",
    "\n",
    "        # ================== Append aligned results ====================\n",
    "        results_aligned.append({\n",
    "            \"date\": snap[\"date\"],\n",
    "            \"nn_return\": port_ret_nn,\n",
    "            \"nn_variance\": port_var_nn,\n",
    "            \"nn_selected_idx\": nn_selected_idx.tolist(),\n",
    "            \"nn_selected_names\": nn_selected_names.tolist(),\n",
    "            \"svm_return\": port_ret_svm,\n",
    "            \"svm_variance\": port_var_svm,\n",
    "            \"svm_selected_idx\": svm_selected_idx.tolist(),\n",
    "            \"svm_selected_names\": svm_selected_names.tolist(),\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {snap['date']} due to error: {e}\")\n",
    "        continue\n",
    "\n",
    "# ================== Compute aligned comparisons ====================\n",
    "\n",
    "rets_nn = [r[\"nn_return\"] for r in results_aligned]\n",
    "rets_svm = [r[\"svm_return\"] for r in results_aligned]\n",
    "vars_nn = [r[\"nn_variance\"] for r in results_aligned]\n",
    "vars_svm = [r[\"svm_variance\"] for r in results_aligned]\n",
    "\n",
    "print(f\"\\nAligned on {len(results_aligned)} snapshots.\")\n",
    "print(f\"NN Mean Return: {np.mean(rets_nn):.6f}, Mean Variance: {np.mean(vars_nn):.6f}\")\n",
    "print(f\"SVM+MVO Mean Return: {np.mean(rets_svm):.6f}, Mean Variance: {np.mean(vars_svm):.6f}\")\n",
    "print(f\"NN Sharpe: {np.mean(rets_nn)/np.std(rets_nn):.6f}\")\n",
    "print(f\"SVM+MVO Sharpe: {np.mean(rets_svm)/np.std(rets_svm):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN Out-of-Sample: Mean Return: 0.06408602500444702 Mean Variance: 0.0017066805664551292\n",
      "SVM+MVO Out-of-Sample: Mean Return: 0.05867508531493715 Mean Variance: 0.0016125466222726714\n",
      "NN Sharpe: 1.8656312618259718\n",
      "SVM+MVO Sharpe: 1.610391182358715\n",
      "NN outperforms SVM+MVO in terms of mean return.\n",
      "SVM+MVO outperforms NN in terms of variance.\n",
      "NN outperforms SVM+MVO in terms of Sharpe ratio.\n"
     ]
    }
   ],
   "source": [
    "print(\"NN Out-of-Sample: Mean Return:\", np.mean(rets_nn), \"Mean Variance:\", np.mean(vars_nn))\n",
    "print(\"SVM+MVO Out-of-Sample: Mean Return:\", np.mean(rets_svm), \"Mean Variance:\", np.mean(vars_svm))\n",
    "print(\"NN Sharpe:\", np.mean(rets_nn) / np.std(rets_nn))\n",
    "print(\"SVM+MVO Sharpe:\", np.mean(rets_svm) / np.std(rets_svm))\n",
    "\n",
    "if np.mean(rets_nn) > np.mean(rets_svm):\n",
    "    print(\"NN outperforms SVM+MVO in terms of mean return.\")\n",
    "else:\n",
    "    print(\"SVM+MVO outperforms NN in terms of mean return.\")\n",
    "if np.mean(vars_nn) < np.mean(vars_svm):\n",
    "    print(\"NN outperforms SVM+MVO in terms of variance.\")\n",
    "else:\n",
    "    print(\"SVM+MVO outperforms NN in terms of variance.\")\n",
    "if np.mean(rets_nn) / np.std(rets_nn) > np.mean(rets_svm) / np.std(rets_svm):\n",
    "    print(\"NN outperforms SVM+MVO in terms of Sharpe ratio.\")\n",
    "else:\n",
    "    print(\"SVM+MVO outperforms NN in terms of Sharpe ratio.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2024-01-31 00:00:00\n",
      "  NN Return: 0.064259, Variance: 0.001499, Selected Assets: 37\n",
      "  SVM+MVO Return: 0.063888, Variance: 0.001444, Selected Assets: 39\n",
      "Date: 2024-02-29 00:00:00\n",
      "  NN Return: 0.047093, Variance: 0.001401, Selected Assets: 63\n",
      "  SVM+MVO Return: 0.047093, Variance: 0.001401, Selected Assets: 63\n",
      "Date: 2024-03-31 00:00:00\n",
      "  NN Return: 0.046532, Variance: 0.001836, Selected Assets: 12\n",
      "  SVM+MVO Return: 0.046532, Variance: 0.001836, Selected Assets: 12\n",
      "Date: 2024-05-31 00:00:00\n",
      "  NN Return: 0.049780, Variance: 0.000689, Selected Assets: 29\n",
      "  SVM+MVO Return: 0.049780, Variance: 0.000689, Selected Assets: 29\n",
      "Date: 2024-06-30 00:00:00\n",
      "  NN Return: 0.072115, Variance: 0.001127, Selected Assets: 63\n",
      "  SVM+MVO Return: 0.046392, Variance: 0.001103, Selected Assets: 44\n",
      "Date: 2024-07-31 00:00:00\n",
      "  NN Return: 0.081135, Variance: 0.001132, Selected Assets: 28\n",
      "  SVM+MVO Return: 0.081519, Variance: 0.001100, Selected Assets: 24\n",
      "Date: 2024-08-31 00:00:00\n",
      "  NN Return: 0.057623, Variance: 0.002214, Selected Assets: 43\n",
      "  SVM+MVO Return: 0.034787, Variance: 0.001329, Selected Assets: 16\n",
      "Date: 2024-09-30 00:00:00\n",
      "  NN Return: 0.069248, Variance: 0.002432, Selected Assets: 9\n",
      "  SVM+MVO Return: 0.069248, Variance: 0.002432, Selected Assets: 9\n",
      "Date: 2024-10-31 00:00:00\n",
      "  NN Return: 0.148178, Variance: 0.003183, Selected Assets: 13\n",
      "  SVM+MVO Return: 0.148178, Variance: 0.003183, Selected Assets: 13\n",
      "Date: 2024-11-30 00:00:00\n",
      "  NN Return: 0.004898, Variance: 0.001556, Selected Assets: 18\n",
      "  SVM+MVO Return: -0.000668, Variance: 0.001609, Selected Assets: 20\n"
     ]
    }
   ],
   "source": [
    "# use results_aligned\n",
    "\n",
    "for r in results_aligned:\n",
    "    print(f\"Date: {r['date']}\")\n",
    "    print(f\"  NN Return: {r['nn_return']:.6f}, Variance: {r['nn_variance']:.6f}, Selected Assets: {len(r['nn_selected_names'])}\")\n",
    "    print(f\"  SVM+MVO Return: {r['svm_return']:.6f}, Variance: {r['svm_variance']:.6f}, Selected Assets: {len(r['svm_selected_names'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'plots/9curve.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m axes \u001b[38;5;241m=\u001b[39m axes\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ax, img_file \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(axes, png_files):\n\u001b[0;32m---> 16\u001b[0m     img \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mimread(img_file)\n\u001b[1;32m     17\u001b[0m     ax\u001b[38;5;241m.\u001b[39mimshow(img)\n\u001b[1;32m     18\u001b[0m     ax\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Hide axes ticks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/matplotlib/pyplot.py:2597\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   2593\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(matplotlib\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mimread)\n\u001b[1;32m   2594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimread\u001b[39m(\n\u001b[1;32m   2595\u001b[0m         fname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath \u001b[38;5;241m|\u001b[39m BinaryIO, \u001b[38;5;28mformat\u001b[39m: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2596\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m-> 2597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m matplotlib\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mimread(fname, \u001b[38;5;28mformat\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/matplotlib/image.py:1544\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fname, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(parse\u001b[38;5;241m.\u001b[39murlparse(fname)\u001b[38;5;241m.\u001b[39mscheme) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1538\u001b[0m     \u001b[38;5;66;03m# Pillow doesn't handle URLs directly.\u001b[39;00m\n\u001b[1;32m   1539\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease open the URL for reading and pass the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1541\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult to Pillow, e.g. with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1542\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m``np.array(PIL.Image.open(urllib.request.urlopen(url)))``.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1543\u001b[0m         )\n\u001b[0;32m-> 1544\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m img_open(fname) \u001b[38;5;28;01mas\u001b[39;00m image:\n\u001b[1;32m   1545\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (_pil_png_to_float_array(image)\n\u001b[1;32m   1546\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image, PIL\u001b[38;5;241m.\u001b[39mPngImagePlugin\u001b[38;5;241m.\u001b[39mPngImageFile) \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[1;32m   1547\u001b[0m             pil_to_array(image))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/PIL/ImageFile.py:126\u001b[0m, in \u001b[0;36mImageFile.__init__\u001b[0;34m(self, fp, filename)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecodermaxblock \u001b[38;5;241m=\u001b[39m MAXBLOCK\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_path(fp):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;66;03m# filename\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename \u001b[38;5;241m=\u001b[39m fp\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'plots/9curve.png'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x2400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List of your PNG files\n",
    "png_files = ['plots/9curve.png', 'plots/9eff.png']  # replace with your file names\n",
    "\n",
    "n = len(png_files)\n",
    "cols = 2\n",
    "rows = (n + 1) // cols\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(8, 24))\n",
    "\n",
    "# Flatten axes for easy iteration (handles 1D/2D arrays)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, img_file in zip(axes, png_files):\n",
    "    img = plt.imread(img_file)\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')  # Hide axes ticks\n",
    "\n",
    "# Hide unused subplots, if any\n",
    "for ax in axes[len(png_files):]:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/9combined.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "# save the figure\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
