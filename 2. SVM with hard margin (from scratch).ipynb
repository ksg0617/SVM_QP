{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We have convex quadratic optimization problem \n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}{\\underset{\\mathbf{w}, b}{\\operatorname{minimize}}} & {\\frac{1}{2}\\|\\mathbf{w}\\|^{2}} \\\\ {\\text { subject to }} & {y_{i}\\left(\\mathbf{w} \\cdot \\mathbf{x}_{i}+b\\right)-1 \\geq 0, i=1, \\ldots, m}\\end{array}\n",
    "$$\n",
    "\n",
    "### this problem can be solve by using Lagrange Multiplier\n",
    "\n",
    "# Lagrange multiplier\n",
    "\n",
    "The Italian-French mathematician Giuseppe Lodovico Lagrangia, also known as JosephLouis Lagrange, invented a strategy for finding the local maxima and minima of a function subject to equality constraint. It is called the method of Lagrange multipliers.\n",
    "\n",
    "let,\n",
    "$$\n",
    "\\begin{array}{c}{f(\\mathbf{w})=\\frac{1}{2}\\|\\mathbf{w}\\|^{2}} \\\\ {g_{i}(\\mathbf{w}, b)=y_{i}\\left(\\mathbf{w} \\cdot \\mathbf{x}_{i}+b\\right)-1, i=1, \\ldots, m}\\end{array}\n",
    "$$\n",
    "and 'm' constraint functions\n",
    "\n",
    "We introduce the Lagrangian function to this function:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c}{\\mathcal{L}(\\mathbf{w}, b, \\alpha)=f(\\mathbf{w})-\\sum_{i=1}^{m} \\alpha_{i} g_{i}(\\mathbf{w}, b)} \\\\ {\\mathcal{L}(\\mathbf{w}, b, \\alpha)=\\frac{1}{2}\\|\\mathbf{w}\\|^{2}-\\sum_{i=1}^{m} \\alpha_{i}\\left[y_{i}\\left(\\mathbf{w} \\cdot \\mathbf{x}_{i}+b\\right)-1\\right]}\\end{array}\n",
    "$$\n",
    "\n",
    "You may have noticed that the method of Lagrange multipliers is used for solving problems with equality constraints, and here we are using them with inequality constraints. This is because the method still works for inequality constraints, provided some additional conditions (the KKT conditions) are met\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Wolfe dual problem \n",
    "\n",
    "The Lagrangian problem has 'm' inequality constraints (where 'm' is the number of training examples) and is typically solved using its dual form. The duality principle tells us that an optimization problem can be viewed from two perspectives. The first one is the primal problem, a minimization problem in our case, and the other one is the dual problem, which will be a maximization problem. What is interesting is that the maximum of the dual problem will always be less than or equal to the minimum of the primal problem (we say it provides a lower bound to the solution of the primal problem). \n",
    "\n",
    "The Lagrangian primal problem is:\n",
    "$$\n",
    "\\begin{array}{ll}{\\min _{\\mathbf{w}, b}} & {\\max _{\\alpha} \\quad \\mathcal{L}(\\mathbf{w}, b, \\alpha)} \\\\ {\\text { subject to }} & {\\alpha_{i} \\geq 0, \\quad i=1, \\ldots, m}\\end{array}\n",
    "$$\n",
    "\n",
    "Solving the minimization problem involves taking the partial derivatives of 'L' with respect to 'w' and 'b'.\n",
    "\n",
    "$$\n",
    "\\begin{array}{c}{\\nabla_{\\mathbf{w}} \\mathcal{L}=\\mathbf{w}-\\sum_{i=1}^{m} \\alpha_{i} y_{i} \\mathbf{x}_{i}=\\mathbf{0}} \\\\ {\\frac{\\partial \\mathcal{L}}{\\partial b}=-\\sum_{i=1}^{m} \\alpha_{i} y_{i}=0}\\end{array}\n",
    "$$\n",
    "\n",
    "From the first equation, we find that:\n",
    " \n",
    "$$\n",
    "\\mathbf{w}=\\sum_{i=1}^{m} \\alpha_{i} y_{i} \\mathbf{x}_{i}\n",
    "$$\n",
    "\n",
    "Let us substitute :\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned} W(\\alpha, b) &=\\frac{1}{2}\\left(\\sum_{i=1}^{m} \\alpha_{i} y_{i} \\mathbf{x}_{i}\\right) \\cdot\\left(\\sum_{j=1}^{m} \\alpha_{j} y_{j} \\mathbf{x}_{j}\\right)-\\sum_{i=1}^{m} \\alpha_{i}\\left[y_{i}\\left(\\left(\\sum_{j=1}^{m} \\alpha_{j} y_{j} \\mathbf{x}_{j}\\right) \\cdot \\mathbf{x}_{i}+b\\right)-1\\right] \\\\ &=\\frac{1}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\alpha_{i} \\alpha_{j} y_{i} y_{i} \\mathbf{x}_{i} \\cdot \\mathbf{x}_{j}-\\sum_{i=1}^{m} \\sum_{j=1}^{m} \\alpha_{i} \\alpha_{j} y_{i} y_{i} \\mathbf{x}_{i} \\cdot \\mathbf{x}_{j}-b \\sum_{i=1}^{m} \\alpha_{i} y_{i}+\\sum_{i=1}^{m} \\alpha_{i} \\\\ &=\\sum_{i=1}^{m} \\alpha_{i}-\\frac{1}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\alpha_{i} \\alpha_{j} y_{i} y_{j} \\mathbf{x}_{i} \\cdot \\mathbf{x}_{j}-b \\sum_{i=1}^{m} \\alpha_{i} y_{i} \\end{aligned}\n",
    "$$\n",
    " \n",
    "The optimization problem is now called the **Wolfe dual problem**\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{array}{cl}{\\underset{\\alpha}{\\operatorname{maximize}}} & {\\sum_{i=1}^{m} \\alpha_{i}-\\frac{1}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\alpha_{i} \\alpha_{j} y_{i} y_{j} \\mathbf{x}_{i} \\cdot \\mathbf{x}_{j}} \\\\ {\\text { subject to }} & {\\alpha_{i} \\geq 0, \\text { for any } i=1, \\ldots, m} \\\\ {} & {\\sum_{i=1}^{m} \\alpha_{i} y_{i}=0}\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "# Karush-Kuhn-Tucker conditions\n",
    "\n",
    "Because we are dealing with inequality constraints, there is an additional requirement: the solution must also satisfy the Karush-Kuhn-Tucker (KKT) conditions. \n",
    " \n",
    "The KKT conditions are first-order necessary conditions for a solution of an optimization problem to be optimal. Moreover, the problem should satisfy some regularity conditions. Luckily for us, one of the regularity conditions is Slaterâ€™s condition, and we just saw that it holds for SVMs. Because the primal problem we are trying to solve is a convex problem, the KKT conditions are also sufficient for the point to be primal and dual optimal, and there is zero duality gap. \n",
    "\n",
    "1. Stationarity Condition:\n",
    "$$\n",
    "\\begin{array}{c}{\\nabla_{\\mathrm{w}} \\mathcal{L}=\\mathbf{w}-\\sum_{i=1}^{m} \\alpha_{i} y_{i} \\mathbf{x}_{i}=\\mathbf{0}} \\\\ {\\frac{\\partial \\mathcal{L}}{\\partial b}=-\\sum_{i=1}^{m} \\alpha_{i} y_{i}=0}\\end{array}\n",
    "$$\n",
    "2. Primal feasibility condition:\n",
    "$$\n",
    "y_{i}\\left(\\mathbf{w} \\cdot \\mathbf{x}_{i}+b\\right)-1 \\geq 0 \\quad \\text { for all } i=1, \\ldots, m\n",
    "$$\n",
    "\n",
    "3. Dual feasibility condition:\n",
    "\n",
    "$$\n",
    "\\alpha_{i} \\geq 0 \\quad \\text { for all } i=1, \\ldots, m\n",
    "$$\n",
    "\n",
    "4. Complementarity slackness condition: \n",
    "$$\n",
    "\\alpha_{i}\\left[y_{i}\\left(\\mathbf{w} \\cdot \\mathbf{x}_{i}+b\\right)-1\\right]=0 \\quad \\text { for all } i=1, \\ldots, m\n",
    "$$\n",
    "\n",
    "\n",
    "# Hypothesis function \n",
    "The SVMs use the same hypothesis function as the Perceptron. The class of an example 'x' is given by: \n",
    "$$\n",
    "h\\left(\\mathbf{x}_{i}\\right)=\\operatorname{sign}\\left(\\mathbf{w} \\cdot \\mathbf{x}_{i}+b\\right)\n",
    "$$\n",
    "\n",
    "When using the dual formulation, it is computed using only the support vectors: \n",
    "\n",
    "$$\n",
    "h\\left(\\mathbf{x}_{i}\\right)=\\operatorname{sign}\\left(\\sum_{j=1}^{S} \\alpha_{j} y_{j}\\left(\\mathbf{x}_{j} \\cdot \\mathbf{x}_{i}\\right)+b\\right)\n",
    "$$\n",
    "\n",
    "# Solving SVMs with a QP solver \n",
    "\n",
    "A QP solver is a program used to solve quadratic programming problems. In the following example, we will use the Python package called CVXOPT\n",
    "\n",
    "This package provides a method that is able to solve quadratic problems of the form\n",
    "\n",
    "$$\n",
    "\\begin{array}{cl}{\\underset{x}{\\operatorname{minimize}}} & {\\frac{1}{2} x^{T} P x+q^{T} x} \\\\ {\\text { subject to }} & {G x \\leq h} \\\\ {} & {A x=b}\\end{array}\n",
    "$$\n",
    "\n",
    "It does not look like our optimization problem, so we will need to rewrite it so that we can solve it with this package.  \n",
    "First, we note that in the case of the Wolfe dual optimization problem, what we are trying to minimize is , so we can rewrite the quadratic problem with  instead of  to better see how the two problems relate\n",
    "\n",
    "$$\n",
    "\\begin{array}{cl}{\\underset{x}{\\operatorname{minimize}}} & {\\frac{1}{2} \\alpha^{T} P \\alpha+q^{T} \\alpha} \\\\ {\\text { subject to }} & {G \\alpha \\leq h} \\\\ {} & {A \\alpha=b}\\end{array}\n",
    "$$\n",
    "\n",
    "We will change the Wolfe dual problem. First, we transform the maximization problem\n",
    "\n",
    "$$\n",
    "\\begin{array}{cl}{\\underset{\\alpha}{\\operatorname{maximize}}} & {-\\frac{1}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\alpha_{i} \\alpha_{j} y_{i} y_{j} \\mathbf{x}_{i} \\cdot \\mathbf{x}_{j}+\\sum_{i=1}^{m} \\alpha_{i}} \\\\ {\\text { subject to }} & {\\alpha_{i} \\geq 0, \\text { for any } i=1, \\ldots, m} \\\\ {} & {\\sum_{i=1}^{m} \\alpha_{i} y_{i}=0}\\end{array}\n",
    "$$\n",
    "\n",
    "into a minimization problem by multiplying by -1. \n",
    "\n",
    "$$\n",
    "\\begin{array}{cl}{\\underset{\\alpha}{\\operatorname{minimize}}} & {\\frac{1}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\alpha_{i} \\alpha_{j} y_{i} y_{j} \\mathbf{x}_{i} \\cdot \\mathbf{x}_{j}-\\sum_{i=1}^{m} \\alpha_{i}} \\\\ {\\text { subject to }} & {-\\alpha_{i} \\leq 0, \\text { for any } i=1, \\ldots, m} \\\\ {} & {\\sum_{i=1}^{m} \\alpha_{i} y_{i}=0}\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "K\\left(\\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{m}\\right)=\\left(\\begin{array}{cccc}{\\mathbf{x}_{1} \\cdot \\mathbf{x}_{1}} & {\\mathbf{x}_{1} \\cdot \\mathbf{x}_{2}} & {\\dots} & {\\mathbf{x}_{1} \\cdot \\mathbf{x}_{m}} \\\\ {\\mathbf{x}_{2} \\cdot \\mathbf{x}_{1}} & {\\mathbf{x}_{2} \\cdot \\mathbf{x}_{2}} & {\\ldots} & {\\mathbf{x}_{2} \\cdot \\mathbf{x}_{m}} \\\\ {\\vdots} & {\\vdots} & {\\ddots} & {\\vdots} \\\\ {\\mathbf{x}_{m} \\cdot \\mathbf{x}_{1}} & {\\mathbf{x}_{m} \\cdot \\mathbf{x}_{2}} & {\\dots} & {\\mathbf{x}_{m} \\cdot \\mathbf{x}_{m}}\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\alpha=\\left(\\alpha_{1}, \\ldots, \\alpha_{m}\\right)^{T} \\text { and } \\mathbf{y}=\\left(y_{1}, \\cdots, y_{m}\\right)^{T}\n",
    "$$\n",
    "\n",
    "\n",
    "We use them to construct a vectorized version of the Wolfe dual problem where 'yyT' denotes the outer product of 'y' \n",
    "\n",
    "$$\n",
    "\\begin{array}{cl}{\\underset{\\alpha}{\\operatorname{minimize}}} & {\\frac{1}{2} \\alpha^{T}\\left(\\mathbf{y y}^{T} K\\right) \\alpha-\\alpha} \\\\ {\\text { subject to }} & {-\\alpha \\leq 0} \\\\ {} & {\\mathbf{y} \\cdot \\alpha=0}\\end{array}\n",
    "$$\n",
    "\n",
    "We are now able to find out the value for each of the parameters $P, q, G, h, A, \\text { and } b$  required by the CVXOPT qp function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cvxopt.solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_examples():\n",
    "    X1 = np.array([[8, 7], [4, 10], [9, 7], [7, 10],\n",
    "                   [9, 6], [4, 8], [10, 10]])\n",
    "    y1 = np.ones(len(X1))\n",
    "    X2 = np.array([[2, 7], [8, 3], [7, 5], [4, 4],\n",
    "                   [4, 6], [1, 3], [2, 5]])\n",
    "    y2 = np.ones(len(X2)) * -1\n",
    "    return X1, y1, X2, y2\n",
    "\n",
    "def get_dataset(get_examples):\n",
    "    X1, y1, X2, y2 = get_examples()\n",
    "    X, y = get_dataset_for(X1, y1, X2, y2)\n",
    "    return X, y\n",
    "\n",
    "def get_dataset_for(X1, y1, X2, y2):\n",
    "    X = np.vstack((X1, X2))\n",
    "    y = np.hstack((y1, y2))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_dataset(get_training_examples) \n",
    "m = X.shape[0] \n",
    "\n",
    "# Gram matrix - The matrix of all possible inner products of X. \n",
    "K = np.array([np.dot(X[i], X[j])\n",
    "              for i in range(m)\n",
    "              for j in range(m)]).reshape((m, m)) \n",
    "P = cvxopt.matrix(np.outer(y, y) * K) \n",
    "q = cvxopt.matrix(-1 * np.ones(m)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let alpha be 1 for all obervations\n",
    "# Equality constraints \n",
    "A = cvxopt.matrix(y, (1, m)) # create 1*m matrix of y \n",
    "b = cvxopt.matrix(0.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inequality constraints \n",
    "G = cvxopt.matrix(np.diag(-1 * np.ones(m))) \n",
    "h = cvxopt.matrix(np.zeros(m)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -3.9356e+00 -7.2072e+00  4e+01  6e+00  2e+00\n",
      " 1: -5.9831e+00 -4.3032e+00  1e+01  2e+00  6e-01\n",
      " 2: -5.6350e-01 -1.1535e+00  2e+00  1e-01  4e-02\n",
      " 3: -6.2758e-01 -7.4538e-01  1e-01  7e-17  6e-15\n",
      " 4: -7.1507e-01 -7.1641e-01  1e-03  6e-17  7e-15\n",
      " 5: -7.1604e-01 -7.1605e-01  1e-05  1e-16  7e-15\n",
      " 6: -7.1605e-01 -7.1605e-01  1e-07  2e-16  6e-15\n",
      "Optimal solution found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'x': <14x1 matrix, tc='d'>,\n",
       " 'y': <1x1 matrix, tc='d'>,\n",
       " 's': <14x1 matrix, tc='d'>,\n",
       " 'z': <14x1 matrix, tc='d'>,\n",
       " 'status': 'optimal',\n",
       " 'gap': 1.341946923683446e-07,\n",
       " 'relative gap': 1.8740985460465638e-07,\n",
       " 'primal objective': -0.7160492848758147,\n",
       " 'dual objective': -0.7160494190705081,\n",
       " 'primal infeasibility': 2.0014830212605044e-16,\n",
       " 'dual infeasibility': 5.755747388725673e-15,\n",
       " 'primal slack': 1.126650697981518e-09,\n",
       " 'dual slack': 1.4870341090223383e-08,\n",
       " 'iterations': 6}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solve the problem \n",
    "solution = cvxopt.solvers.qp(P, q, G, h, A, b) # return # dictionary which contains iterators\n",
    "solution  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(1.1266506979815075e-09),\n",
       " np.float64(2.0459666064661667e-09),\n",
       " np.float64(2.2070588989809616e-09),\n",
       " np.float64(2.6220191259872816e-09),\n",
       " np.float64(3.0034651454015e-09),\n",
       " np.float64(3.145946131947282e-09),\n",
       " np.float64(3.222391455728385e-09),\n",
       " np.float64(5.628814152541863e-09),\n",
       " np.float64(5.671715746139145e-09),\n",
       " np.float64(3.9826382024404936e-08),\n",
       " np.float64(0.25401330158897134),\n",
       " np.float64(0.26450515437944316),\n",
       " np.float64(0.45154418199037816),\n",
       " np.float64(0.46203599748712115)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lagrange multipliers \n",
    "multipliers = np.ravel(solution['x']) \n",
    "multipliers\n",
    "sorted(multipliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support vectors have positive multipliers. \n",
    "has_positive_multiplier = multipliers > 1e-07 \n",
    "sv_multipliers = multipliers[has_positive_multiplier] \n",
    " \n",
    "support_vectors = X[has_positive_multiplier] \n",
    "support_vectors_y = y[has_positive_multiplier] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9, 6],\n",
       "       [4, 8],\n",
       "       [2, 7],\n",
       "       [7, 5]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "support_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing 'w'\n",
    "def compute_w(multipliers, X, y):\n",
    "    return np.sum(multipliers[i] * y[i] * X[i] for i in range(len(y))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.44444446 1.11111114]\n",
      "[0.44444453 1.11111128]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksg\\AppData\\Local\\Temp\\ipykernel_39380\\3416292550.py:3: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return np.sum(multipliers[i] * y[i] * X[i] for i in range(len(y)))\n"
     ]
    }
   ],
   "source": [
    "w = compute_w(multipliers, X, y) \n",
    "w_from_sv = compute_w(sv_multipliers, support_vectors, support_vectors_y)\n",
    " \n",
    "print(w)          # [0.44444446 1.11111114] \n",
    "print(w_from_sv)  # [0.44444446 1.11111114]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute 'b'\n",
    "def compute_b(w, X, y):\n",
    "    return np.sum([y[i] - np.dot(w, X[i]) for i in range(len(X))])/len(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-9.666666925153782)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = compute_b(w, support_vectors, support_vectors_y) # -9.666668268506335 \n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qqq = pd.DataFrame(X)\n",
    "qqq['y']=y\n",
    "qqq[qqq['y']==-1][0]\n",
    "qqq[qqq['y']==-1][1]\n",
    "\n",
    "plt.scatter(qqq[qqq['y']==1][0],qqq[qqq['y']==1][1])\n",
    "plt.scatter(qqq[qqq['y']==-1][0],qqq[qqq['y']==-1][1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This formulation of the SVM is called the **hard margin SVM.** It cannot work when the data is not linearly separable. There are several Support Vector Machines formulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**soft margin SVM** which will be able to work when data is non-linearly separable because of outliers. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
